quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Performance,"til.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-6"" #45 daemon prio=5 os_prio=31 tid=0x00007fb76e8dd000 nid=0x5507 waiting on condition [0x0000000131298000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-5"" #44 daemon prio=5 os_prio=31 tid=0x00007fb76b517800 nid=0x3b0b waiting on condition [0x0000000131195000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.Th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:33115,concurren,concurrent,33115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"til.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-7"" #46 daemon prio=5 os_prio=31 tid=0x00007fb76e8f5000 nid=0x5807 waiting on condition [0x000000013139b000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-6"" #45 daemon prio=5 os_prio=31 tid=0x00007fb76e8dd000 nid=0x5507 waiting on condition [0x0000000131298000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.Th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:32040,concurren,concurrent,32040,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"til.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-8"" #47 daemon prio=5 os_prio=31 tid=0x00007fb76b518000 nid=0x8703 waiting on condition [0x000000013149e000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-7"" #46 daemon prio=5 os_prio=31 tid=0x00007fb76e8f5000 nid=0x5807 waiting on condition [0x000000013139b000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.Th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:30965,concurren,concurrent,30965,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"til.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-9"" #48 daemon prio=5 os_prio=31 tid=0x00007fb76b529800 nid=0x8903 waiting on condition [0x00000001315a1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-8"" #47 daemon prio=5 os_prio=31 tid=0x00007fb76b518000 nid=0x8703 waiting on condition [0x000000013149e000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.Th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:29890,concurren,concurrent,29890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"tion"")). # Cromwell HTTP server settings; webservice {; #port = 8000; #interface = 0.0.0.0; #binding-timeout = 5s; #instance.name = ""reference""; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }. # Cromwell ""system"" settings; system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; #abort-jobs-on-terminate = false. # this tells Cromwell to retry the task with Nx memory when it sees either OutOfMemoryError or Killed in the stderr file.; memory-retry-error-keys = [""OutOfMemory"", ""Out Of Memory"",""Out of memory""]; # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; #graceful-server-shutdown = true. # Cromwell will cap the number of running workflows at N; #max-concurrent-workflows = 5000. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; #max-workflow-launch-count = 50. # Number of seconds between workflow launches; #new-workflow-poll-rate = 20. # Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; #number-of-workflow-log-copy-workers = 10. # Default number of cache read workers; #number-of-cache-read-workers = 25. io {; # throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; # #number-of-requests = 100000; # #per = 100 seconds; # }. # Number of times an I/O operation should be attempted before giving up and failing it.; #number-of-attempts = 5; }. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; #lines = 128000; #bool = 7; #int = 19; #float = 50;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:2051,concurren,concurrent-workflows,2051,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['concurren'],['concurrent-workflows']
Performance,"tor$$anonfun$1.apply(CallCacheWriteActor.scala:19); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. Then I see:. ```; [ERROR] [05/01/2017 17:36:04.203] [cromwell-system-akka.dispatchers.engine-dispatcher-84] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow 5; 3e95ead-9026-4c13-89f9-f6c675214523 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:1:1 failed. JES error code 5. Message: 9: Failed to localize files: failed to copy the follow; ing files: ""gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam -> /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD; /DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam (cp failed: gsutil -q -m cp gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A2; 5E-08.2.bam /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:2113,concurren,concurrent,2113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['concurren'],['concurrent']
Performance,"tor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:115) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; 2016-05-27 11:08:57,270 cromwell-system-akka.actor.default-dispatcher-4 ERROR - Failures during localization; java.lang.UnsupportedOperationException: Could not localize /foo/bar/baz -> /home/conradL/cromwell-executions/badLocalization/8c7774be-7917-4c6a-88c4-55e495bbb9ec/call-BillyBob/foo/bar/baz; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at scala.Option.getOrElse(Option.scala:121) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/922:8798,concurren,concurrent,8798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/922,1,['concurren'],['concurrent']
Performance,"tor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:115) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; 2016-05-27 11:08:57,270 cromwell-system-akka.actor.default-dispatcher-5 ERROR - Failures during localization; java.lang.UnsupportedOperationException: Could not localize /foo/bar/baz -> /home/conradL/cromwell-executions/badLocalization/8c7774be-7917-4c6a-88c4-55e495bbb9ec/call-BillyBob/foo/bar/baz; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at scala.Option.getOrElse(Option.scala:121) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/922:4721,concurren,concurrent,4721,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/922,1,['concurren'],['concurrent']
Performance,"tor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""Abandoned connection cleanup thread"" #22 daemon prio=5 os_prio=31 tid=0x00007fb76a4f5800 nid=0x7103 in Object.wait() [0x000000012ccb5000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x00000006c0624180> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""ForkJoinPool-3-worker-15"" #19 daemon prio=5 os_prio=31 tid=0x00007fb76abaa800 nid=0x6b03 waiting on condition [0x000000012a1e3000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0060ab0> (a java.util.concurrent.CountDownLatch$Sync); at java.util.concurrent.locks.LockSupport.park(Redefined); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2$$anon$4.block(ExecutionContextImpl.scala); at scala.concurrent.forkjoin.ForkJoinPool.managedBlock(Redefined); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2.blockOn(ExecutionContextImpl.scala:44); at scala.concurr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:44654,concurren,concurrent,44654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,tor.scala:103); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSys,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1406:5729,concurren,concurrent,5729,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406,1,['concurren'],['concurrent']
Performance,"tor.scala:103); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-08-08 08:33:09,747] [info] Updating WorkflowManager state. New Data: (4e20eafc-baae-4605-a010-adfa5f32ae46,Actor[akka://cromwell-system/user/WorkflowManagerActor/WorkflowActor-4e20eafc-baae-4605-a010-adfa5f32ae46#-904922324]); [2016-08-08 08:33:09,767] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: Start(Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor#576120716])) message received; [2016-08-08 08:33:10,91] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: ExecutionStoreCreated(Start(Some(Actor[akka://cromwellsystem/user/SingleWorkflowRunnerActor#576120716]))) message received; [2016-08-08 08:33:10,94] [info] SingleWorkflowRunnerActor: workflow ID ←[38;5;2m4e20eafc-baae-4605-a010-adfa5f32ae46←[0m; [2016-08-08 08:33:10,104] [info] WorkflowAc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1261:5527,concurren,concurrent,5527,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261,1,['concurren'],['concurrent']
Performance,"tor.scala:103); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. To a novice user it is not clear whether that this can safely be ignored. This is a problem in particular if this first use actually fails for some other reason. The user will spend time trying to figure out if the problem is caused by a credential issue. I tried to see if I could easily suppress this by setting up ""backends"" to only include ""local"". I pulled down the [application.conf](https://github.com/broadinstitute/cromwell/blob/9f759a54a0b8873f1338f36391f985477d83475a/engine/src/main/resources/application.conf) which sets:. ```; backend {; // Either ""jes"", ""local"", or ""sge"" (case insensitive); defaultBackend = ""local""; // List of backends which this Cromwell supports. Be sure to include the defaultBackend!; backendsAllowed = [ ""local"" ]; ```. and",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/705:4862,concurren,concurrent,4862,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/705,1,['concurren'],['concurrent']
Performance,torActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:103); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(S,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1406:5656,concurren,concurrent,5656,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406,1,['concurren'],['concurrent']
Performance,"torActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:103); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-08-08 08:33:09,747] [info] Updating WorkflowManager state. New Data: (4e20eafc-baae-4605-a010-adfa5f32ae46,Actor[akka://cromwell-system/user/WorkflowManagerActor/WorkflowActor-4e20eafc-baae-4605-a010-adfa5f32ae46#-904922324]); [2016-08-08 08:33:09,767] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: Start(Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor#576120716])) message received; [2016-08-08 08:33:10,91] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: ExecutionStoreCreated(Start(Some(Actor[akka://cromwellsystem/user/SingleWorkflowRunnerActor#576120716]))) message received; [2016-08-08 08:33:10,94] [info] SingleWorkflowRunnerActor: workflow ID ←[38;5;2m4e20eafc-baa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1261:5454,concurren,concurrent,5454,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261,1,['concurren'],['concurrent']
Performance,"torActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:103); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. To a novice user it is not clear whether that this can safely be ignored. This is a problem in particular if this first use actually fails for some other reason. The user will spend time trying to figure out if the problem is caused by a credential issue. I tried to see if I could easily suppress this by setting up ""backends"" to only include ""local"". I pulled down the [application.conf](https://github.com/broadinstitute/cromwell/blob/9f759a54a0b8873f1338f36391f985477d83475a/engine/src/main/resources/application.conf) which sets:. ```; backend {; // Either ""jes"", ""local"", or ""sge"" (case insensitive); defaultBackend = ""local""; // List of backends which this Cromwell supports. Be sur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/705:4789,concurren,concurrent,4789,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/705,1,['concurren'],['concurrent']
Performance,"torActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157),List())WorkflowFailure(scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304),List())WorkflowFailure(scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37),List())WorkflowFailure(scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60),List())WorkflowFailure(akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55),List())WorkflowFailure(akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91),List())WorkflowFailure(scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12),List())WorkflowFailure(scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81),List())WorkflowFailure(akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91),List())WorkflowFailure(akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40),List())WorkflowFailure(akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43),List())WorkflowFailure(akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260),List())WorkflowFailure(akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339),List())WorkflowFailure(akka.dispatch.forkj",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3176:2569,concurren,concurrent,2569,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3176,1,['concurren'],['concurrent']
Performance,"torageRpc.java:578); 	at com.google.cloud.storage.StorageImpl$15.call(StorageImpl.java:419); 	at com.google.cloud.storage.StorageImpl$15.call(StorageImpl.java:416); 	at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); 	at com.google.cloud.storage.StorageImpl.copy(StorageImpl.java:416); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.copy(CloudStorageFileSystemProvider.java:493); 	at java.nio.file.Files.copy(Files.java:1274); 	at better.files.File.copyTo(File.scala:646); 	at cromwell.core.path.BetterFileMethods$class.copyTo(BetterFileMethods.scala:415); 	at cromwell.filesystems.gcs.GcsPath.copyTo(GcsPathBuilder.scala:116); 	at cromwell.core.path.PathCopier$$anonfun$copy$1.apply$mcV$sp(PathCopier.scala:45); 	at cromwell.core.path.PathCopier$$anonfun$copy$1.apply(PathCopier.scala:44); 	at cromwell.core.path.PathCopier$$anonfun$copy$1.apply(PathCopier.scala:44); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.core.path.PathCopier$.copy(PathCopier.scala:44); 	... 24 common frames omitted; 2017-05-01 18:06:39 [cromwell-system-akka.actor.default-dispatcher-1373] INFO c.e.w.l.e.EngineJobExecutionActor - Could not find another cache hit, falling back to running job: GenotypeGVCFsComparison.IndexVCF:-1:1; ```. It seems that the file can't be copied so it invalidates the cache. The wdl task is as follows:. ```; task IndexVCF {; File combined_gvcf; Int disk_size. command {; /usr/gitc/tabix ${combined_gvcf}; }; runtime {; docker: ""broadinstitute/genomes-in-the-cloud@sha256:d7aa37fc8351074a2d6fb949932d3283cdcefdc8e53729dcf7202bee16ab660a""; memory: ""13 GB""; cpu: ""1""; disks: ""local-disk "" + disk_size + "" HDD""; }; output {; File gvcf = ""${combined_gvcf}""; File gvcf_index = ""${combined_gvcf}.tbi""; }; }; ```. I'm going to try running this same task in V24 without the docker hash (to see if it's something to do with the file) and in V26 to see if the issue is resolved there.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2229:6683,cache,cache,6683,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229,2,['cache'],['cache']
Performance,"tput_dir}/${entity_id}-sim-final.acs.seg\"" \n File cnloh_final_cnb_called_segs=\""${output_dir}/${entity_id}-sim-final.cnb_called.seg\""\n File cnloh_final_cnv_segs=\""${output_dir}/${entity_id}-sim-final.cnv.seg\""\n File cnloh_final_titan_hets=\""${output_dir}/${entity_id}-sim-final.titan.het.tsv\""\n File cnloh_final_titan_tn=\""${output_dir}/${entity_id}-sim-final.titan.tn.tsv\""\n }\n}\n"",; ""options"": ""{\n \""default_runtime_attributes\"": {\n \""docker\"": \""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e\"",\n \""zones\"": \""us-central1-a us-central1-b\"",\n\t \""disks\"": \""local-disk 200 SSD\"",\n\t \""memory\"": \""6G\""\n\t }\n}""; },; ""calls"": {; ""case_gatk_acnv_workflow.TumorCalculateTargetCoverage"": [; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-0/execution/stdout"",; ""shardIndex"": 0,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74P2T.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74P2T.bam.bai"",; ""entity_id"": ""SM-74P2T"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28218"",; """,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:46751,Cache,Cache,46751,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,2,"['Cache', 'cache']","['Cache', 'cache']"
Performance,"ttps://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Call caching fails for me with Server version: 10.3.34-MariaDB MariaDB Server and Cromwell 84. . > [2022-10-06 15:14:54,54] [info] BT-322 12ceda02:test.task_A:-1:1 is eligible for call caching with read = true and write = true; [2022-10-06 15:14:54,68] [ESC[38;5;1merrorESC[0m] $aFailed to properly process data; > java.sql.SQLSyntaxErrorException: Every derived table must have its own alias; ...; > [2022-10-06 15:14:54,68] [ESC[38;5;1merrorESC[0m] ejha_for_12ceda02-4906-4840-80a2-514af3ccb801:BackendJobDescriptorKey_CommandCallNode_test.task_A:-1:1 [ESC[38;5;2m12ceda02ESC[0mtest.task_A:NA:1]: Received unexpected event HashError(java.sql.SQLSyntaxErrorException: Every derived table must have its own alias); [2022-10-06 15:14:54,70] [info] BT-322 12ceda02:test.task_A:-1:1 cache hit copying nomatch: could not find a suitable cache hit.; [2022-10-06 15:14:54,70] [info] 12ceda02-4906-4840-80a2-514af3ccb801-EngineJobExecutionActor-test.task_A:NA:1 [ESC[38;5;2m12ceda02ESC[0m]: Could not copy a suitable cache hit for 12ceda02:test.task_A:-1:1. No copy attempts were made. Based on [StackOverflow, the issue seems to be simply that subqueries must be aliased.](https://stackoverflow.com/q/1888779/4107809) Is MariaDB not supported? . The workflow runs jobs that complete as normal. When rerunning, no call caching results are used, and all jobs simply run again. . Cromwell connects to the call caching database and successfully creates tables, for example `CALL_CACHING_AGGREGATION_ENTRY`. . <!-- Which backend are you running? -->; I am running with a SLURM backend. . <!-- Paste/Attach your workflow if possible: -->; I have a very simple example workflow. ; ```; workflow test{; call task_A {}; }. task task_A{; command{; echo 'testing'; }; }; ```. <!-- Paste your configuration if possible, MAKE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6929:1251,cache,cache,1251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6929,2,['cache'],['cache']
Performance,ttributes(StandardInitializationActor.scala:78); cromwell_1 | 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:121); cromwell_1 | 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:154); cromwell_1 | 	at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:42); cromwell_1 | 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$2.apply(BackendWorkflowInitializationActor.scala:146); cromwell_1 | 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$2.apply(BackendWorkflowInitializationActor.scala:146); cromwell_1 | 	at cromwell.backend.BackendLifecycleActor$class.performActionThenRespond(BackendLifecycleActor.scala:44); cromwell_1 | 	at cromwell.backend.standard.StandardInitializationActor.performActionThenRespond(StandardInitializationActor.scala:42); cromwell_1 | 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.applyOrElse(BackendWorkflowInitializationActor.scala:146); cromwell_1 | 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); cromwell_1 | 	at cromwell.backend.standard.StandardInitializationActor.aroundReceive(StandardInitializationActor.scala:42); cromwell_1 | 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); cromwell_1 | 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); cromwell_1 | 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); cromwell_1 | 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); cromwell_1 | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); cromwell_1 | 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); cromwell_1 | 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); cromwell_1 | 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2284:4660,perform,performActionThenRespond,4660,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2284,1,['perform'],['performActionThenRespond']
Performance,ture$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91\; ); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.net.SocketException: Socket is closed; at sun.security.ssl.SSLSocketImpl.getInputStream(SSLSocketImpl.java:2218); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:642); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); at com.google.cloud.storage.spi.DefaultStorageRpc.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2009:2434,concurren,concurrent,2434,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2009,1,['concurren'],['concurrent']
Performance,"tus\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n)""; at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$.StandardException(JesAsyncBackendJobExecutionActor.scala:63); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:411); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:67); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$1.applyOrElse(StandardAsyncExecutionActor.scala:666); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$1.applyOrElse(StandardAsyncExecutionActor.scala:663); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:16442,concurren,concurrent,16442,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['concurren'],['concurrent']
Performance,"txt.gz.tbi; 1608597698326,[W::hts_idx_load2] The index file is older than the data file: /tmp/scratch/focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz.tbi; 1608597702687,[W::hts_idx_load2] The index file is older than the data file: /tmp/scratch/focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz.tbi; 1608597703940,[W::hts_idx_load2] The index file is older than the data file: /tmp/scratch/focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz.tbi; 1608597705033,[E::hts_open_format] Failed to open file s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz; 1608597705033,Could not read s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz; 1608597715816,*** DELOCALIZING OUTPUTS ***; 1608597717532,The user-provided path /tmp/scratch/test_na12878.SR.chr6.txt.gz does not exist.; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:203083,cache,cacheCopy,203083,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,2,['cache'],['cacheCopy']
Performance,"t}""; """""". submit-docker = """"""; # SINGULARITY_CACHEDIR needs to point to a directory accessible by; # the jobs (i.e. not lscratch). Might want to use a workflow local; # cache dir like in run.sh; source /work/share/ac7m4df1o5/bin/cromwell/set_singularity_cachedir.sh; SINGULARITY_CACHEDIR=/work/share/ac7m4df1o5/bin/cromwell/singularity-cache; source /work/share/ac7m4df1o5/bin/cromwell/test.sh ${docker}; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; if [ -z $SINGULARITY_CACHEDIR ]; then; CACHE_DIR=$HOME/.singularity; else; CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; LOCK_FILE=$CACHE_DIR/singularity_pull_flock. # we want to avoid all the cromwell tasks hammering each other trying; # to pull the container into the cache for the first time. flock works; # on GPFS, netapp, and vast (of course only for processes on the same; # machine which is the case here since we're pulling it in the master; # process before submitting).; #flock --exclusive --timeout 1200 $LOCK_FILE \; # singularity exec --containall docker://${docker} \; # echo ""successfully pulled ${docker}!"" &> /dev/null. # Ensure singularity is loaded if it's installed as a module; module load apps/singularity/3.7.3. # Build the Docker image into a singularity image; #IMAGE=$(echo $SINGULARITY_CACHEDIR/pull/${docker}.sif|sed ""s#:#_#g""); #singularity build $IMAGE docker://${docker}. # Submit the script to SLURM; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${cwd}/execution/stdout \; --error=${cwd}/execution/stderr \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $SINGULARITY_CACHEDIR/pull/$docker_image.sif ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }. }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:8872,load,loaded,8872,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,2,['load'],"['load', 'loaded']"
Performance,"ubmit$2(Execute.scala:269) at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16) at sbt.Execute.work(Execute.scala:278) at sbt.Execute.$anonfun$submit$1(Execute.scala:269) at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178) at sbt.CompletionService$$anon$2.call(CompletionService.scala:37) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Cause: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://test-system-6/user/$l#-102797778]] after [30000 ms]. Sender[Actor[akka://test-system-6/system/testActor-24#-1294021439]] sent message of type ""cromwell.engine.workflow.SingleWorkflowRunnerActor$RunWorkflow$"". at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:596) at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:606) at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205) at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:870) at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:109) at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:103) at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:868) at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328) at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279) at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283) at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235) at java.lang.Thread.run(Thread.java:748)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4350:6750,concurren,concurrent,6750,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4350,4,['concurren'],['concurrent']
Performance,"uch files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default"". # Google project which will be billed for the requests; project = ""gred-cumulus-sb-01-991a49c4"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }. http { }; }. default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; # Allowed to be a String, or a list of Strings; disks: ""local-disk 10 SSD""; noAddress: false; preemptible: 0; zones: [""us-west1-a"", ""us-west1-b"", ""us-west1-c""]; }; }; }; }; }. ```. However, when I tried to run a WDL workflow test which used ""gcr.io/broad-cumulus/cellranger:6.1.1"" docker image, the execution failed with the following log:. ```; 2021-09-27 13:47:50,363 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; Skipping auto-registration; 2021-09-27 13:47:55,753 WARN - Skipping auto-registration; 2021-09-27 13:47:55,833 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; Skipping auto-registration; 2021-09-27 13:47:56,493 WARN - Skipping auto-regis",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:4555,cache,cache,4555,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['cache'],['cache']
Performance,"ue; [2022-12-15 21:22:59,01] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.kinship_count:-1:1-20000000009 [9e4f5894main.kinship_count:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,03] [info] BT-322 9e4f5894:main.kinship_count:-1:1 cache hit copying success with aggregated hashes: initial = 40DB3965745EAB4613A3E2804F447EFE, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,03] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.kinship_count:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,12] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.reported_sex:-1:1-20000000001 [9e4f5894main.reported_sex:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,12] [info] BT-322 9e4f5894:main.reported_sex:-1:1 cache hit copying success with aggregated hashes: initial = 91C81CABBB083C238800E3CF59AF537D, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,12] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.reported_sex:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,16] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.sex_aneuploidy:-1:1-20000000003 [9e4f5894main.sex_aneuploidy:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,17] [info] BT-322 9e4f5894:main.sex_aneuploidy:-1:1 cache hit copying success with aggregated hashes: initial = 86896541F0DCB2C2B959EEF37F266B30, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,17] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.sex_aneuploidy:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,32] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7ff",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:22232,cache,cache,22232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['cache'],['cache']
Performance,"uedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-5"" #65 prio=5 os_prio=31 tid=0x00007fb7706d2800 nid=0xab03 waiting on condition [0x00000001329eb000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-20"" #61 daemon prio=5 os_prio=31 tid=0x00007fb76b1f9000 nid=0xa303 waiting on condition [0x00000001325df000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArray",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:14943,concurren,concurrent,14943,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"uedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-6"" #66 prio=5 os_prio=31 tid=0x00007fb7705ef800 nid=0xad03 waiting on condition [0x0000000132aee000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-5"" #65 prio=5 os_prio=31 tid=0x00007fb7706d2800 nid=0xab03 waiting on condition [0x00000001329eb000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:14065,concurren,concurrent,14065,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"uedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-7"" #67 prio=5 os_prio=31 tid=0x00007fb7705a3000 nid=0xaf03 waiting on condition [0x00000001330c6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-6"" #66 prio=5 os_prio=31 tid=0x00007fb7705ef800 nid=0xad03 waiting on condition [0x0000000132aee000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:13187,concurren,concurrent,13187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"uedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-8"" #68 prio=5 os_prio=31 tid=0x00007fb76ef84000 nid=0xb103 waiting on condition [0x0000000132bf1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-7"" #67 prio=5 os_prio=31 tid=0x00007fb7705a3000 nid=0xaf03 waiting on condition [0x00000001330c6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:12309,concurren,concurrent,12309,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"uedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-9"" #69 prio=5 os_prio=31 tid=0x00007fb76f36d800 nid=0xb303 waiting on condition [0x000000012c1b6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-8"" #68 prio=5 os_prio=31 tid=0x00007fb76ef84000 nid=0xb103 waiting on condition [0x0000000132bf1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:11431,concurren,concurrent,11431,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"uency: 30 seconds. # The cache of in-progress aborts. Cromwell will add entries to this cache once a WorkflowActor has been messaged to abort.; # If on the next scan an 'Aborting' status is found for a workflow that has an entry in this cache, Cromwell will not ask; # the associated WorkflowActor to abort again.; cache {; enabled: true; # Guava cache concurrency.; concurrency: 1; # How long entries in the cache should live from the time they are added to the cache.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 100000; }; }. # Cromwell reads this value into the JVM's `networkaddress.cache.ttl` setting to control DNS cache expiration; dns-cache-ttl: 3 minutes; }. docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; enabled = ""false""; }; }. # Here is where you can define the backend providers that Cromwell understands.; # The default is a local provider.; # To add additional backend providers, you should copy paste additional backends; # of interest that you can find in the cromwell.example.backends folder; # folder at https://www.github.com/broadinstitute/cromwell; # Other backend providers include SGE, SLURM, Docker, udocker, Singularity. etc.; # Don't forget you will need to customize them for your particular use case.; backend {; # Override the default backend.; default = slurm. # The list of providers.; providers {; # Copy paste the contents of a backend ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:4318,cache,cache,4318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['cache'],['cache']
Performance,"uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntologyFromOntologyDocument(OWLOntologyManagerImpl.java:974) cwl.ontology.Schema$.$anonfun$loadOntologyFromIri$5(Schema.scala:155) cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85) -------------------------------------------------------------------------------- Parser: org.semanticweb.owlapi.oboformat.OBOFormatOWLAPIParser@6bd28ad6 Stack trace: LINENO: 1 - Could not find tag separator ':' in line. LINE: <?xml version=""1.0""?> org.semanticweb.owlapi.oboformat.OBOFormatOWLAPIParser.parse(OBOFormatOWLAPIParser.java:50) uk.ac.manchester.cs.owl.owlapi.OWLOntologyFactoryImpl.loadOWLOntology(OWLOntologyFactoryImpl.java:193) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.load(OWLOntologyManagerImpl.java:1071) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntology(OWLOntologyManagerImpl.java:1033) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntologyFromOntologyDocument(OWLOntologyManagerImpl.java:974) cwl.ontology.Schema$.$anonfun$loadOntologyFromIri$5(Schema.scala:155) cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85) cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303) LINENO: 1 - Could not find tag separator ':' in line. LINE: <?xml version=""1.0""?> org.obolibrary.oboformat.parser.OBOFormatParser.error(OBOFormatParser.java:1337) org.obolibrary.oboformat.parser.OBOFormatParser.getParseTag(OBOFormatParser.java:760) org.obolibrary.oboformat.parser.OBOFormatParser.parseHeaderClause(OBOFormatParser.java:409) org.obolibrary.oboformat.parser.OBOFormatParser.parseHeaderClauseNl(OBOFormatParser.java:402) org.obolibrary.oboformat.parser.OBOFormatParser.parseHeaderFrame(OBOFormatParser.java:385) org.obolibrary.oboformat.parser.OBOFormatParser.p",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4372:5005,load,loadOntologyFromOntologyDocument,5005,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4372,1,['load'],['loadOntologyFromOntologyDocument']
Performance,"ule00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz.tbi; 1608597646131,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-MergeSRFilesByContig/shard-5/write_lines_1aa3abac483dac7d55fbf1572054f418.tmp to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-MergeSRFilesByContig/shard-5/write_lines_1aa3abac483dac7d55fbf1572054f418.tmp; 1608597648902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz; 1608597650698,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:196097,cache,cacheCopy,196097,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"ult""; scheme = ""application_default""; },; {; name = ""xxxxx-xxxxx""; scheme = ""service_account""; service-account-id = ""xxxxx@xxxxx-xxxxx.iam.gserviceaccount.com""; pem-file = ""/xxxxx/xxxxx.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }. backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; # Google project; project = ""xxxxx-cromwell"". # Base bucket for workflow executions; root = ""gs://xxxxx-cromwell_bucket"". # Make the name of the backend used for call caching purposes insensitive to the PAPI version.; name-for-call-caching-purposes: PAPI. # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in PAPI.; slow-job-warning-time: 24 hours. # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 25000. # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; # account = """"; # token = """"; }. #docker-image-cache-manifest-file = ""gs://xxxxx-xxxxx/xxxxx.json"". # Number of workers to assign to PAPI requests; request-workers = 3. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:1446,throughput,throughput,1446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['throughput'],['throughput']
Performance,"ults = true; }`. **Question 1.**. Can call caching be initiated if there is only the in-memory database, as below is not clear regarding this?; _""Cromwell's call cache is maintained in its database. In order for call caching to be used on any previously run jobs, it is best to configure Cromwell to point to a MySQL database instead of the default in-memory database. This way any invocation of Cromwell (either with run or server subcommands) will be able to utilize results from all calls that are in that database.""_. Secondly, if this can. **Question 2.**. Can call caching be initiated if a scatter, wraps a workflow, which then wraps tools.; Or will the entire workflow need to be in one script? (I have attached an example as zip); And, the options file.; [DsTrim - Broken.zip](https://github.com/broadinstitute/cromwell/files/3842334/DsTrim.-.Broken.zip). **Question 3.**. What exactly triggers callcaching to change from ""CallCachingOff"" to on, in the following result?; `; ""callCaching"": {; ""effectiveCallCachingMode"": ""CallCachingOff"",; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss""; },`. **If the in-memory is the issue, then please close and we will set-up a UAT correctly.; If not any additional assistance or comments will be most apprecitated.** . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL h",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5280:1604,Cache,Cache,1604,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280,1,['Cache'],['Cache']
Performance,unID|A3F4D40B1A8326642B9C761E2FE16F3E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String default_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String default_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String default_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String default_attr:docker|A5281F25296D4311ED0C46422719018E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int runtime_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int runtime_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int runtime_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int runtime_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int numCPUs|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int numCPUs|C74D97B01EAE257E44AA9D,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:3984,queue,queueArn,3984,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['queue'],['queueArn']
Performance,"unexpected token: ""<"" <ERROR> at line 1, column 1. Was expecting one of: ""("" ""@base"" ""@prefix"" ""["" <EMPTY_BLANK_NODE> <FULLIRI> <NODEID> <PNAME_LN> <PNAME_NS> org.semanticweb.owlapi.rdf.turtle.parser.TurtleParser.generateParseException(TurtleParser.java:1034) org.semanticweb.owlapi.rdf.turtle.parser.TurtleParser.jj_consume_token(TurtleParser.java:902) org.semanticweb.owlapi.rdf.turtle.parser.TurtleParser.parseDocument(TurtleParser.java:165) org.semanticweb.owlapi.rdf.turtle.parser.TurtleOntologyParser.parse(TurtleOntologyParser.java:54) uk.ac.manchester.cs.owl.owlapi.OWLOntologyFactoryImpl.loadOWLOntology(OWLOntologyFactoryImpl.java:193) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.load(OWLOntologyManagerImpl.java:1071) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntology(OWLOntologyManagerImpl.java:1033) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntologyFromOntologyDocument(OWLOntologyManagerImpl.java:974) cwl.ontology.Schema$.$anonfun$loadOntologyFromIri$5(Schema.scala:155) cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85) -------------------------------------------------------------------------------- Parser: org.semanticweb.owlapi.oboformat.OBOFormatOWLAPIParser@6bd28ad6 Stack trace: LINENO: 1 - Could not find tag separator ':' in line. LINE: <?xml version=""1.0""?> org.semanticweb.owlapi.oboformat.OBOFormatOWLAPIParser.parse(OBOFormatOWLAPIParser.java:50) uk.ac.manchester.cs.owl.owlapi.OWLOntologyFactoryImpl.loadOWLOntology(OWLOntologyFactoryImpl.java:193) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.load(OWLOntologyManagerImpl.java:1071) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntology(OWLOntologyManagerImpl.java:1033) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntologyFromOntologyDocument(OWLOntologyManagerImpl.java:974) cwl.ontology.Schema$.$anonfun$loadOntologyFromIri$5(Schema.scala:155) cats.effect.internals.IORunLoop$.cats$effect$interna",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4372:4183,load,loadOntologyFromIri,4183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4372,1,['load'],['loadOntologyFromIri']
Performance,"unexpected token: ""<?xml version=\""1.0\""?>"" "">"" at line 1, column 1. Was expecting: <EOF> org.semanticweb.owlapi.krss2.parser.KRSS2OWLParser.parse(KRSS2OWLParser.java:248) uk.ac.manchester.cs.owl.owlapi.OWLOntologyFactoryImpl.loadOWLOntology(OWLOntologyFactoryImpl.java:193) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.load(OWLOntologyManagerImpl.java:1071) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntology(OWLOntologyManagerImpl.java:1033) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntologyFromOntologyDocument(OWLOntologyManagerImpl.java:974) cwl.ontology.Schema$.$anonfun$loadOntologyFromIri$5(Schema.scala:155) cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85) cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303) Encountered unexpected token: ""<?xml version=\""1.0\""?>"" "">"" at line 1, column 1. Was expecting: <EOF> org.semanticweb.owlapi.krss2.parser.KRSS2Parser.generateParseException(KRSS2Parser.java:2103) org.semanticweb.owlapi.krss2.parser.KRSS2Parser.jj_consume_token(KRSS2Parser.java:1968) org.semanticweb.owlapi.krss2.parser.KRSS2Parser.parse(KRSS2Parser.java:124) org.semanticweb.owlapi.krss2.parser.KRSS2OWLParser.parse(KRSS2OWLParser.java:245) uk.ac.manchester.cs.owl.owlapi.OWLOntologyFactoryImpl.loadOWLOntology(OWLOntologyFactoryImpl.java:193) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.load(OWLOntologyManagerImpl.java:1071) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntology(OWLOntologyManagerImpl.java:1033) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntologyFromOntologyDocument(OWLOntologyManagerImpl.java:974) cwl.ontology.Schema$.$anonfun$loadOntologyFromIri$5(Schema.scala:155) cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4372:8136,load,loadOWLOntology,8136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4372,5,['load'],"['load', 'loadOWLOntology', 'loadOntology', 'loadOntologyFromIri', 'loadOntologyFromOntologyDocument']"
Performance,"upported: application/json; 	at cromiam.webservice.QuerySupport.$anonfun$preprocessQuery$3(QuerySupport.scala:67); 	at akka.http.scaladsl.server.Directive$SingleValueModifiers.$anonfun$flatMap$1(Directive.scala:141); 	at akka.http.scaladsl.server.Directive.$anonfun$tflatMap$2(Directive.scala:69); 	at akka.http.scaladsl.server.directives.FutureDirectives.$anonfun$onComplete$3(FutureDirectives.scala:37); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$2(FastFuture.scala:37); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$3(FastFuture.scala:52); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: akka.http.scaladsl.unmarshalling.Unmarshaller$UnsupportedContentTypeException: Unsupported Content-Type, supported: application/json; 	at akka.http.scaladsl.unmarshalling.Unmarshaller$UnsupportedContentTypeException$.apply(Unmarshaller.scala:158); 	at akka.http.scaladsl.unmarshalling.Unmarshaller$EnhancedFromEntityUnmarshaller$.$anonfun$forContentTypes$3(U",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3622:2022,concurren,concurrent,2022,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3622,1,['concurren'],['concurrent']
Performance,"uptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""pool-1-thread-1"" #27 prio=5 os_prio=31 tid=0x00007fb76f4b6000 nid=0x7b03 waiting on condition [0x000000012b643000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-1"" #26 daemon prio=5 os_prio=31 tid=0x00007fb76b442000 nid=0x7903 waiting on condition [0x000000012d905000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(Managed",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:40015,concurren,concurrent,40015,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ur is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridde",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:84926,concurren,concurrent-workflows,84926,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['concurren'],['concurrent-workflows']
Performance,"ure, might be useful as a reference and I'm going to reference it from a different issue: https://broadworkbench.atlassian.net/browse/BA-6172_. I'm trying to get call-caching working for my workflows, and having some trouble identifying a config that will work for the following requirements:. - Using containers (both Singularity and Docker); - Initial localisation strategy: `[hard-link, cached-copy]`; - Local SFS environment; - My input files can be fairly large (~250GB per Bam with up to 16 Bams). . If I can get this working, I'll happily document and update the CallCaching documentation page with what I've found. ## Background information. Version: Cromwell-47. Documentation:; - https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346:1028,Cache,Cache,1028,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346,1,['Cache'],['Cache']
Performance,"urrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2$$anon$4.block(ExecutionContextImpl.scala); at scala.concurrent.forkjoin.ForkJoinPool.managedBlock(Redefined); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2.blockOn(ExecutionContextImpl.scala:44); at scala.concurrent.Await$.ready(package.scala:169); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellServer.scala:29); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellServer.scala); at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:433); at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala); at scala.concurrent.impl.CallbackRunnable.run(Redefined); at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(Redefined); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java). ""cromwell-system-scheduler-1"" #14 prio=5 os_prio=31 tid=0x00007fb7",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:45653,concurren,concurrent,45653,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,usDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:534); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:5438,concurren,concurrent,5438,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,2,['concurren'],['concurrent']
Performance,"uteAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>. This is a minimal example of a config which gets such an error:; `Could not evaluate expression: ""echo "" + memory: Cannot perform operation: echo + WomLong(4)`; ```; include required(classpath(""application"")); webservice {; port = 8000; }; backend {; default=""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int? memory; String? docker; String? docker_user; """"""; submit = """"""; bash ${script}; ${""echo "" + memory}; """"""; }; }; }; } ; ```. This means that the launch command given in the cromwell docs [here](https://cromwell.readthedocs.io/en/stable/backends/SGE/) will not work. A current workaround would be to use an expression like this instead:; `${true=""echo"" false="""" defined(memory)} ${memory}`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:5366,perform,perform,5366,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,1,['perform'],['perform']
Performance,"ution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 9620 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 9624 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 9625 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 9626 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 9627 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 9628 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 9629 a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3732:2288,concurren,concurrent,2288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732,1,['concurren'],['concurrent']
Performance,"utionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2020-08-25 10:40:46,27] [info] WorkflowManagerActor WorkflowActor-282f5595-171e-4296-a7fa-9bd9f7a2f33b is in a terminal state: WorkflowFailedState; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804:2840,concurren,concurrent,2840,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804,1,['concurren'],['concurrent']
Performance,"utor.java:1144); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642); at java.base/java.lang.Thread.run(Thread.java:1570); Suppressed: com.google.api.gax.rpc.AsyncTaskException: Asynchronous task failed; at com.google.api.gax.rpc.ApiExceptions.callAndTranslateApiException(ApiExceptions.java:57); at com.google.api.gax.rpc.UnaryCallable.call(UnaryCallable.java:112); at com.google.cloud.batch.v1.BatchServiceClient.getJob(BatchServiceClient.java:427); at cromwell.backend.google.batch.api.GcpBatchApiRequestHandler.$anonfun$query$1(GcpBatchApiRequestHandler.scala:15); at cromwell.backend.google.batch.api.GcpBatchApiRequestHandler.withClient(GcpBatchApiRequestHandler.scala:29); at cromwell.backend.google.batch.api.GcpBatchApiRequestHandler.query(GcpBatchApiRequestHandler.scala:14); at cromwell.backend.google.batch.actors.GcpBatchBackendSingletonActor$$anonfun$normalReceive$1.$anonfun$applyOrElse$3(GcpBatchBackendSingletonActor.scala:80); at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678); at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception; Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]; at io.grpc.Status.asRuntimeException(Status.java:539); ... 14 common frames omitted; Caused by: javax.net.ssl.SSLHandshakeException: General OpenSslEngine problem; at io.grpc.ne",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7551:3965,concurren,concurrent,3965,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7551,1,['concurren'],['concurrent']
Performance,"utor.java:617); at java.lang.Thread.run(Redefined). ""db-1"" #26 daemon prio=5 os_prio=31 tid=0x00007fb76b442000 nid=0x7903 waiting on condition [0x000000012d905000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""Hikari Housekeeping Timer (pool db)"" #24 daemon prio=5 os_prio=31 tid=0x00007fb76d88f000 nid=0x7503 waiting on condition [0x000000012cebb000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0623fc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecuto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:41212,concurren,concurrent,41212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"utor.java:617); at java.lang.Thread.run(Redefined). ""db-2"" #28 daemon prio=5 os_prio=31 tid=0x00007fb7708d7800 nid=0x7d03 waiting on condition [0x000000012c789000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""pool-1-thread-1"" #27 prio=5 os_prio=31 tid=0x00007fb76f4b6000 nid=0x7b03 waiting on condition [0x000000012b643000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-1"" #26 daemon prio=5 os_prio=31 tid=0x0000",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:39309,concurren,concurrent,39309,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"utor.java:617); at java.lang.Thread.run(Redefined). ""db-3"" #29 daemon prio=5 os_prio=31 tid=0x00007fb76f4b7000 nid=0x7f03 waiting on condition [0x000000012c88c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-2"" #28 daemon prio=5 os_prio=31 tid=0x00007fb7708d7800 nid=0x7d03 waiting on condition [0x000000012c789000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPool",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:38259,concurren,concurrent,38259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,utors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Succeeded; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdlAndAssertOutputs$1(CromwellTestKitSpec.scala:344); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdlAndAssertOutputs(CromwellTestKitSpec.scala:344); at cromwell.WorkflowOutputsSpec.$anonfun$new$4(WorkflowOutputsSpec.scala:38); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4457:5915,concurren,concurrent,5915,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4457,1,['concurren'],['concurrent']
Performance,"utput {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051:1774,concurren,concurrent,1774,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051,1,['concurren'],['concurrent']
Performance,va.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Succeeded; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdlAndAssertOutputs$1(CromwellTestKitSpec.scala:344); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdlAndAssertOutputs(CromwellTestKitSpec.scala:344); at cromwell.WorkflowOutputsSpec.$anonfun$new$4(WorkflowOutputsSpec.scala:38); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4457:5831,concurren,concurrent,5831,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4457,1,['concurren'],['concurrent']
Performance,"va:209). ""Reference Handler"" #2 daemon prio=10 os_prio=31 tid=0x00007fb76b005800 nid=0x3303 in Object.wait() [0x0000000126ac4000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Object.wait(Object.java:502); at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:157); - locked <0x00000006c00301d8> (a java.lang.ref.Reference$Lock). ""main"" #1 prio=5 os_prio=31 tid=0x00007fb76a803000 nid=0xf07 waiting on condition [0x000000010b088000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c061d308> (a scala.concurrent.impl.Promise$CompletionLatch); at java.util.concurrent.locks.LockSupport.park(Redefined); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:199); at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala); at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala); at scala.concurrent.Await$.ready(package.scala:169); at cromwell.Main.cromwell$Main$$waitAndExit(Main.scala); at cromwell.Main$$anonfun$runServer$2.apply$mcI$sp(Main.scala:109); at cromwell.Main.continueIf(Main.scala); at cromwell.Main.runServer(Main.scala:109); at cromwell.Main.runAction(Main.scala:103); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala); at scala.Function0$class.apply$mcV$sp(Fu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:50809,concurren,concurrent,50809,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"valueFrom: ""*""; shellQuote: false; - valueFrom: $(inputs.radius); - valueFrom: ""`""; shellQuote: false; stdout: stdout.txt; ```. ```; name: cwl_caching; testFormat: workflowsuccess; backendsMode: ""only""; backends: [Local]; tags: [localdockertest]. files {; wdl: cwl_caching/cwl_caching.cwl; inputs: cwl_caching/cwl_caching.json; }. metadata {; status: Succeeded; }. workflowType: CWL; workflowTypeVersion: v1.0; ```. ```; cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'rCopy': No coercion defined from '1' of type 'Int' to 'Float'.; 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:688); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3480:1924,concurren,concurrent,1924,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3480,2,['concurren'],['concurrent']
Performance,"vent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:116); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:116); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:116); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-05-25 12:18:24,94] [info] WorkflowManagerActor WorkflowActor-e52409b4-c85a-4285-9453-f47c6b0ae86c is in a terminal state: WorkflowFailedState; [2017-05-25 12:18:24,94] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c#-297741123] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-e52409b4-c85a-4285-9453-f47c6b0ae86c#772660809] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [201",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2296:2461,concurren,concurrent,2461,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2296,1,['concurren'],['concurrent']
Performance,vent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:117); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:117); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:117); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	Suppressed: java.nio.file.NoSuchFileException: /tmp/640585481854205084.zip4511378926145376874/bar/foo.wdl; 		at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); 		at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 		at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); 		at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); 		at java.nio.file.Files.newByteChannel(Files.java:361); 		at java.nio.file.Files.newByteChannel(Files.java:407); 		at java.nio.file.Files.readAllBytes(Files.java:3152); 		at better.files.File.loadBytes(File.scala:163); 		at better.files.File.byteArray(File.scala:16,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1958:4176,concurren,concurrent,4176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1958,1,['concurren'],['concurrent']
Performance,"version ：cromwell-47. Architecture: DOCKER SGE docker-mysql. cromwell server Intermittent appearance qsub：command not found , server To get it back online. The relevant portion of cromwell.conf：. default = ""SGE""; providers {; # Configure the SGE backend; SGE {. # Use the config backend factory; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; #root = ""/GeneCloud001/server/cr_server/cromwell-exe""; #dockerRoot = ""/GeneCloud001/server/cr_server/cromwell-exe""; #script-epilogue = ""chmod -R a+rw * && chmod -R a+rw * && sync""; # Limits the number of concurrent jobs; #concurrent-job-limit = 500; # Define runtime attributes for the SGE backend.; # memory_gb is a special runtime attribute. See the cromwell README for more info.; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; String docker = ""jycloud/base:latest""; String? mnt_db_dir ####数据库挂载目录; String? mnt_input_dir ####输入bam挂载目录; String? mnt_out_dir ####输出挂载目录; String docker_user = ""$EUID""; String num_proc = 1; String? task_queue; String? mount; """"""; submit-docker = """"""; /opt/gridengine/bin/lx-amd64/qsub \; -terse \; -V \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ${""vf="" + memory_gb + ""g""},p=${num_proc} \; -b y docker run --rm -v ${cwd}:${docker_cwd} --user 1002 -m ${(memory_gb + (memory_gb / 2 )) + ""G""} --cpus ${num_proc} -v ${mnt_db_dir}:${mnt_db_dir}:ro -v ${mnt_out_dir}:${mnt_out_dir} -v /mnt/cache/sentieon:/mnt/cache/sentieon ${mount} ${docker} /bin/bash ${docker_script}",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5334:597,concurren,concurrent,597,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334,4,"['cache', 'concurren']","['cache', 'concurrent', 'concurrent-job-limit']"
Performance,"wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${out} \; --error=${err} \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # SINGULARITY_CACHEDIR needs to point to a directory accessible by; # the jobs (i.e. not lscratch). Might want to use a workflow local; # cache dir like in run.sh; source /work/share/ac7m4df1o5/bin/cromwell/set_singularity_cachedir.sh; SINGULARITY_CACHEDIR=/work/share/ac7m4df1o5/bin/cromwell/singularity-cache; source /work/share/ac7m4df1o5/bin/cromwell/test.sh ${docker}; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; if [ -z $SINGULARITY_CACHEDIR ]; then; CACHE_DIR=$HOME/.singularity; else; CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; LOCK_FILE=$CACHE_DIR/singularity_pull_flock. # we want to avoid all the cromwell tasks hammering each other trying; # to pull the container into the cache for the first time. flock works; # on GPFS, netapp, and vast (of course only for processes on the same; # machine which is the case here since we're pulling it in the master; # process before submitting).; #flock --exclusive --timeout 1200 $LOCK_FILE \; # singularity exec --containall docker://${docker} \; # echo ""successfully pulled ${docker}!"" &> /dev/null. # Ensure singularity is loaded if it's installed as a module; module load apps/singularity/3.7.3. # Build the Docker image into a singularity image; #IMAGE=$(echo $SINGULARITY_CACHEDIR/pull/${docker}.sif|sed ""s#:#_#g""); #singularity build $IMAGE docker://${docker}. # Submit the script to SLURM; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${cwd}/execution/stdout \; --error=${cwd}/execution/stderr \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""singularity exec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:8480,cache,cache,8480,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['cache'],['cache']
Performance,"we working on an HPC without root and network and I often get the following message, does this mean that my cache calls are failing.; we using the singularity method of task execution; ```; cromwell-system-akka.dispatchers.engine-dispatcher-27 WARN - BackendPreparationActor_for_bcfd9d26:UnmappedBamToAlignedBam.SamToFastqAndBwaMemAndMba:14:1 [UUID(bcfd9d26)]: Docker lookup failed; cala:35); ```. How do I set it up to enable caching calls?. ------------------------------------------------------------------------------------------; running file; ```; java -jar -Ddocker.hash-lookup.method=local -Ddocker.hash-lookup.enabled=true -Dwebservice.port=8088 -Dwebservice.interface=0.0.0.0 -Dconfig.file=/work/share/ac7m4df1o5/bin/cromwell/3_config/cromwellslurmsingularitynew.conf ./cromwell-84.jar server. ```; config ; ```; # This line is required. It pulls in default overrides from the embedded cromwell; # `reference.conf` (in core/src/main/resources) needed for proper performance of cromwell.; include required(classpath(""application"")). # Cromwell HTTP server settings; webservice {; #port = 8000; #interface = 0.0.0.0; #binding-timeout = 5s; #instance.name = ""reference""; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }. # Cromwell ""system"" settings; system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; #abort-jobs-on-terminate = false. # this tells Cromwell to retry the task with Nx memory when it sees either OutOfMemoryError or Killed in the stderr file.; memory-retry-error-keys = [""OutOfMemory"", ""Out Of Memory"",""Out of memory""]; # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; #graceful-server-shutdown = true. # Cromwell wi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:108,cache,cache,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,2,"['cache', 'perform']","['cache', 'performance']"
Performance,well.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: lenthall.exception.AggregatedException: :; Variable 'non_existent_scatter_variable' not found; 	at lenthall.util.TryUtil$.sequenceIterable(TryUtil.scala:26); 	at lenthall.util.TryUtil$.sequence(TryUtil.scala:33); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableScopes(WorkflowExecutionActor.scala:373); 	... 20 common frames omitted; 	Suppressed: wdl4s.exception.VariableNotFoundException$$anon$1: Variable 'non_existent_scatter_variable' not found; 		at wdl4s.exception.VariableNotFoundException$.apply(LookupException.scala:17); 		at wdl4s.Scope$$anonfun$6.apply(Scope.scala:268); 		at wdl4s.Scope$$anonfun$6.apply(Scope.scala:268); 		at scala.Op,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2020:2860,concurren,concurrent,2860,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2020,1,['concurren'],['concurrent']
Performance,"what are the reasons for ""Could not copy a suitable cache hit""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6484:52,cache,cache,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6484,1,['cache'],['cache']
Performance,"worker-15"" #19 daemon prio=5 os_prio=31 tid=0x00007fb76abaa800 nid=0x6b03 waiting on condition [0x000000012a1e3000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0060ab0> (a java.util.concurrent.CountDownLatch$Sync); at java.util.concurrent.locks.LockSupport.park(Redefined); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2$$anon$4.block(ExecutionContextImpl.scala); at scala.concurrent.forkjoin.ForkJoinPool.managedBlock(Redefined); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2.blockOn(ExecutionContextImpl.scala:44); at scala.concurrent.Await$.ready(package.scala:169); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellServer.scala:29); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellServer.scala); at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:433); at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala); at scala.concurrent.impl.CallbackRunnable.run(Redefined); at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:45288,concurren,concurrent,45288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"x7103 in Object.wait() [0x000000012ccb5000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x00000006c0624180> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""ForkJoinPool-3-worker-15"" #19 daemon prio=5 os_prio=31 tid=0x00007fb76abaa800 nid=0x6b03 waiting on condition [0x000000012a1e3000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0060ab0> (a java.util.concurrent.CountDownLatch$Sync); at java.util.concurrent.locks.LockSupport.park(Redefined); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2$$anon$4.block(ExecutionContextImpl.scala); at scala.concurrent.forkjoin.ForkJoinPool.managedBlock(Redefined); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2.blockOn(ExecutionContextImpl.scala:44); at scala.concurrent.Await$.ready(package.scala:169); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:44895,concurren,concurrent,44895,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"xecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-2"" #43 prio=5 os_prio=31 tid=0x00007fb76e8ee000 nid=0x3f0b waiting on condition [0x000000012ee35000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""_jprofiler_control_sampler"" #34 daemon prio=9 os_prio=31 tid=0x00007fb771044800 nid=0x6307 waiting on condition [0x000000012ab3a000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at com.jprofiler.agent.probe.y.run(ejt:1030). ""_jprofiler_native_sampler"" #37 daemon prio=10 os_prio=31 tid=0x00007fb76d269000 nid=0x5d07 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""_jprofiler_native_comm"" #36 daemon prio=5 os_prio=31 tid=0x00007fb770d7d000 nid=0x3707 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""_jprofiler_sampler"" #35 daemon prio=10 os_prio=31 tid=0x00007fb771027000 nid=0x7707 waiting on condition [0x000000012cb95000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at com.jprofiler.agent.sampler.Sampler.run(ejt:84). ""Attach Listener"" #33 daemon prio=9 os",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:35152,concurren,concurrent,35152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"xecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-3"" #57 prio=5 os_prio=31 tid=0x00007fb76e95e000 nid=0x9b03 waiting on condition [0x00000001322d6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-17"" #56 daemon prio=5 os_prio=31 tid=0x00007fb76b6b6000 nid=0x9903 waiting on condition [0x0000000131db9000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.Thr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:20291,concurren,concurrent,20291,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"xecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-4"" #60 prio=5 os_prio=31 tid=0x00007fb76d42b000 nid=0xa103 waiting on condition [0x0000000132168000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-19"" #59 daemon prio=5 os_prio=31 tid=0x00007fb770631000 nid=0x9f03 waiting on condition [0x00000001324dc000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.Thr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:17261,concurren,concurrent,17261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"xecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-5"" #65 prio=5 os_prio=31 tid=0x00007fb7706d2800 nid=0xab03 waiting on condition [0x00000001329eb000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-20"" #61 daemon prio=5 os_prio=31 tid=0x00007fb76b1f9000 nid=0xa303 waiting on condition [0x00000001325df000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.Thr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:15307,concurren,concurrent,15307,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"xecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-6"" #66 prio=5 os_prio=31 tid=0x00007fb7705ef800 nid=0xad03 waiting on condition [0x0000000132aee000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-5"" #65 prio=5 os_prio=31 tid=0x00007fb7706d2800 nid=0xab03 waiting on condition [0x00000001329eb000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-20"" #61 dae",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:14429,concurren,concurrent,14429,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"xecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-7"" #67 prio=5 os_prio=31 tid=0x00007fb7705a3000 nid=0xaf03 waiting on condition [0x00000001330c6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-6"" #66 prio=5 os_prio=31 tid=0x00007fb7705ef800 nid=0xad03 waiting on condition [0x0000000132aee000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:13551,concurren,concurrent,13551,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"xecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-8"" #68 prio=5 os_prio=31 tid=0x00007fb76ef84000 nid=0xb103 waiting on condition [0x0000000132bf1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-7"" #67 prio=5 os_prio=31 tid=0x00007fb7705a3000 nid=0xaf03 waiting on condition [0x00000001330c6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:12673,concurren,concurrent,12673,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"xecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-9"" #69 prio=5 os_prio=31 tid=0x00007fb76f36d800 nid=0xb303 waiting on condition [0x000000012c1b6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-8"" #68 prio=5 os_prio=31 tid=0x00007fb76ef84000 nid=0xb103 waiting on condition [0x0000000132bf1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:11795,concurren,concurrent,11795,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"xecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); [2018-11-17 09:37:14,33] [error] Error summarizing metadata; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 3785ms.; 	at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:548); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:186); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:145); 	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); 	at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:453); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:249); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:248); 	at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:37); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); [2018-11-17 09:37:53,75] [warn] [0 (WaitingForResponseEntitySubscription)] Response entity was not subscribed after 1 second. Make sure to read the response entity body or call discardBytes() on it. GET /token Empty -> 200 OK Chunked; [2018-11-17 10:11:19,05] [warn] [0 (WaitingForResponseEntitySubscription)] Response entity was not subscribed after 1 second. Make sure to read the response entity body or call discardBytes() on it. GET /token Empty -> 200 OK Chunked; [Guo-1|12:27:21]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4403:3018,concurren,concurrent,3018,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4403,2,['concurren'],['concurrent']
Performance,"y d; }. command <<<; set -euo pipefail; ls ""~{d}""; >>>. output {; String s = read_string(stdout()); }. runtime {; docker: ""debian:stable-slim""; }; }; ```. On a first `141477ef-e8e6-4fb9-ae58-5c2e8a646088` run, callCaching for `task2` is negative, as it should, with this error:; ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [; {; ""message"": ""gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir"",; ""causedBy"": []; }; ],; ""message"": ""[Attempted 1 time(s)] - FileNotFoundException: gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Now though, the directory has been created as a result of the WDL succeeding:; ```; $ gsutil ls gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir; gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir/file; ```. On a second `2690f8a5-4cd4-45e2-a93a-55125a1107f8` run, callCaching for `task2` is negative again though, with this error:; ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [; {; ""message"": ""gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir"",; ""causedBy"": []; }; ],; ""message"": ""[Attempted 1 time(s)] - FileNotFoundException: gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```; However, the directory `gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir` does exist from the previous run, so I am puzzled by the `FileNotFoundException` exception. Is it the case that directories cannot get cached by Crowmell and therefore callCaching does not work for tasks that have `Directory` as inputs?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6509:2058,Cache,Cache,2058,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6509,2,"['Cache', 'cache']","['Cache', 'cached']"
Performance,"y helpful. Any ideas? . ```; cromwell.core.CromwellFatalException: com.google.cloud.storage.StorageException: 410 Gone; {; ""error"": {; ""errors"": [; {; ""domain"": ""global"",; ""reason"": ""backendError"",; ""message"": ""Backend Error""; }; ],; ""code"": 503,; ""message"": ""Backend Error""; }; }. at cromwell.core.CromwellFatalException$.apply(core.scala:17); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:36); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. Metadata (``api/workflows/v2/c43e7d14-36a1-4b0b-95bf-cc381db48b5b/metadata?expandSubWorkflows=false``):. (attached); [cromwell_jes_error_April272017.txt](https://github.com/broadinstitute/cromwell/files/962915/cromwell_jes_error_April272017.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2215:1431,concurren,concurrent,1431,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2215,5,['concurren'],['concurrent']
Performance,y(JesInitializationActor.scala:58); at scala.Option.foreach(Option.scala:257); at cromwell.backend.impl.jes.JesInitializationActor.cromwell$backend$impl$jes$JesInitializationActor$$\; writeAuthenticationFile(JesInitializationActor.scala:58); at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.apply(JesInitializationActor.\; scala:52); at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.apply(JesInitializationActor.\; scala:51); at scala.util.Try$.apply(Try.scala:192); at cromwell.backend.impl.jes.JesInitializationActor.beforeAll(JesInitializationActor.scala:51); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$initSequence$1$$anonfun$apply$1.apply(\; BackendWorkflowInitializationActor.scala:156); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$initSequence$1$$anonfun$apply$1.apply(\; BackendWorkflowInitializationActor.scala:155); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91\; ); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scal,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2009:1426,concurren,concurrent,1426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2009,1,['concurren'],['concurrent']
Performance,"y; [2022-12-15 21:22:59,12] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.reported_sex:-1:1-20000000001 [9e4f5894main.reported_sex:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,12] [info] BT-322 9e4f5894:main.reported_sex:-1:1 cache hit copying success with aggregated hashes: initial = 91C81CABBB083C238800E3CF59AF537D, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,12] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.reported_sex:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,16] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.sex_aneuploidy:-1:1-20000000003 [9e4f5894main.sex_aneuploidy:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,17] [info] BT-322 9e4f5894:main.sex_aneuploidy:-1:1 cache hit copying success with aggregated hashes: initial = 86896541F0DCB2C2B959EEF37F266B30, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,17] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.sex_aneuploidy:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,32] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.month_of_birth:-1:1-20000000024 [9e4f5894main.month_of_birth:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,32] [info] BT-322 9e4f5894:main.month_of_birth:-1:1 cache hit copying success with aggregated hashes: initial = 601F8C709AA96517AA171B340CCA88BF, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,32] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.month_of_birth:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,78] [info] WorkflowExecutionActor-9e4f5894-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:22877,cache,cache,22877,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['cache'],['cache']
Performance,"yncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-08-14 16:14:14,45] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-08-14 16:14:15,67] [error] WorkflowManagerActor Workflow a3d3e011-3a0c-4203-9edb-3d65564a1d1d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:839); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorker",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4004:3677,concurren,concurrent,3677,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-5"" #65 prio=5 os_prio=31 tid=0x00007fb7706d2800 nid=0xab03 waiting on condition [0x00000001329eb000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-20"" #61 daemon prio=5 os_prio=31 tid=0x00007fb76b1f9000 nid=0xa303 waiting on condition [0x00000001325df000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArray",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:15141,concurren,concurrent,15141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-6"" #66 prio=5 os_prio=31 tid=0x00007fb7705ef800 nid=0xad03 waiting on condition [0x0000000132aee000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-5"" #65 prio=5 os_prio=31 tid=0x00007fb7706d2800 nid=0xab03 waiting on condition [0x00000001329eb000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Thread",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:14263,concurren,concurrent,14263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-7"" #67 prio=5 os_prio=31 tid=0x00007fb7705a3000 nid=0xaf03 waiting on condition [0x00000001330c6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-6"" #66 prio=5 os_prio=31 tid=0x00007fb7705ef800 nid=0xad03 waiting on condition [0x0000000132aee000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Thread",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:13385,concurren,concurrent,13385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-8"" #68 prio=5 os_prio=31 tid=0x00007fb76ef84000 nid=0xb103 waiting on condition [0x0000000132bf1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-7"" #67 prio=5 os_prio=31 tid=0x00007fb7705a3000 nid=0xaf03 waiting on condition [0x00000001330c6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Thread",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:12507,concurren,concurrent,12507,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-9"" #69 prio=5 os_prio=31 tid=0x00007fb76f36d800 nid=0xb303 waiting on condition [0x000000012c1b6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-8"" #68 prio=5 os_prio=31 tid=0x00007fb76ef84000 nid=0xb103 waiting on condition [0x0000000132bf1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Thread",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:11629,concurren,concurrent,11629,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-10"" #49 daemon prio=5 os_prio=31 tid=0x00007fb7720cd800 nid=0x8b03 waiting on condition [0x00000001316a4000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-9"" #48 daemon prio=5 os_prio=31 tid=0x00007fb76b529800 nid=0x8903 waiting on condition [0x00000001315a1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:28334,concurren,concurrent,28334,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-11"" #50 daemon prio=5 os_prio=31 tid=0x00007fb7720ce800 nid=0x8d03 waiting on condition [0x00000001317a7000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-10"" #49 daemon prio=5 os_prio=31 tid=0x00007fb7720cd800 nid=0x8b03 waiting on condition [0x00000001316a4000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:27258,concurren,concurrent,27258,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-12"" #51 daemon prio=5 os_prio=31 tid=0x00007fb7720cf000 nid=0x8f03 waiting on condition [0x00000001318aa000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-11"" #50 daemon prio=5 os_prio=31 tid=0x00007fb7720ce800 nid=0x8d03 waiting on condition [0x00000001317a7000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:26182,concurren,concurrent,26182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-13"" #52 daemon prio=5 os_prio=31 tid=0x00007fb76e96e000 nid=0x9103 waiting on condition [0x00000001319ad000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-12"" #51 daemon prio=5 os_prio=31 tid=0x00007fb7720cf000 nid=0x8f03 waiting on condition [0x00000001318aa000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:25106,concurren,concurrent,25106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-14"" #53 daemon prio=5 os_prio=31 tid=0x00007fb77061e000 nid=0x9303 waiting on condition [0x0000000131ab0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-13"" #52 daemon prio=5 os_prio=31 tid=0x00007fb76e96e000 nid=0x9103 waiting on condition [0x00000001319ad000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:24030,concurren,concurrent,24030,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-15"" #54 daemon prio=5 os_prio=31 tid=0x00007fb76b6b4000 nid=0x9503 waiting on condition [0x0000000131bb3000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-14"" #53 daemon prio=5 os_prio=31 tid=0x00007fb77061e000 nid=0x9303 waiting on condition [0x0000000131ab0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:22954,concurren,concurrent,22954,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-16"" #55 daemon prio=5 os_prio=31 tid=0x00007fb76e92f000 nid=0x9703 waiting on condition [0x0000000131cb6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-15"" #54 daemon prio=5 os_prio=31 tid=0x00007fb76b6b4000 nid=0x9503 waiting on condition [0x0000000131bb3000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:21878,concurren,concurrent,21878,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-18"" #58 daemon prio=5 os_prio=31 tid=0x00007fb770630800 nid=0x9d03 waiting on condition [0x00000001323d9000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-3"" #57 prio=5 os_prio=31 tid=0x00007fb76e95e000 nid=0x9b03 waiting on condition [0x00000001322d6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.ut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:18848,concurren,concurrent,18848,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['concurren'],['concurrent']
Performance,"ystem/user/cromwell-service/$b/$a/$8#-1275882726] unexpectedly terminated while conducting 11 polls. Making a new one...; [INFO] [11/12/2016 02:22:12.170] [cromwell-system-akka.actor.default-dispatcher-2686] [akka://cromwell-system/user/cromwell-service/$b/$a] watching Actor[akka://cromwell-system/user/cromwell-service/$b/$a/$9#-1852863496]; [WARN] [11/12/2016 02:22:12.171] [cromwell-system-akka.dispatchers.backend-dispatcher-2573] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-7f0d7c16-d434-4dd7-a7ba-c7e897701c9a/WorkflowExecutionActor-7f0d7c16-d434-4dd7-a7ba-c7e897701c9a/7f0d7c16-d434-4dd7-a7ba-c7e897701c9a-EngineJobExecutionActor-salmonRun.salmonQuant:NA:1/7f0d7c16-d434-4dd7-a7ba-c7e897701c9a-BackendJobExecutionActor-7f0d7c16:salmonRun.salmonQuant:-1:1/JesAsyncBackendJobExecutionActor] JesAsyncBackendJobExecutionActor [UUID(7f0d7c16)salmonRun.salmonQuant:NA:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; 	at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665:2323,concurren,concurrent,2323,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665,4,['concurren'],['concurrent']
Performance,"z.tbi; 1608597511949,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz.tbi; 1608597513691,download: s3://focal-sv-resources/broad-references/v0/sv-resources/resources/v1/hg38_primary_contigs.bed to focal-sv-resources/broad-references/v0/sv-resources/resources/v1/hg38_primary_contigs.bed; 1608597515955,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.tbi; 1608597517316,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:159890,cache,cacheCopy,159890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"z.tbi; 1608597513691,download: s3://focal-sv-resources/broad-references/v0/sv-resources/resources/v1/hg38_primary_contigs.bed to focal-sv-resources/broad-references/v0/sv-resources/resources/v1/hg38_primary_contigs.bed; 1608597515955,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.tbi; 1608597517316,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz.tbi; 1608597520303,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:160518,cache,cacheCopy,160518,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['cache'],['cacheCopy']
Performance,"ze of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13:16,20] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,23] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,25] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-02-11 10:13:16,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-02-11 10:13:16,33] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-02-11 10:13:17,45] [info] SingleWorkflowRunnerActor: Version 37; [2019-02-11 10:13:17,46] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-02-11 10:13:17,59] [info] Unspecified type (Unspecified version) workflow 52999e15-953f-44d6-aaae-1774c74d2910 submitted; [2019-02-11 10:13:17,65] [info] SingleWorkflowRunnerActor: Workflow submitted 52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,65] [info] 1 new workflows fetched; [2019-02-11 10:13:17,66] [info] WorkflowManagerActor Starting workflow 52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,67] [info] WorkflowManagerActor Successfully started WorkflowActor-52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,67] [info] Retrieved 1 workflows from the Workflo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:2443,throttle,throttle,2443,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,1,['throttle'],['throttle']
Performance,"{; ""shardIndex"": 11,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:07.287Z""; },; {; ""shardIndex"": 12,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:07.287Z""; },; {; ""shardIndex"": 13,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:07.288Z""; },; {; ""shardIndex"": 14,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:07.289Z""; },; {; ""shardIndex"": 15,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:07.289Z""; },; {; ""shardIndex"": 16,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:07.289Z""; }; ],; ""case_gatk_acnv_workflow.PadTargets"": [; {; ""Call caching read result"": ""Cache Hit: 6b52652e-a50d-4787-86b1-794e53958ada:case_gatk_acnv_workflow.PadTargets:-1"",; ""executionStatus"": ""Done"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/stdout"",; ""shardIndex"": -1,; ""outputs"": {; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""target_file"": ""/data/target/ice_targets.tsv"",; ""mem"": 1,; ""padding"": 250,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""isWGS"": false; },; ""returnCode"": 0,; ""backend"": ""Local"",; ""end"": ""2016-09-23T13:53:07.897Z"",; ""stderr"": ""/home/lichtens/test_eval/cromwel",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:75958,Cache,Cache,75958,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['Cache'],['Cache']
Performance,"~[cromwell.jar:0.19]; at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2541) ~[cromwell.jar:0.19]; at com.mysql.jdbc.ConnectionImpl.setAutoCommit(ConnectionImpl.java:4882) ~[cromwell.jar:0.19]; at com.zaxxer.hikari.proxy.ConnectionProxy.setAutoCommit(ConnectionProxy.java:334) ~[cromwell.jar:0.19]; at com.zaxxer.hikari.proxy.ConnectionJavassistProxy.setAutoCommit(ConnectionJavassistProxy.java) ~[cromwell.jar:0.19]; at slick.jdbc.JdbcBackend$BaseSession.startInTransaction(JdbcBackend.scala:437) ~[cromwell.jar:0.19]; at slick.driver.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:41) ~[cromwell.jar:0.19]; at slick.driver.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:38) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_72]; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_72]; at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_72]; Caused by: java.io.EOFException: Can not read response from server. Expected to read 4 bytes, read 0 bytes before connection was unexpectedly lost.; at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:2926) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3344) ~[cromwell.jar:0.19]; ... 16 common frames omitted; 2016-04-26 14:06:37,506 cromwell-system-akka.actor.default-dispatcher-15 INFO - WorkflowActor [UUID(143681e1)]: persisting status of GatherBqsrReports to Failed.; 2016-04-26 14:06:37,506 cromwell-system-akka.actor.default-dispatcher-15 ERROR - WorkflowActor [UUID(143681e1)]: Communications link failure. The last packet successfully received from the server was 324 milliseconds ago. The last packet sent successfully to ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/742:2498,concurren,concurrent,2498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/742,1,['concurren'],['concurrent']
Performance,~[cromwell.jar:0.19]; at scala.collection.immutable.List.foldLeft(List.scala:84) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.CallMetadataBuilder$.build(CallMetadataBuilder.scala:232) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowMetadataBuilder$$anonfun$cromwell$engine$workflow$WorkflowMetadataBuilder$$buildWorkflowMetadata$2.apply(WorkflowMetadataBuilder.scala:95) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowMetadataBuilder$$anonfun$cromwell$engine$workflow$WorkflowMetadataBuilder$$buildWorkflowMetadata$2.apply(WorkflowMetadataBuilder.scala:85) ~[cromwell.jar:0.19]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at scala.util.Success.map(Try.scala:237) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.pollAndExecAll(ForkJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/927:2925,concurren,concurrent,2925,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/927,1,['concurren'],['concurrent']
Performance,~[cromwell.jar:0.19];   at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19];   at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19];   at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19];   at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19];   at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19];   at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19];   at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19];   at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19];   at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19];   at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19];   at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19];   at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19];   at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19];   at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19];   at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19];   at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19];   at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19];   at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19];   at scala.concurrent.forkjoin.ForkJoinPool.runWork,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/810:5559,concurren,concurrent,5559,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/810,2,['concurren'],['concurrent']
Performance,"~~Might not make much difference until *all* our PRs are rebased to include this throttle~~. Actually this PR is more about allowing the AWS backend to hook into the existing poll retry logic to allow more resilience in the face of ""429 / Too Many Request"" exceptions during status polling.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4817:81,throttle,throttle,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4817,1,['throttle'],['throttle']
Performance,"…lity. Notes: ; - As w/ the WDL half of this, I will not be accepting passenger requests for random fixups but only on the actual changes (should be easier here); - sbt assembly is borked, I know that; - currently loading in wdl4s as an unmanaged jar, that'll change prior to merging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/365:214,load,loading,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/365,1,['load'],['loading']
Performance,…nking and a wider variety of cacheing strategies,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676:30,cache,cacheing,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676,1,['cache'],['cacheing']
Performance,"…old performance increase under load prior to DB gumming up. . Things to note:; - Effectively removes Metadata acks & failure notices (see #1811) via no longer emitting the messages but does not fully remove them. They still technically exist, I'll remove them as part of a separate PR; - Completely reworks `CromwellApiServiceSpec` to actually be testing `CromwellApiService` and not a general integration test of our REST endpoints. Two specific tests didn't make the cut (#1828 and #1829) I'll address in separate PRs. There were other tests which did not make the cut but were already effectively being tested in their appropriate units.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1836:5,perform,performance,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1836,2,"['load', 'perform']","['load', 'performance']"
Safety,"[31m- should abort a workflow mid run and restart immediately abort.restart_abort_tes *** FAILED *** (1 minute, 52 seconds)[0m; [31m java.lang.Exception: Invalid metadata response:[0m; [31m -Metadata mismatch for calls.scheduled_abort.aborted.executionStatus - expected: ""Failed"" but got: ""Aborted""[0m; [31m at centaur.test.Operations$$anon$13.checkDiff$1(Test.scala:330)[0m",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3571:14,abort,abort,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3571,4,"['Abort', 'abort']","['Aborted', 'abort', 'aborted']"
Safety, (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt: s3://s3.amazonaws.com/concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt; Caused by: java.io.IOException: Could not read from s3://concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt: s3://s3.amazonaws.com/concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoin,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341:1633,recover,recoverWith,1633,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341,1,['recover'],['recoverWith']
Safety," (sleeping); at java.lang.Thread.sleep(Native Method); at com.jprofiler.agent.probe.y.run(ejt:1030). ""_jprofiler_native_sampler"" #37 daemon prio=10 os_prio=31 tid=0x00007fb76d269000 nid=0x5d07 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""_jprofiler_native_comm"" #36 daemon prio=5 os_prio=31 tid=0x00007fb770d7d000 nid=0x3707 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""_jprofiler_sampler"" #35 daemon prio=10 os_prio=31 tid=0x00007fb771027000 nid=0x7707 waiting on condition [0x000000012cb95000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at com.jprofiler.agent.sampler.Sampler.run(ejt:84). ""Attach Listener"" #33 daemon prio=9 os_prio=31 tid=0x00007fb76e3e7800 nid=0x8507 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""db-4"" #31 daemon prio=5 os_prio=31 tid=0x00007fb7706e2000 nid=0x8303 waiting on condition [0x000000012ca92000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-3"" #29 daemon prio=5 os_prio=31 tid=0x00007fb76f4b7000 nid=0x7f03 waiting on condition [0x000000012c88c000]; java.lang",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:36434,Unsafe,Unsafe,36434,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety," 0; 	}; }. My.conf:. include required(classpath(""application"")). system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }. backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory"". system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; config {; project = ""$my_project""; root = ""$my_bucket""; name-for-call-caching-purposes: PAPI; slow-job-warning-time: 24 hours; genomics-api-queries-per-100-seconds = 1000; maximum-polling-interval = 600. # Setup GCP to give more memory with each retry; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; system.memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; memory_retry_multiplier = 4; ; # Number of workers to assign to PAPI requests; request-workers = 3. virtual-private-cloud {; network-label-key = ""network-key""; network-name = ""network-name""; subnetwork-name = ""subnetwork-name""; auth = ""auth""; }; pipeline-timeout = 7 days; genomics {; auth = ""auth""; compute-service-account = ""$my_account""; endpoint-url = ""https://lifesciences.googleapis.com/""; location = ""us-central1""; restrict-metadata-access = false; localization-attempts = 3; parallel-composite-upload-threshold=""150M""; }; filesystems {; gcs {; auth = ""auth""; project = ""$my_project""; caching {; duplication-strategy = ""copy""; }; }; }; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; runtime {; cpuPlatform: ""Intel Cascade Lake""; }; default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; disks: ""local-disk 375 SSD""; noAddress: true; preemptible: 1; maxRetries: 3; system.memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; memory_retry_multiplier = 4; zones: [""us-central1-a"", ""us-central1-b""]; }. include ""papi_v2_reference_image_manifest.conf""; }; }; }; }. gustily ls gs://cromwell-executions/MemoryRetryTest/d54a5a39-4d3b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7451:1952,timeout,timeout,1952,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7451,1,['timeout'],['timeout']
Safety," 2381; 470 pool-10-t 4751; 470 pool-10-t 2381; 282 G1 4751; 282 G1 2381; 188 blaze-tic 4751; 188 blaze-tic 2381; 94 VM 4751; 94 VM 2381; 94 java 4751; 94 java 2381; 94 db-9 4751; 94 db-9 2381; 94 db-8 4751; 94 db-8 2381; 94 db-7 4751; 94 db-7 2381; 94 db-6 4751; 94 db-6 2381; 94 db-5 4751; 94 db-5 2381; 94 db 4751; 94 db-4 4751; 94 db-4 2381; 94 db-3 4751; 94 db-3 2381; 94 db-2 4751; 94 db 2381; 94 db-2 2381; 94 db-20 4751; 94 db-20 2381; 94 db-19 4751; 94 db-19 2381; 94 db-18 4751; 94 db-18 2381; 94 db-17 4751; 94 db-17 2381; 94 db-16 4751; 94 db-16 2381; 94 db-15 4751; 94 db-15 2381; 94 db-1 4751 ...; ```. this is my java command; ```{shell}; java -Xms10M -Xmx125M -Dconfig.file=SGE.conf -jar cromwell-86.jar run xxx.wdl --inputs xxx.json; ```. SGE.conf file:; ```; # Documentation:; # https://cromwell.readthedocs.io/en/stable/backends/SGE. backend {; default = SGE. providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 5. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue = ""xxx""; String? sge_project = ""xxx""; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l num_proc="" + cpu + "",virtual_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; -binding ${""linear:"" + cpu} \; /usr/bin/env bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; }; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7571:1662,timeout,timeout-seconds,1662,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7571,2,['timeout'],['timeout-seconds']
Safety," = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAA",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:84533,abort,abort,84533,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,2,['abort'],"['abort', 'abort-jobs-on-terminate']"
Safety," Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:37:14,145 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(60ec6228)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:23,720 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(a442dc1c)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:37:31,421 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17bed42e)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:40,098 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(e9851ba1)]: Abort received. Aborting 3 EJEAs; `; Cromwell hash: 192ea6025613df967d60e9e975",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:4680,Abort,Aborting,4680,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety," I have created and I want to make a conditional statement. This because according to which sequencing library is made the user wants to trim the cell barcodes or not. So in case the user is providing the metadata with the name of the barcodes in the 4th column the task of trimming the barcode should be ""on"". In contrary, it should be off. This, of course, depends on whether the barcodes are provided in the metadata or not. What I am trying to do is to make the string that is in the barcode with the condition ""?"" (please see the workflow below in the scatter). When the scatter is reading the metadata, in the case in which there is no barcode the WDL is interrupted with this error: . ```; ""message"": ""Failed to evaluate 'scMethTask3.barcode' (reason 1 of 1): Evaluating files_and_metadata_row[3] failed: Failed to find index Success(WomInteger(3)); on array:\n\nSuccess([\""SRR5395068\"", \""SRR5395068_1.fastq.gz\"", \""SRR5395068_2.fastq.gz\""])\n\n3"",; ""causedBy"": []; ```; How can i avoid this? Or is there a way to accomplish what I am trying to do?. ### Which backend are you running? ; Unix terminal within slurm scheduler. ### Example meta_data files:; 1) without barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz	; ```; 2) with barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz ATCGCT	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz ATCGGA; ```; ### Below my workflow:. workflow scMethTask3 {. #information about the monitoring scrip and the number of samples; File? monitoring_script; File meta_data. #information for trimming the cell barcode; File command; Int bases; ; #information for trimming the adapters and low quality reads; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG. #information memory for each task; Int memory_task1; Int memory_task2. #Start the ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5396:1028,avoid,avoid,1028,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396,1,['avoid'],['avoid']
Safety," SGE: Sun Grid Engine; # SLURM: workload manager. # Note that these other backend examples will need tweaking and configuration.; # Please open an issue https://www.github.com/broadinstitute/cromwell if you have any questions; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions"". concurrent-job-limit = 10; # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; ## Warning: If set, Cromwell will run 'check-alive' for every job at this interval; exit-code-timeout-seconds = 360; filesystems {; local {; localization: [; # soft link does not work for docker with --contain. Hard links won't work; # across file systems; ""copy"", ""hard-link"", ""soft-link""; ]; caching {; duplication-strategy: [""copy"", ""hard-link"", ""soft-link""]; hashing-strategy: ""file""; }; }; }. #; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 3; Int requested_memory_mb_per_core = 8000; Int memory_mb = 40000; String? docker; String? partition; String? account; String? IMAGE; """""". submit = """"""; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${out} \; --error=${err} \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # SINGULARITY_CACHEDIR needs to point to a directory accessible by; # the jobs (i.e. not lscratch). Might want to use a workflow local; # cache dir like in run.sh; source /work/share/ac7m4df1o",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:6904,timeout,timeout-seconds,6904,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['timeout'],['timeout-seconds']
Safety," [UUID(bcfd9d26)]: Docker lookup failed; cala:35); ```. How do I set it up to enable caching calls?. ------------------------------------------------------------------------------------------; running file; ```; java -jar -Ddocker.hash-lookup.method=local -Ddocker.hash-lookup.enabled=true -Dwebservice.port=8088 -Dwebservice.interface=0.0.0.0 -Dconfig.file=/work/share/ac7m4df1o5/bin/cromwell/3_config/cromwellslurmsingularitynew.conf ./cromwell-84.jar server. ```; config ; ```; # This line is required. It pulls in default overrides from the embedded cromwell; # `reference.conf` (in core/src/main/resources) needed for proper performance of cromwell.; include required(classpath(""application"")). # Cromwell HTTP server settings; webservice {; #port = 8000; #interface = 0.0.0.0; #binding-timeout = 5s; #instance.name = ""reference""; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }. # Cromwell ""system"" settings; system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; #abort-jobs-on-terminate = false. # this tells Cromwell to retry the task with Nx memory when it sees either OutOfMemoryError or Killed in the stderr file.; memory-retry-error-keys = [""OutOfMemory"", ""Out Of Memory"",""Out of memory""]; # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; #graceful-server-shutdown = true. # Cromwell will cap the number of running workflows at N; #max-concurrent-workflows = 5000. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; #max-workflow-launch-count = 50. # Number of seconds between workflow launches; #new-workflow-poll-rate = 20. # Since the WorkflowLogCopyRouter is init",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:1351,abort,abort,1351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,2,['abort'],"['abort', 'abort-jobs-on-terminate']"
Safety," `womtool validate` (and it validated fine on Terra with the automatic validation they do). But the job would run about halfway and then automatically switch to ""Aborting"" status with no explanation or error message. The workflow would eventually fail after a huge delay (about 22 hours), and there would be no real error message. All tasks that ran were successful (but not all tasks ran). # Minimal WDL example. Here is a working example:. ```wdl; version 1.0. workflow my_workflow {; call my_task; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. And here is a non-working example that still validates fine using `womtool validate`:. ```wdl; version 1.0. workflow my_workflow {; input {; Boolean run_task; }. if (run_task) {; call my_task; }. output {; File out = select_first([my_task.out, stdout()]); }; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. The above gives; ```console; (cromwell) [sfleming@laptop:~/cromwell]$ womtool validate test.wdl ; Success!; ```. # The problem. The problem is that the non-working WDL example above should not validate successfully, as it is NOT a valid WDL. The `stdout()` built-in inside the `select_first()` in the `output` block of the `workflow` is not actually allowed. It will cause a very bizarre error when this WDL is run. # What am I asking for?. 1. Fix `womtool validate` to catch these kinds of errors. Also happens with `stderr()`.; 2. Provide an actionable error message when this kind of edge case ends up being run by Cromwell. Right now it automatically moves to ""Aborting"" status with no error message at all. Very hard to diagnose!. # Other information. I found this error using `miniwdl check`, which correctly identified the error, just FYI. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6976:2275,Abort,Aborting,2275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6976,1,['Abort'],['Aborting']
Safety," about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures and re-attach? Or maybe that the `script.submit` or `script.backgound` checks that the container is really stop and finished before returning a misleading error code?. Thank you in advance!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370:1985,timeout,timeout,1985,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370,1,['timeout'],['timeout']
Safety," akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.Abstra",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:16793,abort,abort,16793,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety," be simply that subqueries must be aliased.](https://stackoverflow.com/q/1888779/4107809) Is MariaDB not supported? . The workflow runs jobs that complete as normal. When rerunning, no call caching results are used, and all jobs simply run again. . Cromwell connects to the call caching database and successfully creates tables, for example `CALL_CACHING_AGGREGATION_ENTRY`. . <!-- Which backend are you running? -->; I am running with a SLURM backend. . <!-- Paste/Attach your workflow if possible: -->; I have a very simple example workflow. ; ```; workflow test{; call task_A {}; }. task task_A{; command{; echo 'testing'; }; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```; include required(classpath(""application"")). webservice {; }. akka {; http {; server {; }; }; }. system {; io {; }; input-read-limits {; }; job-rate-control {; jobs = 2; per = 1 second; }. abort {; scan-frequency: 30 seconds; cache {; enabled: true; concurrency: 1; ttl: 20 minutes; size: 100000; }; }. dns-cache-ttl: 3 minutes; }. workflow-options {; default {; }; }. call-caching {; enabled = true; }. google {; }. docker {; hash-lookup {; }; }. engine {; filesystems {; local {; }; }; }. languages {; WDL {; versions {; ""draft-2"" {; }; ""1.0"" {; }; }; }; CWL {; versions {; ""v1.0"" {; }; }; }; }. backend {; default = ""SLURM"". providers {. SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; runtime-attributes = """"""; Int runtime_minutes = 720; Int cpus = 1; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". exit-code-timeout-seconds = 600. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --constraint=""groups"" \; --qos=ded_reich \; --account=""reich"" \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6929:2537,abort,abort,2537,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6929,1,['abort'],['abort']
Safety, file or directory); 	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048); 	at cromwell.backend.sfs.ProcessRunner.run(ProcessRunner.scala:20); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$isAlive$1(SharedFileSystemAsyncJobExecutionActor.scala:196); 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive$(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(Standard,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:1495,recover,recover,1495,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recover']
Safety," files in directory defuse-data/gmap/cdna; Pointers file is cdna.ref153offsets64meta; Offsets file is cdna.ref153offsets64strm; Positions file is cdna.ref153positions; Offsets compression type: bitpack64; Allocating memory for ref offset pointers, kmer 15, interval 3...Attached existing memory (2 attached) for defuse-data/gmap/cdna/cdna.ref153offsets64meta...done (134,217,744 bytes, 0.00 sec); Allocating memory for ref offsets, kmer 15, interval 3...Attached new memory for defuse-data/gmap/cdna/cdna.ref153offsets64strm...done (234,475,312 bytes, 0.23 sec); Pre-loading ref positions, kmer 15, interval 3......done (276,173,052 bytes, 0.05 sec); Starting alignment; Failed attempt to alloc 18446744073709550532 bytes; Exception: Allocation Failed raised at indexdb.c:2885; /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/log/defuse.12.sh: line 6: 7481 Segmentation fault (core dumped) /usr/local/bin/gmap -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa > /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa.cdna.psl.tmp; ; real 0m1.262s; user 0m0.046s; sys 0m0.564s. ```. Run within the docker container but not using Cromwell, the output of that command looks like this:; ```; Starting defuse command:; /usr/local/bin/gmap -D Program_required_data/deFuse/defuse-data/gmap -d cdna -f psl #<1 > #>1; Reasons:; /mnt/Workflow_runs/2_fusion_detection_tools/BT474/BT474_deFuse_0.8.1/jobs/breakpoints.split.001.fa.cdna.psl m; issing; Success for defuse command:; /usr/local/bin/gmap -D Program_required_data/deFuse/defuse-data/gmap -d est4 -f psl /mnt/Workflow_runs/2_fus; ion_detection_tools/BT474/BT474_deFuse_0.8.1/jobs/breakpoints.split.001.fa > /mnt/Workflow_runs/2_fusion_detection_to; ols/BT474/BT474_deFuse_0.8.1/jobs/breakpoints.split.001.fa.est.4.psl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465:2864,detect,detectFusions,2864,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465,1,['detect'],['detectFusions']
Safety," invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication""}]}; cromwell_1 | 	at cromwell.docker.registryv2.DockerRegistryV2Abstract.$anonfun$getDigestFromResponse$1(DockerRegistryV2Abstract.scala:321); cromwell_1 | 	at map @ fs2.internal.CompileScope.$anonfun$close$9(CompileScope.scala:246); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.$anonfun$close$6(CompileScope.scala:245); cromwell_1 | 	at map @ fs2.internal.CompileScope.fs2$internal$CompileScope$$traverseError(CompileScope.scala:222); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.$anonfun$close$4(CompileScope.scala:244); cromwell_1 | 	at map @ fs2.internal.CompileScope.fs2$internal$CompileScope$$traverseError(CompileScope.scala:222); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.$anonfun$close$2(CompileScope.scala:242); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.close(CompileScope.scala:241); cromwell_1 | 	at unsafeRunAsyncAndForget @ cromwell.docker.DockerInfoActor.$anonfun$startAndRegisterStream$2(DockerInfoActor.scala:163); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.$anonfun$openAncestor$2(CompileScope.scala:261); cromwell_1 | 	at flatMap @ fs2.internal.FreeC$.$anonfun$compile$17(Algebra.scala:545); cromwell_1 | 	at map @ fs2.internal.CompileScope.$anonfun$close$9(CompileScope.scala:246); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.$anonfun$close$6(CompileScope.scala:245); cromwell_1 | 	at map @ fs2.internal.CompileScope.fs2$internal$CompileScope$$traverseError(CompileScope.scala:222); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.$anonfun$close$4(CompileScope.scala:244); cromwell_1 | 	at map @ fs2.internal.CompileScope.fs2$internal$CompileScope$$traverseError(CompileScope.scala:222); cromwell_1 | 2024-01-11 11:09:38 cromwell-system-akka.dispatchers.engine-dispatcher-33 WARN - BackendPreparationActor_for_0845428a:myworkflow.mytask:-1:1 [UUID(0845428a)]: Docker look",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:2629,unsafe,unsafeRunAsyncAndForget,2629,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['unsafe'],['unsafeRunAsyncAndForget']
Safety," launches; #new-workflow-poll-rate = 20. # Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; #number-of-workflow-log-copy-workers = 10. # Default number of cache read workers; #number-of-cache-read-workers = 25. io {; # throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; # #number-of-requests = 100000; # #per = 100 seconds; # }. # Number of times an I/O operation should be attempted before giving up and failing it.; #number-of-attempts = 5; }. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; #lines = 128000; #bool = 7; #int = 19; #float = 50; #string = 128000; #json = 128000; #tsv = 128000; #map = 128000; #object = 128000; }. abort {; # These are the default values in Cromwell, in most circumstances there should not be a need to change them. # How frequently Cromwell should scan for aborts.; scan-frequency: 30 seconds. # The cache of in-progress aborts. Cromwell will add entries to this cache once a WorkflowActor has been messaged to abort.; # If on the next scan an 'Aborting' status is found for a workflow that has an entry in this cache, Cromwell will not ask; # the associated WorkflowActor to abort again.; cache {; enabled: true; # Guava cache concurrency.; concurrency: 1; # How long entries in the cache should live from the time they are added to the cache.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 100000; }; }. # Cromwell reads this value into the JVM's `networkaddress.cache.ttl` setting to control DNS cache expiration; dns-cache-ttl: 3 minutes; }. docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:3279,abort,aborts,3279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['abort'],['aborts']
Safety," os_prio=31 tid=0x00007fb76a05e000 nid=0x3503 in Object.wait() [0x0000000126bc7000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x00000006c00371c0> (a java.lang.ref.ReferenceQueue$Lock); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164); at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209). ""Reference Handler"" #2 daemon prio=10 os_prio=31 tid=0x00007fb76b005800 nid=0x3303 in Object.wait() [0x0000000126ac4000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Object.wait(Object.java:502); at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:157); - locked <0x00000006c00301d8> (a java.lang.ref.Reference$Lock). ""main"" #1 prio=5 os_prio=31 tid=0x00007fb76a803000 nid=0xf07 waiting on condition [0x000000010b088000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c061d308> (a scala.concurrent.impl.Promise$CompletionLatch); at java.util.concurrent.locks.LockSupport.park(Redefined); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:199); at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala); at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala); at scala.concurrent.Await$.re",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:50374,Unsafe,Unsafe,50374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety, scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); languages.wdl.draft3.WdlDraft3LanguageFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:128); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:9094,unsafe,unsafeRunAsync,9094,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['unsafe'],['unsafeRunAsync']
Safety, scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); languages.wdl.draft3.WdlDraft3LanguageFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:138); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); akka,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999:11918,unsafe,unsafeRunAsync,11918,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999,1,['unsafe'],['unsafeRunAsync']
Safety," to Success; 2023-04-18 22:00:18,464 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:106:1]: Status change from Running to Success; 2023-04-18 22:01:20,604 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:111:1]: Status change from Running to Success; 2023-04-18 22:14:47,728 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: Aborting workflow; 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:262:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/9178938377659283430); 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:112:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/8559201934542591362); 2023-04-18 22:14:48,295 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Successfully requested cancellation of projects/16371921765/locations/us-central1/operations/9178938377659283430; 2023-04-18 22:15:56,564 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: Status change from Running to Success; 2023-04-18 22:16:44,505 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Status change from Running to Cancelled; 2023-04-18 22:16:44,539 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: WorkflowExecutionActor [UUID(10fa31a8)] aborted: myco.pull:262:1; 2023-04-18 22:16:45,159 INFO - $f [UUID(10fa31a8)]: Copying workflow logs from /cromwell-workflow-logs/workflow.10fa31a8-acbe-4ab7-a96a-6550ec08df12.log to gs://fc-caa84e5a-8ef7-434e-af9c-feaf6366a042/submissions/93bf6971-bfa1-4cb8-bb22-c8a753f58c49/workflow.logs/workflow.10fa31a8-acbe-4ab7-a96a-6550ec08df12.log; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7121:5981,abort,aborted,5981,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7121,1,['abort'],['aborted']
Safety," two runs at the same time since cromwell db gets locked by the previous run until it is finished? If yes, is there any other way to do it?. PS: I understand that cromwell provides `server` mode where we can submit runs via REST API end points. However, we are working on HPC cluster where we don't have admin privileges to start server and submit requests to api. Backend: `slurm`; Workflow: [Link](https://github.com/biowdl/RNA-seq/blob/develop/RNA-seq.wdl). <details>; <summary>Config</summary>. ```; backend {. default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int time_minutes = 600; Int cpu = 4; #Int memory = 500; String queue = ""short""; String map_path = ""/shared/rna-seq""; String partition = ""compute""; String root = ""/shared/rna-seq/cromwell-executions""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. submit = """"""; task=`echo ${job_name}|cut -d'_' -f3`; echo $task; image=`grep ""\b$task\b"" ${map_path}/map.txt |cut -d',' -f2`; echo $PWD; echo $image; if [ ! -z $image ]; then \; echo ""Inside Singularity exec""; \; echo ""CPU count: "" ${cpu}; \; echo ""time_minutes: "" ${time_minutes}; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""singularity exec -B /shared/rna-seq:/shared/rna-seq $image /bin/bash ${script}""; else \; echo ""No Singularity""; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""/bin/bash ${script}""; fi;; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. </details>. <details>;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6208:1327,timeout,timeout-seconds,1327,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6208,1,['timeout'],['timeout-seconds']
Safety, type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4920:1469,recover,recoverWith,1469,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920,1,['recover'],['recoverWith']
Safety,"![image](https://user-images.githubusercontent.com/165320/46151480-da3c2080-c23c-11e8-97a4-ecfa39139c11.png). We're seeing intermittent connectivity issues w/ message of ""socket timeout, cannot connect to server"" in Pingdom. They last 1-3 minutes and seem to be off and on:; ![image](https://user-images.githubusercontent.com/165320/46151547-05267480-c23d-11e8-865a-f9c1fc1c4e4d.png). From the looks of things this looks to be between pingdom and the load balancer or proxy, as neither Cromwell nor proxy logs are showing signs of distress during these times.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4164:178,timeout,timeout,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4164,1,['timeout'],['timeout']
Safety,""": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello-stdout.log"",; ""commandLine"": ""sleep 60 \necho \""Hello World! Welcome to Cromwell . . . on Google Cloud!\"""",; ""shardIndex"": -1,; ""jes"": {; ""executionBucket"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca"",; ""endpointUrl"": ""https://genomics.googleapis.com/"",; ""googleProject"": ""broad-dsde-alpha""; },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""ubuntu:latest"",; ""maxRetries"": ""0"",; ""cpu"": ""1"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zones"": ""us-central1-b"",; ""memoryMin"": ""2.048 GB"",; ""memory"": ""2.048 GB""; },; ""callCaching"": {; ""allowResultReuse"": false,; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ""inputs"": {; ""addressee"": ""World""; },; ""backendLabels"": {; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663"",; ""wdl-task-name"": ""hello""; },; ""labels"": {; ""wdl-task-name"": ""hello"",; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663""; },; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""message"": ""java.lang.IllegalArgumentException: Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""backend"": ""JES"",; ""end"": ""2018-12-11T16:07:04.207Z"",; ""stderr"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello-stderr.log"",; ""callRoot"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello"",; ""attempt"": 1,; ""executionEvents"": [; {; ""startTime"": ""2018-12-11T16:07:02.746Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2018-12-11T16:07:03.606Z""; },; {; ""startTime"": ""2018-12-11T16:07:03.648Z"",; ""description"": ""RunningJob"",; ""endTime"": ""2018-12-11T16:07:04.116Z""; },; {; ""startTime"": ""2018-12-11T16:07:04.116Z"",; ""des",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4484:1597,Abort,AbortedExecutionHandle,1597,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4484,1,['Abort'],['AbortedExecutionHandle']
Safety,"""Pipeline"" Scopes are added only for ""Pipeline"" Credentials.; Otherwise scopes must be requested when asking for credentials.; `Credential` generator (vs. `Credentials`, the former an older API) still returns an unscoped Credential.; Renamed methods returning Credentials from `credential` to `credentials`.; Now also validating USA Credentials before returning.; Credentials lookups from workflow options are only done for ""Pipeline"" creds and tests.; Removed a `validate(WorkflowOptions)` that wasn't in use since commit 6fbeadc.; Removed scope declarations no longer in use.; Using scope-constants as-much-as-possible from the Google SDKs.; Added an `unsafe` to replace `toTry.get`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4054:654,unsafe,unsafe,654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4054,1,['unsafe'],['unsafe']
Safety,"""bjobs -w ${job_id} |& egrep -qvw 'not found|EXIT|JOBID'"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path+modtime""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3;; hsqldb.log_size=0; """"""; connectionTimeout = 86400000; numThreads = 2; }; insert-batch-size = 2000; read-batch-size = 5000000; write-batch-size = 5000000; metadata {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-metadata-db/;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3;; hsqldb.log_size=0; """"""; connectionTimeout = 86400000; numThreads = 2; }; insert-batch-size = 2000; read-batch-size = 5000000; write-batch-size = 5000000; }; }. services {; MetadataService {; metadata-read-row-number-safety-threshold = 5000000; }; }; ```; The main issue that I can see is that Cromwell is ignoring the increased metadata row count. this is despite my separating out the metadata database and increasing the thresholds on both databases. Prior to running the changes listed above I have ensured that the working directory is completely purged of logs and metadata so as to ensure an unobstructed run. The documentation currently provides no additional guidance on how to overcome the error. Any assistance will be appreciated.; Best wishes,. Matthieu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7203:3641,safe,safety-threshold,3641,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7203,1,['safe'],['safety-threshold']
Safety,"""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; parameter_meta {. }. }. application.conf. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. system.new-workflow-poll-rate=1; ```; google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; }; ]; }. engine {; filesystems {; gcs {; }; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. backend {; default = ""Jes""; providers {; Jes {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""gcid-cromwell"". // Base bucket for workflow executions. // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; caching {; duplication-strategy = ""reference""; }. }; }; }; }; }; }; ```. I executed a haplotype caller wdl with interval scattering. Two of the shards took over 5 before I aborted the workflow, while the others finished in under 1 hour. The timestamps on the RC file, which indicated a 0 return code, were similar to the other shards that finished in under an hour. . This looks like a bug. I've since restarted the worklow with call-caching so it seems unlikely I can reproduce this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3905:5605,abort,aborted,5605,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905,1,['abort'],['aborted']
Safety,"# Thursday. 1. Jobs ""queued in cromwell""; 2. Doug started job, 8 hours to start; 3. Graphite under-reporting. # Friday. 1. No improvement; 2. User1 suggests his own wf running w/ 25k jobs; 3. disk quota hit; 4. **No limit on # jobs running / project**; 4. Console quotas page revealed this; 5. User1 aborts wf; 6. 10 minutes of improvement, then back down to low throughput; 7. **Difficult to understand who is running what**; 8. We found a crude query to ask ^; 9. Bubbled up sub-wf's manually; 10. Found User2 running a large job; 11. User had upped quotas on # cpus, persistent disk and IP's; 12. **Exhausted IP quota, PAPI throttles itself as a result of too many API calls**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3666:300,abort,aborts,300,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3666,1,['abort'],['aborts']
Safety,"# What Happened. On Friday September 14, a user noted that they were unable to retrieve metadata associated with their workflow. Subsequent calls were made to the endpoint directly to retrieve this metadata. During this time, New Relic reported memory exhaustion and extensive (~30 mins) of garbage collection. Ultimately, the instance stopped responding to requests but continued accepting connections, resulting in proxy timeout log messages. ![image](https://user-images.githubusercontent.com/165320/45637785-56827700-ba79-11e8-9176-1991692fcc76.png). # What should have happened. Crowell frontend should have either:. * returned the result in a timely manner ; * failed more gracefully. # What we did to fix it. Rebooted the instances. Subsequent calls to retrieve the metadata also timed out but did not put the frontend back into the ""zombie"" state. # Potential causes. The metadata is too large to fit in memory. The present situation is that there is some processing done between DB and user in order to provide a more structured response. # Potential fixes. The timeout on Cromwell should be increased beyond the current 20s. The metadata could always be larger than the instance has memory. Either a streaming response or deferred computation of the structured result would be better. # Technical Addendums:. Error Message when unresponsive:. ```; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.508796 2018] [proxy:error] [pid 162:tid 139866926597888] [client 130.211.0.195:49012] AH00898: Error reading from remote server returned by /engine/v1/version; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.316500 2018] [proxy:error] [pid 162:tid 139867379803904] (110)Connection timed out: AH00957: HTTP: attempt to connect to 172.17.0.2:8000 (app) failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4105:423,timeout,timeout,423,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4105,2,['timeout'],['timeout']
Safety,"# What happens. When a large workflow is queried for metadata, cromwell spends a considerable amount of time preparing the repsonse. **This usually results in a timeout for the caller.** In some cases, the preparation is so expensive that Cromwell either runs out of memory or enters a zombie-like state(#4105). # What should happen. The caller should receive a timely response, and Cromwell should not be endangered by operations on large workflows. # Speculation: Construction of result. The result is constructed in a two-phase manner: gather all the data, then produce a structured response. This is done for two reasons:. 1. Unstructured metadata is difficult for a human to understand.; 1. There are possibly many duplicates due to the way restarts are handled. ## Recommendation. ~Stream results (using doobie SQL library?) and construct response while gathering data. This should mean that a large pool of data is never present in memory, only the current result set and the partial response.~. Not streaming for now. Instead going to [`foldMap`](https://typelevel.org/cats/typeclasses/foldable.html) large sequence into `Map` monoid, then combine all those maps together into a final result. . There is some manipulation to be done after combining a result. 1. Sort calls by time; 1. Prune duplicates by taking the most recent. [This has some special cases](https://github.com/broadinstitute/cromwell/blob/3d68421b025db26ac3ab53972f69497e90601a47/engine/src/main/scala/cromwell/webservice/metadata/MetadataComponent.scala#L93) that need to be considered. # Speculation: Database table. The metadata table is currently an unindexed monster, comprising 10^6 - 10^9 rows and between 2-3 TB of data. The query has historically been surprisingly performant but is likely going to degrade over time. ## Recommendation . **punt on DB changes**. Believe to be related to #4093 and #4105",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4124:161,timeout,timeout,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4124,1,['timeout'],['timeout']
Safety,"## Discussion \#1; ```; bshifaw [3:59 PM]; Hi Chris, ; The featured joint calling method is using NIO.; https://portal.firecloud.org/#methods/gatk/joint-discovery-gatk4/9/wdl; Is this the method you are referencing? (edited). bshifaw [4:28 PM]; @vdauwera, just confirmed with @jsoto. The wdl isn’t using NIO when importing the GVCFs. Due to a change in the wdl we decide to implement to best leverage the FC data model (using an array of input files instead of a sample name map file). (edited). Collapse; cwhelan [9:48 PM]; right, that’s the method i was using. vdauwera [11:22 PM]; oooh that’s an interesting case that would benefit from the flexible data models work — this would be great to show @andreah; ```. ## Discussion \#2. ```; cwhelan [11:17 AM]; ie it’s trying to localize each gvcf to each shard instance. tjeandet [11:17 AM]; do you have an idea of how many input files each shard has ?. Collapse; cwhelan [11:17 AM]; 555 samples; ```. # Takeaways. Run https://portal.firecloud.org/#methods/gatk/joint-discovery-gatk4/9/wdl in a non-production environment w/ 555 samples and try to reproduce issue w/ hashing timeouts. We predict they will not occur as cromwell production was seeing elevated CPU usage due to it's /stats endpoint being hit repeatedly.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3712:1124,timeout,timeouts,1124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3712,2,"['predict', 'timeout']","['predict', 'timeouts']"
Safety,"## Symptom; I can run test and assembly tasks succesfully on cromwellApiClient subproject, but If I write my own small test class that uses `cromwell.api.CromwellClient` it fails at runtime with:; ```; Detected java.lang.NoSuchMethodError error, which MAY be caused by incompatible Akka versions on the classpath. Please note that a given Akka version MUST be the same across all modules of Akka that you are using, e.g. if you use akka-actor [2.5.3 (resolved from current classpath)] all other core Akka modules MUST be of the same version. External projects like Alpakka, Persistence plugins or Akka HTTP etc. have their own version numbers - please make sure you're using a compatible set of libraries. ; Uncaught error from thread [default-akka.actor.default-dispatcher-5]: akka.actor.ActorCell.addFunctionRef(Lscala/Function2;)Lakka/actor/FunctionRef;, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for for ActorSystem[default]; java.lang.NoSuchMethodError: akka.actor.ActorCell.addFunctionRef(Lscala/Function2;)Lakka/actor/FunctionRef;; ...; ```; I'm essentially seeing exactly the behaviour described in reference [1] below, which is eviction warnings at compile time and then the runtime blow-up. The root cause seems to be that akka-http depends on an older version of akka-actor (2.4.19) than that specified for the project (2.5.3). Running `dependencyTree` task confirms:; ```; [info] +-com.typesafe.akka:akka-http-spray-json_2.12:10.0.9 [S]; [info] | +-com.typesafe.akka:akka-http_2.12:10.0.9 [S]; [info] | | +-com.typesafe.akka:akka-http-core_2.12:10.0.9 [S]; [info] | | +-com.typesafe.akka:akka-parsing_2.12:10.0.9 [S]; [info] | | | +-com.typesafe.akka:akka-actor_2.12:2.4.19 (evicted by: 2.5.3); ```; If I explicitly add dependency on the latest akka-stream as suggested in [2] and [3], the problem goes away:; ```; diff --git a/project/Dependencies.scala b/project/Dependencies.scala; index 0d77e2d3..7254fc61 100644; --- a/project/Dependencies.scala; +++ b/project",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2579:202,Detect,Detected,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2579,1,['Detect'],['Detected']
Safety,"## Why; Emerald empire scattered 100k wide and it didn’t go well. * Cromwell pegged CPU and was unresponsive to HTTP calls, forcing the process manager to kill it; * Papi v2 deadlocked w/ load (talked to Aaron Kemp & Henry Ferrara, No-op for us. ## What; Send 100k wide scatter to Cromwell, make sure it handles it gracefully. ## Measure. * Time it took to complete scatter (TODO: not sure at which point to consider ""complete"", almost certainly it should be before the WDL is run and thus avoids the inherent variance of the backend ); * CPU (should not be pegged); * HTTP Responsiveness (should respond to HTTP calls in a reasonable time: < 2s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4795:490,avoid,avoids,490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4795,1,['avoid'],['avoids']
Safety,"### Description. After playing a while with GCP Batch:; 1. Batch can automatically retry preemption errors.; 2. When Batch retries, there is no signal in the Job status events, we need to check the VM logs.; 3. Cromwell does not get any details about Batch retries, hence, the same jobId is kept even if a VM is recreated.; 4. When the job status events mention that the job failed due to a preemption error, this is final, Batch already exhausted the retries. This removes all the code related to handling preemption errors and parses the job status events to derive the failure reason. Also, this tries detecting the other potential exit codes mapping them to a better error message. Refs:; - [Batch automated task retries](https://cloud.google.com/batch/docs/automate-task-retries); - [Batch exit codes](https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes). <!-- What is the purpose of this change? What should reviewers know? -->. Fixes #7407. This is an example error log produced when getting a preemption error:. ```; [2024-06-21 12:30:09,28] [info] WorkflowManagerActor: Workflow 2cdef371-703c-4c1e-92b5-0e013dcda6c8 failed (during ExecutingWorkflowState): java.lang.Exception: Task myWorkflow.myTask:NA:1 failed: A Spot VM for the job was preempted during run time; ```. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7457:605,detect,detecting,605,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7457,1,['detect'],['detecting']
Safety,"### Description. Fixes job recovery on restart for GCP Batch, addresses #7495. . ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7498:27,recover,recovery,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7498,1,['recover'],['recovery']
Safety,### Description. Part 2 of https://github.com/broadinstitute/cromwell/pull/7432. Detects and retries the new fatal quota errors we've been seeing. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [x] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7439:81,Detect,Detects,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7439,1,['Detect'],['Detects']
Safety,"### Description. Resolves intermittent build breakage caused by 404s of `paleo-core` artifacts. `paleo-core` is deprecated, and so is the library that depends on it, `swagger2markup`. - Remove code and build components; - Clean up docs and provide reasonable replacements when necessary; - Removed the term ""REST"" as redundant because it has taken over as the dominant API type; - Reorganize current `CHANGELOG.md` into sections because we have a substantial number of release notes 🎉 ; - Unrelated one-line change to add timezone to debug image. ```; > docker run -it --entrypoint /bin/bash broadinstitute/cromwell:88-648e536-DEBUG; Version 88-648e536-DEBUG built at 2024-08-08 15:04:21 EDT; root@4ec372b744a8:/# ; ```. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7488:317,redund,redundant,317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7488,1,['redund'],['redundant']
Safety,"### Description. We make this query constantly, looks like the single most frequent one against metadata. All it does is check whether the workflow ID is valid by checking whether >1 metadatum exists for it. We already started down the path of checking summary instead of metadata, see https://github.com/broadinstitute/cromwell/pull/4617. It just makes way more sense to me to check a table with 77M rows than 36B. ```; select ; exists(; select ; `CALL_FQN`, ; `METADATA_KEY`, ; `WORKFLOW_EXECUTION_UUID`, ; `METADATA_TIMESTAMP`, ; `JOB_SCATTER_INDEX`, ; `METADATA_JOURNAL_ID`, ; `JOB_RETRY_ATTEMPT`, ; `METADATA_VALUE_TYPE`, ; `METADATA_VALUE` ; from ; `METADATA_ENTRY` ; where ; `WORKFLOW_EXECUTION_UUID` = '602a4913-d666-4182-b2f1-242fbda817d2'; );; ```. It is potentially implicated in the 2021 database migration that failed at the very end, you can see a bunch of them in this screenshot (2021-11-09):. <img width=""1792"" alt=""Screen Shot 2021-11-09 at 1 14 03 AM"" src=""https://github.com/user-attachments/assets/d3eee3da-9636-4406-a6f9-9862d33cd650"">. ```; Lock wait timeout exceeded; try restarting transaction; [for Statement ""RENAME TABLE `cromwell`.`METADATA_ENTRY` TO `cromwell`.`_METADATA_ENTRY_old`,; `cromwell`.`_METADATA_ENTRY_new` TO `cromwell`.`METADATA_ENTRY`""]; at /usr/bin/pt-online-schema-change line 10922.; ```; [Slack link to contemporary discussion.](https://broadinstitute.slack.com/archives/C02LCC8968N/p1636439602084200); [Contemporary analysis in JIRA.](https://broadworkbench.atlassian.net/browse/WM-906?focusedCommentId=53394). ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7575:1074,timeout,timeout,1074,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7575,1,['timeout'],['timeout']
Safety,"### EDIT: See https://www.traviscistatus.com/incidents/kyf149kl6bvp. Multiple builds are displaying timeouts trying to run the dockerScripts tests. These builds have a heartbeat that give more information as to the timeout:; - https://travis-ci.com/broadinstitute/cromwell/jobs/197403844; - https://travis-ci.com/broadinstitute/cromwell/jobs/197407990; - https://travis-ci.com/broadinstitute/cromwell/jobs/197412193; - https://travis-ci.com/broadinstitute/cromwell/jobs/197420904. May be something that broke in our repo, or an upstream transient error?. Edit:. Just commenting out the `sbt assembly` is not enough. Commenting out just the assembly leads to downstream build errors such as:; - https://travis-ci.com/broadinstitute/cromwell/jobs/197523977",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4933:100,timeout,timeouts,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4933,2,['timeout'],"['timeout', 'timeouts']"
Safety,"### What happened. On 10/10/2018, around 11:15 AM, there was a spike in backpressure and 403 copy failures. It was discovered that a user had submitted workflows attempting to access buckets it did not have access to. . ![image](https://user-images.githubusercontent.com/16748522/46764755-59087300-ccab-11e8-9163-afd953710adf.png); Purple line- backpressure; Light green line- 403 copy failures. ### What was done to fix it. The situation was discussed with the user, and once he aborted all his workflows, Cromwell slowly returned to its normal state. The issue was resolved around 1:50 PM. ### Potential causes. The user had reused a WDL from another user, but he didn't have access to their Google Cloud buckets. This workflow contained job that ran 5000 split intervals against dataset of approx 1300 samples. Each of the 5000 outputs would be copied, per workflow, per sample. Depending on the number of samples the other user had previously run, each interval-output-for-each-sample tried call caching to other user's workspace. This resulted in a lot of attempts to copy files and then failures.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4229:480,abort,aborted,480,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4229,1,['abort'],['aborted']
Safety,"### Wordy Description. * We will no longer update the womtool libraries in Agora and Rawls. We will accept the minor drift which may occur between ""what Rawls thinks Cromwell could run"" and ""what Cromwell can actually run"" until womtool as a service is adopted.; * We will no longer run smoke tests before and after each update. We will rely on swatomation and daily runs to detect problems. ### Current Process:; ![Current Process](https://github.com/broadinstitute/cromwell/blob/develop/scripts/release_processes/firecloud-develop.dot.png?raw=true). ### Proposed New Process:; ![Proposed New Process](https://github.com/broadinstitute/cromwell/blob/cjl_simplify_releases/scripts/release_processes/firecloud-develop.dot.png?raw=true). ### Proposed Hotfix Process:; ![Proposed Hotfix Process](https://github.com/broadinstitute/cromwell/blob/cjl_simplify_releases/scripts/release_processes/firecloud-develop-hotfix.dot.png?raw=true)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4941:375,detect,detect,375,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4941,1,['detect'],['detect']
Safety,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. Hi there,. My workflow runs out of memory. I'm not sure where exactly though. I'm trying to run Mutect2 on a large cohort of samples, but it keeps crashing with an out of memory error. Any help would be extremely helpful as to how I can avoid this issue. I'm using `-Xmx32g` when I call cromwell, and am using GCS as the backend. Here the error:; ```; ### GetPileupSummaries; # These must be created, even if they remain empty, as cromwell doesn't support optional output; touch tumor-pileups.table; touch normal-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-TumorCramToBam/1046545_23163_0_0.bam --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O tumor-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O normal-pileups.table; fi; fi; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fbe6d000000, 4345298944, 0) failed; error='Not enough space' (errno=12); #; # There is insufficient memory for the Java Runtime Environ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5347:463,avoid,avoid,463,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347,1,['avoid'],['avoid']
Safety,"$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryWorker-aaa95e49-59b4-4de6-864d-22920eac6164 terminated. 99 run creation requests, 1 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; ```. Of note, I am running Cromwell 40 with the following `java -Xmx100g -Dconfig.file=google.conf -jar cromwell-40.jar server` on a 16-core highmem system that has 102g of RAM. Of those 102G, only 30G are in use per `htop` (including both active and cache). Cromwell does continue, but the concern, as noted in the error, is that 99 jobs might now be duplicated. If I run with just 1 or 2 jobs, I don't get this message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:4588,abort,abort,4588,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['abort'],['abort']
Safety,(We don't actually have a pluggable SGE backend ATM but just entering this for completeness since the other backends had the same issue). Actual code:. ``` scala; override def abortInitialization(): Unit = ???; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1112:176,abort,abortInitialization,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1112,1,['abort'],['abortInitialization']
Safety,"); at akka.util.SerializedSuspendableExecutionContext.run(SerializedSuspendableExecutionContext.scala); at akka.dispatch.TaskInvocation.run(Redefined); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""Abandoned connection cleanup thread"" #22 daemon prio=5 os_prio=31 tid=0x00007fb76a4f5800 nid=0x7103 in Object.wait() [0x000000012ccb5000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x00000006c0624180> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""ForkJoinPool-3-worker-15"" #19 daemon prio=5 os_prio=31 tid=0x00007fb76abaa800 nid=0x6b03 waiting on condition [0x000000012a1e3000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0060ab0> (a java.util.concurrent.CountDownLatch$Sync); at java.util.concurrent.locks.LockSupport.park(Redefined); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(Redefined); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2$$anon$4.block(ExecutionContextImpl.scala); at scal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:44465,Unsafe,Unsafe,44465,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"* JES; * cromwell-30.jar. I have two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3032:391,detect,detect,391,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032,1,['detect'],['detect']
Safety,"**Commit 1**; Stop invoking scalacheck during the sbt build by replacing a) specs2 with specs2-mock plus pegdown, and b) excluding cats dependencies (also in wdl4s).; Removed cromwell dependency duplications (see the verboseness in excising cats' duplicated dependencies).; Just in case, pass scalatest arguments only to scalatest. **Commit 2**; 3 seconds timeout (instead of the 1 second default) for each of the slick and liquibase databases being compared.; Removed dead docker case class.; Formatting updates for sbt-docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1589:356,timeout,timeout,356,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1589,1,['timeout'],['timeout']
Safety,"**What Happened**; On 9/12/18 5:40 pm, after a Firecloud release, Cromwell 402 stopped responding to status checks. It only recovered after being restarted at 10pm.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4094:124,recover,recovered,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4094,1,['recover'],['recovered']
Safety,"**What happened:**; On 9/12/18 at 3:41 pm, the Cromwell servers' (801 and 802) CPU were pegged. 801 was restarted (4:11 pm) and immediately got pegged again but recovered 35 min later without further intervention. As for 802, the CPU remained pegged so, at 5:21 pm, 802 was restarted and recovered right away.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4093:161,recover,recovered,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4093,2,['recover'],['recovered']
Safety,", each phase having its own configurable timeout. See the Dev Wiki for more details.; 	graceful-server-shutdown = true; max-concurrent-workflows = 5000. io {; throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; number-of-requests = 100000; per = 100 seconds; }; }; }. akka {; # Optionally set / override any akka settings; http {; server {; # Increasing these timeouts allow rest api responses for very large jobs; # to be returned to the user. When the timeout is reached the server would respond; # `The server was not able to produce a timely response to your request.`; # https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows; request-timeout = 600s; idle-timeout = 600s; }; }; }. services {; MetadataService {; #class = ""cromwell.services.metadata.impl.MetadataServiceActor""; config {; metadata-read-row-number-safety-threshold = 2000000; # # For normal usage the default value of 200 should be fine but for larger/production environments we recommend a; # # value of at least 500. There'll be no one size fits all number here so we recommend benchmarking performance and; # # tuning the value to match your environment.; db-batch-size = 700; }; }; }. google {. application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. docker {; hash-lookup {; method = ""remote""; }; }. engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }. call-caching {; enabled = true; }. backend {; default = GCPBATCH; providers {; GCPBATCH {; // life sciences; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; ## Google project; project = ""$PROJECT"". ## Base bucket for workflow executions; root = ""$BUCKET""; name-for-call-caching-purposes: PAPI; #60000/min in google; ##genomics-api-queries-per-100-seconds = 90000; virtual-private-cloud {; network-name = ""$NET""; subnetwo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:9038,safe,safety-threshold,9038,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['safe'],['safety-threshold']
Safety,",; ""doc"": ""Minimum spread within candidate purities before somatics can be used. Default 0.15\n"",; ""id"": ""#somatic_min_purity_spread_purple""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Minimum number of somatic variants required to assist highly diploid fits. Default 300.\n"",; ""id"": ""#somatic_min_total_purple""; },; {; ""type"": [; ""null"",; ""float""; ],; ""doc"": ""Proportion of somatic deviation to include in fitted purity score. Default 1.\n"",; ""id"": ""#somatic_penalty_weight_purple""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional location of somatic variant vcf to assist fitting in highly-diploid samples.\nSample name must match tumor parameter. GZ files supported.\n"",; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#somatic_vcf_purple""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional location of structural variant vcf for more accurate segmentation.\nGZ files supported.\n"",; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#structural_vcf_purple""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional location of failing structural variants that may be recovered.\nGZ files supported.\n"",; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#sv_recovery_vcf_purple""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Number of threads used for amber step\n"",; ""id"": ""#threads_amber""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Number of threads to run cobalt command\n"",; ""id"": ""#threads_cobalt""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Number of threads to use - set to 8 by default"",; ""id"": ""#threads_gridss""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Number of threads\n"",; ""id"": ""#threads_purple""; },; {; ""type"": ""File"",; ""doc"": ""tumour BAM file\n"",; ""secondaryFiles"": [; "".bai""; ],; ""id"": ""#tumor_bam""; },; {; ""type"": [; ""null"",; ""string""; ],; ""doc"": ""sample name of tumor. Must match the somatic snvvcf sample name. (Default: \\${sample}_T)\n"",; ""id"": ""#tumor_sample""; },; {; ""type"": [; ""null"",; ""string""; ],; ""doc"": ""htsjdk SAM/BAM validation level (STRICT (default), LENIENT, o",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:46840,recover,recovered,46840,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['recover'],['recovered']
Safety,",; I am trying to run a workflow written in WDL using Cromwell v.65. The workflow reports the following error in the stdout:; ```[2023-08-11 14:21:11,58] [error] SingleWorkflowRunnerActor received Failure message: Metadata for workflow <UUID> exists in database but cannot be served because row count of 3138431 exceeds configured limit of 1000000.; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow <UUID> exists in database but cannot be served because row count of 3138431 exceeds configured limit of 1000000.```; This is after having edited the `cromwell.conf` as suggested in [this thread](https://github.com/broadinstitute/cromwell/issues/2519). The configuration file used is as follows (edited to remove the main script):; ```; include required(classpath(""application"")); backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 300; runtime-attributes = """"""; Int cpu; Int memory_mb; String? lsf_queue; String? lsf_project; String? docker; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; module load tools/singularity/3.8.3; SINGULARITY_MOUNTS='<redacted>'; export SINGULARITY_CACHEDIR=$HOME/.singularity/cache; LOCK_FILE=$SINGULARITY_CACHEDIR/singularity_pull_flock. export SINGULARITY_DOCKER_USERNAME=<redacted>; export SINGULARITY_DOCKER_PASSWORD=<redacted>. flock --exclusive --timeout 900 $LOCK_FILE \; singularity exec docker://${docker} \; echo ""Sucessfully pulled ${docker}"". bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; singularity exec --containall $SINGULARITY_MOUNTS --bind ${cwd}:${d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7203:980,timeout,timeout-seconds,980,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7203,1,['timeout'],['timeout-seconds']
Safety,"- 0.22; - local backend; - docker; - single workflow. Upshot: I still have jobs running and cromwell is not shutting down. ```; ^C[2016-10-19 18:29:22,42] [info] WorkflowManagerActor: Received shutdown signal. Aborting all running workflows...; [2016-10-19 18:29:22,42] [info] WorkflowManagerActor Aborting all workflows; [2016-10-19 18:29:22,42] [info] WorkflowExecutionActor [51ee236f]: Abort received. Aborting 8 EJEAs; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (2 remaining).; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (1 remaining).; [2016-10-19 18:29:50,48] [info] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] job aborted: case_gatk_acnv_workflow.HetPulldown:8:; 1; [2016-10-19 18:29:50,52] [warn] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] received an unhandled message: JobRunning(51ee236f-; c31a-48c2-bae7-9246439160b0:case_gatk_acnv_workflow.HetPulldown:12:1,Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-51ee236f-c31a-48c2-b; ae7-9246439160b0/WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0/51ee236f-c31a-48c2-bae7-9246439160b0-EngineJobExecutionActor-case_gatk_acnv_workflow.HetPulldown:12:1/51ee236f-c; 31a-48c2-bae7-9246439160b0-BackendJobExecutionActor-51ee236f:case_gatk_acnv_workflow.HetPulldown:12:1#636728322])) in state: WorkflowExecutionAbortingState; [2016-10-19 18:29:50,53] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: java -Xmx4g -jar /root/gatk-protected.jar GetHetCoverage --referen; ce /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/data/ref/Homo_sapiens_assembly19.fasta \; --normal /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1600:210,Abort,Aborting,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600,7,"['Abort', 'abort']","['Abort', 'Aborting', 'abort', 'aborted']"
Safety,"- Added `scalafmt.conf` to the repo, which includes our linting rules.; - This is essentially a copy of the [Leonardo one](https://github.com/DataBiosphere/leonardo/blob/develop/.scalafmt.conf), although I bumped the version and avoided using deprecated syntax. It should be functionally identical.; - Ran the `scalafmt` CLI tool on to apply the formatting rules to all files. We shouldn't need the CLI tool moving forward since IntelliJ is perfectly capable of formatting individual files. ; - Setup:; - Get the `scala` plugin for IntelliJ. You likely already have it.; - Restart IntelliJ. ; - (optional) `Settings > Editor > Code Style > Scala` to turn on ""Reformat on Save"". ; - Planning on creating a Github Action in a a different branch.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7257:229,avoid,avoided,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7257,1,['avoid'],['avoided']
Safety,"- Added recovery functionality using KV service.; - In the next iteration will refactor to use a Doc store (Mongo, Couchbase) generic service implementation or continue using KV service but with a refactor in order to support not just SQL DBs as KV store but any other kind of DB. I think the best may be to work on a DAL or if it's not possible just modify the service to support other providers. Let me know what do you think on this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1250:8,recover,recovery,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1250,1,['recover'],['recovery']
Safety,- Adds a `JesError` class that maps some known JES errors to custom Exceptions to provide better error messages. Simplistic for now but avoid unnecessary stacktrace and give more explicit error messages.; - Tries to read the return code regardless of the final status of the JES job (even if it failed). If it can read it then the return code will be available in metadata.; - Sets the exec.sh content-type to `text/plain` in gcs so it opens in the browser instead of downloading a files.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1856:136,avoid,avoid,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1856,1,['avoid'],['avoid']
Safety,"- Allow a `0` value for CWL `outDirMin` and `tmpDirMin` resource attributes; - Adds an optional section to the language factory to define a command to run after the user's action that will return output files that can only be known at runtime; - Only defined for CWL for now, which will remove unnecessary pull of jq for WDL tasks on PAPI2; - Docker image and command can both be changed in the configuration; - The PAPI2 logic that handles delocalization of those file strips away some redundant pieces in the delocalized paths to reduce the overall length of the path",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4358:487,redund,redundant,487,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4358,1,['redund'],['redundant']
Safety,"- As sentry may drop some metadata, print the metadata to slf4j.; - For centaur-restarting-cromwell, remember if cromwell was alive, and log more of the connection status.; - Pass more jenkins variables through docker.; - Increase papi v2 cwl conformance test timeout due to problematic test 55.; - Use pr branch name during pr builds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4021:260,timeout,timeout,260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4021,1,['timeout'],['timeout']
Safety,"- Better localization and delocalization of directories in PAPI2 using hidden files to cover for empty directories; - IWDR localization is not baked in the CWL code anymore but left to the backend. This allows for the PAPI backend to opt out of it since localization is done directly on the VM.; - ~~Use configurable `job-shell` instead of hardcoded `/bin/bash`~~ It fixes 117 but also makes a bunch of centaur tests fail, so leaving as is for now.; - Refactors Pipelines conversions in v2 (w/ typeclasses !); - Allow for lazy evaluation of file and directory literals so that they can be written when the backend and the appropriate IoFunctions are known. This only partially covers the possible cases. It needs a deeper tech talk discussion. This is orthogonal to the above and only here to avoid a later rebase (the files changed overlap with the refactoring mentioned).; - Partially replaces the custom `MemorySize` with [squants](https://github.com/typelevel/squants); - Turns the CPU runtime validation from an `Int` to a `Int Refined Positive`; - Automatically fits the resources requirements in the task to the [GCE constraints](https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type#specifications)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3697:793,avoid,avoid,793,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3697,1,['avoid'],['avoid']
Safety,"- Cleans up the `IoFunctionSet` a little and add `PathFunctionSet` in it to deal with path manipulation that doesn't involve I/O; - When secondary files are actually secondary directories, list their content and return a `WomMaybeListedDirectory` instead of a `WomSingleFile` or `WomMaybePopulatedFile`; - Make paths absolute as much and as early as possible to avoid ambiguity and having to guess later where they are relative to; - Adjust the `OutputManipulator` to deal with secondary listed directories",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3468:362,avoid,avoid,362,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3468,1,['avoid'],['avoid']
Safety,"- Combines the hotfix and regular release graphs into a single diagram; - Removes the redundant ""re-run swatomation"" step from the end of the release process; - Add the creation of a new ""work in progress version"" PR to firecloud-develop. Rendered Image: . ![](https://github.com/broadinstitute/cromwell/blob/cjl_release_process_fixup/scripts/release_processes/firecloud-develop.dot.png?raw=true)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4962:86,redund,redundant,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4962,1,['redund'],['redundant']
Safety,"- Disabled redundant `lots_of_inputs.test` test, which uses `lots_of_inputs.wdl` like `lots_of_inputs_papiv2.test` and makes analysis confusing; - Removed unused configs, mostly from PAPIv2 Alpha; - Removed unused suites, mostly from PAPIv2 Alpha; - Removed Travis, Jenkins, and CircleCI references from `test.inc.sh`. This includes `case` statements, as well as all functions that were called exclusively in the removed `case` statements.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7336:11,redund,redundant,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7336,1,['redund'],['redundant']
Safety,- Github Actions was throwing a warning because it detected a crash in the script it was running.; - The crash it was seeing was something intentional triggered by a test. We don't want a warning about it. ; - Temporarily redirected stderr so that Github Actions doesn't see the crash.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7106:51,detect,detected,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7106,1,['detect'],['detected']
Safety,- Instruments workflows in OnHold and Aborting; - Instruments the last state the EJEA is before stopping itself,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4398:38,Abort,Aborting,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4398,1,['Abort'],['Aborting']
Safety,"- JES backend. ```; ...snip....; [2016-11-03 19:36:22,19] [info] SingleWorkflowRunnerActor writing metadata to /home/lichtens/test_eval/crsp_validation_input_files/crsp_validation_from_cromwell.json.metadata.json; [2016-11-03 19:36:22,30] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; [2016-11-03 19:36:22,30] [info] WorkflowManagerActor: Received shutdown signal.; [2016-11-03 19:36:22,30] [info] Waiting for 1 workflows to abort...; ....15 minutes gone by....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649:456,abort,abort,456,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649,1,['abort'],['abort']
Safety,"- JES backend; - 0.24; - single workflow mode. When JES returns a 403 AccessDeniedException, should cromwell keep retrying? It delays the result getting back to the user and should have no way of recovering with retries. Proposed solution: When AccessDeniedException is seen from JES, simply end there, instead of initiating any retries...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961:196,recover,recovering,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961,1,['recover'],['recovering']
Safety,- Prefix default locations in CWL with `gs://...` root for PAPI conformance tests; - Retrieve size of files early to avoid unnecessary I/O (there's still too much redundant I/O but it's a step); - Uses `WomObject` to map back JS objects instead of `WomMap` that needs homogoneous value type (or it ends up being `WomAnyType`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3492:117,avoid,avoid,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3492,2,"['avoid', 'redund']","['avoid', 'redundant']"
Safety,"- Refactor all CI TRAVIS_* variables back into create_build_variables(); - Detect hotfixes using git instead of TRAVIS variables; - Using ""force ci"" now runs all sub builds even on push; - All centaur tests should contribute to codecov; - Moved ci source files under src/ci; - Write ci log files under target/ci instead of $PWD; - Write ci generated files under target/ci, instead of sending secrets to src; - Jar file searches now return most recently modified jar; - Added allowPublicKeyRetrieval=true to MySQL url generation; - Removed cloudwell test as the combo of horicromtal + deadlock tests the same features",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5202:75,Detect,Detect,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5202,1,['Detect'],['Detect']
Safety,"- Removed `martha_v2` response parsing; - When traversing files using a mapper function then mapper does the exclusion; - Use serialized class instead of config string replace for Martha request generation; - Request partial responses from Martha; - Use JDK standard responses for missing file attributes (size=0, time=epoch, hash=None); - Copy `timeCreated` from DOS/DRS to file attributes; - Martha `read_string()` uses `gsUri` (gs://bucket/name) instead of `bucket` and `name`; - Martha localization uses safer file paths still based on the DOS/DRS URI; - Reading DOS/DRS content uses the config google auth type, not always Bond-or-USA; - Google config auth types support ADC=SA, passing in scopes to ADC; - Google auth type `UserMode` no longer requires config values that it was ignoring; - Allow skipping docker build-and-push by specifying the `CROMWELL_BUILD_PAPI_DOCKER_IMAGE_DRS`; - `papi-v2-usa` backend now ALWAYS uses the USA just like Terra/FC does",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5912:508,safe,safer,508,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5912,1,['safe'],['safer']
Safety,"- Removes the ability to abort Finalization Actor; - Ensures that the finalization actor runs if the Workflow reaches the `Initialize` state, regardless of what happens next (failure, success, abort), or when it happens (initialization, execution).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/916:25,abort,abort,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/916,2,['abort'],['abort']
Safety,"- Removes the awkward plateauing of running jobs at 2k, 4k, 6k, etc when running several thousand jobs concurrently.; - Does not introduce very long delays into execution store processing like the previous attempt to ""fix"" the execution store.; - Allows us to get a more accurate count of total jobs queued in the system because they will express themselves as EJEAs waiting for tokens rather than pre-queue-queued items of which we have no visibility.; - Adds a dummy backend to test all of the above. Review Notes:; * Start with the `Remove redundant WaitingForQueueSpace status` commit. That's the one which fixes the bug. Everything else is just dummy backend and test infrastructure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6047:543,redund,redundant,543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6047,1,['redund'],['redundant']
Safety,"- Send abort requests through the `JesAPIQueryManager`. This is wanted because currently each JABJEA aborts on its own which has undesired consequences, like flooding the backend thread pool with blocking requests.; - Lift the ""1 second"" maximum limit of the `JesPollingActor` by switching to milliseconds (new limit is 1 millisecond); - Add a second `JesPollingActor` to help with throughput of PAPI requests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3357:7,abort,abort,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3357,2,['abort'],"['abort', 'aborts']"
Safety,"- Single workflow mode; - Local backend (using throttling in custom application.conf); - 0.22; - using docker images; - call-caching enabled (localhost mysql instance); - all data is on local filesystem (not even shared filesystem); - N=2; - One time took 6 minutes before I did ctl-C. The second time it was left overnight and never completed. I did notice that (before I hit Ctl-C) cromwell got the shutdown signal and was aborting running jobs, even though there were none. If this was desired behavior, is there a flag to disable?. What other information can I provide? WDL? application.conf is attached. No other `-D` command line parameters were used. [local_application.conf.txt](https://github.com/broadinstitute/cromwell/files/539083/local_application.conf.txt). I am attempting to run cromwell as part of a larger shell script and I am positive that cromwell is not exiting (I still see MySQL warning messages). The workflow results appear to be there and no jobs are running (according to `top -c`)...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1594:425,abort,aborting,425,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1594,1,['abort'],['aborting']
Safety,"- Upgraded Liquibase to latest; - ~Workaround Liquibase UniqueConstraint ""caching"" bug~ EDIT: Bug was fixed in liquibase!; - Fixed S3 SPI config to avoid Liquibase warnings; - Removed unused DB upgrade environment variables; - Test various DB combinations using centaur local",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091:148,avoid,avoid,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091,1,['avoid'],['avoid']
Safety,"- [x] Needs https://github.com/broadinstitute/wdl4s/pull/47; - [x] Needs https://github.com/broadinstitute/centaur/pull/114; - [x] Needs WDL doc; - [x] Needs Cromwell doc. What it does in a nutshell:. - Enables sub workflows execution; - Sub workflow metadata can be queried separately or injected in the main workflow metadata; - Restarts work; - Aborts should work (work meaning what abort is doing in develop now). To be addressed:; - ~~Sub Workflow Store cleanup~~; - ~~Workflow outputs copying~~ -> https://github.com/broadinstitute/cromwell/issues/1684; - ~~Call logs copying~~; - ~~Provenance: More related to imports, but right now the actual WDL content of a sub workflow is unknown to cromwell (it's in the `WdlNamespace` as a scala object but the actual text is not available).~~; - ~~Stats Endpoint~~",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1682:348,Abort,Aborts,348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1682,2,"['Abort', 'abort']","['Aborts', 'abort']"
Safety,"- branch 0.19_hotfix a6f7c00b71dd22485d5e95c9a30f3dedd2ddeaba; - running with default application.conf. If I abort a running job via POST to the API endpoint `worflows/v1/<uuid>/abort`, this appears in the server logs:. > 2016-05-23 10:21:55,192 cromwell-system-akka.actor.default-dispatcher-6 INFO - CallActor [UUID(87ebf02f):Godot]: Abort function called.; > 2016-05-23 10:21:55,201 cromwell-system-akka.actor.default-dispatcher-5 INFO - WorkflowActor [UUID(87ebf02f)]: Beginning transition from Running to Aborting.; > 2016-05-23 10:21:55,201 cromwell-system-akka.actor.default-dispatcher-5 INFO - WorkflowActor [UUID(87ebf02f)]: transitioning from Running to Aborting.; > 2016-05-23 10:22:00,175 cromwell-system-akka.actor.default-dispatcher-8 INFO - LocalBackend [UUID(87ebf02f):Godot]: Return code: 0; > 2016-05-23 10:22:00,313 cromwell-system-akka.actor.default-dispatcher-2 ERROR - WorkflowActor [UUID(87ebf02f)]: Completion work failed for call Godot.; > java.sql.SQLIntegrityConstraintViolationException: integrity constraint violation: unique constraint or index violation; UK_SYM_WORKFLOW_EXECUTION_ID_SCOPE_NAME_ITERATION_IO table: SYMBOL; > at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at com.zaxxer.hikari.proxy.PreparedStatementProxy.executeUpdate(PreparedStatementProxy.java:61) ~[cromwell-0.19.jar:0.19]; > at com.zaxxer.hikari.proxy.PreparedStatementJavassistProxy.executeUpdate(PreparedStatementJavassistProxy.java) ~[cromwell-0.19.jar:0.19]; > at slick.driver.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$anonfun$run$8$$anonfun$apply$1.apply(JdbcActionComponent.scala:520) ~[cromwell-0.19.jar:0.19]; > at slick.driver.JdbcActionComponen",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/869:109,abort,abort,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/869,5,"['Abort', 'abort']","['Abort', 'Aborting', 'abort']"
Safety,"- cromwell 26; - JES backend; - call caching on local mysql instance; - server mode. Ran a bunch of the initial jobs, but once it really started fan out (thousands of jobs), I got this error message. Trying to replicate now, but not sure if I can. Might be transient. . Regardless, error message is not particularly helpful. Any ideas? . ```; cromwell.core.CromwellFatalException: com.google.cloud.storage.StorageException: 410 Gone; {; ""error"": {; ""errors"": [; {; ""domain"": ""global"",; ""reason"": ""backendError"",; ""message"": ""Backend Error""; }; ],; ""code"": 503,; ""message"": ""Backend Error""; }; }. at cromwell.core.CromwellFatalException$.apply(core.scala:17); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:36); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.for",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2215:853,recover,recoverWith,853,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2215,2,['recover'],['recoverWith']
Safety,"- cromwell pre-0.21 dev snaposhot; - JES backend; - command line execution (single workflow) . Some docker images are bigger than the default boot disk size for JES backend. There should be some safeguards against failure when the docker image is too big to fit in the default boot disk size. What happens?; 1. JES tries to download docker image that is bigger than the VM boot disk size. Disk full error message appears.; 2. Workflow fails. Proposed behavior:. After number 1 happens, attempt to spin the VM with additional boot disk storage and retry running the job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1449:195,safe,safeguards,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1449,1,['safe'],['safeguards']
Safety,"- cromwell pre-0.21 dev snapshot; - JES backend; - command line execution (single workflow) . Current behavior, which happens frequently:; - Mysterious error 500 appears on cromwell stdout. Apologies that I do not have example. ; - cromwell hangs. One time, cromwell was left running overnight and no progress was made.; - ctl-c which ends cromwell; - up arrow and return; - job completes successfully. Can cromwell detect these errors on JES and retry the jobs?. More observations:; - These were never seen on local backend. On JES, these were common.; - All jobs were self-contained. I.e. did not hit a web service nor make a HTTP request.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1450:416,detect,detect,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1450,1,['detect'],['detect']
Safety,"---- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've been told that Message 13 was a generic catch all for ; > something unexpected happening. For instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receives a message 13 that it's ; > a preemption? ; > ; > The problem is that we keep them on separate counters so as to maximize the ; > number of preemptible tries a user gets (we try preemptibles up to N times ; > before falling back to a standard instance) but will retry other retryable ; > errors on their own count. If we're treating transient errors as ; > preemptible when they're not people can wind up on a standard instance ; > before it's necessary. ; > ; > If it's not 100%, is there any way for the error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:11562,abort,aborted,11562,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['abort'],['aborted']
Safety,-7ce25791-3731-4a69-97f1-b7b65ac8ff71)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Ret,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:3185,recover,recoverAsync,3185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recoverAsync']
Safety,"-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Call-to-Backend assignments: wf_hello.hello -> AWSBATCH; 2018-06-11 16:10:36,997 cromwell-system-akka.dispatchers.engine-dispatcher-36 INFO - WorkflowExecutionActor-ab42cf3c-726f-4148-a30f-0f907c843361 [UUID(ab42cf3c)]: Starting wf_hello.hello; 2018-06-11 16:10:37,958 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_wf_hello.hello:-1:1, invalidating cache entry.; cromwell.core.CromwellFatalException: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:2185,recover,recoverWith,2185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,1,['recover'],['recoverWith']
Safety,"-system-akka.dispatchers.engine-dispatcher-28 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2019-07-21 23:34:38,667 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,157 cromwell-system-akka.dispatchers.backend-dispatcher-37 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,233 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PAPI request worker batch interval is 33333 milliseconds; ```. but then it immediately starts printing these errors:; ```; 2019-07-21 23:34:40,010 cromwell-system-akka.actor.default-dispatcher-32 ERROR - Error searching for abort requests; java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""WORKFLOW_STORE_ENTRY"" where (""WORKFLOW_STATE"" = cast('Aborting' as varchar(1677' at line 1; 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120); 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97); 	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:970); 	at com.mysql.cj.jdbc.ClientPreparedStatement.execute(ClientPreparedStatement.java:387); 	at com.zaxxer.hikari.pool.ProxyPreparedStatement.execute(ProxyPreparedStatement.java:44); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java); 	at slick.jdbc.StatementInvoker.results(StatementInvoker.scala:38",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:3703,abort,abort,3703,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,1,['abort'],['abort']
Safety,".client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.n",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:5233,Abort,Aborting,5233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,2,"['Abort', 'abort']","['Aborting', 'abort']"
Safety,.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyn,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:6223,recover,recover,6223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Safety,".scala:49); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2019-01-07 16:21:23,91] [info] WorkflowManagerActor WorkflowActor-18de8166-5f29-4288-9fa4-6741565446fd is in a terminal state: WorkflowFailedState; [2019-01-07 16:21:30,36] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-01-07 16:21:32,96] [info] Workflow polling stopped; [2019-01-07 16:21:32,99] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-01-07 16:21:32,99] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-01-07 16:21:33,02] [info] Aborting all running workflows.; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 secon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4526:7110,Timeout,Timeout,7110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526,3,"['Abort', 'Timeout']","['Aborting', 'Timeout']"
Safety,.trim() Docker image names to avoid pathological re behavior [BA-6478],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5548:30,avoid,avoid,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5548,1,['avoid'],['avoid']
Safety,".util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-5"" #44 daemon prio=5 os_prio=31 tid=0x00007fb76b517800 nid=0x3b0b waiting on condition [0x0000000131195000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-2"" #43 prio=5 os_prio=31 tid=0x00007fb76e8ee000 nid=0x3f0b waiting on condition [0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:33480,Unsafe,Unsafe,33480,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,".util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-6"" #45 daemon prio=5 os_prio=31 tid=0x00007fb76e8dd000 nid=0x5507 waiting on condition [0x0000000131298000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-5"" #44 daemon prio=5 os_prio=31 tid=0x00007fb76b517800 nid=0x3b0b waiting on condition [0x000",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:32405,Unsafe,Unsafe,32405,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,".util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-7"" #46 daemon prio=5 os_prio=31 tid=0x00007fb76e8f5000 nid=0x5807 waiting on condition [0x000000013139b000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-6"" #45 daemon prio=5 os_prio=31 tid=0x00007fb76e8dd000 nid=0x5507 waiting on condition [0x000",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:31330,Unsafe,Unsafe,31330,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,".util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-8"" #47 daemon prio=5 os_prio=31 tid=0x00007fb76b518000 nid=0x8703 waiting on condition [0x000000013149e000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-7"" #46 daemon prio=5 os_prio=31 tid=0x00007fb76e8f5000 nid=0x5807 waiting on condition [0x000",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:30255,Unsafe,Unsafe,30255,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,".util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-9"" #48 daemon prio=5 os_prio=31 tid=0x00007fb76b529800 nid=0x8903 waiting on condition [0x00000001315a1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-8"" #47 daemon prio=5 os_prio=31 tid=0x00007fb76b518000 nid=0x8703 waiting on condition [0x000",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:29180,Unsafe,Unsafe,29180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-stdout.log; 2018-06-13 14:41:14,088 cromwell-system-akka.dispatchers.backend-dispatcher-112 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Running to Succeeded; 2018-06-13 14:41:15,905 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - WorkflowManagerActor Workflow a67833cb-b894-4790-872f-9f3104cab60c failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: target not exists: s3://s3.amazonaws.com/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-rc.txt; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:21335,recover,recoverWith,21335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['recover'],['recoverWith']
Safety,"/gatk4:4.1.0.0--0""; }. command {; set -e; mkdir -p $(dirname ~{outputBam}); gatk --java-options -Xmx~{memory}G \; SplitNCigarReads \; -I ~{inputBam} \; -R ~{referenceFasta} \; -O ~{outputBam} \; ~{prefix('-L ', intervals)}; }. output {; File bam = outputBam; File bamIndex = sub(outputBam, ""\.bam$"", "".bai""); }. runtime {; docker: dockerImage; memory: ceil(memory * memoryMultiplier); }; }; ```; expected behavior: By default nothing happens as intervals is empty. So this should evaluate to an empty string. No intervals flag is passed to GATK.; Actual behaviour:; ```; [2019-07-29 08:49:39,27] [error] WorkflowManagerActor Workflow 3de3bd74-b387-4d35-a704-73a4054387e9 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.Exception: Failed command instantiation; at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5092:1274,recover,recoverWith,1274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092,1,['recover'],['recoverWith']
Safety,"/tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Localization; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Done\ localization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Localization; timeout: 300s; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Running\ user\ action:\; docker\ run\ -v\ /mnt/local-disk:/cromwell_root\ --entrypoint\=/bin/bash\; ubuntu@sha256:1e48201ccc2ab83afc435394b3bf70af0fa0055215c1e26a5da9b50a1ae367c9\; /cromwell_root/script; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: UserAction; timeout: 300s; - commands:; - /cromwell_root/script; entrypoint: /bin/bash; imageUri: ubuntu@sha256:1e48201ccc2ab83afc435394b3bf70af0fa0055215c1e26a5da9b50a1ae367c9; labels:; tag: UserAction; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Starting\ delocalization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Delocalization; timeout: 300s; - commands:; - -c; - /bin/bash /cromwell_root/gcs_delocalization.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Delocalization; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Done\ delocalization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Delocalization; timeout: 300s; - alwaysRun: true; commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/1xxxxxx.sh && chmod u+x /tmp/1xxxxxx.sh; && sh /tmp/1xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.i",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:19424,timeout,timeout,19424,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['timeout'],['timeout']
Safety,"00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-2"" #28 daemon prio=5 os_prio=31 tid=0x00007fb7708d7800 nid=0x7d03 waiting on condition [0x000000012c789000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""pool-1-thread-1"" #27 prio=5 os_prio=31 tid=0x00007fb76f4b6000 nid=0x7b03 waiting on condition [0x000000012b643000]; java.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:38534,Unsafe,Unsafe,38534,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-3"" #29 daemon prio=5 os_prio=31 tid=0x00007fb76f4b7000 nid=0x7f03 waiting on condition [0x000000012c88c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-2"" #28 daemon prio=5 os_prio=31 tid=0x00007fb7708d7800 nid=0x7d03 waiting on condition [0x000000012c789000]; java.lang",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:37484,Unsafe,Unsafe,37484,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"0006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""pool-1-thread-1"" #27 prio=5 os_prio=31 tid=0x00007fb76f4b6000 nid=0x7b03 waiting on condition [0x000000012b643000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-1"" #26 daemon prio=5 os_prio=31 tid=0x00007fb76b442000 nid=0x7903 waiting on condition [0x000000012d905000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.u",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:39588,Unsafe,Unsafe,39588,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"12/232939192-8823373b-c21e-4586-8c1b-516770a212e3.png"">. Because Job Manager breaks on large scatters, and to save money on compute credits, I decided to stop the workflow early rather than let it keep going to find out if the workflow log would eventually show an errors. So far, it seems to have considered everything a success. ```; 2023-04-18 21:59:54,599 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:108:1]: Status change from Running to Success; 2023-04-18 22:00:09,060 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:107:1]: Status change from Running to Success; 2023-04-18 22:00:18,464 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:106:1]: Status change from Running to Success; 2023-04-18 22:01:20,604 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:111:1]: Status change from Running to Success; 2023-04-18 22:14:47,728 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: Aborting workflow; 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:262:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/9178938377659283430); 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:112:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/8559201934542591362); 2023-04-18 22:14:48,295 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Successfully requested cancellation of projects/16371921765/locations/us-central1/operations/9178938377659283430; 2023-04-18 22:15:56,564 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: Status change from Running to Success; 2023-04-18 22:16:44,505 INFO",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7121:4737,Abort,Aborting,4737,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7121,1,['Abort'],['Aborting']
Safety,"153039990-0d0b2c96-a33b-454f-9617-aee83137337a.PNG); [Cromwell-Error.docx](https://github.com/broadinstitute/cromwell/files/8026009/Cromwell-Error.docx); ; <!-- Paste/Attach your workflow if possible: -->; java -Dconfig.file=aws-cromwell-batch.conf -jar cromwell-75.jar run hello.wdl -i hello.inputs. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; include required(classpath(""application"")). aws {. application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""us-east-1""; }; engine {; filesystems {; s3.auth = ""default""; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; docker {; hash-lookup {; enabled = false; # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub and gcr; method = ""remote""; }; }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; concurrent-job-limit = 1000; root = ""s3://cromwell-aws-hello/cromwell-execution""; auth = ""default""; default-runtime-attributes {; queueArn = ""arn:aws:batch:us-east-1:XXXXXXXXX:job-queue/python-batch"" ,; scriptBucketName = ""cromwell-aws-hello"" ; }; filesystems {; s3 {; auth = ""default""; }; }; # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in the cloud.; slow-job-warning-time: 24 hours; }; }; }; }. [Cromwell-Error.docx](https://github.com/broadinstitute/cromwell/files/8026013/Cromwell-Error.docx); ![AWS-Batch](https://user-images.githubusercontent.com/25282254/153040332-625cb61a-062b-4766-96ea-8e129efb2b20.PNG); [config file.docx](https://github.com/broadinstitute/cromwell/files/8026025/config.file.docx). How to give Timeout options for Job definitions?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6671:3332,Timeout,Timeout,3332,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671,1,['Timeout'],['Timeout']
Safety,190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:2266,recover,recoverAsync,2266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recoverAsync']
Safety,"2 other minor things popped up while call caching the 10K joint genotyping: ; - We spend a fair amount of time creating `GcsPathBuilder`s, which we shouldn't since we only need one per workflow in theory. In practice we need to create a few more because we can't quite propagate the same one around everywhere. But before this PR we would create one per job which seems inefficient. One reason for this is that `JobPaths` extends `WorkflowPaths`, so when we convert the latter to the former we effectively re-instantiate a new `WorkflowPaths` every time. This PR changes that so that `JobPaths` takes a `WorkflowPaths` instead as one of its attribute to avoid unnecessary re-allocations.; - A small optimization to the execution store which allows quicker lookup of ""Done"" jobs which we do a lot in `runnableCalls`. Also added a benchmark test that measures the performance of `runnableCalls`. Below are the results before and after this change. The ""size"" corresponds to how many jobs in ""Done"" and ""NotStarted"" states are inserted in the execution store before calling `runnableCalls`. Results are in ms. Before:; ![screen shot 2017-04-19 at 3 24 15 pm](https://cloud.githubusercontent.com/assets/2978948/25305440/fcca5418-2748-11e7-8d2a-6f2c645f2ef3.png). After:; ![screen shot 2017-04-19 at 3 25 18 pm](https://cloud.githubusercontent.com/assets/2978948/25305444/06de3f00-2749-11e7-860f-a1c077f3243f.png)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2198:654,avoid,avoid,654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2198,1,['avoid'],['avoid']
Safety,"2/call-lo; ad_shared_covars/execution/stderr.; [First 3000 bytes]:Traceback (most recent call last):; File ""/home/cromwell-executions/main/9e4f5894-f7e6-4e2f-be4b-f547d6de7fff/call-main/main/788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2/call-load_shared_covars/inputs/-915037270/load_shared_covars.py"",; line 87, in <module>; load_covars(); File ""/home/cromwell-executions/main/9e4f5894-f7e6-4e2f-be4b-f547d6de7fff/call-main/main/788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2/call-load_shared_covars/inputs/-915037270/load_shared_covars.py"",; line 51, in load_covars; assert not np.any(np.isnan(data)); AssertionError. [2022-12-15 21:28:38,49] [info] WorkflowManagerActor: Workflow actor for 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff completed with status 'Failed'. The workflow will be removed from the workflow store.; [2022-12-15 21:28:52,23] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2022-12-15 21:28:53,46] [info] Workflow polling stopped; [2022-12-15 21:28:53,46] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2022-12-15 21:28:53,46] [info] Aborting all running workflows.; [2022-12-15 21:28:53,46] [info] 0 workflows released by cromid-b254006; [2022-12-15 21:28:53,47] [info] WorkflowStoreActor stopped; [2022-12-15 21:28:53,47] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2022-12-15 21:28:53,47] [info] WorkflowLogCopyRouter stopped; [2022-12-15 21:28:53,47] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2022-12-15 21:28:53,47] [info] JobExecutionTokenDispenser stopped; [2022-12-15 21:28:53,47] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2022-12-15 21:28:53,47] [info] WorkflowManagerActor: All workflows finished; [2022-12-15 21:28:53,47] [info] WorkflowManagerActor stopped; [2022-12-15 21:28:53,71] [info] Connection pools shut down; [2022-12-15 21:28:53,71] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down JobStoreAc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:49166,Timeout,Timeout,49166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,2,"['Abort', 'Timeout']","['Aborting', 'Timeout']"
Safety,200 files scattered 200x fails to call cache due to GCS hash timeout,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4873:61,timeout,timeout,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4873,1,['timeout'],['timeout']
Safety,"2018-06-13 14:29:44,774 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-13 14:29:45,255 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Parsing workflow as WDL draft-2; 2018-06-13 14:29:46,004 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Call-to-Backend assignments: demux_only.illumina_demux -> AWSBATCH; 2018-06-13 14:29:46,085 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - AWSBATCH [UUID(a67833cb)]: Key/s [preemptible, dx_instance_type] is/are not supported by backend. Unsupported attributes will not be part of job executions.; 2018-06-13 14:29:47,088 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:2519,Timeout,TimeoutException,2519,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['Timeout'],['TimeoutException']
Safety,"28000000; }. # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; 	graceful-server-shutdown = true; max-concurrent-workflows = 5000. io {; throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; number-of-requests = 100000; per = 100 seconds; }; }; }. akka {; # Optionally set / override any akka settings; http {; server {; # Increasing these timeouts allow rest api responses for very large jobs; # to be returned to the user. When the timeout is reached the server would respond; # `The server was not able to produce a timely response to your request.`; # https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows; request-timeout = 600s; idle-timeout = 600s; }; }; }. services {; MetadataService {; #class = ""cromwell.services.metadata.impl.MetadataServiceActor""; config {; metadata-read-row-number-safety-threshold = 2000000; # # For normal usage the default value of 200 should be fine but for larger/production environments we recommend a; # # value of at least 500. There'll be no one size fits all number here so we recommend benchmarking performance and; # # tuning the value to match your environment.; db-batch-size = 700; }; }; }. google {. application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. docker {; hash-lookup {; method = ""remote""; }; }. engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }. call-caching {; enabled = true; }. backend {; default = GCPBATCH; providers {; GCPBATCH {; // life sciences; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; ## Google projec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:8861,timeout,timeout,8861,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,2,['timeout'],['timeout']
Safety,"28a:myworkflow.mytask:-1:1 is not eligible for call caching; ```; <!-- Which backend are you running? -->; Used backend: ; GCPBATCH. Callcaching works with PAPIv2, not on GCPBATCH.; <!-- Paste/Attach your workflow if possible: -->; workflow used for testing:; ```; workflow myworkflow {; call mytask; }. task mytask {; String str = ""!""; command <<<; echo ""hello world ${str}""; >>>; output {; String out = read_string(stdout()); }. runtime{; docker: ""eu.gcr.io/project/image_name:tag""; cpu: ""1""; memory: ""500 MB""; disks: ""local-disk 5 HDD""; zones: ""europe-west1-b europe-west1-c europe-west1-d""; preemptible: 2; noAddress: true; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; We are using cromwell through broadinstitute/cromwell:87-ecd44b6 image.; cromwell configuration:; ```; include required(classpath(""application"")). system.new-workflow-poll-rate=1. // increase timeout for http requests..... getting meta-data can timeout for large workflows.; akka.http.server.request-timeout=600s. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; system {; 	job-rate-control {; 	 jobs = 100; 	 per = 1 second; 	}; input-read-limits {; lines = 128000000; bool = 7; int = 19; float = 50; string = 1280000; json = 12800000; tsv = 1280000000; map = 128000000; object = 128000000; }. # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; 	graceful-server-shutdown = true; max-concurrent-workflows = 5000. io {; throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; number-of-requests = 100000; per = 100 se",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:7426,timeout,timeout,7426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['timeout'],['timeout']
Safety,"357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:16724,abort,abort,16724,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"3d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3876:2750,detect,detect,2750,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876,1,['detect'],['detect']
Safety,48:00 cromwell-system-akka.dispatchers.backend-dispatcher-84 ERROR - GcpBatchAsyncBackendJobExecutionActor [UUID(119e11a5)wf_hello.hello:NA:1]: Error attempting to Recover(StandardAsyncJob(projects/broad-dsde-cromwell-dev/locations/us-central1/jobs/job-7ce25791-3731-4a69-97f1-b7b65ac8ff71)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecuti,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:2932,recover,recover,2932,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recover']
Safety,"501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py\"", line 155, in TryFunc\n return func(*args, **kwargs), None\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 41, in _ReadNoProxyWithCleanFailures\n return gce_read.ReadNoProxy(uri)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py\"", line 50, in ReadNoProxy\n request, timeout=timeout_property).read()\n File \""/usr/lib/python2.7/urllib2.py\"", line 401, in open\n response = self._open(req, data)\n File \""/usr/lib/python2.7/urllib2.py\"", line 419, in _open\n '_open', req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 379, in _call_chain\n result = func(*args)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1211, in http_open\n return self.do_open(httplib.HTTPConnection, req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1184, in do_open\n r = h.getresponse(buffering=True)\n File \""/usr/lib/python2.7/httplib.py\"", line 1072, in getresponse\n response.begin()\n File \""/usr/lib/python2.7/httplib.py\"", line 408, in begin\n version, status, reason = self._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n)""; at cromwell.backend.impl.jes.JesAsyncBackendJobExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:14675,timeout,timeout,14675,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['timeout'],['timeout']
Safety,"501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py\"", line 155, in TryFunc\n return func(*args, **kwargs), None\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 41, in _ReadNoProxyWithCleanFailures\n return gce_read.ReadNoProxy(uri)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py\"", line 50, in ReadNoProxy\n request, timeout=timeout_property).read()\n File \""/usr/lib/python2.7/urllib2.py\"", line 401, in open\n response = self._open(req, data)\n File \""/usr/lib/python2.7/urllib2.py\"", line 419, in _open\n '_open', req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 379, in _call_chain\n result = func(*args)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1211, in http_open\n return self.do_open(httplib.HTTPConnection, req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1184, in do_open\n r = h.getresponse(buffering=True)\n File \""/usr/lib/python2.7/httplib.py\"", line 1072, in getresponse\n response.begin()\n File \""/usr/lib/python2.7/httplib.py\"", line 408, in begin\n version, status, reason = self._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n)""; java.lang.Exception: Task m2.Mutect2.M2:1:1 failed. J",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:8155,timeout,timeout,8155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['timeout'],['timeout']
Safety,"501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py\"", line 155, in TryFunc\n return func(*args, **kwargs), None\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 41, in _ReadNoProxyWithCleanFailures\n return gce_read.ReadNoProxy(uri)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py\"", line 50, in ReadNoProxy\n request, timeout=timeout_property).read()\n File \""/usr/lib/python2.7/urllib2.py\"", line 401, in open\n response = self._open(req, data)\n File \""/usr/lib/python2.7/urllib2.py\"", line 419, in _open\n '_open', req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 379, in _call_chain\n result = func(*args)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1211, in http_open\n return self.do_open(httplib.HTTPConnection, req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1184, in do_open\n r = h.getresponse(buffering=True)\n File \""/usr/lib/python2.7/httplib.py\"", line 1072, in getresponse\n response.begin()\n File \""/usr/lib/python2.7/httplib.py\"", line 408, in begin\n version, status, reason = self._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n); gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DN",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:11488,timeout,timeout,11488,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['timeout'],['timeout']
Safety,60-second DB timeout on all metadata queries [BA-5858],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5087:13,timeout,timeout,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5087,1,['timeout'],['timeout']
Safety,"62/). I would like to apologize in advance for any ignorance regarding the documentation that I might have missed. It is not my intention to ask for what I would have known if I had read the documentation better, I am merely trying to grasp the concepts that are abstracted in the Cromwell metadata as described by [the paragraph about metadata in the Cromwell docs](https://cromwell.readthedocs.io/en/stable/SubWorkflows/). When executing a workflow written in WDL and executed with Cromwell (the scientific workflow engine) one can extract metadata out of the Cromwell database. Within this metadata, the following ""executionEvents"" are available for each ""workflow.task"" in the ""calls"" objects. Pending; Requesting ExecutionToken; WaitingFor ValueStore; PreparingJob; CallCache Reading; RunningJob; Updating CallCache; Updating JobStore. From the documentation:; [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) allows Cromwell to detect when a job has been run in the past so that it doesn't have to re-compute results, saving both time and money. The main purpose of the [Job Store table](https://cromwell.readthedocs.io/en/stable/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores/#job-store-job_store_entry) is to support resuming execution of a workflow when Cromwell is restarted by recovering the outputs of completed jobs. I couldn't find a description of the Execution Token nor of the [Value Store](https://cromwell.readthedocs.io/en/stable/developers/bitesize/workflowExecution/jobKeyValueStore/) in [the docs](https://cromwell.readthedocs.io/en/develop/developers/Arch). My questions are the following:. What is the engine waiting on when a task/job is ""Pending""?; Is Requesting an Execution Token something that happens for every task because of security reasons, or does it have to do with the allowed capacity for Cromwell? What types of token are we talking about?; What happens during Value Store, where are which values s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5579:1261,detect,detect,1261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5579,1,['detect'],['detect']
Safety,"6a458800 nid=0xbb03 waiting on condition [0x0000000133591000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-12"" #72 prio=5 os_prio=31 tid=0x00007fb76f1b0800 nid=0xb903 waiting on condition [0x00000001332cc000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-11"" #71 prio=5 os_prio=31 tid=0x00007fb76b05a800 nid=0xb703 waiting on condition [0x00000001331c9000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchroniz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:8566,Unsafe,Unsafe,8566,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"6a6e9800 nid=0xc303 waiting on condition [0x000000013399d000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-15"" #75 prio=5 os_prio=31 tid=0x00007fb76e11d000 nid=0xbf03 waiting on condition [0x0000000133797000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-14"" #74 prio=5 os_prio=31 tid=0x00007fb76dd8e800 nid=0xbd03 waiting on condition [0x0000000133694000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchroniz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:5929,Unsafe,Unsafe,5929,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"6b05a800 nid=0xb703 waiting on condition [0x00000001331c9000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-10"" #70 prio=5 os_prio=31 tid=0x00007fb76bac5800 nid=0xb503 waiting on condition [0x0000000132fa6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-9"" #69 prio=5 os_prio=31 tid=0x00007fb76f36d800 nid=0xb303 waiting on condition [0x000000012c1b6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronize",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:10324,Unsafe,Unsafe,10324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"6cc59800 nid=0xc903 waiting on condition [0x0000000134093000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #79 prio=5 os_prio=31 tid=0x00007fb76d1d1000 nid=0xc703 waiting on condition [0x0000000133f90000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-17"" #78 prio=5 os_prio=31 tid=0x00007fb76de1b800 nid=0xc503 waiting on condition [0x0000000133df7000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchroniz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:3292,Unsafe,Unsafe,3292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"6cc5b800 nid=0xcb03 waiting on condition [0x000000013455e000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #80 prio=5 os_prio=31 tid=0x00007fb76cc59800 nid=0xc903 waiting on condition [0x0000000134093000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #79 prio=5 os_prio=31 tid=0x00007fb76d1d1000 nid=0xc703 waiting on condition [0x0000000133f90000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchroniz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:2413,Unsafe,Unsafe,2413,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"6d1d1000 nid=0xc703 waiting on condition [0x0000000133f90000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-17"" #78 prio=5 os_prio=31 tid=0x00007fb76de1b800 nid=0xc503 waiting on condition [0x0000000133df7000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-16"" #77 prio=5 os_prio=31 tid=0x00007fb76a6e9800 nid=0xc303 waiting on condition [0x000000013399d000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchroniz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:4171,Unsafe,Unsafe,4171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"6dd8e800 nid=0xbd03 waiting on condition [0x0000000133694000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-13"" #73 prio=5 os_prio=31 tid=0x00007fb76a458800 nid=0xbb03 waiting on condition [0x0000000133591000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-12"" #72 prio=5 os_prio=31 tid=0x00007fb76f1b0800 nid=0xb903 waiting on condition [0x00000001332cc000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchroniz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:7687,Unsafe,Unsafe,7687,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"6de1b800 nid=0xc503 waiting on condition [0x0000000133df7000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-16"" #77 prio=5 os_prio=31 tid=0x00007fb76a6e9800 nid=0xc303 waiting on condition [0x000000013399d000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-15"" #75 prio=5 os_prio=31 tid=0x00007fb76e11d000 nid=0xbf03 waiting on condition [0x0000000133797000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchroniz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:5050,Unsafe,Unsafe,5050,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"6e11d000 nid=0xbf03 waiting on condition [0x0000000133797000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-14"" #74 prio=5 os_prio=31 tid=0x00007fb76dd8e800 nid=0xbd03 waiting on condition [0x0000000133694000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-13"" #73 prio=5 os_prio=31 tid=0x00007fb76a458800 nid=0xbb03 waiting on condition [0x0000000133591000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchroniz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:6808,Unsafe,Unsafe,6808,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"6f1b0800 nid=0xb903 waiting on condition [0x00000001332cc000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-11"" #71 prio=5 os_prio=31 tid=0x00007fb76b05a800 nid=0xb703 waiting on condition [0x00000001331c9000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-10"" #70 prio=5 os_prio=31 tid=0x00007fb76bac5800 nid=0xb503 waiting on condition [0x0000000132fa6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchroniz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:9445,Unsafe,Unsafe,9445,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"76bac5800 nid=0xb503 waiting on condition [0x0000000132fa6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-9"" #69 prio=5 os_prio=31 tid=0x00007fb76f36d800 nid=0xb303 waiting on condition [0x000000012c1b6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-8"" #68 prio=5 os_prio=31 tid=0x00007fb76ef84000 nid=0xb103 waiting on condition [0x0000000132bf1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronize",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:11202,Unsafe,Unsafe,11202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"76ef84000 nid=0xb103 waiting on condition [0x0000000132bf1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-7"" #67 prio=5 os_prio=31 tid=0x00007fb7705a3000 nid=0xaf03 waiting on condition [0x00000001330c6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-6"" #66 prio=5 os_prio=31 tid=0x00007fb7705ef800 nid=0xad03 waiting on condition [0x0000000132aee000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronize",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:12958,Unsafe,Unsafe,12958,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"76f36d800 nid=0xb303 waiting on condition [0x000000012c1b6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-8"" #68 prio=5 os_prio=31 tid=0x00007fb76ef84000 nid=0xb103 waiting on condition [0x0000000132bf1000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-7"" #67 prio=5 os_prio=31 tid=0x00007fb7705a3000 nid=0xaf03 waiting on condition [0x00000001330c6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronize",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:12080,Unsafe,Unsafe,12080,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"7705a3000 nid=0xaf03 waiting on condition [0x00000001330c6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-6"" #66 prio=5 os_prio=31 tid=0x00007fb7705ef800 nid=0xad03 waiting on condition [0x0000000132aee000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-5"" #65 prio=5 os_prio=31 tid=0x00007fb7706d2800 nid=0xab03 waiting on condition [0x00000001329eb000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronize",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:13836,Unsafe,Unsafe,13836,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"7705ef800 nid=0xad03 waiting on condition [0x0000000132aee000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-5"" #65 prio=5 os_prio=31 tid=0x00007fb7706d2800 nid=0xab03 waiting on condition [0x00000001329eb000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-20"" #61 daemon prio=5 os_prio=31 tid=0x00007fb76b1f9000 nid=0xa303 waiting on condition [0x00000001325df000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$C",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:14714,Unsafe,Unsafe,14714,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"7fb76d42b000 nid=0xa103 waiting on condition [0x0000000132168000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-19"" #59 daemon prio=5 os_prio=31 tid=0x00007fb770631000 nid=0x9f03 waiting on condition [0x00000001324dc000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-18"" #58 daemon prio=5 os_prio=31 tid=0x00007fb770630800 nid=0x9d03 waiting on condition [0x00",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:17543,Unsafe,Unsafe,17543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"7fb76e95e000 nid=0x9b03 waiting on condition [0x00000001322d6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-17"" #56 daemon prio=5 os_prio=31 tid=0x00007fb76b6b6000 nid=0x9903 waiting on condition [0x0000000131db9000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-16"" #55 daemon prio=5 os_prio=31 tid=0x00007fb76e92f000 nid=0x9703 waiting on condition [0x00",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:20573,Unsafe,Unsafe,20573,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"7fb7706d2800 nid=0xab03 waiting on condition [0x00000001329eb000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-20"" #61 daemon prio=5 os_prio=31 tid=0x00007fb76b1f9000 nid=0xa303 waiting on condition [0x00000001325df000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-4"" #60 prio=5 os_prio=31 tid=0x00007fb76d42b000 nid=0xa103 waiting on condition [0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:15589,Unsafe,Unsafe,15589,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:2514,unsafe,unsafeToFuture,2514,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['unsafe'],['unsafeToFuture']
Safety,"9:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockCon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051:1963,unsafe,unsafeToFuture,1963,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051,1,['unsafe'],['unsafeToFuture']
Safety,":06.270Z"",; ""cromwellVersion"": ""52""; },; {; ""cromwellId"": ""cromid-0fe86cb"",; ""description"": ""PickedUp"",; ""timestamp"": ""2020-09-02T09:23:04.924Z"",; ""cromwellVersion"": ""52""; }; ],; ""metadataSource"": ""Unarchived"",; ""actualWorkflowLanguageVersion"": ""v1.0"",; ""submittedFiles"": {; ""workflow"": ""{\n \""$graph\"": [\n {\n \""class\"": \""CommandLineTool\"",\n \""doc\"": \""AMBER is designed to generate a tumor BAF file for use in PURPLE from a provided VCF of likely heterozygous SNP sites.\\n\\nWhen using paired reference/tumor bams,\\nAMBER confirms these sites as heterozygous in the reference sample bam then calculates the\\nallelic frequency of corresponding sites in the tumor bam.\\nIn tumor only mode, all provided sites are examined in the tumor with additional filtering then applied.\\n\\nThe Bioconductor copy number package is then used to generate pcf segments from the BAF file.\\n\\nWhen using paired reference/tumor data, AMBER is also able to:\\n1. detect evidence of contamination in the tumor from homozygous sites in the reference; and\\n2. facilitate sample matching by recording SNPs in the germline\\n\"",\n \""requirements\"": [\n {\n \""dockerPull\"": \""quay.io/biocontainers/hmftools-amber:3.3--0\"",\n \""class\"": \""DockerRequirement\""\n },\n {\n \""expressionLib\"": [\n \""var get_start_memory = function(){ /* Start with 2 Gb */ return 2000; }\"",\n \""var get_max_memory_from_runtime_memory = function(max_ram){ /* Get Max memory and subtract heap memory */ return max_ram - get_start_memory(); }\""\n ],\n \""class\"": \""InlineJavascriptRequirement\""\n },\n {\n \""coresMin\"": 16,\n \""ramMin\"": 32000,\n \""class\"": \""ResourceRequirement\""\n },\n {\n \""class\"": \""ShellCommandRequirement\""\n }\n ],\n \""baseCommand\"": [\n \""AMBER\""\n ],\n \""arguments\"": [\n {\n \""prefix\"": \""-Xms\"",\n \""separate\"": false,\n \""valueFrom\"": \""$(get_start_memory())m\"",\n \""position\"": -2\n },\n {\n \""prefix\"": \""-Xmx\"",\n \""separate\"": false,\n \""valueFrom\"": \""$(get_max_memory_from_runtime_memory(runtime.ram))m\",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:61826,detect,detect,61826,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['detect'],['detect']
Safety,":55:17,31] [info] Aborting all running workflows.; [2023-02-04 08:55:17,31] [info] JobExecutionTokenDispenser stopped; [2023-02-04 08:55:17,31] [info] WorkflowStoreActor stopped; [2023-02-04 08:55:17,32] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2023-02-04 08:55:17,32] [info] WorkflowLogCopyRouter stopped; [2023-02-04 08:55:17,32] [info] WorkflowManagerActor All workflows finished; [2023-02-04 08:55:17,32] [info] WorkflowManagerActor stopped; [2023-02-04 08:55:17,32] [info] Connection pools shut down; [2023-02-04 08:55:17,33] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] SubWorkflowStoreActor stopped; [2023-02-04 08:55:17,33] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] JobStoreActor stopped; [2023-02-04 08:55:17,33] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2023-02-04 08:55:17,33] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] CallCacheWriteActor stopped; [2023-02-04 08:55:17,33] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2023-02-04 08:55:17,33] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] KvWriteActor Shutting down: 0 queued messages to process; [2023-02-04 08:55:17,33] [info] DockerHashActor stopped; [2023-02-04 08:55:17,34] [info] IoProxy stopped; [2023-02-04 08:55:17,34] [info] ServiceRegistryActor stopped; [2023-02-04 08:55:17,37] [info] Database closed; [2023-02-04 08:55:17,37] [info] Stream materializer shut down; [2023-02-04 08:55:17,40] [info] Automatic shutdown of the async connection; [2023-02-04 08:55:17,40] [info] Gracefully shutdown sentry threads.; [2023-02-04 08:55:17,40] [info] Shutdown finish",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999:16512,Timeout,Timeout,16512,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999,7,['Timeout'],['Timeout']
Safety,:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:1218,recover,recover,1218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Safety,":expanse_figures.CBL_assoc:-1:1-20000000025 [b303ae23expanse_figures.CBL_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 13:07:47,67] [info] BT-322 58e64982:expanse_figures.CBL_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = C3078AB9F63DD3A59655953B1975D6CF.; [2023-03-29 13:07:47,67] [info] 58e64982-cf3d-4e77-ad72-acfda8299d1b-EngineJobExecutionActor-expanse_figures.CBL_assoc:NA:1 [58e64982]: Call cache hit process had 0 total hit failures before completing successfully; ```. Can someone help me diagnose why call caching isn't near instantaneous, and what I can do to make it much faster? Happy to provide more information as necessary. Thanks!. Config:; ```; # See https://cromwell.readthedocs.io/en/stable/Configuring/; # this configuration only accepts double quotes! not singule quotes; include required(classpath(""application"")). system {; abort-jobs-on-terminate = true; io {; number-of-requests = 30; per = 1 second; }; file-hash-cache = true; }. # necessary for call result caching; # will need to stand up the MySQL server each time before running cromwell; # stand it up on the same node that's running cromwell; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true""; user = ""root""; password = ""pass""; connectionTimeout = 5000; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. docker {; hash-lookup {; enabled = true; method = ""remote""; }; }. backend {; # which backend do you want to use?; # Right now I don't know how to choose this via command line, only here; default = ""Local"" # For running jobs on an interactive node; #default = ""SLURM"" # For running jobs by submitting them from an interactive node to the cluster; providers { ; # For running jobs on an interactive node; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigB",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:3104,abort,abort-jobs-on-terminate,3104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['abort'],['abort-jobs-on-terminate']
Safety,"; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051:1865,unsafe,unsafeToFuture,1865,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051,1,['unsafe'],['unsafeToFuture']
Safety,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4526:7422,Timeout,Timeout,7422,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526,8,['Timeout'],['Timeout']
Safety,"; }; }. # Here is where you can define the backend providers that Cromwell understands.; # The default is a local provider.; # To add additional backend providers, you should copy paste additional backends; # of interest that you can find in the cromwell.example.backends folder; # folder at https://www.github.com/broadinstitute/cromwell; # Other backend providers include SGE, SLURM, Docker, udocker, Singularity. etc.; # Don't forget you will need to customize them for your particular use case.; backend {; # Override the default backend.; default = slurm. # The list of providers.; providers {; # Copy paste the contents of a backend provider in this section; # Examples in cromwell.example.backends include:; # LocalExample: What you should use if you want to define a new backend provider; # AWS: Amazon Web Services; # BCS: Alibaba Cloud Batch Compute; # TES: protocol defined by GA4GH; # TESK: the same, with kubernetes support; # Google Pipelines, v2 (PAPIv2); # Docker; # Singularity: a container safe for HPC; # Singularity+Slurm: and an example on Slurm; # udocker: another rootless container solution; # udocker+slurm: also exemplified on slurm; # HtCondor: workload manager at UW-Madison; # LSF: the Platform Load Sharing Facility backend; # SGE: Sun Grid Engine; # SLURM: workload manager. # Note that these other backend examples will need tweaking and configuration.; # Please open an issue https://www.github.com/broadinstitute/cromwell if you have any questions; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions"". concurrent-job-limit = 10; # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:5666,safe,safe,5666,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['safe'],['safe']
Safety,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. **Which backend are you running?**. broadinstitute/cromwell:36. **Paste/Attach your workflow if possible**. For any workflow, when I query its metadata endpoint with `excludeKey=calls` parameter, it returns a response with all `""calls""` key nevertheless. This doesn't seem to happen to other keys, like `inputs` or `submittedFiles`. Excluding `calls` would make a huge difference for us, because for large workflows it takes a long time for Cromwell to aggregate all calls, the response becomes large, and sometimes it even timeouts. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4362:1126,timeout,timeouts,1126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4362,1,['timeout'],['timeouts']
Safety,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Backend:; Local, no conf file. <!-- Paste/Attach your workflow if possible: -->. Workflow: Files are here:; https://github.com/FredHutch/reproducible-workflows/tree/master/CWL/SingleStepWorkflow. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Details (see also [this post](https://gatkforums.broadinstitute.org/wdl/discussion/23265/cwl-workflow-fails-running-locally#latest)):. I can run this workflow just fine using cwltool/cwl-runner as follows:. ```; cwl-runner bwa-memWorkflow.cwl localInputs.yml; ```. When I try and run it with cromwell I get an error that ""The job was aborted from outside Cromwell"" but I definitely did not abort it myself. Here is the command I used to run this workflow in Cromwell:. ```; java -jar cromwell-36.jar run bwa-memWorkflow.cwl -i localInputs.yml -p bwa-pe.cwl.zip; ```. (`bwa-pe.cwl.zip` just contains the dependency `bwa-pe.cwl`). And here's the full output of it:. https://gist.github.com/dtenenba/61bcf60f129b817cd894ee222789369a. My ultimate goal is to switch over to the AWS Batch back end (in case you are wondering why I don't just stick with cwltool) but first I wanted to get the workflow running locally in cromwell. Any ideas about this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4587:1287,abort,aborted,1287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4587,2,['abort'],"['abort', 'aborted']"
Safety,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. # Description. I believe this is a bug. I tried to use `stderr()` in the `output` section of a `workflow`, rather than the output section of a `task`. The resulting WDL validated fine using `womtool validate` (and it validated fine on Terra with the automatic validation they do). But the job would run about halfway and then automatically switch to ""Aborting"" status with no explanation or error message. The workflow would eventually fail after a huge delay (about 22 hours), and there would be no real error message. All tasks that ran were successful (but not all tasks ran). # Minimal WDL example. Here is a working example:. ```wdl; version 1.0. workflow my_workflow {; call my_task; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. And here is a non-working example that still validates fine using `womtool validate`:. ```wdl; version 1.0. workflow my_workflow {; input {; Boolean run_task; }. if (run_task) {; call my_task; }. output {; File out = select_first([my_task.out, stdout()]); }; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. The above gives; ```console; (cromwell) [sfleming@laptop:~/cromwell]$ womtool validate test.wdl ; Success!; ```. # The problem. The problem is that the non-working WDL example above should not validate successfully, as it is NOT a valid WDL. The `stdout()` built-in inside the `select_first()` in the `output` block of the `workflow` is not actually allowed. It will cause a very bizarre err",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6976:822,Abort,Aborting,822,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6976,1,['Abort'],['Aborting']
Safety,"> We use lots of different runtime attributes (bsub job submission parameters) in our workflows.  ; > ; > With existing Cromwell backends, you need to put all the possible runtime attributes in the code itself.  Would it be possible to let the user specify arbitrary runtime parameters only in the configuration file (not in the source code) that Cromwell just passes to the submit command?  ; > ; > We want to avoid asking for Cromwell code changes every time we decide to use a new bsub parameter. -- Pfizer",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1217:411,avoid,avoid,411,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1217,1,['avoid'],['avoid']
Safety,"@Horneth I think this was originally added by you (but git blame might be lying...), could you sanity check this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4085:95,sanity check,sanity check,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4085,1,['sanity check'],['sanity check']
Safety,"@kshakir commented on [Mon Jan 23 2017](https://github.com/broadinstitute/centaur/issues/134). For each test name, it would be helpful to log the workflowId, as the name of the WDL workflow doesn't always match the name of the test. Additionally, a brief message of when the test was detected as starting & stopped would help debug stuck workflows. If this is deemed too verbose, the above could be logged at level debug.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2891:284,detect,detected,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2891,1,['detect'],['detected']
Safety,"@mcovarr commented on [Thu Sep 07 2017](https://github.com/broadinstitute/wdltool/issues/48). Per the link below, enhance wdltool to be able to detect malformed expressions. Expressions that can't be evaluated are okay and expected due to values not being available, but malformed expressions are not okay. https://gatkforums.broadinstitute.org/wdl/discussion/10311/error-evaluating-output-files-that-serve-as-input-files-for-following-step#latest",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2869:144,detect,detect,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2869,1,['detect'],['detect']
Safety,"@mwalker174 originally reported:. > Hi all, I’ve got a critical problem where call caching times out on a particular WDL task (`“message”: “Hashing request timed out for gs://...“’). This makes some sense since the task is checking ~200 files on each of ~200 shards. This is on cromwell v36/papiv2. I thinking bundling the files would probably fix this, but is there any way to increase the timeout limit in the server settings? Would upgrading to v38 help? Thanks!. There's currently a non-configurable 5 minute timeout per GCS hash request. Assuming no batching (which I didn't come across) for ~40K individual requests that's about 100 requests/second to GCS. I'm pretty sure w/ GCS request throttling & internal cromwell backoffs at least one of those requests would fail to return in 5 minutes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4873:391,timeout,timeout,391,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4873,2,['timeout'],['timeout']
Safety,"@ruchim ; As a user, I want to abort a series of workflows by their label. Does not involve JESification at this time.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2333:31,abort,abort,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2333,1,['abort'],['abort']
Safety,"A 503 StorageException seems to have failed one of the centaur JES jobs, and hence the workflow. Via the [cromwell.log](https://console.cloud.google.com/storage/browser/cloud-cromwell-dev/cromwell_execution/travis/centaur_workflow/0310fa51-e985-4c54-8cdb-5058155f452e/call-centaur/cromwell_root/logs/). ```java; 2017-08-25 05:43:25,399 cromwell-system-akka.dispatchers.engine-dispatcher-51 ERROR - WorkflowManagerActor Workflow dabddbe7-a385-4df4-be97-c1ef7b884823 failed (during ExecutingWorkflowState): Could not evaluate composeEngineFunctions.y = read_int(stderr()) + x + read_string(blah); java.lang.RuntimeException: Could not evaluate composeEngineFunctions.y = read_int(stderr()) + x + read_string(blah); 	at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:190); 	at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:189); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at wdl4s.wdl.WdlTask.$anonfun$evaluateOutputs$2(WdlTask.scala:189); 	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:157); 	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:157); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:157); 	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:155); 	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104); 	at wdl4s.wdl.WdlTask.evaluateOutputs(WdlTask.scala:182); 	at cromwell.backend.wdl.OutputEvaluator$.evaluateOutputs(OutputEvaluator.scala:15); 	at cromwell.backend.stan",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2576:949,recover,recoverWith,949,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2576,1,['recover'],['recoverWith']
Safety,A batch endpoint to abort workflows,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3753:20,abort,abort,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3753,1,['abort'],['abort']
Safety,"A bunch of jobs were finished which Cromwell didn't detect. The context: ; - Trying to run jprofiler to get a profile of the run described in #820. Full stack dump:. ```; Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode):. ""cromwell-system-akka.actor.default-dispatcher-27"" #115 prio=5 os_prio=31 tid=0x00007fb76d052800 nid=0xf503 waiting on condition [0x0000000135d74000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0021d00> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""ForkJoinPool-3-worker-5"" #82 daemon prio=5 os_prio=31 tid=0x00007fb76cc73800 nid=0xcd03 waiting on condition [0x0000000134661000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0041f30> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""pool-1-thread-20"" #81 prio=5 os_prio=31 tid=0x00007fb76cc5b800 nid=0xcb03 waiting on condition [0x000000013455e000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(Th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:52,detect,detect,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,2,"['Unsafe', 'detect']","['Unsafe', 'detect']"
Safety,A cleanly shut down Cromwell instance cleans up the workflow store to null out the Cromwell instance ID field for the workflow store entries it was running. When a new Cromwell instance comes up it will consider those workflow store entries to be fair game for pickup because those instance ID fields are null. However an uncleanly shut down Cromwell does not null out the Cromwell instance ID field of its running workflows before it exits. When a new Cromwell instance comes up it will see that those workflow store entries appear to be owned by another Cromwell and will only pick them up if the heartbeats on those rows are older than the workflow heartbeat TTL (default 10 minutes). . The problem here was some vestigial logic for the way restarts used to work that no longer makes sense in the 2/3-implemented horizontal Cromwell system. It is now entirely reasonable to see workflows in Running or Aborted state with or without a heartbeat timestamp depending on whether the Cromwell that was previously running those workflows was shut down cleanly or not.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3675:905,Abort,Aborted,905,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3675,1,['Abort'],['Aborted']
Safety,"A couple of separate ""belt-and-braces"" fixes to BW-478 which allowed errors in job preparation engine execution to leave workflows stuck in a zombie/Running/""call Starting"" state:. * Stop the engine function itself from throwing an exception; * Put a safety catch in the Job Execution actor to catch any other exceptions thrown by engine functions; * Put a safety catch in the `ErrorOr` `flatMap` function to automatically catch any exception thrown in `for`-comprehension chains.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6161:251,safe,safety,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6161,2,['safe'],['safety']
Safety,A different approach to #1126 from that proposed in the ticket. This records the container ID at `docker run` time and uses that to `docker kill` the process on abort.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2581:161,abort,abort,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2581,1,['abort'],['abort']
Safety,"A few observations:; - If the EJEA is aborted, we could stop hashing the remaining files; - If the EJHA is done, it could stop hashing the remaining files; - Since the EJHA already blocks work into chunks of 100 (and BackendFileHashers tend to be synchronous), it could simply not send the next batch if it knows it doesn't need to; - If the set of initial hashes are a cache miss (and cache writing is disabled), we don't need to send the files for hashing in the first place",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1503:38,abort,aborted,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1503,1,['abort'],['aborted']
Safety,"A few workflows that we aborted in Cromwell-as-a-Service have the status ""Aborted"" (e.g. ""429e0aaf-c429-4438-a12d-734f1f444801"") but the subworkflow that was running when the parent workflow was aborted is still in the ""running"" status. When trying to abort the subworkflow that is still running (""34074359-f8ed-4402-ba65-c92ab550e999""), I see the error:. ```; {; ""status"": ""error"",; ""message"": ""Couldn't abort 34074359-f8ed-4402-ba65-c92ab550e999 because no workflow with that ID is in progress""; }; ```. It looks like Cromwell thinks the workflow is not running, but it's metadata says that it is.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3654:24,abort,aborted,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3654,5,"['Abort', 'abort']","['Aborted', 'abort', 'aborted']"
Safety,"A partial implementation of the WES 1.0 standard directly embedded in the Cromwell server (modulo auth, but that's another story altogether). Why partial? Because I wanted to keep PR sizes down and didn't want to be rebasing every 3 days. Also this lays the basic infrastructure, so might as well get commentary on that. It's not hurting anything to have a partial implementation. Why status & abort? Because they were the easiest to do.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4425:394,abort,abort,394,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4425,1,['abort'],['abort']
Safety,"A recent review of Travis test failures revealed that some workflows were failing due to timeouts on functions like read_lines() or read_int() timing out:. ```cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'int_reader.int': Failed to read_int(""""gs://cloud-cromwell-dev/cromwell_execution/travis/globs/57f6e677-c2aa-4d96-bf33-9591fce20da7/call-int_reader/shard-3/stdout"""") (reason 1 of 1): Futures timed out after [10 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:851)```. It's possible that being queued in the I/O actor can take longer than the 10s timeout and thus that is the issue. It's possible this timeout needs to be raised or output evaluation needs to be retried, but this needs a fix as the outputs being evaluated already exist, so this is a bad failure mode. AC: Depending on the potential causes for such behavior, either retry this evaluation, raise the timeout or explore another solution to ensure that jobs dont fail because of this timeout.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057:89,timeout,timeouts,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057,5,['timeout'],"['timeout', 'timeouts']"
Safety,"A significant amount of GotC failures are due to out of memory / disk errors.; Design a mechanism that allow to specify custom retry strategies that can modify runtime parameters based on failure modes. For example, “Retry on return code X with double the amount of memory and / or disk”. Thoughts:; - `currentAttempt()` wdl function to be used as a variable in a memory / disk formula; - monitor the job (monitoring script ?) to detect disk / memory overflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1847:430,detect,detect,430,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1847,1,['detect'],['detect']
Safety,A strict version of `mapValues` that avoids the nasty surprises of the original.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3943:37,avoid,avoids,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3943,1,['avoid'],['avoids']
Safety,"AWS backend ""aborts"" inappropriately with large files",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4960:13,abort,aborts,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4960,1,['abort'],['aborts']
Safety,Abort Local with docker kills the script but not the docker container,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1126:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1126,1,['Abort'],['Abort']
Safety,Abort aborts workflow but not jobs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,2,"['Abort', 'abort']","['Abort', 'aborts']"
Safety,Abort all connected up. Closes #1253,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1397:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1397,1,['Abort'],['Abort']
Safety,Abort harder,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2829:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2829,1,['Abort'],['Abort']
Safety,Abort more,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3069:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3069,1,['Abort'],['Abort']
Safety,Abort sub workflows,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3260:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3260,1,['Abort'],['Abort']
Safety,Abort support for JES PBE,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/753:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/753,1,['Abort'],['Abort']
Safety,Abort support for Local PBE,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/672:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/672,1,['Abort'],['Abort']
Safety,"Abort was just failing everything, this fixes it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/379:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/379,1,['Abort'],['Abort']
Safety,Abort wiring in shadow world. Closes #671,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/759:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/759,1,['Abort'],['Abort']
Safety,Abort with JES has race conditions,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/700:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/700,1,['Abort'],['Abort']
Safety,Aborted jobs still submits additional preemtible tasks to JES,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3758:0,Abort,Aborted,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758,1,['Abort'],['Aborted']
Safety,AbortedResponse handler in WorkflowExecutionActor seems superfluous,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1376:0,Abort,AbortedResponse,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1376,1,['Abort'],['AbortedResponse']
Safety,"Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3873,Abort,Aborting,3873,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:37:14,145 cromwell-system-akka.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:4034,Abort,Aborting,4034,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:37:14,145 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(60ec6228)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:23,720 cromwell-system-akka.d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:4196,Abort,Aborting,4196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3550,Abort,Aborting,3550,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:37:14,145 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(60ec6228)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:23,720 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(a442dc1c)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:37:31,421 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17bed42e)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:40,098 cromwell-system-akka.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:4519,Abort,Aborting,4519,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3388,Abort,Aborting,3388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,Aborts jobs on shutdown.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397:0,Abort,Aborts,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397,1,['Abort'],['Aborts']
Safety,Aborts v2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2808:0,Abort,Aborts,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2808,1,['Abort'],['Aborts']
Safety,Actual code:. ``` scala; override def abortInitialization(): Unit = ???; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1109:38,abort,abortInitialization,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1109,3,['abort'],['abortInitialization']
Safety,Add Recover to Backend Actor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/663:4,Recover,Recover,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/663,1,['Recover'],['Recover']
Safety,"Add Trivy github action and remediate [BW-466, BW-476]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6152:28,remediat,remediate,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6152,1,['remediat'],['remediate']
Safety,Add ability to abort on-hold workflows,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4311:15,abort,abort,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4311,1,['abort'],['abort']
Safety,Add abort by label,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2333:4,abort,abort,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2333,1,['abort'],['abort']
Safety,Add abort support in HtCondor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1402:4,abort,abort,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1402,1,['abort'],['abort']
Safety,"Add abort, workflow store delete to coordinated access actor [WA-334]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906:4,abort,abort,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906,1,['abort'],['abort']
Safety,Add akka http request-timeout idle-timeout examples in the config; To allow users to get metadata results from large workflows. Also delete the now duplicated cromwell.examples.conf file; And delete the backends section which has been split into; separate files. https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows. https://github.com/broadinstitute/cromwell/issues/2519,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4776:22,timeout,timeout,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4776,2,['timeout'],['timeout']
Safety,Add an abort test with subworkflows when they're back,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2815:7,abort,abort,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2815,1,['abort'],['abort']
Safety,Add default logback.xml to womtool to avoid spammy netty logs [BA-6580],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5794:38,avoid,avoid,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5794,1,['avoid'],['avoid']
Safety,Add example akka config to avoid metadata timeout issue,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4776:27,avoid,avoid,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4776,2,"['avoid', 'timeout']","['avoid', 'timeout']"
Safety,Add flatten type detection,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2959:17,detect,detection,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2959,1,['detect'],['detection']
Safety,Add recovery / abort to HtCondorBackend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/885:4,recover,recovery,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/885,2,"['abort', 'recover']","['abort', 'recovery']"
Safety,Add retry on abort option,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4205:13,abort,abort,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4205,1,['abort'],['abort']
Safety,"Add safety net around expression ""toString""ing for missing expressions [BA-5950]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5197:4,safe,safety,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5197,1,['safe'],['safety']
Safety,Add test timeouts to more quickly fail everlasting tests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5937:9,timeout,timeouts,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5937,1,['timeout'],['timeouts']
Safety,Add tests for aborts,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2088:14,abort,aborts,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088,1,['abort'],['aborts']
Safety,"Added Docker specification content types to the manifest unmarshaller.; Updated spec for testing GCR w/o authentication, as cromwell's Google Credentials utility now automatically detects the application default credentials on a system.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/715:180,detect,detects,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/715,1,['detect'],['detects']
Safety,Added Workflow Abort functionality,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/128:15,Abort,Abort,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/128,1,['Abort'],['Abort']
Safety,Added an extra akka message check just in case we miss the abort message when our scala future eventually runs.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1947:59,abort,abort,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1947,1,['abort'],['abort']
Safety,Added changelog message regarding webservice timeout,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1850:45,timeout,timeout,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1850,1,['timeout'],['timeout']
Safety,Added the new database tables for Job Avoidance,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/276:38,Avoid,Avoidance,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/276,2,['Avoid'],['Avoidance']
Safety,Added the new database tables for Job Avoidance. It may be better to do all Avoidance work in a branch rather than merging into DEVELOP one piece at a time.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/276:38,Avoid,Avoidance,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/276,2,['Avoid'],['Avoidance']
Safety,Additional null safety for GCS IO [CROM-6670],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6118:16,safe,safety,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6118,1,['safe'],['safety']
Safety,"Addresses [WX-1282](https://broadworkbench.atlassian.net/browse/WX-1282). PR replaces the INNER JOIN statement against `pg_largeobject` with a `lo_get` statement to avoid ""Permission Denied"" errors that comes from scanning the `pg_largeobject` table (which enforces owner/role permissions for each row which needs to be taken in consideration for the new Workflows App ecosystem). [WX-1282]: https://broadworkbench.atlassian.net/browse/WX-1282?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7228:165,avoid,avoid,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7228,1,['avoid'],['avoid']
Safety,Adds submitted and aborted status to Metadata,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2114:19,abort,aborted,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2114,1,['abort'],['aborted']
Safety,Adjust CromIAM timeout to match Cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4615:15,timeout,timeout,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4615,1,['timeout'],['timeout']
Safety,"After I have aborted a job, VM's are not being apropriately killed. Manually killing the VM does not have the desire effect, as if the task was preemptible PAPI will launch another VM until all of your preemptible tries have been consumed",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3758:13,abort,aborted,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758,1,['abort'],['aborted']
Safety,"After abort a workflow, can I rerun the task based on the workflow id?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7465:6,abort,abort,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7465,1,['abort'],['abort']
Safety,"Allow ""AbortedResponse"" even when not aborting",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2503:7,Abort,AbortedResponse,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2503,2,"['Abort', 'abort']","['AbortedResponse', 'aborting']"
Safety,Allow GCP global pipeline timeout to be configurable,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5273:26,timeout,timeout,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273,1,['timeout'],['timeout']
Safety,Allow GCP global pipeline timeout to be configurable [CI clone],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5315:26,timeout,timeout,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5315,1,['timeout'],['timeout']
Safety,Allow abort in CromIAM to go to a different Cromwell server,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4142:6,abort,abort,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4142,1,['abort'],['abort']
Safety,Allow for a DB write to trigger abort of a workflow,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3344:32,abort,abort,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3344,1,['abort'],['abort']
Safety,Allowed the CallActor to receive an updated abort function,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/326:44,abort,abort,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/326,1,['abort'],['abort']
Safety,Also creates #852. Following things need to be done (separate stories?) :; - [ ] Add docker support (#884); - [ ] Add recovery and abort (#885); - [x] Add continueOnErrorCode support; - [ ] Find a way to add condor specific runtime attributes to Condor ClassAds (#886). Anything more to support?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/861:118,recover,recovery,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/861,2,"['abort', 'recover']","['abort', 'recovery']"
Safety,"Alternative to #5588 which completely removes this redundant queuing mechanism which doesn't seem to be doing what it thinks it was doing, and is worse in any case than the existing token distribution safety mechanisms.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5590:51,redund,redundant,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5590,2,"['redund', 'safe']","['redundant', 'safety']"
Safety,"An attempt to document my observation of our general purpose debugging process - will hopefully help the next generation of Cromwell fire troubleshooters. * Moves the release processes under a new ""processes"" banner instead of awkwardly sitting in ""scripts""; * Adds a general-purpose recover process . See the process rendered and [in situ](https://github.com/broadinstitute/cromwell/tree/cjl_all_purpose_mess_remover/processes/troubleshooting). NB: If this gets approval, I'll update our playbook to link to this as our ""general purpose fallback process""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4991:284,recover,recover,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4991,1,['recover'],['recover']
Safety,And another possible bug: why are we trying to upload an auth file when running in application default auth mode for both genomics and filesystems?. ```; [ERROR] [01/27/2017 14:39:36.100] [cromwell-system-akka.dispatchers.engine-dispatcher-5] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow 732474fd-88b0-4a5e-ad19-5ee5cd71d141 failed (during InitializingWorkflowState): Failed to upload authentication file; java.io.IOException: Failed to upload authentication file; 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$1.applyOrElse(JesInitializationActor.scala:81); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$1.applyOrElse(JesInitializationActor.scala:80); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinP,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1924:957,recover,recoverWith,957,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1924,1,['recover'],['recoverWith']
Safety,ApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); cromwell_1 | at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); cromwell_1 | at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); cromwell_1 | at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); cromwell_1 | at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); cromwell_1 | at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); cromwell_1 | at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); cromwell_1 | at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); cromwell_1 | at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); cromwell_1 | at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); cromwell_1 | at akka.dispatch.forkjoin.ForkJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4337:2818,recover,recoverWith,2818,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4337,2,['recover'],['recoverWith']
Safety,"As FireCloud ( @cbirger ) , I often abort workflows. Although JES currently makes a best effort to abort calls, sometimes those calls fail to abort. I would like to know through the cromwell call/workflow status the difference between ""definitely aborted"" and ""unknown abort status"". The reason this is important is that if I know the status is ""unknown"" I know that I might be at risk for being billed for machines I don't want and should take further action. </end of PO comment>. Technically this might require a little research and specifically work on JES. Ideally:; 1. We change our overall workflow status to aborting; 2. When we cancel a JES operation, the status of that operation will reflect reality.; 3. We can poll that operation until it reaches a terminal state (e.g. cancelled). This may actually just be for us to keep running the workflow like normal and just add canceled to the list of terminal states).; 4. Once all tasks are in a terminal state, the workflow status is Aborted. Question will be... if JES fails to terminate a job, should we change it's status to something like 'LOST' or 'UNKNOWN' after N minutes, or should we wait indefinitely. Since it's a best effort cancellation in JES we should handle this case",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1139:36,abort,abort,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1139,8,"['Abort', 'abort', 'risk']","['Aborted', 'abort', 'aborted', 'aborting', 'risk']"
Safety,"As a **workflow runner**, I want **certain parameters to be ignored in the hashing process**, so that I can **call cache on more workflows when the result is exactly the same**.; - Effort: **?**; - Risk: **Medium** ; - We should err on the side of hashing a workflow differently if we are not absolutely confident that the parameter does not impact the result.; - Which parameters are ignored is NOT user-editable. This is to prevent users from accidentally ignoring parameters that do impact the result.; - Business value: **Medium**. Some parameters, such as `preemptible_attempts` and `CPU`, don't affect the outcome of the workflow but workflows with different CPU values will not call cache. @LeeTL1220 and @geoffjentry to provide additional thoughts and context if helpful.; Related issue #1210",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2604:198,Risk,Risk,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2604,1,['Risk'],['Risk']
Safety,"As a pipeline author, I would like the ability to add assert-like statements to my tasks. For example, after the task has run check the output log for the presence of some message. This is important because we often find problems in the pipelines that we could have detected with some basic checks that could be part of the pipeline. They are different than unit tests because these problems often only arise under unusual data conditions that are hard to predict. The current approach on this is for @yfarjoun to test run these ideas just within the command block of WDL as a way to figure out what this feature really looks like. Then from that set of we can figure out what features need to be added in order to support this in a natural, DRY way",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1146:266,detect,detected,266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146,2,"['detect', 'predict']","['detected', 'predict']"
Safety,"As a side effect to enable abort support in HtCondor, this PR makes the polling (for checking job status) asynchronous, and the polling interval to be configurable.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1403:27,abort,abort,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1403,1,['abort'],['abort']
Safety,"As a user who runs cromwell in a production setting (like @ktibbett), I need to be able to manage the lifecycle of workflows in the system. After running many workflows, they consume a lot of space on disk and even within the cromwell environment. I woul to be able to delete them through a REST endpoint. Add a new endpoint at DELETE/workflows/{version}/{id} which effectively removes this workflow from the system. This should include; - removing all output files for the workflows and calls; - removing all metadata from the metadata service; - removing all workflows/calls from the call caching service. attempting to remove a workflow in a non-terminal state should result in an error (it should either finish or be aborted first). --; In detail specification:. https://docs.google.com/document/d/1aJn5HzvDgYbvBlEG4z0KO8oZgaQ3lFu2hE8QzRC0_18/edit?usp=sharing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1292:721,abort,aborted,721,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292,1,['abort'],['aborted']
Safety,"As a user with a controlled file system (like GOTC or the cromwell execution directory) where the I know that a file path is immutable and uniquely identifying, I would like to run the cromwell server in a mode where the file path can be used in call caching rather than computing the actual hash. I will take on the risk that if I break that contract (by modifying files), workflows will not execute properly. I want to do this because it will be a big performance gain when I have many files and I know that their paths are unique. @cjllanwarne gets credit for raising this as a cool feature, @jsotobroad and @dshiga agreed",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1271:317,risk,risk,317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1271,2,['risk'],['risk']
Safety,"As a user, I would like to be able to call an endpoint in cromwell which takes in a workflow submission identifier and have cromwell clean up all intermediate outputs of the workflow (files that are not declared as outputs of the workflow).; If there is a job avoidance feature when this is worked on, these cleaned up tasks internal to the workflow should not be eligible for job avoidance. However, the overall workflow would be eligible.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/601:260,avoid,avoidance,260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/601,2,['avoid'],['avoidance']
Safety,"As discussed in https://github.com/broadinstitute/cromwell/issues/6235, developers of workflows for GCP who store their images in Google Container Repositories can be exposed to large Google GCS egress charges when users attempt to run workflows in different continental regions, resulting in many trans-continental container pulls. There currently does not seem to be a satisfactory way to guard against this:. - We can't make our image repositories private because we want to make the workflows available to the public via Terra.; - We can't make the repositories requester-pays because the pipelines API does not support pulling images from requester-pays repositories.; - We can mirror our repositories to different regions, but we are still dependent on our users to configure their workflows to point to the right region and take good-faith extra steps to help us avoid these charges. Some possible ideas were suggested by @freeseek in https://github.com/broadinstitute/cromwell/issues/6235:. - Convince Google to support requester-pays buckets for container pulls in PAPI.; - Modify some combination of Cromwell/PAPI to cache images rather than pulling them for each task that is run.; - Develop infrastructure within Cromwell to know what region the workflow is running in and automatically select the right GCR mirror to pull from.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442:870,avoid,avoid,870,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442,1,['avoid'],['avoid']
Safety,"As far as I can tell, the timeline is:. - Shutdown signal received; - The job is aborted in JES but not removed from the JobStore; - On restart, the job is recovered because it remains in the JobStore, but in JES it's already been aborted; - On the console, a ""Job Failed"" message appears.; - The EJEA actor dies in an unexpected way (this concerns me *most*. Why isn't the failure cause recorded!). Not tested on SFS backends.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2050:81,abort,aborted,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050,3,"['abort', 'recover']","['aborted', 'recovered']"
Safety,"As pointed out by @davidbernick, there are some vulnerabilities in Cromwell's docker image. Beyond that, it's a good idea to periodically update the underlying image. This is not deemed to be a critical issue (yet) from a security perspective, but we should make sure to clear this up when we get a chance. $ docker run -it --rm -e CLAIR_ADDR=http://clair.bits-infosec.broadinstitute.org:6060 -e CLAIR_OUTPUT=High -e CLAIR_THRESHOLD=10 -e DOCKER_USER=davidbernick -e DOCKER_PASSWORD='xxxxx' broadinstitute/klar broadinstitute/cromwell:dev; clair timeout 1m0s; docker timeout: 1m0s; no whitelist file; Analysing 10 layers; Got results from Clair API v1; Found 139 vulnerabilities; Unknown: 3; Negligible: 47; Low: 38; Medium: 44; High: 7. CVE-2017-12424: [High] ; Found in: shadow [1:4.4-4.1]; Fixed By: ; In shadow before 4.5, the newusers tool could be made to manipulate internal data structures in ways unintended by the authors. Malformed input may lead to crashes (with a buffer overflow or other memory corruption) or other unspecified behaviors. This crosses a privilege boundary in, for example, certain web-hosting environments in which a Control Panel allows an unprivileged user account to create subaccounts.; https://security-tracker.debian.org/tracker/CVE-2017-12424; -----------------------------------------; CVE-2018-13347: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; mpatch.c in Mercurial before 4.6.1 mishandles integer addition and subtraction, aka OVE-20180430-0002.; https://security-tracker.debian.org/tracker/CVE-2018-13347; -----------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; htt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4979:546,timeout,timeout,546,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979,2,['timeout'],['timeout']
Safety,"As predicted in the ticket, simply removing the swagger `default` allows this value to be exercised correctly from the UI.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5277:3,predict,predicted,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5277,1,['predict'],['predicted']
Safety,Asynced the standard backend execute/recover.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1947:37,recover,recover,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1947,1,['recover'],['recover']
Safety,"At least ""allows result reuse"" and ""results cloned"" (the latter being transformed from an fk to a boolean). Returning hashes might be helpful too for diagnosing why executions don't avoid.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/559:182,avoid,avoid,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/559,1,['avoid'],['avoid']
Safety,"At the moment there's a single Cromwell server underlying the CromIAM server, and all Cromwell traffic is directed there. Modify CromIAM to allow for a second Cromwell address (defaulting to the main one), and direct abort requests to that Cromwell. NB: This is an intermediate term hack, so err on the side of quick & pragmatic instead of perfect & beautiful",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4142:217,abort,abort,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4142,1,['abort'],['abort']
Safety,At this moment there is a way to submit jobs to a HPC with a command line that is executed. With drmaa this is also possible. The only problem is that with drmaa v1 you can only get status of jobs submitted in the same session. This means for recovering after a restart you must rely on command line methods like in the current implementation. Drmaa v2 have the possibility to track jobs outside it's session but there is almost no support for v2 yet. Here is the implementation inside queue:; https://github.com/broadgsa/gatk/tree/master/public/gatk-queue/src/main/scala/org/broadinstitute/gatk/queue/engine/drmaa,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1355:243,recover,recovering,243,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1355,1,['recover'],['recovering']
Safety,Atomic file copying to avoid partial inputs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1426:23,avoid,avoid,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1426,1,['avoid'],['avoid']
Safety,Attempts to unflakify the abort tests by making them fail reliably if they fail > 20% of the time.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3321:26,abort,abort,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3321,1,['abort'],['abort']
Safety,Automatic detection+localization of index files,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1412:10,detect,detection,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1412,1,['detect'],['detection']
Safety,Avoid ConfigFactory.load all over the place,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/796:0,Avoid,Avoid,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/796,1,['Avoid'],['Avoid']
Safety,Avoid hashing Scopes. Closes #1457,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1468:0,Avoid,Avoid,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1468,1,['Avoid'],['Avoid']
Safety,Avoid unnecessary IO,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3602:0,Avoid,Avoid,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3602,1,['Avoid'],['Avoid']
Safety,Avoid unnecessary token refreshing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1429:0,Avoid,Avoid,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1429,1,['Avoid'],['Avoid']
Safety,Avoiding copying input file for docker by mounting volume,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3447:0,Avoid,Avoiding,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3447,1,['Avoid'],['Avoiding']
Safety,"BCS appears to have already wired through a `timeout` runtime attribute. This would be valuable as an option in PAPIv2 as well, especially as we are encountering problems with non-terminating actions. See https://gatkforums.broadinstitute.org/gatk/discussion/comment/58454/#Comment_58454 for a motivating use case. The code to amend with a custom timeout is where we build the `Pipeline` for the request: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/GenomicsFactory.scala#L135",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4946:45,timeout,timeout,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4946,2,['timeout'],['timeout']
Safety,Backend Store performing recovery closes #751,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1241:25,recover,recovery,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1241,1,['recover'],['recovery']
Safety,"Backend: AWS Batch. Workflow: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/frankenstein.wdl. Input file: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/map-variantcall-hg38.json. Possibly related to #4412 but not sure as I don't see the same error message. When submitting a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-anothe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563:893,timeout,timeouts,893,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563,1,['timeout'],['timeouts']
Safety,"Based on a post I made on the openWDL slack; was asked to make a ticket here. ## quick summary; Cromwell handles `/` in strings inconsistently. In some cases, it is dropped without throwing an error, in other cases it will cause an error immediately. If the string is in the WDL file itself, womtool does not detect any issues with it but it will not be handled as expected as runtime. ## use case and how to reproduce; [goleft indexcov ](https://github.com/brentp/goleft/tree/master/indexcov#indexcov) defaults to this value for --excludePattern:; `""^chrEBV$|_random$|Un_|^HLA|_alt$|hap$""`. So I set `String excludePattern = ""^chrEBV$|_random$|Un_|^HLA|_alt$|hap$""` in my WDL. That passes miniwdl check and womtool. But... * Terra will accept `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$ `as a variable default or as hardcoded variable, but will handle it incorrectly -- it will not error, but it will be changed into `^chrEBV$|^NC|_random$|Un_|^HLA-|_alt$|hapd$`; * Terra will not accept `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$` as an input variable via JSON; it will fail to import; * Terra will not accept `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$` as an input variable if entered manually; it will throw token recognition error in the workflow menu and not allow you to submit; * Terra will accept the escaped version `^chrEBV$|^NC|_random$|Un_|^HLA\\-|_alt$|hap\\d$` as an input if entered manually or hardcoded, and will interpret it as `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$`. Only tested via Terra-Cromwell, as I was previously told local-Cromwell is a lower development priority. ## expected behavior; 1. A user inputting a string as a variable vs that exact same string being a hardcoded default should be handled the same way.; 2. If Cromwell is supposed to handle `/` by requiring they be escaped as `//`, that should be documented if it isn't already.; 3. womtool should throw a warning when it sees a hardcoded variable/default with a `/` inside of it, and that wa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7167:309,detect,detect,309,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7167,1,['detect'],['detect']
Safety,"Based on this [user error report](https://support.terra.bio/hc/en-us/community/posts/360043437191-Cromwell-WorkFlow-getting-aborted-intermittently-without-any-exception?page=1#community_comment_360005588172). Investigation is needed but at first glance:. * The job succeeds; * ~The workflow result copy is probably relatively long, given the size of output files~; * ~Cromwell's ""on shutdown"" logic is triggered too soon, and interrupts the result copy, which manifests as a workflow abort~; * The workflow apparently aborts shortly after the job succeeds. EDIT: Running in server mode didn't seem to help, so this is probably unrelated to the shutdown logic triggering too early, and more likely something else - an uncaught exception with the large output file perhaps?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4960:124,abort,aborted-intermittently-without-any-exception,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4960,3,['abort'],"['abort', 'aborted-intermittently-without-any-exception', 'aborts']"
Safety,Because sometimes things other than cromwell can cancel jobs. Also might make restarts after aborts a little more resilient in case of unexpected race conditions (not a guarantee TM),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2503:93,abort,aborts,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2503,1,['abort'],['aborts']
Safety,"Because writing to the call caching store and the job store is not atomic, the following chain of events is possible and not necessarily desirable:. - A job start; - A cache hit is found; - The outputs are copied; - The hashes / simpletons are written to the DB; - ** Cromwell Stops **: This is after the hashes are written successfully but before the EJEA had a chance to write the outputs to the job store and mark the job as complete.; - Cromwell starts; - The workflow is restarted; - The job is not found in the job store; - At this point the EJEA has a state to check if there are hashes existing for this job already. If there is, it disables call caching (so that the EJEA doesn't try to call cache to himself, and that we don't write to the hash store again - which would fail because of the unique index in the call cache table).; - However since we've disabled call caching we then proceed to try and recover the job, which fails because it was never run (since we found a cache hit the first time), and then falls back to running the job for reals. This is not great because this job already has all the outputs it needs, files have been copied already, but we run the job on top of it, which seems to increase the likelihood of having empty files at least locally when trying to read outputs and cause `cannot create an Int from """"` types of failures. Maybe a better way would be to re-use the outputs that have been written to the cache to make the job succeed and bypass all the rest. Relevant code in the EJEA: https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L153",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3074:912,recover,recover,912,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3074,1,['recover'],['recover']
Safety,Better testing of Abort endpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1396:18,Abort,Abort,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1396,1,['Abort'],['Abort']
Safety,"Better theory is https://github.com/broadinstitute/firecloud-develop/pull/1556. IMO the risk/reward for the upgrade no longer checks out, especially since we are in a bit of a crunch mode and relatively ill-equipped to deal with unexpected problems.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4701:88,risk,risk,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4701,1,['risk'],['risk']
Safety,"Binding"": {; ""prefix"": ""-somatic_min_total""; },; ""default"": 300,; ""id"": ""#purple-2.44.cwl/somatic_min_total""; },; {; ""type"": [; ""null"",; ""float""; ],; ""doc"": ""Proportion of somatic deviation to include in fitted purity score. Default 1.\n"",; ""inputBinding"": {; ""prefix"": ""-somatic_penalty_weight""; },; ""default"": 1,; ""id"": ""#purple-2.44.cwl/somatic_penalty_weight""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional location of somatic variant vcf to assist fitting in highly-diploid samples.\nSample name must match tumor parameter. GZ files supported.\n"",; ""inputBinding"": {; ""prefix"": ""-somatic_vcf""; },; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#purple-2.44.cwl/somatic_vcf""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional location of structural variant vcf for more accurate segmentation.\nGZ files supported.\n"",; ""inputBinding"": {; ""prefix"": ""-structural_vcf""; },; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#purple-2.44.cwl/structural_vcf""; },; {; ""type"": ""File"",; ""doc"": ""Optional location of failing structural variants that may be recovered.\nGZ files supported.\n"",; ""inputBinding"": {; ""prefix"": ""-sv_recovery_vcf""; },; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#purple-2.44.cwl/sv_recovery_vcf""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Number of threads\n"",; ""inputBinding"": {; ""prefix"": ""-threads""; },; ""default"": 2,; ""id"": ""#purple-2.44.cwl/threads""; },; {; ""type"": ""string"",; ""doc"": ""Name of the tumor sample. This should correspond to the value used in AMBER and COBALT.\n"",; ""inputBinding"": {; ""prefix"": ""-tumor""; },; ""id"": ""#purple-2.44.cwl/tumor""; },; {; ""type"": [; ""null"",; ""boolean""; ],; ""doc"": ""Tumor only mode. Disables somatic fitting.\n"",; ""inputBinding"": {; ""prefix"": ""-tumor_only""; },; ""default"": false,; ""id"": ""#purple-2.44.cwl/tumor_only""; }; ],; ""outputs"": [; {; ""type"": ""Directory"",; ""outputBinding"": {; ""glob"": ""$(inputs.output_dir)/""; },; ""id"": ""#purple-2.44.cwl/outdir""; }; ],; ""id"": ""#purple-2.44.cwl""; },; {; ""class"": ""Workflow"",; ""id"": ""#main"",; ""l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:36318,recover,recovered,36318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['recover'],['recovered']
Safety,Bonus: Increase timeout on SimpleWorkflowActorSpec that sometimes takes longer than 10s from call-start to end-of-workflow.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4313:16,timeout,timeout,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4313,1,['timeout'],['timeout']
Safety,Bring abort to HtCondor. Closes #1402,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1403:6,abort,abort,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1403,1,['abort'],['abort']
Safety,Bug fix: subworkflow rows should not be included in the metadata safety limit count when subworkflows are not being expanded.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5788:65,safe,safety,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5788,1,['safe'],['safety']
Safety,Bump file read timeout to maybe reduce test failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4036:15,timeout,timeout,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4036,1,['timeout'],['timeout']
Safety,"CI clone of ""Implement recoverAsync for AWS backend"" #5216",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5291:23,recover,recoverAsync,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5291,1,['recover'],['recoverAsync']
Safety,CPU on Cromwell machines pegged and unable to recover without restart,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4093:46,recover,recover,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4093,1,['recover'],['recover']
Safety,"CWL was treating output glob strings as if they were filenames, and thus was not returning the filename that Cromwell expects, namely `glob-${md5(fileName)}.list`. The implementation boils down to `OutputEvaluator` trying to detect whether the output of the expression is a glob. If it is _is_ a glob, it changes the output to be the filename as listed above.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2828:225,detect,detect,225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2828,1,['detect'],['detect']
Safety,Caching actor: Don't emit fatal timeout messages that must be ignored,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4085:32,timeout,timeout,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4085,1,['timeout'],['timeout']
Safety,Caching timeout on S3,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977:8,timeout,timeout,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977,1,['timeout'],['timeout']
Safety,"Can be reproduced using the following workflow. ```wdl; version 1.0. task crash {; command <<<; kill -9 $$; >>>; }. workflow crash {; call crash ; }. ```; We use a configuration with the following values:; ```HOCON; backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 120; default-runtime-attributes {; maxRetries: 2; }; }; }; }; }; workflow-options {; workflow-failure-mode = ""ContinueWhilePossible""; }; ```; On Cromwell 37 the workflow will be run. Jobs will be killed and retried.; On Cromwell 39, the retries will not happen any more.; This is very annoying, as our cluster kills jobs that exceed the memory limit, and some java based jobs seem to have random memory spikes. Having only 1 try means basically that a workflow with 50-100 jobs will usually fail, unless we give some jobs an insane memory parameter. This is probably caused by the refactoring in:; https://github.com/broadinstitute/cromwell/pull/4654/files; EDIT: This statement was not meant to put a blame on someone. I understand that code needs to be refactored at times and that bugs can creep in. I will look if I can fix the issue myself but maybe @cjllanwarne can also have a quick look? That would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4998:369,timeout,timeout-seconds,369,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4998,1,['timeout'],['timeout-seconds']
Safety,"Can you add more details? Is this at the REST endpoint level, or an internal thing? How would the user experience this issue?. Do you mean/ for example. If you abort a workflow/call running on JES via the REST endpoint, Cromwell will return a 200 OK indicating ""yes, I will try to abort this"" as opposed to 200 OK ""yes, I have aborted this"". If it's at the REST endpoint level the ""Request received"" sounds right to me. The user shouldn't have to wait for the abort to occur, and for some backends it's a best-efforts anyway (like JES' cancel operation command)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1409:160,abort,abort,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1409,4,['abort'],"['abort', 'aborted']"
Safety,"Centaur has its own timeouts before it gives up. So does `test.inc.sh`. Setting the value in `test.inc.sh` should also set the value for centaur. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/src/main/resources/reference.conf#L37-L38. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/bin/test.inc.sh#L130-L136. Currently values for centaur are set through multiple `-Dkey=value` settings inside `test_cromwel.sh`. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/test_cromwell.sh#L127-L134. A couple options among others:; - This can be another `getopts` argument wired into `test_cromwell.sh`; - This could be an environment variable that overrides a default, as is currently used for setting database connection info; https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/resources/build_application.inc.conf#L15-L28. A/C:; - Tests timeout at approximately the same duration in the centaur executable and the heartbeat generated by `test.inc.sh`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3874:20,timeout,timeouts,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3874,2,['timeout'],"['timeout', 'timeouts']"
Safety,Centaur tests poorly assess the format of output files in the metadata. To avoid regressions it would be preferable to have better coverage of this.; The main issue comes from the fact that files path are dynamic and hard to validate with the static test definitions centaur has.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3160:75,avoid,avoid,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3160,1,['avoid'],['avoid']
Safety,"Changed ""webservice.timeout"" to ""webservice.binding-timeout"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1859:20,timeout,timeout,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1859,2,['timeout'],['timeout']
Safety,Clarify exit-code-timeout-seconds docs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4905:18,timeout,timeout-seconds,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4905,1,['timeout'],['timeout-seconds']
Safety,"Closed the previous Pull Request (#841) since I ended up moving things to a new branch, but I'll address previous comments here:. Q: ""Is the system section [of application.conf] mandatory? It looks like this would throw if it's missing"" ; A: I don't think the section is mandatory in 0.19_hotfix, but looking back, it is mandatory in develop, something that needs a patch surely. Q: ""How can this [val serverMode = CromwellServer.isServerMode] be false ?"" ; A: You're right, it wasn't wired to be false ever in my previous PR. I've since changed it, please review it!. Documentation for this config change has also been updated and ready for review. Question for the reviewers: I arbitrarily moved the config to the backend stanza of the config file, since the system stanza doesn't exist anymore and the ""abortJobsOnTerminate"" config is also within the backends stanza. Is there a better place for it?. **MainSpec tests failing--it must be my changes, checking that out now, but hopefully the remaining changes can be examined in parallel**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/889:806,abort,abortJobsOnTerminate,806,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/889,1,['abort'],['abortJobsOnTerminate']
Safety,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4567:84,safe,safety,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567,1,['safe'],['safety']
Safety,Closes #4908. Combines the changes in #4909 (sanity check in case this happens again) and #4923 (should prevent it happening in the first place) and adds a test case for one scenario which was found to cause this problem,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4924:45,sanity check,sanity check,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4924,1,['sanity check'],['sanity check']
Safety,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run does-not-exist.wdl; ```; The output shows a stacktrace and then hangs. It should likely exit with a non-zero status, following the convention of other command-line tools and allowing for failure detection.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4061:251,detect,detection,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4061,1,['detect'],['detection']
Safety,Configurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(Stand,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:7835,recover,recover,7835,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recover']
Safety,"Copy of #5441. This adds a mechanism of gzipping the list of output files in the AWS backend to avoid the container override size limit. This mechanism was already in place for the inputs, this will simply utilize it for the outputs as well. I tried testing it, but the new `proxy` file doesn't seem to have been used. I guess the docker image needs to be updated for that. Does anyone have any ideas on how to do that? I couldn't find where this docker image gets used in cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5447:96,avoid,avoid,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447,1,['avoid'],['avoid']
Safety,"Correct understanding of ask timeouts, remove dead code.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/314:29,timeout,timeouts,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/314,1,['timeout'],['timeouts']
Safety,"Create a Python program which can interact with Cromwell but provide batching capabilities, submitting a batch of workflows but then being able to manage them as a group, e.g. getting status, aborting, etc.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2171:192,abort,aborting,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2171,1,['abort'],['aborting']
Safety,"Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1290:349,avoid,avoid,349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1290,1,['avoid'],['avoid']
Safety,CromIAM support for a Cromwell abort server.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4263:31,abort,abort,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4263,1,['abort'],['abort']
Safety,Cromwell .20 returns 500 on abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253:28,abort,abort,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253,1,['abort'],['abort']
Safety,"Cromwell 37 errors when the backend submit configuration contains an expression like:; `${""-l h_vmem="" + memory + ""G""}`: ; <details>; <summary> error message </summary>; <pre><code>; cromwell.core.CromwellFatalException: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:582,recover,recoverWith,582,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,1,['recover'],['recoverWith']
Safety,Cromwell 404ing 'abort' on batched workflows,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4497:17,abort,abort,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4497,1,['abort'],['abort']
Safety,Cromwell Didn't Detect Finished Jobs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:16,Detect,Detect,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Detect'],['Detect']
Safety,Cromwell Version: 37 (I just saw the reversion. is it safe to revert from 37 to 36.1?). It looks like the only labels that are added to the PAPI2 VMs are the `cromwell-id` and the `task-name`. None of the custom labels are being added to the VM. If this is one of the issues that was solved in v36.1 Then I will gladly revert so long as it is safe!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4692:54,safe,safe,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4692,2,['safe'],['safe']
Safety,"Cromwell has REST api to query the workflow status,'RUNNING','Aborted' etc. Does Cromwell has any event system to subscribe these events? With these events, outside apps can monitor the workflow in real time.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6756:62,Abort,Aborted,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6756,1,['Abort'],['Aborted']
Safety,Cromwell throws `java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor` exception when it tries to recover a running job. Stacktrace:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:208,recover,recover,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,2,"['Recover', 'recover']","['Recover', 'recover']"
Safety,"Cromwell version: 30-9a7de06. Minimized from one of our WDLs:; ```wdl; workflow Test {; Boolean do; Int n. if (do) {; call Optional; }. scatter (i in range(n)) {; call Scattered; }. call Gather {; input:; # HERE: select_first returns String, and Scattered.out is an Array[String]; ins = if defined(Optional.out) then select_first([Optional.out]) else Scattered.out; }; output {; Gather.out; }; }. task Optional {; command {; echo ""Hey!""; }; output {; String out = read_string(stdout()); }; }. task Scattered {; command {; echo ""Hello!""; }; output {; String out = read_string(stdout()); }; }. task Gather {; Array[String] ins. command {; cat ${write_lines(ins)}; }; output {; String out = read_string(stdout()); }; }; ```. This WDL runs successfully, but in code review I noticed the weird type mismatch between the branches. I asked @cjllanwarne about it and he thought it was an old ""feature"" that had been purged to avoid bugs / confusion. I'd expect something like this to be rejected.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3478:918,avoid,avoid,918,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3478,1,['avoid'],['avoid']
Safety,"Cromwell's requester pays logic works by trying to perform GCS operations without specifying a project to bill. If the operation is successful, great, all done. If the operation is not successful and the error message looks like a requester pays error, the operation is retried with the project to bill specified. IIRC this system is in place because always specifying the project to bill resulted in the project being billed even if the bucket was not requester pays. It's unfortunate this logic needs to be so clunky when GCS does have the concept of [provisional user projects](https://developers.google.com/resources/api-libraries/documentation/storage/v1/java/latest/com/google/api/services/storage/Storage.Buckets.GetIamPolicy.html#setProvisionalUserProject-java.lang.String-) but this concept is not supported in the Google Storage API used by the GCS filesystem. Anyway the ""is this requester pays"" logic used to look for exact matches to an error message string, i.e. exactly this:; ```; Bucket is requester pays bucket but no user project provided.; ```; However with increasing probability (the `requester_pays_engine_functions` Centaur test fails about 50% of the time with the baseline Cromwell code) we are seeing error messages that actually look like this:; ```; 400 Bad Request; POST https://storage.googleapis.com/upload/storage/v1/b/cromwell_bucket_with_requester_pays/o?projection=full&uploadType=multipart; {; ""error"": {; ""code"": 400,; ""message"": ""Bucket is requester pays bucket but no user project provided."",; ""errors"": [; {; ""message"": ""Bucket is requester pays bucket but no user project provided."",; ""domain"": ""global"",; ""reason"": ""required""; }; ]; }; }; ```. The changes here accommodate either version of the error message with a `null`-safe `contains` check courtesy Apache StringUtils.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6556:1766,safe,safe,1766,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6556,1,['safe'],['safe']
Safety,"Current known issues (feel free to add/remove/edit):; - [ ] Today the existing BatchCompute cluster consists of 1 pre-allocated machine slowing down parallel CI tests (run `bcs c` to see the current size). `OnDemand` clusters are available but take time to spin up the VM instance even [without docker](https://github.com/broadinstitute/cromwell/issues/3518).; - [ ] Like all integration tests there may be intermittent failures/timeouts connecting to external resources. While retry support could be copied out of the PAPI backends and into each backend, once [retries are available across all backends](https://github.com/broadinstitute/cromwell/issues/3161) the CI should be setup to retry failures.; - [ ] The BCS backend is leaking _some_ finished and failed jobs, hitting the job quota after a day or two. It's possible a [nightly cron job](https://github.com/broadinstitute/cromwell/issues/3555) could clean out the leaked jobs but for users this issue should really be fixed elsewhere.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3554:429,timeout,timeouts,429,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3554,1,['timeout'],['timeouts']
Safety,"Currently WorkflowActor aborts BJEAs directly. A whole bunch of much nicerness would occur if it aborted the EJEA and let that ripple down the abort to the BJEA (or something else, depending on its FSM state)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1504:24,abort,aborts,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1504,3,['abort'],"['abort', 'aborted', 'aborts']"
Safety,"Currently when Cromwell starts it updates all `Running` workflows to `RestartableRunning` and `Aborting` to `RestartableAborting` so that it knows which workflows were already running and need to be restarted.; If multiple Cromwells are started against the same DB, they can't all keep changing all workflow statuses as they start.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3347:95,Abort,Aborting,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3347,1,['Abort'],['Aborting']
Safety,"Currently when a user issues a request to abort a workflow, the process is purely in-memory. This means that there's no way to manually trigger an abort other than the web endpoint. Instead, do something similar to the workflow store. Record the request in a table, and have cromwell monitor that table looking for workflows it is running, and when it sees such a thing use that to trigger the abort. . It is **not** a bad state if there's a workflow in the table which Cromwell doesn't know about. Make sure any updates to this table are [locked](https://github.com/broadinstitute/cromwell/issues/3342)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3344:42,abort,abort,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3344,3,['abort'],['abort']
Safety,"Currently, there is no library function to flatten an array of array of files (`Array[Array[File]]`). A scatter, where each task call produces an array of files, is a natural way of ending up with such a structure. In order to flatten this array, you can write a task that takes the it as an argument, and manipulate it with python code. However, this task will also download all the files, taking significant time and disk space. To work around this, you can coerce the files into strings (their paths), and manipulate the paths. . You can see an example [here](https://github.com/HumanCellAtlas/skylab/blob/master/10x/count/count.wdl#L195). The `chunk_reads_join` task flattens the `fastq_chunks` file array, which is coerced into an `Array[Array[String]]`. In order to avoid this circuitous implementation, this pull requests implements a generic flatten operation for ragged array types.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2825:772,avoid,avoid,772,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2825,1,['avoid'],['avoid']
Safety,"Currently, to process a glob on JES, Cromwell does an `ls` of the google cloud storage location. The problem with this is that ls is eventually consistent, which leads to bugs like #843 . JES has added a feature (#28858407) where they now return the number of files that matched the glob as part of their metadata. These appear as events of the form. `{Description: ""copied 3 file(s) to \""gs://my-bucket/out/\"""",; StartTime: {Seconds: 1470063955,; Nanos: 748725437}},`. In Cromwell, when processing these globs, we should poll (with adjustable maximum timeout) for this number of files to appear via the ls. If they do not appear after the timeout, the task should fail with an appropriate error message. If we are processing globs on GCS and NOT using JES, the best we can do is just grab and go via the ls (as we are doing currently for JES).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1395:552,timeout,timeout,552,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1395,2,['timeout'],['timeout']
Safety,"Currently, we are storing a relatively not-so-useful state data in the WMA. It is only used in case of abort to get the WA corresponding to the abortable workflow ID. The current scheme of things can be replaced by a relatively simple actor selection method for abort, which may have varied opinions amongst developers but I believe it provides a more bang for buck.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/606:103,abort,abort,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/606,3,['abort'],"['abort', 'abortable']"
Safety,Cut out KV Store in JES abort to sidestep a race condition. Closes #1253,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1410:24,abort,abort,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1410,1,['abort'],['abort']
Safety,DSDEEPB-852 Provide web endpoint to abort a workflow.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/114:36,abort,abort,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/114,1,['abort'],['abort']
Safety,Data migration for restart/recover,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1119:27,recover,recover,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1119,1,['recover'],['recover']
Safety,Database-driven abort.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4401:16,abort,abort,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4401,1,['abort'],['abort']
Safety,"Dear developers,. During testing I ran into the problem that the `HashPathStrategy` does not include the last modified date of the file. It assumes: ""if the path is there, it is the same file"". This is not necessarily the case. Files can be modified or replaced.Therefore the current `HashPathStrategy` is a big liability when trying to get reproducible results. By adding a ""last modified date"" to the `HashPathStrategy` this will ensure that nothing has happened to the file from the user or system side. This of course is not as safe as the `HashFileStrategy` since it does not protect against filesystem or hardware errors, but it provides a lot more safety compared to the current `HashPathStrategy`. ; This is also how Snakemake checks if files are the same and it works quite well. Alternatively there could be an option in the Configfile that allows you to set this behaviour. Please let me know what you think of this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4405:532,safe,safe,532,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4405,2,['safe'],"['safe', 'safety']"
Safety,Default the SWRA to abortJobsOnTerminate = true.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1664:20,abort,abortJobsOnTerminate,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1664,1,['abort'],['abortJobsOnTerminate']
Safety,Delete zipped imports in the end of test to avoid `NoSuchFileException` on retries [BA-6136],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5319:44,avoid,avoid,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5319,1,['avoid'],['avoid']
Safety,"Description:; * Adds a statistics recorder into the WriteMetadataActor to count rows being sent per workflow. If the counter goes about a given limit, we get an alert back which the write actor converts into a log message. . Food for reviewers' thoughts:. * Does the set of configuration options make sense?; * And what might be sensible default values?; * I'm not a huge fan of how subworkflows' parents are detected here. Is there a more direct way to find out a parent?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6641:409,detect,detected,409,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6641,1,['detect'],['detected']
Safety,Detect error 500 in JES backend and retry job,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1450:0,Detect,Detect,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1450,1,['Detect'],['Detect']
Safety,Detect incorrect Docker Machine usage,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/362:0,Detect,Detect,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/362,1,['Detect'],['Detect']
Safety,Detect stalled workflows,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4486:0,Detect,Detect,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4486,1,['Detect'],['Detect']
Safety,Do not become catatonic on abort.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1382:27,abort,abort,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1382,1,['abort'],['abort']
Safety,Docs HPCSlurmWithLocalScratch.md redundant?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7357:33,redund,redundant,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7357,1,['redund'],['redundant']
Safety,Document a general-purpose recovery process,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4991:27,recover,recovery,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4991,1,['recover'],['recovery']
Safety,"Doesn't close #751 but stabs in its general direction. There is no persistent DB here, I replaced the DB-based approach in `KeyValueServiceActor` with a simple `Map` since the DB-based approach always failed to write due to referential integrity constraints that aren't going to be fixed (no `EXECUTION` or `WORKFLOW_EXECUTION` data for FK constraints). Ruchi and/or I will have a follow-on PR to put a DB table behind this. Also fixed ""abort"" to message the KV service instead of metadata service, removed now-unused `EXECUTION_INFO`-related methods, other assorted cleanup.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1197:437,abort,abort,437,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1197,1,['abort'],['abort']
Safety,Don't abort prematurely on workflow restart,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2820:6,abort,abort,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2820,1,['abort'],['abort']
Safety,"Don't flog me yet, I will clean this up. There's about a week of cruft from trying various things in here... _Diagram Disclaimer: the below diagrams are now obsolete!_. FAQ:. Q: Why did you change stuff unrelated to aborts?; A: Probably because I had a hard time debugging and it facilitated debugging.; A: Bug fixes; A: Probably no good reason, and it might just be cruft changes. This is how workflow aborts work for the LOCAL backend!. ![workflow aborts local - new page](https://cloud.githubusercontent.com/assets/58551/15552622/4698923c-2289-11e6-9435-3860675d4bf0.png). This is how workflow aborts work for the JES backend!. ![workflow aborts jes - new page](https://cloud.githubusercontent.com/assets/58551/15552644/620972d4-2289-11e6-900f-d2998ebe5544.png). :-D",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/893:216,abort,aborts,216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/893,5,['abort'],['aborts']
Safety,"During CromIAM Perf testing, strange behavior occurred where when querying metadata for a workflow right after aborting it yielded a non empty value in the `failures` field, which later disappeared.; Below is an example metadata with the failure:. ```; {; ""calls"": {; ""wf_hello.hello"": [; {; ""preemptible"": false,; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello-stdout.log"",; ""commandLine"": ""sleep 60 \necho \""Hello World! Welcome to Cromwell . . . on Google Cloud!\"""",; ""shardIndex"": -1,; ""jes"": {; ""executionBucket"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca"",; ""endpointUrl"": ""https://genomics.googleapis.com/"",; ""googleProject"": ""broad-dsde-alpha""; },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""ubuntu:latest"",; ""maxRetries"": ""0"",; ""cpu"": ""1"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zones"": ""us-central1-b"",; ""memoryMin"": ""2.048 GB"",; ""memory"": ""2.048 GB""; },; ""callCaching"": {; ""allowResultReuse"": false,; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ""inputs"": {; ""addressee"": ""World""; },; ""backendLabels"": {; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663"",; ""wdl-task-name"": ""hello""; },; ""labels"": {; ""wdl-task-name"": ""hello"",; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663""; },; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""message"": ""java.lang.IllegalArgumentException: Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""backend"": ""JES"",; ""end"": ""2018-12-11T16:07:04.207Z"",; ""stderr"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello-stderr.log"",; ""callRoot"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4484:111,abort,aborting,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4484,1,['abort'],['aborting']
Safety,"During code review for #1836 @cjllanwarne noted that `processSource` in what is currently named `WorkflowStoreActor` and most likely `WorfklowStoreSubmitActor` by the time this is acted upon looked suspicious as we had (we think) intended json validation to not happen until later and a workflow ID would always be handed back to the user. Further, the failed Future doesn't appear to be getting handed back to the API at all (I think), which would lead to a timeout response. Further since the sources are being processed monadically it is possible for a user to have multiple borked files but only the first will be reported (if we were reporting). Check into what's up here - either don't perform this check on submission or ensure that appropriate error messages are handed back",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1882:459,timeout,timeout,459,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1882,1,['timeout'],['timeout']
Safety,"EAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:37:14,145 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(60ec6228)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:23,720 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(a442dc1c)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:37:31,421 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17bed42e)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:40,098 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(e9851ba1)]: Abort received. Aborting 3 EJEAs; `; Cromwell hash: 192ea6025613df967d60e9e975693144035379d7",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:4825,Abort,Abort,4825,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,12,['Abort'],"['Abort', 'Aborting']"
Safety,"EDIT: to safe yourself some data entry, you can use branch [cjl_initial_work_dir_requirement_4](https://github.com/broadinstitute/cromwell/tree/cjl_initial_work_dir_requirement_4) as an entry point with the centaur test and a Spec already added. This seems to be a pretty common pattern but relies on `JSON.stringify(inputs)` working in our expression evaluator:; ```yml; # A common use case: stringy the inputs JSON and provide that file as another input file. cwlVersion: v1.0; $graph:; - id: stringify_inputs; class: CommandLineTool; baseCommand: ['grep', 'number', 'inputs.json']; requirements:; - class: DockerRequirement; dockerPull: ""python:3.5.0""; - class: InitialWorkDirRequirement; listing:; - entryname: 'inputs.json'; entry: $(JSON.stringify(inputs)). stdout: ""number_field"". # TODO CWL: Set the types more appropriately (depends on issue #3059); inputs:; - id: number; type: string; default: 27; - id: str; type: string; default: wooooo; - id: boolean; type: string; default: True; outputs:; - id: number_field_output; type: string; outputBinding:; glob: number_field; loadContents: true; outputEval: $(self[0].contents.trim()); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3090:9,safe,safe,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3090,1,['safe'],['safe']
Safety,"Enable Cromwell to emit statsd messages to a configurable host. It'd be awesome if this whole thing could be configurable on/off but if that's a pain it's not a big deal. The default host could be a non-existent UDP thing or something. The main key for this is the infrastructure, but some initial things to instrument. Things with lifetime counts are for creating a time series, e.g. ""how many of X happened in the last N time units"". - Lifetime count of submitted workflows; - Lifetime count of completed workflows; - Lifetime count of aborted workflows; - Current count of both pending & running workflows; - Lifetime count of retry events, e.g. GCS & PAPI; - Probably best broken up so GCS & PAPI separate if possible",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2467:538,abort,aborted,538,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2467,1,['abort'],['aborted']
Safety,"Ensure GCS file systems use custom configuration.; When an exception/timeout occurs during asyncHashing, report it as a failure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2512:69,timeout,timeout,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2512,1,['timeout'],['timeout']
Safety,"Error message: A timeout occurred waiting for a future to complete. Queried 100 times, sleeping 100 milliseconds between each query. tc: ServicesStore should not deadlock. https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/566/. Update 10/22; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/708/. Update 10/28:; https://fc-jenkins.dsp-techops.broadinstitute.org/view/Testing/view/Test%20Runners/job/cromwell-test-runner/831/. Update 11/03:; https://fc-jenkins.dsp-techops.broadinstitute.org/view/Testing/view/Test%20Runners/job/cromwell-test-runner/1003/. Update 11/06:; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1076/. Update 11/09:; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1166/. Further:; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1337; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1422; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1445; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1489; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1525; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1590",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328:17,timeout,timeout,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328,1,['timeout'],['timeout']
Safety,"Exec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:17000,abort,abort,17000,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"Extending mcovarr's work in #6366 . Big shoutout to mcovarr!!!. [Per @mbookman]; This pull request is an initial update to address:. CROM-6718: FR: Add flag for minimizing chance of GCP cross-region network egress charges being incurred. This PR specifically focuses on the risks of egress charges incurred due to call caching. The framing of the approach here, which is a bit broader than originally noted in CROM-6718, is:; Make call caching location-aware, prioritizing copies that minimize egress charges.; Add a workflow option enabling control of what egress charges can be incurred for call cache copying.; The new workflow option would be:. call_cache_egress: [none, continental, global]. where the values affect whether call cache copies can incur egress charges:; none: only within-region copies are allowed, which generate no egress charges; continental: within content copies are allowed; within-content copies have reduced costs, such as $0.01 / GB in the US; global: copies across all regions are allowed. Cross-content egress charges can be much higher (ranging from $0.08 / GB up to $0.23 / GB). ### CURRENT STATUS OF PR:; With the changes in this PR, Cromwell successfully checks the location of the source and destination file to be copied, compares the location, and makes a decision of whether or not it should be copied based on the call_cache_egress option. If it should be copied, the files are copied as normal. If it should not be copied, the cache attempt fails and the workflow runs instead.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6369:274,risk,risks,274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6369,2,['risk'],['risks']
Safety,"First implementation of a pluggable LocalBackend. This is more a light basic implementation and a starting point to iterate over.; What is implemented : ; - Support for non-docker jobs; - Support for docker jobs; - Support for ""ContinueOnReturnCode"" ""FailOnStderr"" and ""docker"" runtime attributes; - Engine functions; - Abort. Things to think about:; - How to share code between backends ? runtime attributes validation, engine functions, shared filesystem code.. ; - Testing. Note: some code is duplicated from the engine as it's still used by the current non-PBE implementation. Eventually this will replace all local backend code in the engine. Currently adding more tests for ; - [x] abort. ~~\- [ ] engine functions~~; - [x] input localization; - [x] expression evaluation; - [x] coercion ; - [x] scatter",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/712:320,Abort,Abort,320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/712,2,"['Abort', 'abort']","['Abort', 'abort']"
Safety,Fix abort.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/379:4,abort,abort,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/379,1,['abort'],['abort']
Safety,Fix metadata count safety limit when not expanding subwfs [BA-6576],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5788:19,safe,safety,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5788,1,['safe'],['safety']
Safety,Fix recover on restart [WX-927],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7498:4,recover,recover,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7498,1,['recover'],['recover']
Safety,Fix typo in docs about Exit code timeout,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4289:33,timeout,timeout,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4289,1,['timeout'],['timeout']
Safety,Fix-up abort wiring,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1414:7,abort,abort,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1414,1,['abort'],['abort']
Safety,Fixed timeout error during mysql testing.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/398:6,timeout,timeout,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/398,1,['timeout'],['timeout']
Safety,Fixing broken abort functionality [0.18],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/504:14,abort,abort,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/504,1,['abort'],['abort']
Safety,Fixing broken abort functionality [develop],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/503:14,abort,abort,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/503,1,['abort'],['abort']
Safety,Fixing https://github.com/broadinstitute/cromwell/issues/4050. While doing this I notice that this check alive command is not used at all. First did a restructure of the statuses and now there is also a status Running. - [x] Add timeout on `WaitingForReturnCode` step; - [x] Make timeout a config value. Meanwhile please give already feed on this of course ;),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112:229,timeout,timeout,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112,2,['timeout'],['timeout']
Safety,Fixing immediate issue of database connection pool timeouts,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/363:51,timeout,timeouts,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/363,1,['timeout'],['timeouts']
Safety,Fixing two bugs in syntax error detection,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/109:32,detect,detection,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/109,1,['detect'],['detection']
Safety,Flaky Test: abort a workflow mid run and restart immediately,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3392:12,abort,abort,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3392,1,['abort'],['abort']
Safety,"Follow up on https://github.com/broadinstitute/cromwell/pull/4112. This will reduce the load on the JVM a lot. I did indeed a stress test on our system with 50.000 async qsub/qstat jobs but this was outside the jvm. Inside the jvm this ends up in blocking threads to cromwell. When the timeout is set to 120 seconds, `isAlive` will only run once each 120 seconds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4220:286,timeout,timeout,286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4220,1,['timeout'],['timeout']
Safety,"From a quick reading of the parent `WorkflowManagerActor` code it appears the default supervision strategy with ""restart on generic Exception"" is being used. Simply restarting a crashed `WorkflowActor` FSM appears to put it back into its initial `WorkflowUnstartedState` where it wouldn't do anything to progress a workflow until it receives a `StartWorkflowCommand` which is not being re-sent. So it looks like this would create a zombie workflow, though it does appear to be abortable.; ```; ERROR akka.actor.OneForOneStrategy - Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; java.lang.RuntimeException: Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials(GoogleAuthMode.scala:175); 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials$(GoogleAuthMode.scala:173); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.validateCredentials(GoogleAuthMode.scala:237); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:250); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:237); 	at cromwell.filesystems.drs.DrsPathBuilderFactory.withOptions(DrsPathBuilderFactory.scala:86); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.val",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4916:477,abort,abortable,477,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916,1,['abort'],['abortable']
Safety,"From forum post https://gatkforums.broadinstitute.org/wdl/discussion/12361/continue-on-sigterm-code#latest. The situation:; * Local backend; * The python script spawns a monitor which will SIGTERM it when the task completes; * The `128+SIGTERM` exit code was specified as valid in the runtime attributes; * However, Cromwell has already assumed that the job was aborted before it checks against the `continueOnReturnCode` values, the workflow fails instead of continuing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3896:362,abort,aborted,362,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3896,1,['abort'],['aborted']
Safety,Fully add or remove 403 status for abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2415:35,abort,abort,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2415,1,['abort'],['abort']
Safety,GCP Batch backend not recovering workflows on restart,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:22,recover,recovering,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recovering']
Safety,GCS storage appears to be created redundantly,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1437:34,redund,redundantly,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1437,1,['redund'],['redundantly']
Safety,"Got a centaur failure with `Failed to upload auth file` caused by the following exception, not considered retryable:. ```; 017-04-18 21:11:49,413 cromwell-system-akka.dispatchers.engine-dispatcher-64 ERROR - WorkflowManagerActor Workflow 6e23463e-3fc6-4b18-aeb0-fc7c920cd758 failed (during InitializingWorkflowState): Failed to upload authentication file; java.io.IOException: Failed to upload authentication file; 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.applyOrElse(JesInitializationActor.scala:63); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.applyOrElse(JesInitializationActor.scala:62); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.core.CromwellFatalException: com.google.cloud.storage.StorageExc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183:692,recover,recoverWith,692,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183,2,['recover'],['recoverWith']
Safety,HOTFIX: Forcibly abort workflows stuck in aborting for longer than 10 minutes…,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/986:17,abort,abort,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/986,2,['abort'],"['abort', 'aborting']"
Safety,"HPC; # Singularity+Slurm: and an example on Slurm; # udocker: another rootless container solution; # udocker+slurm: also exemplified on slurm; # HtCondor: workload manager at UW-Madison; # LSF: the Platform Load Sharing Facility backend; # SGE: Sun Grid Engine; # SLURM: workload manager. # Note that these other backend examples will need tweaking and configuration.; # Please open an issue https://www.github.com/broadinstitute/cromwell if you have any questions; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions"". concurrent-job-limit = 10; # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; ## Warning: If set, Cromwell will run 'check-alive' for every job at this interval; exit-code-timeout-seconds = 360; filesystems {; local {; localization: [; # soft link does not work for docker with --contain. Hard links won't work; # across file systems; ""copy"", ""hard-link"", ""soft-link""; ]; caching {; duplication-strategy: [""copy"", ""hard-link"", ""soft-link""]; hashing-strategy: ""file""; }; }; }. #; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 3; Int requested_memory_mb_per_core = 8000; Int memory_mb = 40000; String? docker; String? partition; String? account; String? IMAGE; """""". submit = """"""; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${out} \; --error=${err} \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""/b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:6590,timeout,timeout-seconds,6590,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['timeout'],['timeout-seconds']
Safety,Have Workflow Actor call Abort to Lifecycle Actors,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/671:25,Abort,Abort,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/671,1,['Abort'],['Abort']
Safety,"Hello I am trying to re-use an existing workflow for Mutect2 available here: https://app.terra.bio/#workspaces/terra-outreach/CHIP-Detection-Mutect2 to run on SLURM with Singularity configuration. There are multiple steps similar to Mutect2 public workflow available here: https://github.com/broadinstitute/gatk/blob/master/scripts/mutect2_wdl/mutect2.wdl , but still attaching the modified WDL with additional steps. . So when we run this with the given configuration using the following; export SINGULARITY_CACHEDIR=$PWD/singularity_cache; export SINGULARITY_TMPDIR=$PWD/tmpdir; module load singularity; rm -rf nohup.out && nohup java -Dconfig.file=$PWD/cromwell_singularity.conf -jar $PWD/cromwell-84.jar run $PWD/mutect2_modified.wdl --inputs $PWD/inputs.json &. The issue is that the first step of splitting intervals runs fine, but as it starts mutect2, it starts copying of the complete execution directory making here is the directory structure. cromwell-executions/; └── Mutect2; └── e5769b79-5e02-44a5-a4f8-38745e152beb; ├── call-M2; │ └── shard-0; │ ├── execution; │ └── inputs; │ ├── -1816294717; │ ├── 1855713868; │ │ └── run_cromwell_only.tmp; │ │ └── cromwell-executions; │ │ └── Mutect2; │ │ └── e5769b79-5e02-44a5-a4f8-38745e152beb; │ ├── 2035192126; │ └── 891763929; └── call-SplitIntervals; ├── execution; │ ├── glob-0fc990c5ca95eebc97c4c204e3e303e1; │ └── interval-files; ├── inputs; │ └── -1816294717; └── tmp.c9d96672. As you can see that run_cromwell_only.tmp is being made and that happens to fall in an endless loop and eventually, it errors stating the file name is too long to copy. Can you help me how to avoid this behavior of making circular paths when copying files for execution? Also, note it does not happen in the first step of SplitIntervals but happens in the Mutect2 call. [mutect2_gatk.wdl.txt](https://github.com/broadinstitute/cromwell/files/9813528/mutect2_gatk.wdl.txt); [cromwell_singularity.conf.txt](https://github.com/broadinstitute/cromwell/files/981352",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6934:131,Detect,Detection-,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6934,1,['Detect'],['Detection-']
Safety,"Hello, I am running Cromwell 36 configured with the GCS/JES backend to run jobs on GCP. When running massive batches of workflows, I frequently encounter the IP-address quota from Google. To avoid this, I've reconfigured my default VPC to allow private google access (see #1325). I've added the following to my Cromwell configuration (other unrelated configuration entries removed):; ```; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; default-runtime-attributes {; noAddress: true; }; }; }; }; }; ```. This appears to have the desired effect, as my instances are now launching without an external IP, however, the jobs end up failing because docker cannot fetch the image `stedolan/jq` (as it resides on docker hub). Is there a way to configure Cromwell to use a different image for that pipeline action?. I could reconfigure the VPC to allow access to docker(hub), but that would require connecting a NAT instance which would increase the cost of using Cromwell. ---. Edit: Cromwell 36. Sorry!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676:191,avoid,avoid,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676,1,['avoid'],['avoid']
Safety,"Hi Guys,. This is more of a question/request than a bug report. Apologies if this is not the place to ask. Im trying to run Cromwell with an AWS backend. A number of our workflows make extensive use of very large reference files. To avoid localising the same huge file over and over (wasting time and space) I want to copy these reference files to an additional volume during batch node initialisation and mount to each container (rather than using File arguments I would use a simple String argument to prevent localisation - I appreciate this is a hack). I am already doing this with a different pipeline framework with some success, however it requires the JobDefinition to specify the mount locations between the node(host) and job container. Is it possible to provide additional mount/volume instructions to the aws batch backend in the cromwell.conf?. If this is possible, I cannot see any specific examples in the Cromwell docs. If this is not currently possible, could I request adding the ability to define additional mount points as a feature request??. Kind Regards,; Jon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6334:233,avoid,avoid,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334,1,['avoid'],['avoid']
Safety,"Hi Guys. We are running pipelines through Cromwell on AWS Batch using S3 and have noticed some behaviour we didn't initially expect. We have a task that has quite a significant setup cost. As such we want to process a number of samples through this task rather than instantiating the task for every sample. We can then parallelise this task to process batches of samples. The task takes an Array of structs:. ```; struct Sample {; String id; File file1; File file2; }; ```. The struct is serialised to the task using write_json() and the tool consumes the resulting json before processing the samples one after the other. It is important that the output files can be matched back to their original inputs via the supplied id. The tool outputs a single file per sample to a directory and produces a reports.json that looks like:. ```; [; {; ""id"": ""1""; ""file"": ""outputs/report.txt""; },; ...; ]; ```. I was hoping we could use the read_json() function to parse the output.json into an array of the following struct:. ```; struct Report {; String id; File file; }; ```. and pass this to the next task (or drive a scatter) in the pipeline. However, the File objects parsed in this manner are not resolved to actual task outputs and neither have their address updated or delocalised at the end of the task. Conceptually, it seems like resolving Files within read_* generated structs would be handled the same way as raw File outputs. However, looking at how the delocalisation occurs in the Cromwell task script I understand why this would be difficult to implement. The wdl spec dose not specifically state that File outputs generated this way will be respected but then again it does not say that they won't. a) Could I put forward a feature request for the spec to detect File outputs generated from read_* functions and delocalise them?; b) Or put a note in the wdl/Cromwell spec that File objects generated from read_*() functions may not be detected in the output?. Thanks,; Jon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6795:1762,detect,detect,1762,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6795,2,['detect'],"['detect', 'detected']"
Safety,"Hi team,. I try to configure cromwell to run ExomeGermlineSingleSample_v3.1.9.wdl on Slurm, and I follow your guide, but I have an error that ${docker_script} : No such file or directory; /cromwell-executions/ExomeGermlineSingleSample/118135f5-ce0e-437b-9fd2-332dd614bded/call-GenerateSubsettedContaminationResources/execution/script : No such file or directory; I attached the run file; #!/bin/bash; #SBATCH --nodes=1; #SBATCH --time=2:00:00. module load jdk. java -Dconfig.file=/mainfs/wrgl/broadinstitute_warp_development/tutorials/cromwell-slurm_5.config \; -jar /mainfs/wrgl/broadinstitute_warp_development/tutorials/cromwell-85.jar \; run /mainfs/wrgl/broadinstitute_warp_development/warp/ExomeGermlineSingleSample_v3.1.9.wdl \; -i /mainfs/wrgl/broadinstitute_warp_development/tutorials/Exom_test.json. #### Configuration file ###. include required(classpath(""application"")). system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false; }. backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; temporary-directory = ""$(mktemp -d /tmp/tmp.XXXXXX)"". runtime-attributes = """"""; Int runtime_minutes = 60; Int cpu = 1; Int memory_mb = 3900; String? docker; """""". submit = """""" \; 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; -p batch,scavenger \; -c ${cpu} \; --mem $(( (${memory_mb} >= ${cpu} * 3900) ? ${memory_mb} : $(( ${cpu} * 3900 )) )) \; -N 1 \; --exclusive \; --wrap ""/bin/bash ${script}""'; """""". submit-docker = """""" \. # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; module load apptainer; if [ -z $APPTAINER_CACHEDIR ];; then CACHE_DIR=$HOME/.apptainer/cache; else CACHE_DIR=$APPTAINER_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR; LOCK_FILE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7086:950,abort,abort,950,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7086,2,['abort'],"['abort', 'abort-jobs-on-terminate']"
Safety,"Hi!. I am attempting to run a tool that requires a directory structure as an input. One issue is that the tool will error out if there are files in the directory that are not the expected type. In our infrastructure we occasionally will have `.md5` files next to the important files which leads to the error in running the tool. To avoid this issue when launching Cromwell workflows I am attempting to list the good files in the listing attribute of the Directory input data type. The files I list are being staged in the inputs folder for the step but they are not being staged in the input directory folder path. Cromwell uses the empty input directory folder path as input to the tool which causes it to fail. . Example.cwl; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""ls""]; arguments: [""$(inputs.dir)""]. requirements:; - class: DockerRequirement; dockerPull: ""ubuntu:xenial"". inputs:; dir:; type: Directory. outputs:; example_out:; type: stdout. stdout: output.txt; ```; Input.yaml:; ```; dir: ; class: Directory; listing:; - class: File; path: ./data/1.txt; - class: File; path: ./data/2.txt; ```. staged files:; ```; => find inputs/; inputs/; inputs/1465754395; inputs/1465754395/2.txt; inputs/1465754395/1.txt; inputs/-143808698; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d/.file; ```. And the generated Cromwell command; ```; [d14c14d1example.cwl:NA:1]: 'ls' '/cromwell-executions/example.cwl/d14c14d1-ce96-44fc-9315-d1c431011f83/call-example.cwl/inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d'; ```. Tested using Cromwell 35 and 37. Cwltool works as expected after adding a basename to the input directory in the yaml. . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4670:332,avoid,avoid,332,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4670,1,['avoid'],['avoid']
Safety,"Hi, . I have been getting PoolTime out error (below) for all human jobs. Smaller ones e.g mouse genomes are a success. There are multiple steps in the wdl with outputs at every stage. Each output is copied from the S3//temp//cromwell_executions folder to S3://output folder successfully except the largest one i.e. .bam file. The .bam file is successfully copied from EC2 instance temp folder to S3://temp folder but it does copy from S3://temp/cromwell_executions to S3://output.; ; Both core environment and workflows have been set up using the templates provided by AWS genomics workflow. AWS batch jobs show a success notification, however it is only Cromwell that sends a status of ""failure"".. . Cromwell metadata : ; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""Timeout waiting for connection from pool"",; ""causedBy"": []; }; ],; ""message"": ""Unable to execute HTTP request: Timeout waiting for connection from pool""; }; ],; ""message"": ""software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool""; }; ],; ""message"": ""[Attempted 1 time(s)] - CompletionException: software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool""; }; ],; ```. Options.json; ```; ""final_workflow_outputs_dir"": ""s3://singleronbio-de-tmp/output/2023/Aug/2023-00578"",; ""final_call_logs_dir"": ""s3://singleronbio-de-tmp/log/2023/Aug/2023-00578/230719005"",; ""final_workflow_log_dir"": ""s3://singleronbio-de-tmp/workflow_log/2023/Aug/2023-00578/230719005"",; ""backend"": ""AWSBATCH"",; ""base_url"": ""XXXX"",; ""route_submit"": ""/api/workflows/v1"",; ""route_valid"": ""/api/womtool/v1/describe"",; ""route_status"": ""/api/workflows/v1/{id}/status"",; ""route_outputs"": ""/api/workflows/v1/{id}/outputs"",; ""write_to_cache"": true,; ""read_from_cache"": true; }; ```. Thank you, ; Lakshmi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7200:812,Timeout,Timeout,812,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7200,4,['Timeout'],['Timeout']
Safety,"Hi, . I want to use reference disk to avoid cp input to local disk. I have some trouble finding how to create image and manifest file. ; I see from the link below that files has path and crc32c. can path be like gs://...? and what is crc32c. https://github.com/broadinstitute/cromwell/blob/2a69691ec56ba0e8f279b8f006ba796bb9cfaf05/docs/backends/Google.md?plain=1#L529",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7211:38,avoid,avoid,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7211,1,['avoid'],['avoid']
Safety,"Hi, is it possible to invalidate cache with timeout? I am asking because we do not keep the results of calls infinitely, only for 6 weeks. I assume that cache will be kept in DB and call will try to copy a directory that is corrupted (we delete files but not directory structure). Otherwise we would have to access DB and remove calls manually. Rafal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5174:44,timeout,timeout,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5174,1,['timeout'],['timeout']
Safety,"Hi,. I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch; Cromwell version: 51; Error log:. Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; nmerged.bam)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977:13,timeout,timeout,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977,2,"['Timeout', 'timeout']","['TimeoutException', 'timeout']"
Safety,"Hi,. I have built a WDL workflow which works well with SLURM but now I am trying to get it to be able to be run on a standalone server. . I have Slurm as my provider and have created one for Local. ` Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. run-in-background = true; exit-code-timeout-seconds = 300; workflow-reset = true; read_from_cache = true; write_to_cache = true; system.file-hash-cache=true; concurrent-job-limit = 2. runtime-attributes = """"""; String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg""; """""". submit = ""singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}"". filesystems {; local {; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; } ## end local; } ## end file systems; } ## end config; } ## End Local`. Oddly, when running the workflow I get a submit docker error. ie. as per below. I have no idea why it's looking for docker as I'm not knowingly using it. I'm not using docker in my run time parameters. I have been able to get standalone working on another workflow by passing a singularity container to each task command output but I was wondering if there was a more elegant solution I could use such as just changing to a pre-made provider. I have searched Google and through here but not found anything. I did find one issue here but they seemed to want to use docker where as I don't. . Thanks for the help!. `task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_scri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862:342,timeout,timeout-seconds,342,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862,1,['timeout'],['timeout-seconds']
Safety,"Hi,. Sorry for submitting an issue here but I'm consistently getting a ""Something has gone wrong"" error trying to log in to your Jira. I'm hoping someone can offer some guidance for an issue I'm having running a CWL workflow with Cromwell on GCP. I'm using bcbio to generate CWL to do joint calling. This worked fine when I tested it with a single sample to shake out any issues with the pipeline. However when scaling up to a 20 sample batch there's an issue with the get_parallel_regions_jointvc step. This step appears to be localizing multiple copies of the reference genome data (one for each sample) to the same disk. This really blows up the storage requirements as the number of samples increase and ends up exhausting the storage allocated to the worker instance. Is this expected behaviour or is there some kind of configuration I'm missing that would avoid this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5131:862,avoid,avoid,862,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5131,1,['avoid'],['avoid']
Safety,"Hi. I'm trying to enable call caching using a local file database and I can't seem to get it to work. Everything that I try does not seem to make a difference, and each run always starts from the first task. I'm running cromwell in run mode from the command line, and I am testing on both cromwell 43 and cromwell 47. I also have write-to-cache and read-from-cache set to true in my options.json (although I understand that is the default behaviour). I am unable to use a mySQL or postgres database at this current time. Is there something that I'm missing? Is there any additional information that is needed to help diagnose this?. My cromwell.conf is as follows:. backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 600. runtime-attributes = """"""; Int cpus; Float memory_mb; String lsf_queue; String lsf_project; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpus} \; -R rusage[mem=${memory_mb}] \; /usr/bin/env bash ${script}; """""". job-id-regex = ""Job <(\\d+)>.*"". kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=100000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 86400000; numThreads = 1; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5370:819,timeout,timeout-seconds,819,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5370,1,['timeout'],['timeout-seconds']
Safety,"Hi，I have learned that cromwell does not support 【cpu and memory】runtime attributes for local backends（see https://cromwell.readthedocs.io/en/develop/RuntimeAttributes/）. Thus, when running a workflow locally， How can we avoid concurent jobs that may crash the workflow by running out of memory？ I know that maximum job number can be limited, and some jobs can be parallelized wildly for they require only little resources , however, some jobs should not be paralleized for need of large memory. So, we need to check available resource before submit a job. . best wishes!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6458:221,avoid,avoid,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6458,1,['avoid'],['avoid']
Safety,"HtCondor backend should be responsive to abort requests from the engine, and kill and remove the job from it's internal queue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1402:41,abort,abort,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1402,1,['abort'],['abort']
Safety,HtCondorInitializationActor will explode on abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1109:44,abort,abort,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1109,1,['abort'],['abort']
Safety,"I am experimenting by running some workflows with a MySQL database to avoid problems like #3387, but after a successful run and several days without re-running the pipeline (or a similar one) I would like to clean up the database to free some space. I would appreciate if this is included in the cromwell documentation...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3415:70,avoid,avoid,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3415,1,['avoid'],['avoid']
Safety,"I am running the program [deFuse](https://bitbucket.org/dranew/defuse) version 0.8.1 using a local backend. When I run the progam locally inside a docker container, the program completes successfully. When I run it using Cromwell/WDL, it raises the following error:; ```; Starting defuse command:; /usr/local/bin/gmap -D defuse-data/gmap -d cdna -f psl #<1 > #>1; Reasons:; /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakp; oints.split.001.fa.cdna.psl missing; Failure for defuse command:; /usr/local/bin/gmap -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a; -ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa > /cromwell-executions/detectFusions/96242; 9bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa.cdna.psl.tmp; Reason:; Job command with nonzero return code; Return codes: 139; Job output:; Running on 2ecb3961d54d; Note: /usr/local/bin/gmap.avx2 does not exist. For faster speed, may want to compile package on an AVX2 machine; GMAP version 2018-07-04 called with args: /usr/local/bin/gmap.sse42 -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa; Checking compiler assumptions for SSE2: 6B8B4567 327B23C6 xor=59F066A1; Checking compiler assumptions for SSE4.1: -103 -58 max=198 => compiler zero extends; Checking compiler options for SSE4.2: 6B8B4567 __builtin_clz=1 __builtin_ctz=0 _mm_popcnt_u32=17 __builtin_popcount=17 ; Finished checking compiler assumptions; Pre-loading compressed genome (oligos)......done (78,222,840 bytes, 19098 pages, 0.00 sec); Pre-loading compressed genome (bits)......done (78,222,864 bytes, 19098 pages, 0.02 sec); Looking for index files in directory defuse-data/gmap/cdna; Pointers file is cdna.ref153offsets64meta; Offsets file is cdna.ref153offsets64strm; Positions file is cdna.re",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465:395,detect,detectFusions,395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465,3,['detect'],['detectFusions']
Safety,"I am using latest cromwell develop docker container. When I run ""abort"" command it internall executes docker.kill script that looks like:; ""; #!/bin/bash ; docker kill `cat /pipelines/cromwell-executions/vsearch/81c51e4e-756c-47f7-8dd6\; -57b9c2981162/call-global_search/execution/docker_cid`; ""; However, docker_cid is never created. So, all the abort commands that I do stop the cromwell tasks but never stop docker containers that were started by it. My docker-stack configuration is https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml. It uses slightly modified cromwell:develop container https://github.com/antonkulaga/cromwell-client/blob/master/services/cromwell/Dockerfile. I also share docker sockets there and everything functions well with the exception of abort.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4011:65,abort,abort,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4011,3,['abort'],['abort']
Safety,"I cant get the sra filesystem to work. Here is the error:. ```; [2020-08-21 11:08:59,62] [info] WorkflowManagerActor Workflow dbd5cdc0-c79a-42cd-b929-56ddb1115467 failed (during InitializingWorkflowState): common.exception.AggregatedMessageException: Failed to instantiate backend filesystem:; Cannot find a filesystem with name sra in the configuration. Available filesystems: ftp, s3, gcs, oss, drs, http; 	at common.validation.Validation$ValidationChecked$.$anonfun$unsafe$2(Validation.scala:98); 	at cats.syntax.EitherOps$.valueOr$extension(either.scala:66); 	at common.validation.Validation$ValidationChecked$.unsafe$extension(Validation.scala:98); 	at cromwell.backend.BackendConfigurationDescriptor.configuredPathBuilderFactories$lzycompute(backend.scala:109); 	at cromwell.backend.BackendConfigurationDescriptor.configuredPathBuilderFactories(backend.scala:108); 	at cromwell.backend.BackendConfigurationDescriptor.pathBuilders(backend.scala:120); 	at cromwell.backend.standard.StandardInitializationActor.pathBuilders$lzycompute(StandardInitializationActor.scala:62); 	at cromwell.backend.standard.StandardInitializationActor.pathBuilders(StandardInitializationActor.scala:62); 	at cromwell.backend.google.pipelines.common.PipelinesApiInitializationActor.$anonfun$workflowPaths$2(PipelinesApiInitializationActor.scala:137); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(Abst",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793:469,unsafe,unsafe,469,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793,2,['unsafe'],['unsafe']
Safety,"I encountered this error when running a WDL:; ```message: Runtime validation failed; causedBy: ; message: Task hello has an invalid runtime attribute docker = !! NOT FOUND !!; ```. I understand that it requires a docker attribute. The issue is that the error gets found at runtime. This should be caught when validating the WDL. . The risk is that users could ""run half their tasks and only find out mid-workflow that one needs an extra parameter"" (ChrisL's words).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2932:335,risk,risk,335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2932,1,['risk'],['risk']
Safety,"I have a task with an input 'File' (WDL concept) that is actually a folder. I did not know what would happen, as I could not find explanation of the concept in either wdl or cromwell docs. . For the 'File's that are actually files, cromwell reports ```Localisation via hard-link has failed ... invalid cross device link``` as expected (different disks), and then seems to successfully soft-link (as expected). For the folder, the same hard link error appears, but then there is no soft link error, and the folder is (recursively) copied. The folder hard link would also fail due to folders not being hard-linkable. But the soft-link should probably succeed. I am running an SGE backend, with no modification of the 'localisation' settings https://cromwell.readthedocs.io/en/latest/backends/HPC/#shared-filesystem. What is the expected behaviour? Should I avoid using folders for WDL 'File's? Any advice would be appreciated. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3785:855,avoid,avoid,855,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3785,1,['avoid'],['avoid']
Safety,"I have been experimenting some random failures due to docker containers being killed for some reason on my system (not only https://github.com/broadinstitute/cromwell/issues/3370), but if I re-run the workflow with caching enabled then this calls end without failure and the pipeline can continue and work. Nevertheless, it is tedious to re-run a whole pipeline due to random failures and rely on caching for avoid re-computation. This is something that can be avoided by providing a configuration option for retry jobs (cromwell level) or add to some tasks a runtime attribute (WDL level) to set the number of retries that can be done per-task. Do you think that this is possible in the near future to avoid re-running a whole pipeline due to a random failure in a concrete task(s)?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3417:409,avoid,avoid,409,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3417,3,['avoid'],"['avoid', 'avoided']"
Safety,I haven't seen this happen live but in theory 'take' can throw an exception if the underlying queue changes between checking `available` and running `dequeue`. That might happen if an unluckily timed `abort` removes an actor from the queue between those checks.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4165:201,abort,abort,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4165,1,['abort'],['abort']
Safety,"I know the I/O actor is not very trendy these days, but even if it ends up going away I thought this was a small change that could help with handling IO pressure in a better way.; Currently if an actor receives a backpressure message it waits more or less 5 seconds and retries. This uses configurable exponential backoff instead with a higher randomization of waiting times to avoid spikes as much as possible.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4043:378,avoid,avoid,378,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4043,1,['avoid'],['avoid']
Safety,I noticed that I got a few failures using the default 1 second timeout here. . So this PR ups that to 10 seconds... 🤞,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4385:63,timeout,timeout,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4385,1,['timeout'],['timeout']
Safety,I noticed that in swagger abort is marked as POST request despite it contains only two parameters: version and id. I suggest to make it a get request,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2318:26,abort,abort,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2318,1,['abort'],['abort']
Safety,"I ran a super small WDL, then checked its status to make sure it was running. Then, I tried to abort it -- the request returned a 500: ""The server was not able to produce a timely response to your request."". However, when I check the status of the workflow, it says it has been successfully aborted. I included the WDL i ran against https://cromwell.gotc-int.broadinstitute.org/swagger/index.html?url=/swagger/cromwell.yaml in case you'd like to try it yourself. task echoHelloWorld {; command {; echo 'Hello, World!'; }; runtime {; docker: ""phusion/baseimage""; disks: ""local-disk 10 HDD""; memory: ""1 GB""; preemptible: 3; }; }. workflow printHelloAndGoodbye {; call echoHelloWorld; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253:95,abort,abort,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253,2,['abort'],"['abort', 'aborted']"
Safety,"I think this is what most people mean when they write declarations in WDL. Some are a bit different from how they're currently interpreted but I think it should make things easier and safer (eg. if it's an optional that has a default, don't force a `select_first`...). ## Example; ```; workflow foo {; Int a ; Int b = 5 ; Int c = b ; Int? d ; Int? e = 6; Int? f = d ; }; ```. |Declaration | Type | *Must* be supplied | **Can** be supplied | Notes | ; |-------|-----------|-------|-|-|; | `Int a` | Int | Yes | Yes | |; | `Int b = 5` | Int | No | Yes | |; | `Int c = b` | Int | No | ~~Yes~~ **No** | Intermediate value. Shouldn't be overridden because it has variable references. |; | `Int? d` | Int? | No | Yes | |; | `Int? e = 5` | ~~Int?~~ **Int** | No | Yes | Can be treated as an `Int` because it has a default |; | `Int? f = d` | Int? | No | ~~Yes~~ **No** | Intermediate value. Shouldn't be overridden because it has variable references. |",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2565:184,safe,safer,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2565,1,['safe'],['safer']
Safety,"I write a new backend. But the hostname verification needs to be canceled due to an internal environment problem. However, after the verification is canceled, the following information is displayed when submitting job:. ```; akka.stream.scaladsl.TcpIdleTimeoutException: TCP idle-timeout encountered on connection to xxxxxxx, no bytes passed in the last 1 minute; java.lang.OutOfMemoryError: GC overhead limit exceeded; ```. If the hostname verification is not canceled, the service is normal. The code is as follows:. ```; val badSslConfig: AkkaSSLConfig = AkkaSSLConfig().mapSettings(s =>; s.withLoose(; s.loose; .withDisableHostnameVerification(true); ); ). val badctx: HttpsConnectionContext = Http().createClientHttpsContext(badSslConfig). private def makeRequest[A](request: HttpRequest)(implicit um: Unmarshaller[ResponseEntity, A]): Future[A] = {. for {; response <- withRetry(() => {. val rsp = if (vkConfiguration.region == ""xxxxxxxxx""){; Await.result({Http().singleRequest(request, badctx)}, Duration.Inf); }else{; Await.result(Http().singleRequest(request), Duration.Inf); }; if (rsp.status.isFailure() && rsp.status.intValue() == 429) {; Future.failed(new RateLimitException(rsp.status.defaultMessage())); } else {; Future.successful(rsp); }; }); data <- if (response.status.isFailure()) {; response.entity.dataBytes.runFold(ByteString(""""))(_ ++ _).map(_.utf8String) flatMap { errorBody =>; Future.failed(new RuntimeException(s""Failed VK request: Code ${response.status.intValue()}, Body = $errorBody"")); }; } else {; Unmarshal(response.entity).to[A]; }; } yield data; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6149:280,timeout,timeout,280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6149,1,['timeout'],['timeout']
Safety,I'm not actually going to wait for :+1: but will wait for it to go green. My claim will be out of safety but really it's because I'm lazy and will do it later.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2664:98,safe,safety,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2664,1,['safe'],['safety']
Safety,"I'm not sure what this was originally intended for?. It was always set to `List.empty` and never read from, so I think it's safe to remove it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3303:124,safe,safe,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3303,1,['safe'],['safe']
Safety,"I'm working on mutation calling based on cromwell, the **Failed to summarize metadata** comes out for several shards in the scatter, then the following processes are aborted. How to fixed this error?. ```; [2018-11-17 09:04:45,38] [info] BackgroundConfigAsyncJobExecutionActor [3df56d2bPreProcessingForVariantDiscovery_GATK4.MarkDuplicates:5:1]: job id: 56011; [2018-11-17 09:04:45,48] [info] BackgroundConfigAsyncJobExecutionActor [3df56d2bPreProcessingForVariantDiscovery_GATK4.MarkDuplicates:5:1]: Status change from - to WaitingForReturnCodeFile; [2018-11-17 09:37:07,47] [error] Failed to summarize metadata; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 3785ms.; 	at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:548); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:186); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:145); 	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); 	at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:453); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:249); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:248); 	at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:37); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); [2018-11-17 09:37:14,33] [error] Error summarizing metadata; java.sql.SQLTransientConnectionException: db - Connecti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4403:166,abort,aborted,166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4403,1,['abort'],['aborted']
Safety,"I've downloaded the new jar file, still showing version 30.2, but still seeing the problem in some situations:. 2018-02-21 11:03:39,563 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - Abort requested for workflow f0bff6e2-77a6-46f5-b226-13a64339a286.; 2018-02-21 11:03:39,564 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - WorkflowExecutionActor-f0bff6e2-77a6-46f5-b226-13a64339a286 [UUID(f0bff6e2)]: Aborting workflow; 2018-02-21 11:03:39,567 cromwell-system-akka.dispatchers.engine-dispatcher-50 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Several SGE queue jobs continue to run/stay in the queue waiting state. Terminating the server with a ctrl-C does not affect the queued jobs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3325:197,Abort,Abort,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3325,2,['Abort'],"['Abort', 'Aborting']"
Safety,ID-734 Increase Timeout for DRSHub Communication,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7198:16,Timeout,Timeout,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7198,1,['Timeout'],['Timeout']
Safety,"IET false --VALIDATION_STRINGENCY LENIENT; [2016-10-19 18:29:50,53] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: executing: docker run --rm -v /home/lichtens/test_eval/cromwell-ex; ecutions/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12:/root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12; -i broadinstitute/gatk-protected:3c44b2f93e29e360af41ba403465df02931f8e86 /bin/bash < /home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160; b0/call-HetPulldown/shard-12/execution/script; [2016-10-19 18:29:50,53] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: command: ""/bin/bash"" ""/home/lichtens/test_eval/cromwell-executions; /case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/execution/script.submit""; [2016-10-19 18:29:50,54] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: job id: 9788; [2016-10-19 18:30:17,05] [info] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] job aborted: case_gatk_acnv_workflow.HetPulldown:4:; 1; [2016-10-19 18:30:17,10] [warn] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] received an unhandled message: JobRunning(51ee236f-; c31a-48c2-bae7-9246439160b0:case_gatk_acnv_workflow.HetPulldown:7:1,Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-51ee236f-c31a-48c2-ba; e7-9246439160b0/WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0/51ee236f-c31a-48c2-bae7-9246439160b0-EngineJobExecutionActor-case_gatk_acnv_workflow.HetPulldown:7:1/51ee236f-c31; a-48c2-bae7-9246439160b0-BackendJobExecutionActor-51ee236f:case_gatk_acnv_workflow.HetPulldown:7:1#132070105 ; .... snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1600:3758,abort,aborted,3758,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600,1,['abort'],['aborted']
Safety,"If `abort-all-workflows-on-terminate` is true, Cromwell will send a message directly to the WorkflowManagerActor which will trigger jobs to be aborted on the backend side but the workflow store is not made aware of that. Which means on restart, all ""aborted"" workflows will be restarted. Possible fix: instead of going to the WMA directly, send the abort command to the WorkflowStore first like it's done for single workflow abort, and have the WorkflowStore notify the WMA when all workflows have been removed. There might be a race condition between workflow submission and workflow deletion from the store so it might need some carefully ordered messages.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2492:4,abort,abort-all-workflows-on-terminate,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2492,5,['abort'],"['abort', 'abort-all-workflows-on-terminate', 'aborted']"
Safety,"If a java.util.concurrent.ExecutionException, check if it has an inner cause that is actually fatal.; Pre-update to SBT 1.x.; The cromwell multi-project with SBT 1.x consumes a lot of memory. To avoid OOME:; - Bump IntelliJ from 768 to something like 1786.; - Bump .sbtopts to something like 3072. Build updates while prepping fo sbt version bump, including using sbt-doge temporarily for simplified SBT 0.13 cross version build syntax.; DRYed out a bit of the module dependency graph.; Publish hotfix libraries, not just executables.; Replaced deprecated commons-lang with commons-text replacements.; Fixed test class errantly nested within its companion object.; Fixed test that was only liquibasing metadata, and not the original database.; Removed unused TestActorSystem.; Fixed missing test in ""common"".; Removed old ""common"" sbt files.; Cleaner version of assembly.; SwaggerUI injected into the cromwell-version.conf, so it only needs to be editted in one place.; Replaced usage of 'sbt ""project foo"" sometask' with 'sbt foo/sometask'.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2821:195,avoid,avoid,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2821,1,['avoid'],['avoid']
Safety,"If a timeout is not provided, GCP defaults to [setting a timeout of 7 days](https://developers.google.com/resources/api-libraries/documentation/genomics/v2alpha1/java/latest/com/google/api/services/genomics/v2alpha1/model/Pipeline.html), after which the pipeline will abort. Occasionally pipelines genuinely need to run >7 days. These changes allow this value to be user-configured.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5273:5,timeout,timeout,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273,3,"['abort', 'timeout']","['abort', 'timeout']"
Safety,"If a workflow fails, I would like cromwell to tell me somewhere really obvious, like perhaps the last thing it prints to stdout:. * The first thing that failed.; * Whether it was a failure that occurred while cromwell was running the wdl, or whether it occurred during the execution of a task.; * If a cromwell error, which line of which wdl it happened on.; * If a task, which task and the stderr file. I think a lot of this already exists, but I'm suggesting for it to be super-simple and in one place. . Here's an example of the desired output:. ```; Lots of output. . .; . . .; Your workflow failed while executing task HelloWorld, See cromwell-executions/Hello/c44566iifgg57/call-HelloWorld/execution/stderr for details.; ```. Or. ```; Cromwell failed while executing line 346 of HelloWorld.wdl. The index 6 is out of bounds for the array popular_salutations.; ```. This would be extremely helpful whenever a non-expert runs a workflow, for example our mutect2 wdl. Currently I debug with a mix of home-brewed greps through the cromwell metadata, fishing through cromwell-executions, and running the darn thing myself. It would be really great just to tell them to look at the last line of stdout. A few points:; * The first error is all you need because you can iterate and fix bugs one at a time.; * It's crucial to let the user know very explicitly if this is a cromwell error or a within-task error.; * Stack traces from running the tool could be useful and acceptable, but cromwell stack traces with all that stuff about akka and spark would be overwhelming.; * Rigor isn't important here. For example, the order of execution is not prescribed to the point that the first task to fail will be the same every time, but for this it doesn't matter. Just reporting the first error on the wall clock is sufficient.; * It doesn't matter which workflows, subworkflows etc fails. Just the task.; * Putting this on the last line of stdout has the added virtue of avoiding hassle in a screen session.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3226:1964,avoid,avoiding,1964,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3226,1,['avoid'],['avoiding']
Safety,"Im testing moving LSAPI runs to Batch with v86, and I keep getting the following error:. textPayload: ""docker: invalid spec: /mnt/disks/cromwell_root:/mnt/disks/cromwell_root:: empty section between colons."". for this command:; Executing runnable container:{image_uri:""gcr.io/google.com/cloudsdktool/cloud-sdk:434.0.0-alpine"" commands:""-c"" commands:""printf '%s %s\\n' \""$(date -u '+%Y/%m/%d %H:%M:%S')\"" Starting\\ container\\ setup."" entrypoint:""/bin/sh"" volumes:""/mnt/disks/cromwell_root:/mnt/disks/cromwell_root:""} timeout:{seconds:300} labels:{key:""logging"" value:""ContainerSetup""} for Task task/job-49cc8a88-722b-43067ba4-ab34-48bc00-group0-0/0/0 in TaskGroup group0 of Job job-49cc8a88-722b-43067ba4-ab34-48bc00. The docker volumes are defined as:; volumes:""/mnt/disks/cromwell_root:/mnt/disks/cromwell_root:""}. Shouldn't there be a rw permissions entry after the last colon? As far as I know, there is no way for users to modify the docker launch config to fix this. Is there something I have malformed or missing in my conf file?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7408:518,timeout,timeout,518,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7408,1,['timeout'],['timeout']
Safety,"Implement a JES PBE, this ticket covers the basics but should NOT include:; - retry support; - call caching support; - recovery support; - metadata support; - abort support. as these are covered in other tickets with the ""JES PBE""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/657:119,recover,recovery,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/657,2,"['abort', 'recover']","['abort', 'recovery']"
Safety,Implement recover in TES and improve abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4197:10,recover,recover,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4197,2,"['abort', 'recover']","['abort', 'recover']"
Safety,Implement recoverAsync for AWS backend [BA-4857],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216:10,recover,recoverAsync,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216,1,['recover'],['recoverAsync']
Safety,"In PR #2925, the `no_new_calls` test sometimes generates a cromwell log [message](https://github.com/broadinstitute/cromwell/blob/c6ed64617c51c572863b87d324fa8e68fa085b1a/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L118-L121):. > Cromwell server was restarted while this workflow was running. As part of the restart process, Cromwell attempted to reconnect to this job, however it was never started in the first place. This is a benign failure and not the cause of failure for this workflow, it can be safely ignored. This occurs when cromwell is restarted while `shouldSucceed` is still running. `shouldSucceed` finishes, and then a `Restarting calls: no_new_calls.delayedTask:NA:1` is generated, even though `boundToFail` has already failed and NoNewCalls should be started. The easiest way to reproduce this locally and see the delay is to increase the sleep in the wdl from 100 to something like 300 (five minutes). FYI if cromwell is not restarted, `delayedTask` does not start, does not fail, and does not have a metadata stanza for the call.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2926:554,safe,safely,554,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2926,1,['safe'],['safely']
Safety,"In [CROM-6338](https://broadworkbench.atlassian.net/browse/CROM-6338) Denis reports that Cromwell is unexpectedly failing to retry 503s and provides the following sample error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""503 Service Unavailable\nBackend Error""; }; ],; ""message"": ""Could not read from gs://broad-epi-cromwell/workflows/ChipSeq/ce6a5671-baf6-4734-a32b-abf3d9138e9b/call-epitope_classifier/memory_retry_rc: 503 Service Unavailable\nBackend Error""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://broad-epi-cromwell/workflows/ChipSeq/ce6a5671-baf6-4734-a32b-abf3d9138e9b/call-epitope_classifier/memory_retry_rc: 503 Service Unavailable\nBackend Error""; }; ]; ```. In https://github.com/broadinstitute/cromwell/issues/6154 @freeseek reports that Cromwell is unexpectedly failing to retry 504s and provides the following sample error:; ```; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=me",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6155:993,Timeout,Timeout,993,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6155,1,['Timeout'],['Timeout']
Safety,"In a configuration like. ```wdl; workflow ifs_in_scatters {; call hello. scatter (n in range(5)) {; if (true) {; call goodbye { input: i = hello.out }; }; }; }; ```; When the conditional graph is created, all nodes outside of the scatter get ""wrapped"" in an `OuterGraphInputNode` that gets passed into the inner conditional graph so that nodes in the inner graph can reference nodes outside of the scatter.; The issue is that those OGINs are created with `preserveScatterIndex = true` even though the node they're pointing to is outside of the scatter. This was preventing the `ExecutionStore` from detecting the `i = hello.out` expression as being runnable because it was looking for a `hello.out` node in `Done` state at index `n`, which doesn't exist since `call hello` is outside the scatter.; This PR changes that to use the `preserveIndexForOuterLookups ` value of the conditional / scatter instead, which in this case will be `false`, because the scatter node does set `preserveScatterIndex = false` to build its inner graph (in this case the if).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3158:599,detect,detecting,599,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3158,1,['detect'],['detecting']
Safety,"In order to de-risk a time-sensitive deployment, I am staging this as an option",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5351:15,risk,risk,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5351,1,['risk'],['risk']
Safety,In particular the behavior for `Timeout` is indirectly tested currently by CromwellApiServiceSpec. Test this behavior directly in the (currently nonexistent) CromwellServerActorSpec instead,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1829:32,Timeout,Timeout,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1829,1,['Timeout'],['Timeout']
Safety,In response to:. * Issue with being too generous in production https://broadworkbench.atlassian.net/browse/PROD-137; * The (hopefully) safe but low sanity limit introduced in https://github.com/broadinstitute/firecloud-develop/pull/1652. AC: We should be able to:; * Perf test various values for this field in a repeatable way; * Make sure we don't regress on the level we find; * Set the production value to something more informed than just a wild guess,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4990:135,safe,safe,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4990,1,['safe'],['safe']
Safety,"In situations where a workflow is being restarted, receives an abort command but is only at materialization stage or initialization stage we currently assume it's ok to stop right here and declare it `Aborted`. This assumption is wrong as jobs are probably running and we need to re-connect to them so we can 1) abort them 2) update their status.; In the same vein, if a workflow is in the workflow store waiting to be restarted and gets aborted, we need to still pick it up and reconnect to jobs to abort them.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2829:63,abort,abort,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2829,5,"['Abort', 'abort']","['Aborted', 'abort', 'aborted']"
Safety,In the course of responding to a bug report with Abort @mcovarr quickly found several other related bugs in Abort which points to the fact that we should have better testing of the Abort endpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1396:49,Abort,Abort,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1396,3,['Abort'],['Abort']
Safety,Increase connection pool timeout for unit tests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4525:25,timeout,timeout,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4525,1,['timeout'],['timeout']
Safety,"Inspired by stuff I've seen on other projects, e.g. https://github.com/atom/atom/blob/master/CONTRIBUTING.md. I believe Github will detect this file and show a little ""read this first"" indicator when opening a pull request: https://github.blog/2012-09-17-contributing-guidelines/. This is potentially a team discussion broader than ""two thumbs & merge"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4922:132,detect,detect,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4922,1,['detect'],['detect']
Safety,"Instrumentation was scheduled on a timer out of band of the actor's thread, and in some cases accessing non thread safe state inside the actor (in a read only fashion but still it can cause incoherent values: https://stackoverflow.com/questions/37690525/multiple-threads-checking-map-size-and-conccurency); Instead use messages to self to schedule instrumentation on the actor's thread",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4402:115,safe,safe,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4402,1,['safe'],['safe']
Safety,Intended to avoid multiple people iterating on this having to regenerate the same set of placeholders:; - Project definition in sbt; - Basic ecs backend factory; - Better defaults in the BackendLifecycleActorFactory trait; - Fixed a few typos in nearby files... 😳,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1563:12,avoid,avoid,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1563,1,['avoid'],['avoid']
Safety,Investigate unused abort registration in CallActor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/464:19,abort,abort,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/464,1,['abort'],['abort']
Safety,Is there a way to detect whether my program is running as part of a Cromwell task?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5235:18,detect,detect,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5235,1,['detect'],['detect']
Safety,"It looks like upgrading from `Constructor` to `SafeConstructor` does not make much difference, Cromwell errors out and refuses to proceed with a similar message in both cases. But it seems like a good move anyway. With `SafeConstructor`:. `java -jar /Users/anichols/Projects/cromwell/server/target/scala-2.12/cromwell-70-1a6c161-SNAP.jar run test3.cwl`; ```; could not determine a constructor for the tag tag:yaml.org,2002:javax.script.ScriptEngineManager; ```. With `Constructor`:. `java -jar cromwell-69.jar run test3.cwl`:; ```; could not determine a constructor for the tag '!!javax.script.ScriptEngineManager'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6510:47,Safe,SafeConstructor,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510,2,['Safe'],['SafeConstructor']
Safety,"It seemed to go like this in FC:. - Cromwell received some workflows in a batch; - [...] some unknown amount of time passed; - `abort` was run on the remaining workflows; - Some workflows aborted fine (but always took at least 2 `abort` calls to abort); - Others returned 404s; - It's unknown whether these also took 2 `abort` calls to abort, and only the second returned 404. Because rawls is retrying on any error, these `abort` calls are now chewing up a lot of Cromwell resource unnecessarily",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4497:128,abort,abort,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4497,7,['abort'],"['abort', 'aborted']"
Safety,"It would be easier to consume errors if they were wrapped in JSON. I believe 400 errors are already represented this way already, but a 500 response timeout returns with Content-Type:`text/plain`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/947:149,timeout,timeout,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/947,1,['timeout'],['timeout']
Safety,"It would be helpful, for our production pipeline, if Cromwell's copying of workflow outputs and logs had an option to ""flatten"" the outputs by writing all to a single directory, instead of including the directory hierarchy when copying. We are careful to avoid file name collisions in our output and logs so handling that case wouldn't be an issue for us -- but might for other users.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641:255,avoid,avoid,255,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641,1,['avoid'],['avoid']
Safety,"JDOM was removed in https://github.com/broadinstitute/cromwell/pull/6785 and this branch is updated from `develop`:. ```; root(aen_bw_1228)> | 81> whatDependsOn org.jdom jdom2; [ ... ]; [error] whatDependsOn org.jdom jdom2; [error] ^; ```. ( This is the normal output when `whatDependsOn` does not find something, see https://github.com/broadinstitute/cromwell/pull/6775 https://github.com/broadinstitute/cromwell/pull/6776 ). For Protobuf, the new MySQL pulls in a safe version ≥ 3.16.1:. ```; +-mysql:mysql-connector-java:8.0.29; | +-com.google.protobuf:protobuf-java:3.19.4; ```. which evicts older versions used by other dependencies. ```; +-io.opencensus:opencensus-proto:0.2.0; | +-com.google.protobuf:protobuf-java:3.19.4; | +-com.google.protobuf:protobuf-java:3.5.1 (evicted by: 3.19.4); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6793:466,safe,safe,466,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6793,1,['safe'],['safe']
Safety,JES Recover Closes #751,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1197:4,Recover,Recover,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1197,1,['Recover'],['Recover']
Safety,"JES abort claims success on successful abort request, not successful abort",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1409:4,abort,abort,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1409,3,['abort'],['abort']
Safety,JES aborts seem to not (always?) abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1976:4,abort,aborts,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1976,2,['abort'],"['abort', 'aborts']"
Safety,"JES job recovery attempted despite ""abort-jobs-on-terminate"" enabled",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2050:8,recover,recovery,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050,2,"['abort', 'recover']","['abort-jobs-on-terminate', 'recovery']"
Safety,JES/SFS/Spark initialization specs use same timeout as TES.; Sleep more on no_new_calls to help JES from restarting delayedTask2.; Sleep a bit on local before sync'ing to help avoid (reworded) NoSuchFileException's.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2975:44,timeout,timeout,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2975,2,"['avoid', 'timeout']","['avoid', 'timeout']"
Safety,"JIRA Ticket: https://broadworkbench.atlassian.net/browse/CROM-6867. Hi. . I've been running the GATK-SV pipeline with AWS backend and, sometimes, due to some intermittent errors the tasks are aborted half-way trough. Then Cromwell re-launches the task but some files, generated by the previous run, are already there what makes the pipeline to fail. With this in mind, I'm preparing do a change in cromwell that remove all the files (except for the script which gets created for each run/job) from the task folder before it starts and I would like to ask:; 1. if this makes sense?; 2. if there is any problem on doing this. can the same folder be used twice? or does each task has its own “workspace”? or Will this change impact any other downstream jobs as we will remove everything except “script” file?. Thanks in advance",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6651:192,abort,aborted,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6651,1,['abort'],['aborted']
Safety,Jackson-databind: vulnerable version 2.13.2 -> safe version 2.13.2.1+ (used 2.13.3); Nimbus JOSE+JWT: vulnerable version 9.9.3 => safe version 9.22 (used 9.23). ```; root(aen_bw_1227_2)> | 80> whatDependsOn com.fasterxml.jackson.core jackson-databind 2.13.2; [...]; [error] Expected '2.13.3'; [error] whatDependsOn com.fasterxml.jackson.core jackson-databind 2.13.2; [error] ^; ```; ```; root(aen_bw_1227_2)> | 80> whatDependsOn com.nimbusds nimbus-jose-jwt 9.9.3; [...]; [error] Expected '9.23'; [error] whatDependsOn com.nimbusds nimbus-jose-jwt 9.9.3; [error] ^; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6776:47,safe,safe,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6776,2,['safe'],['safe']
Safety,Jes flavored abort failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1405:13,abort,abort,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1405,1,['abort'],['abort']
Safety,JesInitializationActor will explode on abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1110:39,abort,abort,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1110,1,['abort'],['abort']
Safety,"Jira:; https://broadworkbench.atlassian.net/browse/BA-5756?atlOrigin=eyJpIjoiYjk0YjlhYzYyN2Y2NGRkY2FiMGIwNWFmZDk5M2ZiMWEiLCJwIjoiaiJ9. ```; version: cromwell41; backend: SGE; hard disk：Network shared storage （lustre）; ```. When the rc file has been generated, the task status is still running. This state will last a long time.; When we set ""exit-code-timeout-seconds = 18000"", such tasks will fail after 10 hours `(""message"": ""The job was aborted from outside Cromwell"") `. Despite the fact that the task has been completed normally. This situation does not always happen, and only a few tasks will encounter this problem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5051:352,timeout,timeout-seconds,352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5051,2,"['abort', 'timeout']","['aborted', 'timeout-seconds']"
Safety,Job preparation safety for engine functions [BW-478],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6161:16,safe,safety,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6161,1,['safe'],['safety']
Safety,Jobs that SIGTERM themselves are assumed to have been aborted,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3896:54,abort,aborted,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3896,1,['abort'],['aborted']
Safety,JoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecuti,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:7941,recover,recover,7941,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recover']
Safety,"KE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```; include required(classpath(""application"")). webservice {; }. akka {; http {; server {; }; }; }. system {; io {; }; input-read-limits {; }; job-rate-control {; jobs = 2; per = 1 second; }. abort {; scan-frequency: 30 seconds; cache {; enabled: true; concurrency: 1; ttl: 20 minutes; size: 100000; }; }. dns-cache-ttl: 3 minutes; }. workflow-options {; default {; }; }. call-caching {; enabled = true; }. google {; }. docker {; hash-lookup {; }; }. engine {; filesystems {; local {; }; }; }. languages {; WDL {; versions {; ""draft-2"" {; }; ""1.0"" {; }; }; }; CWL {; versions {; ""v1.0"" {; }; }; }; }. backend {; default = ""SLURM"". providers {. SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; runtime-attributes = """"""; Int runtime_minutes = 720; Int cpus = 1; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". exit-code-timeout-seconds = 600. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --constraint=""groups"" \; --qos=ded_reich \; --account=""reich"" \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*"". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]. caching {; duplication-strategy: [; ""soft-link""; ]. hashing-strategy: ""path"". check-sibling-md5: false; }; }; }. default-runtime-attributes {; failOnStderr: false; continueOnReturnCode: 0; }; }; }; }; }. services {; MetadataService {; }. Instrumentation {; }; HealthMonitor {; config {; }; }; LoadController {; config {; }; }; }. database {; driver = ""slick.jdbc.MySQLProfile$"". db {; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://database.host/callcachingdatabase?rewriteBatchedStatements=true""; user =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6929:3270,timeout,timeout-seconds,3270,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6929,1,['timeout'],['timeout-seconds']
Safety,"LBjtl7_8otWYjTQgx_mw9zYqD3Byb2R1Y3Rpb25RdWV1ZQ; > ; > broad-wgs-prod5, 2018-01-16T15:37:16Z, 2018-01-16T17:45:43Z, ggp-1801918915849415035, us-central1-c/n1-standard-16; > broad-wgs-prod5, 2018-01-16T15:37:16Z, 2018-01-16T15:46:25Z, ggp-1347601243842424591, us-east1-c/n1-standard-16; > broad-wgs-prod5, 2018-01-16T15:37:16Z, 2018-01-16T17:14:27Z, ggp-17952768368412969986, us-east1-c/n1-standard-16; > broad-wgs-prod5, 2018-01-16T20:41:42Z, 2018-01-16T22:28:14Z, ggp-17459223747282221022, us-central1-b/n1-standard-2; > broad-wgs-prod5, 2018-01-16T22:37:32Z, 2018-01-16T23:38:28Z, ggp-1817239588482439788, us-east1-c/n1-standard-2; > broad-wgs-prod5, 2018-01-16T23:46:08Z, 2018-01-17T14:57:19Z, ggp-3754421722448645101, us-east1-d/n1-standard-2. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #11 Jan 17, 2018 12:32PM ; > Mike, ; > ; > For comparison - the previous reported workflow also had a Message 14: type pre-emption. Which is what cromwell normally detects as pre-emption. Not sure what is difference between the above pre-emptions and the one below. Info as follows:; > ; > OPSID; > operations/ENOi-PyPLBioyJKO-s3GhY0BIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > ; > broad-wgs-prod5, 2018-01-16T15:37:17Z, 2018-01-16T16:43:22Z, ggp-10163246050849367080, us-east1-d/n1-standard-16. > ------------------------------- ; > gdk@google.com <gdk@google.com> Jan 17, 2018 01:36PM; > Accepted by gdk@google.com. > ------------------------------- ; > gdk@google.com <gdk@google.com> #12 Jan 17, 2018 02:52PM ; > The difference between 13 and 14 here is simply when PAPI notices that the VM has been shut down. They mean essentially the same thing, and cromwell should be able to retry with the same logic.; > ; > It looks like this might have been exacerbated because changed the shutdown behavior of VMs so that they won't stay around for 24h for debugging before the holidays. This means that when a VM is preempted it shuts down faster than it used to, and so ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:9057,detect,detects,9057,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['detect'],['detects']
Safety,Lazy centaur checkDescription timeout handling [NOJIRA],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5424:30,timeout,timeout,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5424,1,['timeout'],['timeout']
Safety,"Lenthall and wdl4s have been folded into the main Cromwell repo so the Lenthall and wdl4s repos should be deprecated. Add a message to the Readme, make it impossible for users to make new issues/PR's, etc. As a **user looking at the wdl4s or Lenthall repos**, I want **it to be obvious that they are deprecated, and to have useful information about where to find the artifacts within Cromwell**, so that **I am not tempted to develop against the deprecated repos**. - Effort: Small; - Risk: Small to Medium; - We'll have to keep an eye on the repos for a little while to make sure all interested parties got the message; - Business value: Small to Medium; - Having clear communication to users is important!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2733:485,Risk,Risk,485,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2733,1,['Risk'],['Risk']
Safety,LocalInitializationActor will explode on abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1111:41,abort,abort,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1111,1,['abort'],['abort']
Safety,"Log from broad-dsde-dev here: https://gist.githubusercontent.com/scottfrazer/a0838aa2180e7972da84c4730975b9f5/raw/d0cebdd317489298d6553e5602bfd4b775ab9ea3/gistfile1.txt. Most specifically:. ```; WorkflowActor [UUID(0790bc7e)]: Beginning transition from Running to Aborting.; WorkflowActor [UUID(0790bc7e)]: transitioning from Running to Aborting.; JES Run [UUID(0790bc7e):hello]: Status change from Running to Success; ERROR - 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Precondition check failed."",; ""reason"" : ""failedPrecondition""; } ],; ""message"" : ""Precondition check failed."",; ""status"" : ""FAILED_PRECONDITION""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Precondition check failed."",; ""reason"" : ""failedPrecondition""; } ],; ""message"" : ""Precondition check failed."",; ""status"" : ""FAILED_PRECONDITION""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/700:264,Abort,Aborting,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/700,2,['Abort'],['Aborting']
Safety,Logically revert #4263 since multi-Cromwell deployments no longer need a specific abort server. A simple git revert has a ton of conflicts but hopefully this shouldn't be too tough.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4596:82,abort,abort,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4596,1,['abort'],['abort']
Safety,"Logs captured from alpha environment:; ```; November 2nd 2018, 20:16:15.000 | 2018-11-03 00:16:15 [cromwell-system-akka.actor.default-dispatcher-57321] ERROR c.e.w.w.WorkflowStoreSubmitActor - Workflow com.mysql.jdbc.exceptions.jdbc4.MySQLTransactionRollbackException: Lock wait timeout exceeded; try restarting transaction submit failed.; -- | --.   | November 2nd 2018, 20:16:15.000 | 2018-11-03 00:16:15 [cromwell-system-akka.actor.default-dispatcher-57321] ERROR c.e.w.w.WorkflowStoreSubmitActor - Workflow com.mysql.jdbc.exceptions.jdbc4.MySQLTransactionRollbackException: Lock wait timeout exceeded; try restarting transaction submit failed.   | November 2nd 2018, 10:16:21.000 | 2018-11-02 14:16:21 [cromwell-system-akka.actor.default-dispatcher-42970] ERROR c.e.w.w.WorkflowStoreEngineActor - Error trying to fetch new workflows; com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure. The last packet successfully received from the server was 1 milliseconds ago. The last packet sent successfully to the server was 1 milliseconds ago.; 	at sun.reflect.GeneratedConstructorAccessor75.newInstance(Unknown Source); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:990); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3562); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3462); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3905); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2530); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2683); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2491); 	at com.mysql.jdbc.ConnectionImpl.setAutoCommit(ConnectionImpl.java:4807); 	at com.zaxxer.hikari.pool.ProxyConnection.setAutoCommit(ProxyConnection.java:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4360:279,timeout,timeout,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4360,2,['timeout'],['timeout']
Safety,Longer timeout and more actor names for MetadataBuilderActorSpec BT-53,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6093:7,timeout,timeout,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6093,1,['timeout'],['timeout']
Safety,Make PAPIv2 batch request timeouts configurable,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4973:26,timeout,timeouts,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4973,1,['timeout'],['timeouts']
Safety,Make find ' ' in filename safe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3864:26,safe,safe,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3864,1,['safe'],['safe']
Safety,"Makes use of the helper traits created in the I/O actor PR to abstract managing of backpressure messages etc.. . Creates a `DockerClientHelper` trait to isolate the timeout management logic (if the docker actor never responds, ensures that we don't hang forever). The changes in `HttpFlowWithRetry` add exponential backoff retries (previously they were simple immediate retries) to HTTP responses, and list explicitly the HTTP codes that are retryable.; More specifically, Http ""failures"", as in ""the http request itself failed"", are not retried, since akka already does that by default under the hood. Only Http responses with a retryable error code are retried asynchronously, following the same model as the I/O actor.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2077:165,timeout,timeout,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2077,1,['timeout'],['timeout']
Safety,Malformed UUID causes Call Cache Diff to timeout,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2453:41,timeout,timeout,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2453,1,['timeout'],['timeout']
Safety,Map literal declarations don't detect upstream dependencies,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1951:31,detect,detect,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1951,1,['detect'],['detect']
Safety,Metadata entry count safety limit should apply to matched rows only [52-hotfix],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5725:21,safe,safety,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5725,1,['safe'],['safety']
Safety,Metadata entry count safety limit should apply to matched rows only [BA-6484],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5584:21,safe,safety,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5584,1,['safe'],['safety']
Safety,Metadata entry count safety limit should apply to matched rows only [BA-6484]. [BA-6484]: https://broadworkbench.atlassian.net/browse/BA-6484,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5725:21,safe,safety,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5725,1,['safe'],['safety']
Safety,"MetadataValue.apply was throwing an NPE exception when passed null, even though it had a call to .getOrElse("""").; Consolidated standard backend runtimeAttributeDefinitions implementation.; Added a GoogleAuthModeSpec.assumeHasApplicationDefaultCredentials in tests that use application default credentials.; Refactored away JesBackendLifecycleActorFactory's toJes, only used in one place where a similar standard method now exists.; Refactored away JesBackendLifecycleActorFactory's staticRuntimeAttributeDefinitions, only used in specs.; CromwellServer no longer hard codes the binding timeout.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1823:586,timeout,timeout,586,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1823,1,['timeout'],['timeout']
Safety,Methods to Improve handling of GCP backend timeouts,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7551:43,timeout,timeouts,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7551,1,['timeout'],['timeouts']
Safety,Modified the reference docker submit command to avoid bash redirection,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1734:48,avoid,avoid,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1734,1,['avoid'],['avoid']
Safety,Modified the reference docker submit command to avoid bash redirection. Closes #1556,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1562:48,avoid,avoid,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1562,1,['avoid'],['avoid']
Safety,"Most of the genomic file types we work with in the variant discovery pipelines are typically accompanied by an index file with a conventionally predictable name (eg my_callset.vcf comes with my_callset.vcf.idx). Right now, as a WDL author, I have to supply these index files explicitly in my inputs json files and in several places in my workflows. This is very tedious, so it would be glorious to have Cromwell just automatically recognize when file inputs and outputs are one of a defined index-associated types, search for the corresponding indices based on given naming conventions, and implicitly co-localize the index files that it finds (but not fail if it doesn't find them, because sometimes we work without indices!).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1412:144,predict,predictable,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1412,1,['predict'],['predictable']
Safety,"My WDL pipeline failed to run with Cromwell 55 configured with the `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` Google API with a long list of errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; I was under the expectation that this had been handled in issue #5344 and that Cromwell would retry to access the files until available (the files do indeed exist at the time of this writing).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154:279,Timeout,Timeout,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154,3,['Timeout'],['Timeout']
Safety,"My jobs aren't starting because there's a giant queue of jobs that have been cancelled, but are waiting to start before they can be aborted. This shouldn't happen. If a job is aborted before it can be launched it shouldn't take a long time to process it. . I heard this might be fixed in 30 already, but if it's not, it would be great to have it fixed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2966:132,abort,aborted,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2966,2,['abort'],['aborted']
Safety,"NFS , cromwell and IO timeouts.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3648:22,timeout,timeouts,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3648,1,['timeout'],['timeouts']
Safety,"Never merge this.; Need sanity checking. Testing method which I used was to run HelloWorld workflow using Cromwell built from this branch and wait for successful Carboniting (or OOM, or Cromwell starting to crumble in other ways).; Modifications made in this branch force Cromwell to copy the given number of root workflow's metadata jsons into the resulted carbonited json as if they were subworkflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525:24,sanity check,sanity checking,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525,1,['sanity check'],['sanity checking']
Safety,No more unsafe `List.head` in SWRA. Closes #1615,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1616:8,unsafe,unsafe,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1616,1,['unsafe'],['unsafe']
Safety,"No regressions tests in the PR, but the following cases were tested manually:. - A bad key, never becoming good: failed after 5 minutes; - A bad key, becoming good after 1 minute: the workflow picked up and succeeded; - A bad key, followed by an abort after 1 minute: the workflow aborted successfully",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6983:246,abort,abort,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6983,2,['abort'],"['abort', 'aborted']"
Safety,No timeout on rc file,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4050:3,timeout,timeout,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4050,1,['timeout'],['timeout']
Safety,Note:; * [x] Resolve rebase on top of test timeouts before merging. Worth checking before merging:. * [x] Travis CI; * [x] Jenkins PAPIv2 CI ([jenkins results](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-papiv2-alpha1/776/)),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5929:43,timeout,timeouts,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5929,1,['timeout'],['timeouts']
Safety,"Nothing really clever about this PR, just cashing in on past investments in separation of concerns. 1. Remove SBT projects; - Clean compile time 64s -> 53s on M1; - Dependencies removed, no longer subject to security updates or conflicts (see https://github.com/broadinstitute/cromwell/pull/6948); 2. Remove Centaur integration tests; - Slightly improved Travis build time; - Less stuff to port when we leave Travis; 3. Sever connections between CWL and the rest of Cromwell; - Because of Cromwell's extremely compartmentalized design, only two files really reference CWL directly:; - Entry point for server mode; - Entry point for command-line Womtool; - Only small logic updates needed; 4. Can now safely delete top-level `cwl` directory because nothing depends on it. ---. Reviewer's guide:; - Commits up through [Remove obsolete tests](https://github.com/broadinstitute/cromwell/pull/6955/commits/7a26149d9e70818edf852a16b114809ca9c0dc29) are self-contained and pass CI on their own; - [No longer minimal](https://github.com/broadinstitute/cromwell/pull/6955/commits/557d7b72a97651bcdca8ee27590ebfa29473ad05) removes most of the code; - [Remove *.cwl files](https://github.com/broadinstitute/cromwell/pull/6955/commits/eb4eaef0574ec06a256d38bb222d01ebc44a7e9f) speaks for itself",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6955:700,safe,safely,700,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6955,1,['safe'],['safely']
Safety,Objectives. confirm:. - deadlocks are not observed; - work is distributed correctly; - abandoned workflows are recovered; - abort workflow is functional,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4241:111,recover,recovered,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4241,2,"['abort', 'recover']","['abort', 'recovered']"
Safety,"Omits the first of the two CCHE migrations which appears to be made unnecessary by the second, hopefully avoiding Götterdämmerung.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5040:105,avoid,avoiding,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5040,1,['avoid'],['avoiding']
Safety,"One of Morgan's input files was missing an md5 in its object metadata. Cromwell was dutifully falling back to our backup option, which is to read every byte of the file into memory and calculate the hash itself. This resulted in extraordinary network and CPU usage that destabilized the instance and caused a continual crash/reboot cycle. We think this is also what Lori ran into with the featured workspaces. Now, we detect & avoid this condition, print a warning, and carry on without call caching:; ```; 41183c60:ImputationBeagle.SubsetVcfToRegion:3:1:; Hash error ([Attempted 1 time(s)] - Exception:; File of type BlobPath requires hash in object metadata, not present for; https://lz8b0d07a4d28c13150a1a12.blob.core.windows.net/sc-94fd136b-4231-4e80-ab0c-76d8a2811066/hg38/inputs/palantir_merged_input_samples.liftedover.vcf.gz),; disabling call caching for this job.; ```. Obviously, we'd like to enhance this in the future so that call caching is still possible for these jobs, but we have to walk before we can run. ---. Visualization eye candy section!. Swiftly downloading a file on the datacenter multi-gigabit LAN:. ![Screenshot 2024-05-02 at 19 24 04](https://github.com/broadinstitute/cromwell/assets/1087943/46484bbd-30e0-4f88-8f6c-05b50649c557). Telltale CPU curve as we chew through one file after another:. ![Screenshot 2024-05-03 at 11 32 13](https://github.com/broadinstitute/cromwell/assets/1087943/7916ce63-8d4c-46f7-a86a-b3313edf0d77). Flame graph showing the smoking gun, `generateMd5FileHashForPath`:. ![Screenshot 2024-05-02 at 14 02 25](https://github.com/broadinstitute/cromwell/assets/1087943/0d06f3ad-8155-4b43-bef7-6d9ccce35132)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7419:418,detect,detect,418,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7419,2,"['avoid', 'detect']","['avoid', 'detect']"
Safety,"One possible solution: We should probably create a trait which loads all the configuration (once per application), and let classes mix it in to avoid doing ConfigFactory.load() at multiple places",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/796:144,avoid,avoid,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/796,1,['avoid'],['avoid']
Safety,Only execute isAlive once per timeout,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4220:30,timeout,timeout,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4220,1,['timeout'],['timeout']
Safety,"Order of events:. Start with #3344 and #3342 as those are the requirements for having multiple writer-Cromwell nodes share a database. Next, start #4239 to re-create a deadlocking issue, and address it with a solution:; #4249; #4240 . Generate test cases to make sure Cromwell is able to recover appropriately in case of shutdown:; #4242 ; And test cases to ensure that Cromwell is running/aborting workflows as expected across multiple nodes:; #4241",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4369:288,recover,recover,288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4369,2,"['abort', 'recover']","['aborting', 'recover']"
Safety,"Order of events:. Start with #3344 and #3342 as those are the requirements for having multiple writer-Cromwell nodes to safely share a database. Next, start #4239 de-serialize workflow heartbeats, and build a test case #4414 to re-create the deadlocking issue we've seen before in production. Follow up that work with a solution that addresses the deadlock. Below are two ideas brainstormed in the past:; #4249; #4240. Generate test cases to make sure Cromwell is able to recover appropriately in case of shutdown:; #4242. And test cases to ensure that Cromwell is running/aborting workflows as expected across multiple nodes:; #4241 . GDoc of plan as of March '19; https://docs.google.com/document/d/10AGE3foZsKOHlgUpq3BE4mkUYphcjYyxMt0miQz4FGk/edit?usp=sharing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4370:120,safe,safely,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4370,3,"['abort', 'recover', 'safe']","['aborting', 'recover', 'safely']"
Safety,"P2 - to optimize the case of restarting a single cromwell, upon shutdown the heartbeat related information (server, timestamp) should be deleted so the workflow is immediately picked up on restart rather than after the timeout period",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4242:219,timeout,timeout,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4242,1,['timeout'],['timeout']
Safety,"PAPI error code 2. Execution failed: pulling image: docker login: generic::unknown: retry budget exhausted (10 attempts): running docker login: exit status 1 (standard error: ""WARNING! Using --password via the CLI is insecure. Use --password-stdin.\nError response from daemon: Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers). Seen [here](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-papiv2/170)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4438:380,Timeout,Timeout,380,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4438,1,['Timeout'],['Timeout']
Safety,PAPI requests don't get removed from queue when aborted,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2750:48,abort,aborted,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2750,1,['abort'],['aborted']
Safety,"PR 1 of 3:; 1. Remove cromwell-core dependency from cloud-support; 2. Run jes centaur on travis; 3. Generate coverage for integration tests. ---. Instead of credentials requiring WorkflowOptions, any String => String will do, including Map[String, String].; Retrieving credentials only requires actorSystem/executionContext when retrying.; Moved logback dependencies from common library over to testing.; Added mockito to all artifact tests.; Fixed akka-stream-testkit dependency appearing in core's main instead of test.; Split confusingly named baseDependencies into configDependencies ++ catsDependencies.; Other dependency cleanup to reduce duplicates and extra transitive dependencies.; Log stderr from centaur'ed cromwell failures.; The total attempt time to connect to cromwell for a test is now longer than the timeout of a cromwell restart.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2938:819,timeout,timeout,819,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2938,1,['timeout'],['timeout']
Safety,"Part II of the ""unexpected `JobFailedNonRetryable` during cache output copying"" saga. - Stop using the confusing `JobFailedNonRetryable` response when an output copy fails (it's not like `JobSucceeded` which correctly tells us that the job has already succeeded); - Update the tests to reflect this; - Add some sanity checks to the EJEA's handlers (specifically - did we copy the right outputs, and did we fetch the right outputs from the database)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4091:311,sanity check,sanity checks,311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4091,1,['sanity check'],['sanity checks']
Safety,"Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synchronization` primitive works fine. It is also **just 2 lines of extra code**. So if the akka solution is quite elegant as well I would like to learn about that. If not, well, it is not too bad having 2 lines of understandable commented code that is not ""the proper way of doing things(TM)"".~~. * I used the SFS scalatests to make sure everything worked correctly. However this did not test whether the thread safety was working correctly. I have added a test wdl in centaur: `standardTestCases/cached_copy/cached_copy.wdl`. This workflow creates 10 jobs that read the same input file. This workflow will crash if the `cached-inputs` cache is not used in a thread-safe way. I tested this manually with `java -Dbackend.providers.Local.config.filesystems.local.localization.0=""cached-copy"" -jar server/target/scala-2.12/cromwell-41-*-SNAP.jar run centaur/src/main/resources/standardTestCases/cached_copy/cached_copy.wdl` . Is there a way to integrate such a test in scalatest file? I have tried the `.par` method, but that did not quite work. I hope you will consider this PR as it solves an important issue for us. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900:3013,safe,safety,3013,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900,2,['safe'],"['safe', 'safety']"
Safety,"Perf testing has shown that removing this query improves CC time and reduces DB load (see last row in CC google doc); Unclear if it's worth keeping it as a configurable thing ?; This keeps storing the individual hashes, it just stops using them for ""fast"" cache miss detection.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4121:267,detect,detection,267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4121,1,['detect'],['detection']
Safety,"Persist when jobs are ""Started"" (or running) and ""Aborted"" on the engine side for the duration of the workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3378:50,Abort,Aborted,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3378,1,['Abort'],['Aborted']
Safety,Potential hotfix candidate but would be nice to know why these empty queues are appearing in the first place. An attempt to recover from (though probably not fix the underlying cause of) tokens going missing due to stack traces like:; ```; [cromwell-system-akka.actor.default-dispatcher-1158] ERROR akka.actor.OneForOneStrategy - dequeue on empty queue; java.util.NoSuchElementException: dequeue on empty queue; 	at scala.collection.immutable.Queue.dequeue(Queue.scala:155); 	at cromwell.engine.workflow.tokens.TokenQueue.recursingDequeue(TokenQueue.scala:63); 	at cromwell.engine.workflow.tokens.TokenQueue.dequeue(TokenQueue.scala:50); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1(RoundRobinQueueIterator.scala:46); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1$adapted(RoundRobinQueueIterator.scala:46); 	at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:415); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1169); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1159); 	at scala.collection.immutable.StreamIterator.$anonfun$next$1(Stream.scala:1058); 	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1052); 	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:144); 	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:132); 	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:104); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.findFirst(RoundRobinQueueIterator.scala:48); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:32); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:10); 	at scala.collection.Iterator$SliceIterator.next(Itera,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4909:124,recover,recover,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909,1,['recover'],['recover']
Safety,Preemptible recovery from a checkpointing file [BW-460],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6137:12,recover,recovery,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6137,1,['recover'],['recovery']
Safety,"Produces a 500 error like; ```; {; ""status"": ""error"",; ""message"": ""Statement cancelled due to timeout or client request""; }; ```; TODO: if the 55-second request timeout fires and kills the request, we should still make sure the 60-second query kill also gets logged somehow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5087:94,timeout,timeout,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5087,2,['timeout'],['timeout']
Safety,"Proposed in the [PR for HTTPS imports](https://github.com/broadinstitute/cromwell/pull/2758) by @kcibul:. As a **user configuring Cromwell**, I want **the option to disable HTTPs imports**, so that **I can protect my system from possible security risks**. - Effort: Small; - Risk: Small; - Business value: Medium",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2773:247,risk,risks,247,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2773,2,"['Risk', 'risk']","['Risk', 'risks']"
Safety,Provides WES status and abort endpoints directly to the Cromwell server.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4425:24,abort,abort,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4425,1,['abort'],['abort']
Safety,Pull docker images before running tests with short timeouts BT-144 BT-146,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6228:51,timeout,timeouts,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6228,1,['timeout'],['timeouts']
Safety,Put the binding timeout back to where it was in spray-land,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2478:16,timeout,timeout,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2478,1,['timeout'],['timeout']
Safety,Q: Ever wonder why the first two `checkDescriptions` always failed in centaur?; A: Because out timeout handler was eager instead of lazy 🤦‍♂,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5424:95,timeout,timeout,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5424,1,['timeout'],['timeout']
Safety,Race conditions aborting,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1521:16,abort,aborting,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1521,1,['abort'],['aborting']
Safety,Rawls is periodically getting 404s when calling our `abort` endpoint. It resends the 404 every minute.; ; These accumulate over time so eventually Rawls is sending multiple aborts per minute.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4767:53,abort,abort,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4767,2,['abort'],"['abort', 'aborts']"
Safety,Recover from Docker image hash failures to fail the workflow.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/333:0,Recover,Recover,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/333,1,['Recover'],['Recover']
Safety,Recover from database failures when reading the workflow store. Develop edition,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2150:0,Recover,Recover,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2150,1,['Recover'],['Recover']
Safety,Recover from database failures when reading the workflow store. Hotfix edition,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2149:0,Recover,Recover,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2149,1,['Recover'],['Recover']
Safety,Recover support for Local PBE,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/666:0,Recover,Recover,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/666,1,['Recover'],['Recover']
Safety,Recovery functionality for HtCondor backend. Closes #1249.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1250:0,Recover,Recovery,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1250,1,['Recover'],['Recovery']
Safety,Recovery support for JES PBE,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/751:0,Recover,Recovery,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/751,1,['Recover'],['Recovery']
Safety,Recovery support for SGE Backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1162:0,Recover,Recovery,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1162,1,['Recover'],['Recovery']
Safety,Ref #3259: SGE jobs still in queue after workflow abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3325:50,abort,abort,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3325,1,['abort'],['abort']
Safety,Refactor job aborts via EJEA,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1504:13,abort,aborts,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1504,1,['abort'],['aborts']
Safety,Reflect Backend Abort Status more accurately,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1139:16,Abort,Abort,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1139,1,['Abort'],['Abort']
Safety,Remove abort server support from CromIAM,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4596:7,abort,abort,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4596,1,['abort'],['abort']
Safety,Remove cromiam abort BA-4596,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5215:15,abort,abort,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5215,1,['abort'],['abort']
Safety,Remove redundant WaitingForQueueSpace execution status [BA-6487 prereq],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5590:7,redund,redundant,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5590,1,['redund'],['redundant']
Safety,Remove redundant WaitingForQueueSpace status [BW-387],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6034:7,redund,redundant,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6034,1,['redund'],['redundant']
Safety,"Remove redundant nested /project directory, ignore BSP [no JIRA]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5931:7,redund,redundant,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5931,1,['redund'],['redundant']
Safety,Removes gap between workflow submission and abort request,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2759:44,abort,abort,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2759,1,['abort'],['abort']
Safety,"Replacing `awaitCond` with `eventually` we should also get a better failure message than ""timeout expired""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1442:90,timeout,timeout,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1442,1,['timeout'],['timeout']
Safety,RequestSender.java:140); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequest(NettyRequestSender.java:111); at org.asynchttpclient.DefaultAsyncHttpClient.execute(DefaultAsyncHttpClient.java:240); at org.asynchttpclient.DefaultAsyncHttpClient.executeRequest(DefaultAsyncHttpClient.java:209); at org.asynchttpclient.BoundRequestBuilder.execute(BoundRequestBuilder.java:35); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1(AsyncHttpClientBackend.scala:53); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1$adapted(AsyncHttpClientBackend.scala:42); at cats.effect.IO$.$anonfun$async$1(IO.scala:1042); at cats.effect.IO$.$anonfun$async$1$adapted(IO.scala:1040); at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:329); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:118); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); at cats.effect.IO.unsafeRunAsync(IO.scala:269); at cats.effect.IO.unsafeToFuture(IO.scala:341); at cromwell.languages.util.ImportResolver$.$anonfun$httpResolverWithHeaders$1(ImportResolver.scala:92); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$2(package.scala:25); at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); at scala.collection.immutable.List.foldLeft(List.scala:86); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$1(package.scala:22); at cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.wdl$draft3$transforms$wdlom2wom$FileElementToWomBundle$$importWomBundle(FileElementToWomBundle.scala:101); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$11(FileElementToWomBundle.scala:74); at cats.instances.VectorInstances$$anon$1.$anonfun$traverse$2(vector.scala:77); at cats.i,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3977:2654,unsafe,unsafeRunAsync,2654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977,1,['unsafe'],['unsafeRunAsync']
Safety,Rerun the abort id,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7465:10,abort,abort,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7465,1,['abort'],['abort']
Safety,Response timeout does not return Content-Type: `application/json`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/947:9,timeout,timeout,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/947,1,['timeout'],['timeout']
Safety,Restart / Recover should not kick off until services have initialized and Liquibase has run,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1196:10,Recover,Recover,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1196,1,['Recover'],['Recover']
Safety,Restart/recover migration. Closes #1119,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1340:8,recover,recover,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1340,1,['recover'],['recover']
Safety,Return 503 on timeouts,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3458:14,timeout,timeouts,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3458,1,['timeout'],['timeouts']
Safety,Return Spray timeouts as Json Closes #947,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1133:13,timeout,timeouts,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1133,1,['timeout'],['timeouts']
Safety,Robustify aborts to PRECONDITION_FAILED [BT-450] [CROM-6829],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6568:10,abort,aborts,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6568,1,['abort'],['aborts']
Safety,"Run @ruchim 's Travis test in a different mode, or branch, that shuts down and brings back up. Does a count right after to check that the right number of jobs are recovered (no duplicates). TO DO:; - [ ] Make sure you have as many JES jobs as you think you have; - [ ] If not, fix it!; - [ ] If so, yay!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2111:163,recover,recovered,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2111,1,['recover'],['recovered']
Safety,Run abort tests sequentially in centaur,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3042:4,abort,abort,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3042,1,['abort'],['abort']
Safety,Run mode should default to abort all jobs on CTRL-C,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2794:27,abort,abort,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2794,1,['abort'],['abort']
Safety,SFS job recovery. Closes #1162 Closes #666,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1319:8,recover,recovery,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1319,1,['recover'],['recovery']
Safety,"Safety net against long running ""log an event"" actions",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4947:0,Safe,Safety,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4947,1,['Safe'],['Safety']
Safety,Sanity Check,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3117:0,Sanity Check,Sanity Check,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3117,1,['Sanity Check'],['Sanity Check']
Safety,"Scala 2.13 upgrade per CROM-6036. The overwhelming majority of this is nitpicky noise, I added a few comments at sites where that is not the case. Sources of noise:. 1. `Traversable` and the whole `Traversable` family are gone.; 2. Stronger opinions about the presence of absence of parens in function invocations.; 3. `scala.collection.JavaConverters` moved to `scala.jdk.CollectionConverters`; 4. `MapView` introduced with some breaking changes to the usage of maps; 5. Conversion of `Array` to `IndexedSeq` now has to be explicit; 6. Instances of `case` matches being incomplete are now detected where previously they were not; 7. Right-biasing of `Either`s makes for a lot of annoying `toOption` / `swap` instead of `right` / `left`; 8. Type ascriptions required for some anonymous functions; 9. Explicit `.` now required for some method invocations; 10. `Stream` is no more, long live `LazyList`; 11. `Symbol` literal `'` syntax no longer supported; 12. etc etc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6724:590,detect,detected,590,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6724,1,['detect'],['detected']
Safety,Scale the HMSASpec ask timeout for jenkins.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4267:23,timeout,timeout,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4267,1,['timeout'],['timeout']
Safety,"Searched the codebase for `request-timeout`, found that we seem to use 40 seconds not the 55 previously discussed. Copied the config stanza from `cromwell/server/src/main/resources/application.conf` to CromIAM.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4615:35,timeout,timeout,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4615,1,['timeout'],['timeout']
Safety,"See https://broadworkbench.atlassian.net/browse/BA-6497. (not sure if I created the issue correctly so to be on the safe side I also report it here, sorry if superfluous)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5561:116,safe,safe,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5561,1,['safe'],['safe']
Safety,See https://doc.akka.io/docs/akka-http/current/routing-dsl/testkit.html#increase-timeout,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4250:81,timeout,timeout,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4250,1,['timeout'],['timeout']
Safety,"Seen in [Jenkins build 577](https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/577/) but also in a couple of my branches, where I seem to be able to reliably trigger it w/ some seemingly unrelated changes. . Sometimes they manifest as timeouts, in other cases the [wrong data is coming back](https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/577/testReport/junit/cromwell.webservice/MetadataBuilderActorSpec/MetadataParser_should_support_nested_lists/). In my branches I'm reliably able to get `should build workflow scope tree from metadata events` fail by simply changing the package of `CromwellApiServiceSpec` (see branch `jg_hmm`). That error is not in this jenkins run, but I've seen those other failures in some of my other experiments (see branch `jg_refactor_reality` although that's just a series of me making strange edits to see what happens)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4288:259,timeout,timeouts,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4288,1,['timeout'],['timeouts']
Safety,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4000:923,timeout,timeout,923,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000,1,['timeout'],['timeout']
Safety,Set back sensible defaults for abort on terminate,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2811:31,abort,abort,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2811,1,['abort'],['abort']
Safety,Set non-default timeouts on CWL conformance run_test.sh invocation.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3931:16,timeout,timeouts,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3931,1,['timeout'],['timeouts']
Safety,Setting the tmpdir to be word accessible poses a security risk at some high-performance compute clusters. Original idea seems to make it world writabled: https://github.com/broadinstitute/cromwell/pull/2053. I would even prefer to set it to be *only* accessable for the `cromwell` user (mod: `700`)... or just leave it as it was provided...,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3721:58,risk,risk,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3721,1,['risk'],['risk']
Safety,SgeInitializationActor will explode on abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1112:39,abort,abort,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1112,1,['abort'],['abort']
Safety,Shuffle job paths to avoid cache-vs-live collisions [BA-6236],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5415:21,avoid,avoid,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5415,1,['avoid'],['avoid']
Safety,Shuffle job paths to avoid cache-vs-live collisions: Part II [BA-6236],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5429:21,avoid,avoid,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5429,1,['avoid'],['avoid']
Safety,Slightly better abort behavior in standard backend.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2032:16,abort,abort,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2032,1,['abort'],['abort']
Safety,"So far cromwell is working great on our cluster. Thanks a lot for this splendid effort. However there is one issue we run into that other people might run into as well. Our cluster uses NFS as its filesystem backend. This means that if one node completes a job, it might take a while before the files that were created. In other workflows we can set an I/O timeout option: if the file did not appear within 3 minutes, the job failed. How do we do this in cromwell. All the settings I have available is `number-of-requests`, `per` and `number-of-attempts`. Currently we have; ```HOCON; io {; number-of-requests = 10; per = 10 seconds; number-of-attempts = 180; }; ```; This should make sure that failing files are attempted for 180 seconds, but this is not very elegant. There is a `timeout` option in I/O. But this is the timeout for the I/O operation to respond. If the file is not ""visible"" then the I/O operation will respond immediately, and the job will have failed. Is there a fix to this option in the current cromwell configuration?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3648:357,timeout,timeout,357,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3648,3,['timeout'],['timeout']
Safety,So it can manipulated safely by concurrent workflow actors in the WorkflowManagerActorSpec and tests pass consistently.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/76:22,safe,safely,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/76,1,['safe'],['safely']
Safety,Socket timeout talking to GcsFileSystemProvider,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/826:7,timeout,timeout,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/826,1,['timeout'],['timeout']
Safety,"Some aborted workflows still have ""running"" subworkflows",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3654:5,abort,aborted,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3654,1,['abort'],['aborted']
Safety,"Some background about this fix:; 1. Centaur submits a workflow to Cromwell; 2. Workflow succeeds; 3. Some non-CentaurTestException exception occurs; 4. Centaur swallows it silently and resubmits the workflow.; 5. The resubmitted workflow succeeds using call cached results; 6. Centaur test fails because tasks of the workflow are not expected to be call cached. This fix alters #4 from the above list and makes Centaur to report the exception before resubmitting the workflow. It doesn't solve the flakiness problem itself, but at least we'll see what happened the during the failed attempt. Regarding the non-CentaurTestException which caused flakiness in the first place I suspect the TimeoutExceptions happening in CentaurCromwellClient, but not 100% sure",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6197:687,Timeout,TimeoutExceptions,687,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6197,1,['Timeout'],['TimeoutExceptions']
Safety,"Some light reading for @Horneth and @kshakir. This is largely Frankensteining of ""Olde Style"" code. Known missing or broken, I need to confirm that appropriate tickets exist for the restoration of the following:; - [x] #753 Abort; - [x] #751 Recover; - [x] #749 Preemptibility; - [x] #785 Persistence of any data (note this would not be a ticket to create a KV / metadata service, but to integrate this backend with such a service); - [x] #809 Hashing (prereq for #750); - [x] #750 Caching; - [x] #806 implement Firecloud style auth upload / deletion (the code is not present here); - [x] #808 Retries were removed from command script upload and JES run creation. Lessons learned:; - [x] #811 #812 Actor Factories should be responsible for sanity-checking configs .; - [x] #813 Initialization actors should have the ability to create workflow-level resources that can be shared by the other actors collaborating in the workflow execution. e.g. GCS Filesystems need only be created once per workflow, not for every call.; - [x] It's not clear how workflow logging (or logging in general) should work.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/797:224,Abort,Abort,224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/797,2,"['Abort', 'Recover']","['Abort', 'Recover']"
Safety,Spike: abort 404s,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4767:7,abort,abort,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4767,1,['abort'],['abort']
Safety,StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65);,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:1608,recover,recoverAsync,1608,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,2,['recover'],['recoverAsync']
Safety,Statically detect bad ~{struct} interpolations,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3917:11,detect,detect,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3917,1,['detect'],['detect']
Safety,"Still need to performance-test on alpha. Problematic pairs:; * fetch <-> heartbeat (already coordinated); * fetch <-> abort (newly coordinated); * fetch <-> delete (newly coordinated). Example queries from MySQL deadlock printout in prod:. **Abort**; ```; update ; `WORKFLOW_STORE_ENTRY` ; set ; `WORKFLOW_STATE` = 'Aborting' ; where ; `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '109a9d01-10b6-425d-8381-12a9d3a2c134'. ```; **Delete from workflow store**; ```; delete `WORKFLOW_STORE_ENTRY` ; from ; `WORKFLOW_STORE_ENTRY` ; where ; `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c4961523-321b-4172-abe8-e1e4eba94f43'. ```; **Fetch startable workflows**; ```; select ; `WORKFLOW_EXECUTION_UUID`, ; `WORKFLOW_DEFINITION`, ; `WORKFLOW_URL`, ; `WORKFLOW_ROOT`, ; `WORKFLOW_TYPE`, ; `WORKFLOW_TYPE_VERSION`, ; `WORKFLOW_INPUTS`, ; `WORKFLOW_OPTIONS`, ; `WORKFLOW_STATE`, ; `SUBMISSION_TIME`, ; `IMPORTS_ZIP`, ; `CUSTOM_LABELS`, ; `CROMWELL_ID`, ; `HEARTBEAT_TIMESTAMP`, ; `HOG_GROUP`, ; `WORKFLOW_STORE_ENTRY_ID` ; from ; `WORKFLOW_STORE_ENTRY` ; where ; (; (`HEARTBEAT_TIMESTAMP` is null) ; or (; `HEARTBEAT_TIMESTAMP` < '2020-09-18 05:08:18.823'; ); ) ; and (; not (`WORKFLOW_STATE` = 'On Hold'); ) ; order by ; `SUBMISSION_TIME` ; limit ; 30 for ; update; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906:118,abort,abort,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906,3,"['Abort', 'abort']","['Abort', 'Aborting', 'abort']"
Safety,"Stopped closing a scala `Future` over akka's `context.become`.; Flipped the default for `requestsAbortAndDiesImmediately` from `false` to `true`.; When killing a Standard backend job with rADI false, both the rc and the standard error are required.; Writing the stderr on abort for the SFS backend.; When rADI is true, sending a backend status of `""Aborted""`.; In the engine, set and store the `ExecutionStatus.Aborted`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2032:272,abort,abort,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2032,3,"['Abort', 'abort']","['Aborted', 'abort']"
Safety,"Stumbled upon this while trying to abort a CWL workflow that was failing to submit to JES.; Aborting it was not short circuiting the ""try 10 times"" mechanism of creating JES runs or polling.; The workflow do end up in `Aborted` state after the 10 retries so we don't necessarily have to hotfix it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3069:35,abort,abort,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3069,3,"['Abort', 'abort']","['Aborted', 'Aborting', 'abort']"
Safety,"Subset of https://github.com/broadinstitute/cromwell/pull/7359 concerned with upgrading the Google Cloud SDK only, in order to deploy separately & de-risk.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7361:150,risk,risk,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7361,1,['risk'],['risk']
Safety,Support for aborting On Hold workflows.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4368:12,abort,aborting,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4368,1,['abort'],['aborting']
Safety,Sync centaur timeouts with heartbeats in test.inc.sh,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3874:13,timeout,timeouts,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3874,1,['timeout'],['timeouts']
Safety,TEST Recovering Jobs: Turn Cromwell off/on,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2111:5,Recover,Recovering,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2111,1,['Recover'],['Recovering']
Safety,"Tagged all database specific tests as `IntegrationTest`.; As mysql now has 5s to timeout, increased the testing timeout used to detect if mysql is available.; Removed unused code and optimized imports.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/398:81,timeout,timeout,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/398,3,"['detect', 'timeout']","['detect', 'timeout']"
Safety,Test reliability: filesystem startup timeouts and docker health check errors [BA-6164],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5341:37,timeout,timeouts,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5341,1,['timeout'],['timeouts']
Safety,"Tests like `abort.scheduled_abort` should run also run sequentially, inline with the current Restart test cases. Otherwise, Travis+Centaur produces false negative results, requiring the test to be re-run, and delaying PR merges. There are currently two suites of tests: those that run in parallel, and those that run sequentially. The ""sequential"" tests are currently _only_ of the category ""start a workflow job, restart, then ensure that job still succeeds"". [Here ](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L2955) is an example`abort.scheduled_abort` (wf 6d64cc05) running in parallel with a restart test case. The workflow job is supposed to abort. But because cromwell restarts during the middle of the job, TES is unable to tell if the job was running and marks it as [`Failed` instead of `Aborted`](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L3534). The [`RestartTestCaseSpec`](https://github.com/broadinstitute/cromwell/blob/4a37f95cc49567505ad50907f85c4fa046ac596e/centaur/src/it/scala/centaur/RestartTestCaseSpec.scala) could be renamed to `Sync` or `Sequential` and also used for these additional tests. In theory this should only take a half-day to test and fix and would save frustration across multiple PRs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3010:12,abort,abort,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3010,4,"['Abort', 'abort']","['Aborted', 'abort']"
Safety,"The 'should' string is:; ```; should abort a workflow mid run and restart immediately abort.restart_abort_tes *** FAILED ***; ```. The error message is:; ```; Metadata mismatch for calls.scheduled_abort.aborted.executionStatus - expected: ""Failed"" but got: ""Aborted""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3392:37,abort,abort,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3392,4,"['Abort', 'abort']","['Aborted', 'abort', 'aborted']"
Safety,"The CopyWorkflowActor regularly gets timeouts when trying to copy the gigabytes of data that are typically associated with production workflows. Also this duplicates the amount of disk space used for a workflow. . This remedies that problem by hardlinking the files. It is much much faster, and the cromwell-executions folder can be safely removed afterward. This is very beneficial for people who run cromwell on a cluster backend in `run` mode. Ping @illusional . I have included a test case.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6672:37,timeout,timeouts,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6672,2,"['safe', 'timeout']","['safely', 'timeouts']"
Safety,"The Horicromtal Deadlock test started failing the morning of Monday, 2/26 with Docker pull failures:; ```; Head ""https://registry-1.docker.io/v2/dockercloud/haproxy/manifests/latest"":; error parsing HTTP 429 response body:; invalid character 'S' looking for beginning of value: ; ""Server capacity exceeded.\n""; ```; I was able to pull the [`dockercloud/haproxy` image](https://hub.docker.com/r/dockercloud/haproxy) locally but found that it was last updated 6 years ago. My suspicion is that ancient images are stored in a much less hot level of cache in the bowels of Docker Hub and may be more susceptible to capacity issues and timeouts. In order to adopt a current, official HAProxy image, I had to make a very basic config and we were off to the races. This is because the `dockercloud` image was a bit customized with special sauce to automatically configure itself by detecting running Docker containers. As a bonus, the new Alpine-based image is actually smaller than the ancient one, albeit only 25 MB vs. 43 MB.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7376:631,timeout,timeouts,631,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7376,2,"['detect', 'timeout']","['detecting', 'timeouts']"
Safety,"The JobStore currently remembers Successful jobs (and their outputs) and Failed jobs.; It would be very useful to also remember when jobs are started (from a backend perspective) and aborted.; Indeed, not knowing if a job has already been aborted or even started forces us to create a backend actor for every single job of a restarting workflow to reconnect to it, and if this workflow was aborting, abort the backend job. Again.; Or at least that's the issue, we don't know if it's ""again"" or not because we don't know if the job had already been aborted before restart, or even been started, so we try anyway.; This behavior while guaranteeing that we don't leave any job unaborted even after restart is suboptimal and generates unnecessary requests and load on the system.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3378:183,abort,aborted,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3378,5,['abort'],"['abort', 'aborted', 'aborting']"
Safety,"The Methods Cromwell recently experienced an outage nearly identical to the Terra one, where requests to GCS and the database timed out. Like on Terra, a server restart fixed it. Their server is running version `54-97597a4` that [definitely has](https://github.com/broadinstitute/cromwell/commits/54_hotfix) the [PR](https://github.com/broadinstitute/cromwell/pull/5994) we did to handle null messages. In this PR I fixed another possible source of NPEs in `cromwell.engine.io.gcs.GcsBatchFlow#recoverCommand`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6118:494,recover,recoverCommand,494,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6118,1,['recover'],['recoverCommand']
Safety,"The `DrsCloudNioFileSystemProvider` was wrapping the retries of the `DrsPathResolver` with another set of `CloudNioRetry` retries. The product of these two retries at the previous configuration values would wait around 35 minutes (~:20 + 10 x ~3:30) to fail for each doomed attempt. That combined with a fairly wide scatter and a typo'd DRS path for `file` in code like . ```; task size {; input {; File file; }. Int file_size = ceil(size(file)); ...; }; ```; would completely block all 10 of the `IoActor`s [NIO threads](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/server/CromwellRootActor.scala#L108). These changes remove the nested retries in the engine and dial back the patience for retries. If we want the retries to be more patient we'll probably have to make other code that is competing for `IoActor` threads more patient as well. Utility files for reproducing this error can be cherry picked from commit `ff7bddc8830802f7a606177d0eaf19c8f47ca865`. I don't know how to programatically link Google accounts to NIH accounts in Bond to be able to include this Centaur test in CI, though maybe we don't need to be linked to make sure this negative case errors within a reasonable timeout?. Workflow`9635fbf0-00b1-4635-b482-5a782cda5cd5` induced this problem in production, its metadata shows multiple `HaplotypeCaller` shards erroring out with ; ```; Failed to evaluate input 'disk_size' (reason 1 of 1): [Attempted 1 time(s)] - RuntimeException: Unexpected response during DRS resolution: RuntimeException: Could not access object 'drs://dg4.DFC/...'. Status: 500, reason: 'Internal Server Error', Martha location: 'https://.../martha_v3', message: 'Received error while resolving DRS URL. getaddrinfo ENOTFOUND dg4.dfc'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6439:1229,timeout,timeout,1229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6439,1,['timeout'],['timeout']
Safety,"The `WdlCall` expression handler contained a `traverse` which generated OGINs for outer lookups in parallel.; To avoid that, the PR pre-generates the OGINs for the expressions to use and doesn't let the `traverse` create its own.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2976:113,avoid,avoid,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2976,1,['avoid'],['avoid']
Safety,"The `database.sql` artifact should follow the ""convention over configuration"" nature of [Rails](http://guides.rubyonrails.org/active_record_basics.html#convention-over-configuration-in-active-record), [GORM](http://docs.grails.org/3.1.11/guide/GORM.html#domainClasses), [Hibernate](http://docs.jboss.org/hibernate/orm/5.2/quickstart/html_single/#tutorial_annotations), etc. Classes should be named after the table, and replacing under_scores with CapitalizedNames. We can either:; 1. Liquibase update the database to match the scala names; 2. Update the scala to match the database naming; 3. Ignore the issue and not have a naming convention. Updating the scala seems like the least intrusive, avoiding any possibly time consuming database migrations and index rebuilding at the moment. However, developers may have put more effort into naming the scala tables and less into the sql tables. Liquibase updates could follow as part of a future issue, if/when the schema needs other modifications.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1423:695,avoid,avoiding,695,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1423,1,['avoid'],['avoiding']
Safety,The batched heartbeat writer and workflow picker upper both try to lock multiple rows in the workflow store table inside a transaction and were often observed to deadlock. These two workflow store accesses are now routed through an actor that effectively serializes access to the workflow store table (other accesses are not affected). If this manages to run the gauntlet of gulls [batch abort](https://github.com/broadinstitute/cromwell/issues/3753) would likely need to be added to this system. Known shortcomings:; - ~~Should probably give more thought as to the thread on which the blocking happens.~~ now on the IO dispatcher; - ~~Should consider actor supervision because if this one actor ever dies that will be bad times.~~ default Akka supervision is reasonable here; - ~~May keep one writer Cromwell from tripping over itself but wouldn't keep multiple writer Cromwells from tripping over each other~~ ticketed in #3795,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3761:388,abort,abort,388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3761,1,['abort'],['abort']
Safety,"The documentation for the new HPC backend 'check-alive' feature is a bit scattered an incomplete. https://cromwell.readthedocs.io/en/stable/backends/HPC/; > This option will implicitly enable polling with the check-alive option. https://github.com/broadinstitute/cromwell/releases:; > When the value exit-code-timeout-seconds is set, check-alive command is now only called once every timeout interval instead of each poll. https://github.com/broadinstitute/cromwell/blob/73ad264b4c7919d0bbd344fecbc903f819f5e16c/cromwell.example.backends/SGE.conf#L27; > # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). I had a look at the code and PRs implementing the feature, but am not confident I can even see how the feature is implemented, so cannot write the docs as a PR myself!. I had assumed, when you specify `exit-code-timeout-seconds`, cromwell would poll `check-alive` at that interval, and after 2 failed `check-alive`s with no rc file, would mark the job as failed. Now I think there is some other polling mechanism, perhaps to cap polling load per backend instead of scaling it with number of jobs. . Is it possible to revisit that documentation and actually explain what is happening? Alternatively just a link here to the `unrelated to this timeout` documentation?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877:310,timeout,timeout-seconds,310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877,5,['timeout'],"['timeout', 'timeout-seconds']"
Safety,"The following workflow failed in cromwell (4ef40f07-ff52-426b-9610-3c9dc66ec67e) on production. Looking metadata we have no logs for the step that failed. ```; {; ""executionStatus"": ""Failed"",; ""shardIndex"": 5,; ""outputs"": {. },; ""runtimeAttributes"": {. },; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""input_bam"": ""SortAndFixSampleBam.output_bam"",; ""ref_fasta"": ""ref_fasta"",; ""ref_dict"": ""ref_dict"",; ""disk_size"": ""agg_medium_disk"",; ""dbSNP_vcf"": ""dbSNP_vcf"",; ""known_snps_sites_vcf"": ""known_snps_sites_vcf"",; ""dbSNP_vcf_index"": ""dbSNP_vcf_index"",; ""known_indels_sites_vcf_index"": ""known_indels_sites_vcf_index"",; ""input_bam_index"": ""SortAndFixSampleBam.output_bam_index"",; ""recalibration_report_filename"": ""sample_name + \"".recal_data.csv\"""",; ""known_snps_sites_vcf_index"": ""known_snps_sites_vcf_index"",; ""ref_fasta_index"": ""ref_fasta_index"",; ""sequence_group_interval"": ""subgroup"",; ""known_indels_sites_vcf"": ""known_indels_sites_vcf""; },; ""failures"": [{; ""failure"": ""Call failed to initialize: Could not persist runtime attributes: Timeout after 5059ms of waiting for a connection."",; ""timestamp"": ""2016-04-23T09:14:54.651Z""; }],; ""backend"": ""JES"",; ""end"": ""2016-04-23T09:14:56.000Z"",; ""attempt"": 1,; ""executionEvents"": [],; ""start"": ""2016-04-23T09:14:45.000Z""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/737:1045,Timeout,Timeout,1045,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/737,1,['Timeout'],['Timeout']
Safety,"The idea being, sending an abort message does not get an `AbortSuccess` or `AbortFailed` response directly. Instead, the actor being aborted will cause its operation to fail and the parent should expect an ""Aborted"" response instead of a Success or Failure (although a Success or Failure may still be returned anyway). In this way, it's acceptable for a backend implementor to simply do nothing when `abort` is received. The parent will just continue waiting for a result of some sort.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/759:27,abort,abort,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/759,6,"['Abort', 'abort']","['AbortFailed', 'AbortSuccess', 'Aborted', 'abort', 'aborted']"
Safety,"The mint team submits workflows in ""on hold"" status and then has a separate service to start those workflows. If there is an issue with any of the on-hold workflows, there is no way to abort them until after they start running. It would be convenient to have the ability to abort those workflows so that our starter service queue is not full of these erroneous workflows that would fail.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4311:185,abort,abort,185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4311,2,['abort'],['abort']
Safety,"The test `cromwell.backend.standard.callcaching.StandardFileHashingActorSpec` has a failing test that asserts ""send a timeout to the ioActor the command doesn't hash"". It intermittently fails when run, suggesting a race condition may exist",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2816:118,timeout,timeout,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2816,1,['timeout'],['timeout']
Safety,"The zeroth localizers checks to see if a file exists before re-localizing. The copy localizer should therefore copy-to-temp-then-rename. Current broken behavior:; - Run SGE task with a large `input.bam` and copy localization; - Cromwell starts copying to `<call_root>/input.bam`; - Kill the job during localization; - Restart cromwell; - Cromwell detects the partial `<call_root>/input.bam` exists.; - The job continues without relocalizing. Better behavior:; - Run SGE task with a large `input.bam` and copy localization; - Cromwell starts copying to `<call_root>/input.bam.tmp`; - Kill the job during localization; - Restart cromwell; - Cromwell doesn't detects the partial `<call_root>/input.bam` exists.; - The job continues with relocalizing. And when cromwell isn't killed:; - Run SGE task with a large `input.bam` and copy localization; - Cromwell starts copying to `<call_root>/input.bam.tmp`, ensuring to overwrite previous results; - When copying finishes rename `<call_root>/input.bam.tmp` to `<call_root>/input.bam`; - The job continues. NOTE: Most people do not like copying inputs anyway, so this hasn't been a major issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1426:347,detect,detects,347,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1426,2,['detect'],['detects']
Safety,"There are two general cases where the `WorkflowExecutionActor` switches the workflow to a failed state:; - [`handleRetryableFailure()`](https://github.com/broadinstitute/cromwell/blob/f64c16e62c84b66ddba14705bede5e6fde8376b0/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L292-L306); - [`handleExecutionFailure()`](https://github.com/broadinstitute/cromwell/blob/f64c16e62c84b66ddba14705bede5e6fde8376b0/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L240-L255). In _both_ cases, the method:; - is triggered by the failure of an individual job, but; - fails the entire workflow, and; - doesn't send signals to other jobs to stop. Thus other jobs stay running, while the workflow gets deregistered from the system. Because the workflow manager can no longer delegate aborting the running jobs, this issue may also be related to #1414 and #1504.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2029:864,abort,aborting,864,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2029,1,['abort'],['aborting']
Safety,"There were a **lot** of general cleanup rabbit holes I wanted to go down but figured I'd stick w/ a safe harbor of ""do what the ticket asks"" and then follow up with a series of more targeted clean up PRs. If people would prefer (or start asking for changes along those lines) I'll pull this back and do this. Along similar lines there's a clear overlap erupting between wes2cromwell code and cromiam, however only in places where I could literally use cromiam code as-is did I do so (and even then I left it in cromiam instead of a shared package). While I was working on this it became clear that the ideal shared abstraction is still too early to tell so I'd rather see it play out a bit before going down that road.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3901:100,safe,safe,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3901,1,['safe'],['safe']
Safety,"There's some flakiness in the abort tests right now, I believe the reason is we tend to abort too quickly when a workflow is being restarted (for example if we receive an abort message during materialization, we'll assume that it's fine to stop without going further, which is not the case if the workflow is being restarted as jobs might be running and need to be reconnected to)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2820:30,abort,abort,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2820,3,['abort'],['abort']
Safety,"There’s a new response from JES that should be retryable: ; JES error code 2. Message: Instance failed to start due to preemption.” . THE CATCH: Henry chatted with Google, and it sounds like JES error code 2 isn’t *always* about preemption. (But in the past few weeks we’ve had a handful of cloud workflows failing each day from this response, always with the message about preemption). So to be safe, we want to make this one retryable based on the combination of the code + the message. . Example: ; UUID: 7da0394b-371f-4fdb-ae70-737833c4fbfa; OPERATION_ID: operations/EKmIx96ALBjh373VhLH0ui8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. Relevant workflow metadata: ; ""failures""; : [; {; ""causedBy"": [],; ""message"": ""Task PairedEndSingleSampleWorkflow.CollectUnsortedReadgroupBamQualityMetrics:2:1 failed. JES error code 2. Message: Instance failed to start due to preemption.""; }; ],",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2970:396,safe,safe,396,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2970,1,['safe'],['safe']
Safety,These inputs used to live in the WORKFLOW_EXECUTION_AUX table in .19. We use this table as kind of a back pocket sanity check if our workflow results are not making sense. Is it possible to get this table revived in .20?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1238:113,sanity check,sanity check,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1238,1,['sanity check'],['sanity check']
Safety,These race conditions can leave Cromwell in an inconsistent state:; - The `WEA` might receive an `AbortedResponse` outside of the `Aborting` state. It should do something appropriate; - The `WEA` might receive something other than `AbortedResponse` while in the `Aborting` state. It should do something appropriate,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1521:98,Abort,AbortedResponse,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1521,4,['Abort'],"['AbortedResponse', 'Aborting']"
Safety,"Things Changed; - added a cromwell-master, which refreshes status metadata. There can be only one.; - created a pool of cromwell_norefresh, which has status refresh turned off, these can scale; - found race condition when multiple cromwells try to create the liquibase lock table at once, configured to have master go first; - updated scripts/compose to handle the above two kinds of cromwell; - increased to 100 batches of 10 workflow each; - changed timeout script to show number of completed workflows and break when done; - delete database at start of run, so the above works; - ran heartbeats in auto-commit mode rather than in a single transaction; - dump out logs at end of run for debugging. Things I'd like to share; - Lock Ordering in SELECT...FOR UPDATE no es bueno, there are great feature in MYSQL v8 (SKIP LOCKED) but we can't use those yet; - how to configure mysql for query logging, and what it shows; - heartbeat batches were never a batched update, just a big transaction; - slick terminology can give give the wrong intuition; - impact of cleaning db before each run; - No deadlocks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4508:452,timeout,timeout,452,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4508,1,['timeout'],['timeout']
Safety,"This PR adds `WORKFLOW_NAME`, `TASK_INPUTS`, `TASK_DISKS`, and `MONITORING_CONFIG` environment variables for `MonitoringAction` in PAPIv2 backend. These variables are used to pass details about task inputs and disk mappings (both in JSON form), along with an image-specific config string (e.g. `project-id.dataset-id.table-id` for `quay.io/broadinstitute/cromwell-monitor-bigquery`), into the container specified through the existing `monitoring_image` option. It also adds `bigquery.insertdata` OAuth scope, to be used for streaming monitoring data into BigQuery (@adrazhi seems to approve scope extension).; ; This PR will enable us to:; - stream monitoring data at scale into BQ (much more so than was possible through Stackdriver),; - build detailed models for prediction of runtime resource utilization, using BQ or external tools (e.g. Looker); - easily detect runtime failure modes such as running OOM. (Please see https://github.com/broadinstitute/cromwell-task-monitor-bbq for more info on BQ use case). However, the proposed changes are not specific to BQ (apart from the scope), and could be used for other `monitoring_image` implementations in the future, thanks to the new `monitoring_config` option for PAPIv2 backend. **Please note**: this is an initial implementation that's **not yet ready for a merge**. For example, `TASK_INPUTS` are not serialized correctly yet. We intend to add more commits to implement it fully. However, we're soliciting early feedback and review. Interested parties: @kshakir, @benjamincarlin, @rexwangcc, @mohawkTrail, @ruchim, @abaumann",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028:765,predict,prediction,765,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028,2,"['detect', 'predict']","['detect', 'prediction']"
Safety,"This PR:; - embiggens PK of `CALL_CACHING_ENTRY` table updates it's associated FK constraints; - embiggens PK of `CALL_CACHING_AGGREGATION_ENTRY` table; - embiggens PK of `CALL_CACHING_DETRITUS_ENTRY` table; - embiggens PK of `CALL_CACHING_SIMPLETON_ENTRY` table. And sets the auto-increment counter of above tables and `CALL_CACHING_HASH_ENTRY` table to 20,000,000,000. See more details for this change in [this document](https://docs.google.com/document/d/1AyWVVf2pHA6BukYo3yPG2f3CZ8dTWk7F-c9KZC9WMjg/edit#heading=h.a69ivurg93m6). Reference - https://github.com/broadinstitute/cromwell/pull/5379. Remediation for https://broadworkbench.atlassian.net/browse/PROD-707",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6815:599,Remediat,Remediation,599,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6815,1,['Remediat'],['Remediation']
Safety,"This adds a mechanism of gzipping the list of output files in the AWS backend to avoid the container override size limit. This mechanism was already in place for the inputs, this will simply utilize it for the outputs as well. I haven't tested this yet (hence the draft) and from what I have seen it isn't being tested by centaur either, so I'll still need to have a look at that.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5441:81,avoid,avoid,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5441,1,['avoid'],['avoid']
Safety,"This allows the `ORDER BY` to use the index and is also more consistent because varying workflow lengths could cause sorting by ID to actually be incorrect with respect to end time. 190 ms, uses `IX_WORKFLOW_METADATA_SUMMARY_ENTRY_MAS_ET`:; ```; select ; SQL_NO_CACHE `WORKFLOW_EXECUTION_UUID`, ; `WORKFLOW_NAME`, ; `WORKFLOW_STATUS`, ; `START_TIMESTAMP`, ; `END_TIMESTAMP`, ; `SUBMISSION_TIMESTAMP`, ; `PARENT_WORKFLOW_EXECUTION_UUID`, ; `ROOT_WORKFLOW_EXECUTION_UUID`, ; `METADATA_ARCHIVE_STATUS`, ; `WORKFLOW_METADATA_SUMMARY_ENTRY_ID` ; from ; `WORKFLOW_METADATA_SUMMARY_ENTRY` ; where ; (; (; `WORKFLOW_STATUS` in ('Succeeded', 'Aborted', 'Failed'); ) ; and (; `METADATA_ARCHIVE_STATUS` is null; ); ) ; and (; `END_TIMESTAMP` <= '2021-05-11 21:21:21.265212'; ) ; order by ; `END_TIMESTAMP` ; limit ; 20;; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6350:634,Abort,Aborted,634,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6350,1,['Abort'],['Aborted']
Safety,"This bug was caused by ""userland"" (WDL) code throwing an exception where we did not expect it, causing issues for the ""kernel"" (Cromwell). Now we encapsulate possible exceptions in an `ErrorOr` so they can be safely evaluated in the `case Left(f) =>`. As a stylistic point, I changed `WomMap()` to `WomMap.apply()` to make it more obvious that we're really doing a lot of custom stuff and it's not just a regular old constructor. As a bonus, the new code detects all previous stuck-aborting workflows and fails them. After:; ```; INFO - WorkflowManagerActor: Workflow 1432d67e-3e95-40c8-acbd-d42f75040f1b failed (during ExecutingWorkflowState): cromwell.engine.workflow.lifecycle.execution.WdlRuntimeException: Failed to evaluate 'example2' (reason 1 of 1): Evaluating { ""second"": test, ""lowerLayer"": example1 } failed: Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; INFO - WorkflowManagerActor: Workflow actor for 1432d67e-3e95-40c8-acbd-d42f75040f1b completed with status 'Failed'. The workflow will be removed from the workflow store.; ```; ```; {; 	""status"": ""Failed"",; 	""id"": ""1432d67e-3e95-40c8-acbd-d42f75040f1b""; }; ```. Before:; ```; ERROR - Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; java.lang.UnsupportedOperationException: Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; 	at wom.values.WomMap.<init>(Wo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7385:209,safe,safely,209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385,3,"['abort', 'detect', 'safe']","['aborting', 'detects', 'safely']"
Safety,"This change enables blob filesystems to be opened that do not belong to the workspace, including public containers, and containers the requesting user has access to via WSM. This involves frequent 'refreshing' of the stored open filesystem in the underlying NIO implementation. To minimize the number of redundant requests to WSM, the SAS token for each open filesystem is stored and checked for expiration before attempting to reopen a previously open filesystem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7140:304,redund,redundant,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7140,1,['redund'],['redundant']
Safety,"This draft PR is one half (WIP) of migrating the E2E workflow test over to the dsp-reusable-workflows repo. There is a sibling branch on that repo as well that also needs to be developed along side this (same branch name [`WX-1307-port`], but no PR made just yet.). The PR here simply reduces the workflow so that the run-script job simply generates a token and passes all required inputs to the test script workflow housed in the other repo. If this migration moves forward, the sibling branch must either be merged first or merged at the same time as this one to avoid having to worry about branch referencing in the workflow. If migrating is optional, then you can choose to move forward or drop it if other priorities pop up (since the vanilla `WX-1307` branch can be used to cover the e2e testing from the Cromwell repo).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7239:565,avoid,avoid,565,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7239,1,['avoid'],['avoid']
Safety,"This enables tests on a lot of Slick code that's not actually used yet in the New Worlde (nothing writes to the core engine tables since only Recover needs that and Recover hasn't been implemented). So these changes are valuable iff the New Worlde ultimately uses an engine Slick API that looks a lot like that in the Olde Worlde. My guess is that will end up being true, but at this point that's only a guess. Some things here will certainly be nixed (ExecutionEvents) or are likely to be heavily modified (anything caching-related).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/942:142,Recover,Recover,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/942,2,['Recover'],['Recover']
Safety,"This fixes the docker deadlock by doing 2 things:. - makes sure that all `HttpResponse`s are either consumed or discarded to avoid https://github.com/akka/akka/issues/19538; - removes the decoupling between the `tokenFlow` and the `manifestFlow`; This is believed to be the main cause of the problem. Decoupling the token request from the manifest request can create a situation where all the connections are being used for token requests, and no connection is available to make a manifest request which makes the stream freeze.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2287:125,avoid,avoid,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2287,1,['avoid'],['avoid']
Safety,"This fixes the issue in PAPI2 and revert it to what it was before for PAPI1. Since this change was targeted at CWL and we don't plan to support CWL on PAPI1, it seems safer to revert it entirely in case it still breaks something else",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3802:167,safe,safer,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3802,1,['safe'],['safer']
Safety,This is a small batch to fix #4857 by implementing recoverAsync in AwsBatchAsyncBackendJobExecutionActor. I have tested this in our environment and it appears to work.; Implementation is based on pattern in other AsyncBackendJobExecutionActor classes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216:51,recover,recoverAsync,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216,1,['recover'],['recoverAsync']
Safety,"This is the test `""assign default runtime attributes""` in `MaterializeWorkflowDescriptorActorSpec`. To avoid having to recompute the defaults separately in every backend, it should be filled in by the MWDA",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1068:103,avoid,avoid,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1068,1,['avoid'],['avoid']
Safety,"This makes the job-avoidance conf really optional.; Also fixed a small bug when you don't specify the path for inputs, the default one was not correct.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/284:19,avoid,avoidance,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/284,1,['avoid'],['avoidance']
Safety,"This might be the only remaining type of transient failure which hasn't been patched, but it does pop up fairly frequently. It looks like the usual pattern of retries would work here. ```; cromwell.core.CromwellFatalException: java.util.NoSuchElementException; 	at cromwell.core.CromwellFatalException$.apply(core.scala:17); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.util.NoSuchElementException; 	at java.util.ArrayList$Itr.next(ArrayList.java:854); 	at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:43); 	at scala.collection.IterableLike$class.head(IterableLike.scala:107); 	at s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1966:522,recover,recoverWith,522,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1966,2,['recover'],['recoverWith']
Safety,"This proposal should give us more flexibility regarding docker tags while keeping the call caching safety on false positive / negative. Docker runtime attributes with docker hashes do not need any additional processing. All logic in this ticket applies to docker runtime attributes with a ""floating"" tag, which will be referred as ""tag"" in this issue. In all cases, if Cromwell fails to retrieve the docker hash for a task, for any reason, the corresponding call(s) will NOT be eligible for call caching, neither read nor write, regardless of the call caching configuration in effect. **When to get the hashes and what to do with them:**. 1. Cromwell will lookup the hashes corresponding to docker tags, for all docker attributes in all tasks in a workflow and its subworkflows, at Materialization time.; If the runtime attribute value can't be determined, the task in question will be ineligible for call caching. The only case when that should be true is if the attribute is an expression with variables depending on previous tasks being run. 2. If the hash lookup succeed, Cromwell will use that hash to perform any call cache read / write according to the call caching configuration in effect. It will also provide that hash, along with the original floating tag, to the backend when the job gets dispatched. 3. Backends will choose wether to use the hash or the floating tag. They will report to the engine which one they used, so that the engine can send this information to the metadata. **How to get the hash:**. 1. How to get the hash depends on the backend. Which means, at this time, that only workflows for which the backend is known statically at workflow submission time will be supported. 2. If the task is expected to run on the **Local Backend**, Cromwell will attempt to find the hash corresponding to the tag on the machine where it's being run. This first attempt must be done without executing a `pull` to avoid overriding the current local image, if it exits, with the remote rep",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2048:99,safe,safety,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048,1,['safe'],['safety']
Safety,"This removes several classes of confusing `ERROR` log messages that we're receiving under healthy conditions. For example, when calling `checkAccess` in the filesystem provider, which is intended to be used to detect whether a path exists, the library would log an error containing the path string. This resulted in several `ERROR` level log messages every time we created a directory structure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6982:210,detect,detect,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6982,1,['detect'],['detect']
Safety,This should also fix transient failures of the `abort a workflow mid run and restart immediately abort.restart_abort_tes` centaur test,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4197:48,abort,abort,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4197,2,['abort'],['abort']
Safety,"This test appears to not deal with eventual consistency correctly and sporadically fails for what looks like bogus reasons:. ```; [info] WorkflowExecutionActorSpec:; [info] WorkflowExecutionActor; [info] - should retry a job 2 times and succeed in the third attempt *** FAILED ***; [info] cromwell.core.package$CromwellFatalException: org.scalatest.exceptions.TestFailedException: ""Running"" was not equal to ""Preempted""; [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:44); [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:43); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:344); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:343); [info] at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); [info] at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); [info] ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1311:636,recover,recoverWith,636,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1311,2,['recover'],['recoverWith']
Safety,"This ticket is a reminder to discuss what @cjllanwarne was saying Friday afternoon. I don't believe this is a release blocker, but noting it for future consideration:. The interplay between restart and caching is possibly not ideal. When restarting with cache read turned on, Cromwell will begin potentially expensive hash calculations on jobs that may have previously been running. However, Cromwell currently does this calculation before checking if jobs were actually running and recoverable, which if they were would make hash calculation unnecessary. . On the other hand, perhaps determining whether a job is running in the backend is more expensive than calculating hashes. Shrug. . Anyway, the current scheme is likely more accidental than intentional and would benefit from some discussion.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1441:483,recover,recoverable,483,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1441,1,['recover'],['recoverable']
Safety,This was requested by @patmagee - I agree that it's a good idea. Find a way to detect if a liquibase migration is pending if Cromwell starts. Add a config option (defaulting to a safe mode) such that if this option is enabled and a liquibase migration is required that the process will exit with an error message stating:. - That a migration is necessary; - Encouragement to the user to backup their database and/or do further testing if in a production environment; - Describe how to override (including via command line) the setting to allow Cromwell to start properly.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2429:79,detect,detect,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2429,2,"['detect', 'safe']","['detect', 'safe']"
Safety,"This will enable us to protect from inadvertendyl putting secrets in code:. See: https://cloudplatform.googleblog.com/2017/07/help-keep-your-Google-Cloud-service-account-keys-safe.html with this section:; ""prevent committing keys to external source code repositories""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2479:175,safe,safe,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2479,1,['safe'],['safe']
Safety,Thread-safe getFileSystem method in S3FileSystemProvider [BA-6156],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5328:7,safe,safe,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5328,1,['safe'],['safe']
Safety,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:15599,Timeout,Timeout,15599,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,8,['Timeout'],['Timeout']
Safety,Timeout persisting runtime attributes causes workflow failure.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/737:0,Timeout,Timeout,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/737,1,['Timeout'],['Timeout']
Safety,Timeout when running dockerScripts,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4933:0,Timeout,Timeout,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4933,1,['Timeout'],['Timeout']
Safety,"To avoid lossy conversion during Centaur tests metadata assertions for a shard that has multiple attempts, this PR adds attempt number in the flattened metadata structure:. ```; <workflow_name>.<task_name>.<shard_index>.<attempt_number>.<metadata_key>; ```. Closes https://broadworkbench.atlassian.net/browse/BW-482",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6158:3,avoid,avoid,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6158,1,['avoid'],['avoid']
Safety,"To make cromwell a bit more self-sufficient as a ""server"", having access to `wdltool inputs` as an API endpoint would avoid needing to orchestrate a client having access to wdltool for that functionality.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2651:118,avoid,avoid,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2651,1,['avoid'],['avoid']
Safety,"To make cromwell a bit more self-sufficient as a ""server"", having access to `wdltool validate` as an API endpoint would avoid needing to orchestrate a client having access to wdltool for that functionality. . This issue is a companion to #2651.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2652:120,avoid,avoid,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2652,1,['avoid'],['avoid']
Safety,"Transient ""failure"" in metadata during Abort",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4484:39,Abort,Abort,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4484,1,['Abort'],['Abort']
Safety,"Tried to re-run the 10K JG workflow with CC on, the workflow failed almost immediately with multiple errors like; ```; [ERROR] [04/18/2017 21:11:44.685] [cromwell-system-akka.dispatchers.service-dispatcher-86] [akka://cromwell-system/user/cromwell-service/ServiceRegistryActor/MetadataService/metadata-summary-actor] Failed to summarize metadata; java.util.concurrent.RejectedExecutionException: Task slick.basic.BasicBackend$DatabaseDef$$anon$2@64919660 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@1dce40e4[Running, pool size = 20, active threads = 20, queued tasks = 1000, completed tasks = 800]; 	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047); 	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823); 	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369); 	at slick.util.AsyncExecutor$$anon$2$$anon$3.execute(AsyncExecutor.scala:120); 	at slick.basic.BasicBackend$DatabaseDef$class.runSynchronousDatabaseAction(BasicBackend.scala:233); 	at slick.jdbc.JdbcBackend$DatabaseDef.runSynchronousDatabaseAction(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$class.runInContext(BasicBackend.scala:210); 	at slick.jdbc.JdbcBackend$DatabaseDef.runInContext(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$class.run$1(BasicBackend.scala:153); 	at slick.basic.BasicBackend$DatabaseDef$class.runInContext(BasicBackend.scala:157); 	at slick.jdbc.JdbcBackend$DatabaseDef.runInContext(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$class.runInContext(BasicBackend.scala:179); 	at slick.jdbc.JdbcBackend$DatabaseDef.runInContext(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$class.runInternal(BasicBackend.scala:78); 	at slick.jdbc.JdbcBackend$DatabaseDef.runInternal(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$class.run(BasicBackend.scala:75); 	at slick.jdbc.JdbcBackend$DatabaseDef.run(JdbcBackend.scala:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182:654,Abort,AbortPolicy,654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182,1,['Abort'],['AbortPolicy']
Safety,"Triggered by investigation of DSDEEPB-1934. This caused problems in the past but I was unable to recreate a situation where a backend would update an abort function mid-call. Even if our backends no longer update abort functions mid-call, there's no reason not to allow it for potential future backends.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/326:150,abort,abort,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/326,2,['abort'],['abort']
Safety,Try to reproduce hashing timeouts in a cromwell that's not being spammed on /stats,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3712:25,timeout,timeouts,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3712,1,['timeout'],['timeouts']
Safety,"Trying to read an object that has the same name as a variable causes Cromwell to abort not just the workflow, but the entire Cromwell instance; WDL file:; ```; task TestTask {; 	; 	command {; 		echo ""Hello World!"" > hello_world.txt; 	}. 	output {; 		File exists = ""hello_world.txt""; 	}. 	runtime {; 		docker: will_fail.docker; 		memory: will_fail.memory; 		disks: ""local-disk "" + will_fail.small_disk + "" HDD""; 	}; }. workflow KillsCromwell {; 	String test_string. # This here kills Cromwell; 	Object runtime_params = read_object(runtime_params). 	call TestTask ; }; ```. Inputs File:; ```; {; 	""KillsCromwell.test_string"": ""This is a string"",; 	""KillsCromwell.runtime_params"": {; 		""genomes_cloud_image"": ""broadinstitute/genomes-in-the-cloud:2.2.4-1469632282"",; 		""small_disk"": 100,; 		""medium_disk"": 200,; 		""large_disk"": 300,; 		""x_large_disk"": 400,; 		""preemptible_tries"": 3; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1946:81,abort,abort,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1946,1,['abort'],['abort']
Safety,Trying to recover space to prevent us hitting the 10 TB limit.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4880:10,recover,recover,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4880,1,['recover'],['recover']
Safety,Tune staleness threshold for backpressure to avoid unwanted slowdowns [CROM-6850],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6638:45,avoid,avoid,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6638,1,['avoid'],['avoid']
Safety,Tweaks to test timeouts.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4296:15,timeout,timeouts,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4296,1,['timeout'],['timeouts']
Safety,Unable to recover running jobs in AWS backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:10,recover,recover,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Safety,Under test as: https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-papiv2-alpha1/773/console. EDIT: Timeout needed to be reset to 10h. Retrying as:; * PAPIv2-alpha: https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-papiv2-alpha1/774/console; * PAPIv2-beta: https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-papiv2-beta/165/console,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5924:118,Timeout,Timeout,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5924,1,['Timeout'],['Timeout']
Safety,"Unstick abort, develop edition.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2154:8,abort,abort,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2154,1,['abort'],['abort']
Safety,"Unstick abort, hotfix edition.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2153:8,abort,abort,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2153,1,['abort'],['abort']
Safety,"Until we work this out, we shouldn't risk accidentally CCing, or spamming awful error messages.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3125:37,risk,risk,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3125,1,['risk'],['risk']
Safety,"Unwired the `DockerHashLookupWorkerActor`.; Removed the unused docker registry client.; Cleaned up spray dependencies now that the spray-client is no longer used.; Refactored places that were sending spray classes down into the business logic.; Turned back on deprecation warnings.; Fixed scalaz flatMap deprecation warning by adding Y.A. implicit import.; Undeprecated `TerminalUtil` used by the single workflow runner's pretty printer.; Removed empty files from git.; Bumped timeout for `SharedFileSystemJobExecutionActorSpec.recoverSpec` up to 10 seconds dilated.; Increased sleep for `WorkflowExecutionActorSpec.""retry a job 2 times""` up to 3 seconds dilated.; Updated scalatest to 3.0.0.; Removed leftover bits of `DontUseMainSpecTest`.; Printing test times, minimal stack traces, and resummarizing tests failures, via http://www.scalatest.org/user_guide/running_your_tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1338:477,timeout,timeout,477,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1338,2,"['recover', 'timeout']","['recoverSpec', 'timeout']"
Safety,Up the CWL conformance test timeout BT-272,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6336:28,timeout,timeout,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6336,1,['timeout'],['timeout']
Safety,Update requester-pays error detection [BT-574],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6689:28,detect,detection,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6689,1,['detect'],['detection']
Safety,"Update the Docker image regex to expect port numbers to have 4 or 5 digits. Interested in thoughts on this approach, because it's not bulletproof in either direction:; * 1-3 digit port numbers are technically possible, though I've never observed one associated with a Docker repo; * 4-5 digit numeric image tags are technically possible. If this doesn't feel like a safe solution, I can take a stab at a more aggressive update to this regex.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7367:366,safe,safe,366,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7367,1,['safe'],['safe']
Safety,"Using `better.files` for metadata writing, and removed heaviest duplicated `FileUtil` similarity.; Refactored `Main` to avoid test issues with `DelayedInit`, `System.exit`, consistent error generation, etc.; Removed blocking code from `SingleWorkflowRunnerActor`, and passing `Shutdown` as message now, instead of killing system directly.; Reused / refactored some of the test methods for actors, and removed bitrotted `DefaultWorkflowManagerActor`.; Left comments (warnings?) about possible bugs / improvements to workflow `Actor` messaging.; Replaced use of Java `SystemProperties` with Scala `sys.props`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/265:120,avoid,avoid,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/265,1,['avoid'],['avoid']
Safety,"Version 1.67 is vulnerable and versions 1.69+ are safe. Upgraded to latest version 1.70. `sbt assembly` succeeds. The line; ```; addDependencyTreePlugin; ```; enables a handy new command; ```; whatDependsOn org.bouncycastle bcprov-jdk15on 1.67; ```; shows what chain of artifacts uses a leaf-node dependency – an inversion of the traditional dependency tree:; ```; [info] org.bouncycastle:bcprov-jdk15on:1.67; [info] +-org.bouncycastle:bcpkix-jdk15on:1.67; [info] +-io.grpc:grpc-xds:1.46.0; [info] +-io.grpc:grpc-googleapis:1.46.0; [info] +-com.google.api:gax-grpc:2.18.1; [info] +-com.google.cloud:google-cloud-resourcemanager:1.4.0; [info] +-org.broadinstitute:cloud-nio-spi_2.13:80-d24645a-SNAP [S]; [info] +-org.broadinstitute:cloud-nio-util_2.13:80-d24645a-SNAP [S]; ```. With this PR's fix in place, the output is a gratifying ""this isn't used anywhere"" error:; ```; root(aen_bw_1227)> | 80> whatDependsOn org.bouncycastle bcprov-jdk15on 1.67; [error] Expected 'org.broadinstitute'; [error] Expected '1.70'; [error] whatDependsOn org.bouncycastle bcprov-jdk15on 1.67; [error] ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6775:50,safe,safe,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6775,1,['safe'],['safe']
Safety,"WARNING: This PR is huge and needs to be reviewed carefully, we have already performed many manual tests + ported many other tests from PAPI. ## Intro. The main goal is to refactor Batch backend to include [PipelinesApiRequestManager](https://github.com/broadinstitute/cromwell/blob/5448b85bf334e0970665a69549e796199acc8bd7/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/api/PipelinesApiRequestManager.scala) and [PipelinesApiRequestWorker](https://github.com/broadinstitute/cromwell/blob/5448b85bf334e0970665a69549e796199acc8bd7/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/api/PipelinesApiRequestWorker.scala). This also fixes a few missing details from the initial Batch integration (#7177), for example:; 1. Missing metrics are now published.; 2. The job status is queried before deleting it to try preventing the deletion of jobs that are in a final state (PAPI can abort jobs but Batch deletes them instead). I have been trying to split this into multiple smaller PRs, please let me know if you can find any piece that can be submitted independently, previous PRs:. - #7413; - #7394 ; - #7411; - #7410 ; - #7393; - #7421; - #7428. <details>; <summary>Questions (already resolved)</summary>. ## Questions. 1. There is a centaur test included in the Batch suite, still, this seems to invoke a papi test [testCentaurGcpBatch.sh](https://github.com/broadinstitute/cromwell/blob/5448b85bf334e0970665a69549e796199acc8bd7/src/ci/bin/testCentaurGcpBatch.sh#L25) (see [papi_v2alpha1_gcsa.test](https://github.com/broadinstitute/cromwell/blob/5448b85bf334e0970665a69549e796199acc8bd7/centaur/src/main/resources/standardTestCases/papi_v2alpha1_gcsa.test#L3), the test itself says that batch backend is not used). Could this be related to the false-alarms from codecoverage's bot?; 2. There warnings raised by codecov which seem wrong, for example, the lines mentioned on [GcpBatchGroupedRequests](https",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412:975,abort,abort,975,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412,1,['abort'],['abort']
Safety,WM-2428: Include full error context when failing to abort TES jobs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7354:52,abort,abort,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7354,1,['abort'],['abort']
Safety,WX-1110[risk=low] Added endpoint to fetch failed tasks by workflow id,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7165:8,risk,risk,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7165,1,['risk'],['risk']
Safety,WX-1339 Make `throwExceptionOnExecuteError` false for PAPI aborts,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7245:59,abort,aborts,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7245,1,['abort'],['aborts']
Safety,WX-1748 Increase read timeout for engine function evaluation,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7485:22,timeout,timeout,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7485,1,['timeout'],['timeout']
Safety,WX-757 Fix workflow stuck in aborting after WDL type error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7385:29,abort,aborting,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385,1,['abort'],['aborting']
Safety,"Was meaning to push this before heading out for the week: . A work-in-progress Job Store writer which skirts the problem of databases by not actually ever using one. Instead, every Job has a known filesystem location and we just write to and (not yet) read back from disk. Currently has all of the hooks and wiring needed to write jobs the JobStore and clear them out on workflow completion. All that should be left is to update the JobStoreReader to read back the JSON from an appropriate file (but the tests are there for the JSON implicits and they seems good). NB I went down the JSON route because I anticipate an eventual DB schema more like the metadata, to avoid having to store multiple MBs or GBs in a single cell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1155:665,avoid,avoid,665,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1155,1,['avoid'],['avoid']
Safety,"We are seeing infrequent situations where JES says a call is done but cromwell thinks it is still running. I see the call starting in the logs:; ```; 2017-02-08 18:55:58,500 cromwell-system-akka.dispatchers.engine-dispatcher-963 INFO - WorkflowExecutionActor-fa7e25a2-f51f-4763-9f8a-5a2e5cd1c954 [UUID(fa7e25a2)]: Starting calls: pon_gatk_workflow.PadTargets:NA:1; ```. Then the only suspicious things I see later in the logs are these messages (which could be completely unrelated however they do start to appear 2.5 minutes after the job completes on JES):; ```; 2017-02-08 19:17:07,588 cromwell-system-akka.dispatchers.backend-dispatcher-424 INFO - The JES polling actor Actor[akka://cromwell-system/user/cromwell-service/$b/$a/$Enn#762671444] unexpectedly terminated while conducting 100 polls. Making a new one...; java.lang.NullPointerException: null; 2017-02-08 19:17:07,588 cromwell-system-akka.dispatchers.backend-dispatcher-246 ERROR - null; ```. For future reference by a FireCloud admin the operations id is ELSl1PihKxjdhp-6gvr3weYBILma7PWMHyoPcHJvZHVjdGlvblF1ZXVl and the workflow id is fa7e25a2-f51f-4763-9f8a-5a2e5cd1c954 and the workflow was aborted 3PM Feb 8.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1965:1158,abort,aborted,1158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1965,1,['abort'],['aborted']
Safety,"We are trying to abort a job in FireCloud using cromwell .21. The workflow appears stuck in submitted state. It appears to have no calls. Is there are race condition that is we abort too soon the job is stuck?. ```; ~/projects/rawls [develop*] $ curl -H ""Authorization: Bearer `gcloud auth print-access-token`"" https://cromwell2.dsde-alpha.broadinstitute.org/api/workflows/v1/b29c0ef3-e988-4d70-8093-bdd1a039170d/status; {; ""status"": ""Submitted"",; ""id"": ""b29c0ef3-e988-4d70-8093-bdd1a039170d""; }(dvoet@wm163-585) 14:55; ~/projects/rawls [develop*] $ curl -H ""Authorization: Bearer `gcloud auth print-access-token`"" https://cromwell2.dsde-alpha.broadinstitute.org/api/workflows/v1/b29c0ef3-e988-4d70-8093-bdd1a039170d/metadata; {; ""submittedFiles"": {; ""inputs"": ""{\""test.hello.name\"":\""subject_HCC1143\""}"",; ""workflow"": ""task hello {\n String? name\n\n command {\n echo 'hello ${name}!'\n }\n output {\n File response = stdout()\n }\n runtime {\n docker: \""ubuntu\""\n }\n}\n\nworkflow test {\n call hello\n}"",; ""options"": ""{\n \""default_runtime_attributes\"": {\n \""zones\"": \""us-central1-c us-central1-f\""\n },\n \""google_project\"": \""broad-dsde-alpha\"",\n \""auth_bucket\"": \""gs://cromwell-auth-broad-dsde-alpha\"",\n \""refresh_token\"": \""cleared\"",\n \""final_workflow_log_dir\"": \""gs://fc-ceb841f8-e512-4f70-831b-eb2c43af9b42/d938c20b-916d-4bd5-a132-533c72a84eb0/workflow.logs\"",\n \""account_name\"": \""test.firec@gmail.com\"",\n \""jes_gcs_root\"": \""gs://fc-ceb841f8-e512-4f70-831b-eb2c43af9b42/d938c20b-916d-4bd5-a132-533c72a84eb0\""\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""b29c0ef3-e988-4d70-8093-bdd1a039170d"",; ""inputs"": {. },; ""submission"": ""2017-01-20T16:37:31.589Z""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1885:17,abort,abort,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1885,2,['abort'],['abort']
Safety,We currently do not have automated CWL testing in Centaur. this puts us at risk for regressions. Start w/ ; * 1st-workflow.cwl; * three_step.cwl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2835:75,risk,risk,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2835,1,['risk'],['risk']
Safety,"We have a cromwell task that failed with the unhelpful error message: `Unexpected failure in EJEA (root cause not captured).` After investigation we found that the task was being killed by the google cloud compute system:; ```; ""error"": {; ""code"": 1,; ""message"": ""Operation canceled at 2017-07-23T01:01:39-07:00 because it is older than 6 days""; },; ```; It would be better if cromwell was able to report this error directly, so we don't have to look at the gcloud operations properties. Looking at the cromwell logs I see this sequence:; ```; 2017-07-23 08:09:42 [cromwell-system-akka.actor.default-dispatcher-3449] WARN c.e.w.l.e.WorkflowExecutionActor - WorkflowExecutionActor-f15009ab-b5bc-47ac-b832-3d0ae2f15888 [UUID(f15009ab)]: WorkflowExecutionActor [UUID(f15009ab)] received an unhandled message: AbortedResponse(PairedEndSingleSampleWorkflow.MarkDuplicates:-1:1) in state: WorkflowExecutionInProgressState; 2017-07-23 08:09:47 [cromwell-system-akka.actor.default-dispatcher-3375] ERROR c.e.workflow.WorkflowManagerActor - WorkflowManagerActor Workflow f15009ab-b5bc-47ac-b832-3d0ae2f15888 failed (during ExecutingWorkflowState): Unexpected failure in EJEA (root cause not captured).; ```. In this case the google operations ID is `EOjNtPvUKxiOgeqc763--TEgn6KQ6Z4NKg9wcm9kdWN0aW9uUXVldWU`. I can provide more operations IDs if necessary.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2496:806,Abort,AbortedResponse,806,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496,1,['Abort'],['AbortedResponse']
Safety,"We have a workflow that scatters 95 wide as its first step. When running with `read_from_cache=false` the workflow chugs along normally. When we allow the same workflow to read from the cache, Cromwell seems to lock up after the first handful of cache hits(~30). Cromwell will stop responding to api requests and after some time with logs being written the workflow that was getting the cache hits will hit 503 and timeout errors. When running the workflow with `read_from_cache=false` we run into none of these errors. Timeout Error. ```; 2016-05-05 17:37:02,285 cromwell-system-akka.actor.default-dispatcher-25 WARN - Configured registration timeout of 1 second expired, stoppingw; ```. 503 Error. ```; Exception occurred while attempting to copy outputs from gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/ccba2c79-c998-4f03-b736-af097391db66/call-SplitGvcf/shard-50 to gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/7164dc88-af61-4ea6-8a73-f0b79594ae9a/call-SplitGvcf/shard-50. com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable. {. ""code"" : 503,. ""errors"" : [ {. ""domain"" : ""global"",. ""message"" : ""Backend Error"",. ""reason"" : ""backendError"". } ],. ""message"" : ""Backend Error"". }. at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[cromwell.jar:0.19]. at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[cromwell.jar:0.19]. at com.google.api.client.google",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/794:415,timeout,timeout,415,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/794,3,"['Timeout', 'timeout']","['Timeout', 'timeout']"
Safety,We've seen a few times the IO error `Failed to evaluate job outputs [...] Futures timed out after [10 seconds]` in tests. I hypothesize that file read times from buckets may suffer from occasional outliers due to 🌩. I know this is the right timeout to change thanks to [this branch](https://github.com/broadinstitute/cromwell/compare/aen_make_it_timeout?expand=1) where I induced error `Failed to evaluate job outputs [...] Futures timed out after [10 microseconds]`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4036:241,timeout,timeout,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4036,1,['timeout'],['timeout']
Safety,We've seen in centaur local testing that we fail at times because cromwell is reading an output file before all of its contents has been written and we have no way to detect this (except via the artificial expectation management of centaur). There was some handwaving about filesystems and putting in sleeps but both of these seem like the wrong path. @kcibul this seems like this could go under the reliability umbrella?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1868:167,detect,detect,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1868,1,['detect'],['detect']
Safety,"What do you mean ""metricable"" isn't a word?. \<looks quickly on google\>. ... ""I am **not** using [Metrizable](https://www.yourdictionary.com/metrizable)...!"". ---. Additional commentary and/or things worth sanity checking me on:. * We lose ""root workflow ID"" and ""bucket"" from the log message.; * We gain task name and hog group in the metric path.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5470:207,sanity check,sanity checking,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5470,1,['sanity check'],['sanity checking']
Safety,When Cromwell comes online it detects if there are any unfinished workflows in the system and will pick those back up.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/755:30,detect,detects,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/755,1,['detect'],['detects']
Safety,"When I run scatter I get ""shard-n"". It would be nice to have an option to define shards names, for instance in some pipelines I would be happy to use sample names instead of shard-n to be able to easily detect what sample was processed in a wrong way.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2265:203,detect,detect,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2265,1,['detect'],['detect']
Safety,"When [creating the zipped import resolver](https://github.com/broadinstitute/cromwell/blob/develop/languageFactories/language-factory-core/src/main/scala/cromwell/languages/util/ImportResolver.scala#L133), we create a zip file on disk with format; ```; /tmp/imports_workflow_a5756f37-a2dc-4e57-be4e-0d0531812fef_5414558548666056879.zip; ```; and then unzip it into a directory like; ```; /tmp/imports_workflow_a5756f37-a2dc-4e57-be4e-0d0531812fef_5414558548666056879.zip3360767550785143230; ```; We immediately clean up the temporary file, but the temporary directory sticks around forever. Import resolvers are instantiated from scratch during restart, so it should be safe to clean up the directory as soon as the workflow stops. ---. For extra credit:. Writing temporary items to disk has two very undesirable features:; - Global scope; - Infinite lifetime. Find and adopt a solution that lets us completely isolate workflows from each other's filesystem side-effects, and lets the JVM do the cleanup for us with garbage collection as soon as the workflow stops. I think we could pull this off with NIO, but I have not done my homework on this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4406:670,safe,safe,670,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4406,1,['safe'],['safe']
Safety,"When a user specifies a Docker image of `whoever/myimage:latest`, they almost certainly don't want to avoid to a version of the job that was latest two years ago. This is the current behaviour. Please instead resolve the string to a Docker image hash and use that for call caching instead. The final Docker hash used should end up in call-level metadata so we can know what it is later. Tagging @abaumann so he knows I made this and can chime in to agree with me if there are any questions. Further Refinement from Office Hours:; - Supported Docker Image Repository; -- must have: DockerHub, GCR; -- should have: ECR if it doesn't delay, otherwise a separate ticket; -- bonus points to verify this works against Quay ; - Only API v2 supported ; - Support for both public and private images",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1617:102,avoid,avoid,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1617,1,['avoid'],['avoid']
Safety,"When cromwell is halted (ctl-c), it aborts jobs, but then starts new jobs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1600:36,abort,aborts,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600,1,['abort'],['aborts']
Safety,"When cromwell needs to recover many jobs at once, or check many caches at once it seems to only do one at a time. I found this with the following wdl, scattered by about 300 ways, run on broad filesystem with the SGE backend. I'd be happy to provide the json and list of inputs if needed. ```; task Decapitate {; 	File inputFile; 	String outputFilename. 	command {; 		tail -n+2 ${inputFile}; 	}; 	runtime {; 		memory: ""4 GB""; 	}; 	output {; 		Array[Array[String]] outputMatrix=read_tsv(stdout()); 	}; }. task BreakUpRow {; 	String PROJECT; 	String SAMPLE; 	String DATA_TYPE; 	String COMPARE_SAMPLE ; 	String COMPARE_PROJECT; 	String CLEAN_SAMPLE; 	String CLEAN_COMPARE_SAMPLE. 	command {; 		#do nothing; 	}; 	runtime {; 		memory: ""1 GB""; 	}; 	output {; 		Array[File] bams = [""/seq/picard_aggregation/${PROJECT}/${CLEAN_SAMPLE}/current/${CLEAN_SAMPLE}.bam"", ""/seq/picard_aggregation/${COMPARE_PROJECT}/${CLEAN_COMPARE_SAMPLE}/current/${CLEAN_COMPARE_SAMPLE}.bam""]; 		Array[File] indexes = [""/seq/picard_aggregation/${PROJECT}/${CLEAN_SAMPLE}/current/${CLEAN_SAMPLE}.bai"", ""/seq/picard_aggregation/${COMPARE_PROJECT}/${CLEAN_COMPARE_SAMPLE}/current/${CLEAN_COMPARE_SAMPLE}.bai""]; 		File genotypes = ""/seq/references/reference_genotypes/non-hapmap/${PROJECT}/Homo_sapiens_assembly19/${CLEAN_SAMPLE}.vcf""; 		String PROJECT_out = ""${PROJECT}""; 		String SAMPLE_out = ""${SAMPLE}""; 		String DATA_TYPE_out = ""${DATA_TYPE}""; 		String COMPARE_SAMPLE_out = ""${COMPARE_SAMPLE}""; 		String COMPARE_PROJECT_out = ""${COMPARE_PROJECT}""; 		String CLEAN_SAMPLE_out = ""${CLEAN_SAMPLE}""; 		String CLEAN_COMPARE_SAMPLE_out = ""${CLEAN_COMPARE_SAMPLE}""; 	}. }. task Fingerprint {; String PICARD; File input_bam; File input_bam_index; File haplotype_database_file; File genotypes; String vcf_sample; String output_name; ; command {; java -Dsamjdk.buffer_size=131072 -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -Xmx1024m \; -jar ${PICARD} \; CheckFingerprint \; INPUT=${input_bam} \; OUTPUT=${output_name} \; GENOTYPES=${genotypes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1844:23,recover,recover,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1844,1,['recover'],['recover']
Safety,"When running a task on docker locally, if the task is aborted, the script that feeds the wdl command into the docker container will be destroyed - but not the docker itself. ; Before abort:. ```; wm0e9-683:cromwell tjeandet$ ps -ef | grep c72df249-ccf5-460b-8654-9606a655c669; 1250915218 11002 10990 0 6:13PM ?? 0:00.00 /bin/bash -c cat cromwell-executions/w/c72df249-ccf5-460b-8654-9606a655c669/call-t/script | docker run --rm -v /Users/tjeandet/Codebase/cromwell/cromwell-executions/w/c72df249-ccf5-460b-8654-9606a655c669/call-t:/root/w/c72df249-ccf5-460b-8654-9606a655c669/call-t -i ubuntu:latest /bin/bash <&0; 1250915218 11004 11002 0 6:13PM ?? 0:00.15 docker run --rm -v /Users/tjeandet/Codebase/cromwell/cromwell-executions/w/c72df249-ccf5-460b-8654-9606a655c669/call-t:/root/w/c72df249-ccf5-460b-8654-9606a655c669/call-t -i ubuntu:latest /bin/bash; 1250915218 11015 2783 0 6:13PM ttys000 0:00.00 grep c72df249-ccf5-460b-8654-9606a655c669; ```. After:. ```; wm0e9-683:cromwell tjeandet$ ps -ef | grep c72df249-ccf5-460b-8654-9606a655c669; 1250915218 11004 1 0 6:13PM ?? 0:00.15 docker run --rm -v /Users/tjeandet/Codebase/cromwell/cromwell-executions/w/c72df249-ccf5-460b-8654-9606a655c669/call-t:/root/w/c72df249-ccf5-460b-8654-9606a655c669/call-t -i ubuntu:latest /bin/bash; 1250915218 11021 2783 0 6:14PM ttys000 0:00.00 grep c72df249-ccf5-460b-8654-9606a655c669; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1126:54,abort,aborted,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1126,2,['abort'],"['abort', 'aborted']"
Safety,"When running jobs on backends with job runtime limits such as LSF or SLURM, jobs reaching the runtime limits are killed by the backend. [Cromwell never detects that this occurs](http://gatkforums.broadinstitute.org/wdl/discussion/9542/does-cromwell-detect-task-failures-based-on-check-alive), and will wait forever for a job that is already dead. It would be helpful to configure periodic checks for whether tasks are still alive, and enter failure modes for non-zero return codes when unfinished tasks are no longer alive.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2281:152,detect,detects,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2281,2,['detect'],"['detect-task-failures-based-on-check-alive', 'detects']"
Safety,"When the workflow-restart config option is set to false, incomplete workflows will not be restarted, and instead all incomplete workflows are aborted. . Note: Still need to update ReadMe. Questions: The sys.addShutdownHook takes about a minute to start startup (there's a minute long wait between the logs) which seems too long?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/841:142,abort,aborted,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/841,1,['abort'],['aborted']
Safety,"When there are a lot (~1800 or as many as 3000) of cache hits in a workflow, timeouts or other errors talking to Google often occur (probably can all be fixed with added retries). Impact: High. Prevents us from using call caching for 20k sample sets, which in turn requires us to manually stitch together outputs from multiple workflows to gather all the successes together, which is prone to error. When it is feasible to use, this problem still often requires us to repeatedly relaunch the same workflow, duplicating the data many times. This can be tested by the same WDL used to verify #1185",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1444:77,timeout,timeouts,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1444,1,['timeout'],['timeouts']
Safety,"When using a docker hash in V25 in order to get call caching I get the following error: . ```; 2017-05-01 18:06:39 [cromwell-system-akka.actor.default-dispatcher-3670] ERROR c.e.w.l.e.EngineJobExecutionActor - Failed copying cache results for job GenotypeGVCFsComparison.IndexVCF:-1:1, invalidating cache entry.; java.io.IOException: Failed to copy gs://dsde-palantir/SnapshotExperiment2015/CEUTrio/gvcfs/NA12878.d691bf66-37af-4375-9c09-6a6607f322e8.g.vcf.gz to gs://broad-dsde-methods/cromwell-execution-25/GenotypeGVCFsComparison/c5bc4f99-d969-49ca-9e2e-a3dac7a4b12a/call-IndexVCF/dsde-palantir/SnapshotExperiment2015/CEUTrio/gvcfs/NA12878.d691bf66-37af-4375-9c09-6a6607f322e8.g.vcf.gz; 	at cromwell.core.path.PathCopier$$anonfun$copy$2.applyOrElse(PathCopier.scala:49); 	at cromwell.core.path.PathCopier$$anonfun$copy$2.applyOrElse(PathCopier.scala:48); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.core.path.PathCopier$.copy(PathCopier.scala:48); 	at cromwell.backend.standard.StandardCacheHitCopyingActor.duplicate(StandardCacheHitCopyingActor.scala:36); 	at cromwell.backend.callcaching.CacheHitDuplicating$$anonfun$copySimpletons$1.apply(CacheHitDuplicating.scala:66); 	at cromwell.backend.callcaching.CacheHitDuplicating$$anonfun$copySimpletons$1.apply(CacheHitDuplicating.scala:62); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2229:963,recover,recoverWith,963,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229,1,['recover'],['recoverWith']
Safety,"When we traced through the new actor model there was a lot of wonkiness in how an abort request works its way through the system. There were end runs being made, direct shortcuts to specific things, etc. . Our thoughts were:; - Make sure that the whole process is async. An abort request returns immediately and the workflow is set to an `aborting` state (as opposed to `aborted` and ??? if it fails to abort); - Have the abort request work its way through the workflow in a more logical manner, working down from the `WorkflowActor` to the `WorkflowExecutionActor` to the `EJEA`, etc; - Have each stage watch for all of its children (not necessarily in a supervision sense) to complete or not before sending the success/failure response back up",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1414:82,abort,abort,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1414,6,['abort'],"['abort', 'aborted', 'aborting']"
Safety,"Whenever I run workflow with mistake a see some weird errors in cromwell-server akka-logs while nothing is written to cromwell-workflow-logs; Here is a simple example:; ```; workflow worms {. File samplesFile. #Name \t File; Array[Array[File]] samples = read_tsv(samplesFile). scatter (sample in samples) {; call stats {; input:; fileName = sample(0), #simple mistake with ""()"" instead of ""[]""; file = sample(1); }; }; }. task stats {. String fileName; File file. command {; /opt/sratoolkit/sra-stat ${file} > stats.txt; }. runtime {; docker: ""itsjeffreyy/sratoolkit""; }. output {; File stats = ""stats.txt""; }; }; ```; Here I make quite typical mistake by using () instead of []. But when I send it to the server for execution it I get an empty cromwell-workflow-logs folder and errors are displayed only in stdout of the cromwell server. What I expect is to see them in cromwell-workflow-logs (if my expectations are wrong, it would be nice to have in documentation a description of which log folder are for which type of errors); By the way, according to the error I get, you use runtime reflection to search for ""sample"" function (that is perceived as a function instead of array due to round brackets). Not the safest, way to identify functions, IMHO.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2210:1215,safe,safest,1215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2210,1,['safe'],['safest']
Safety,Wire through PAPIv2 timeouts,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4946:20,timeout,timeouts,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4946,1,['timeout'],['timeouts']
Safety,"With Cromwell 30.2:. Attempting to abort a workflow mostly running on SGE triggers lots of these errors:. 2018-02-09 11:52:46,500 cromwell-system-akka.dispatchers.engine-dispatcher-96 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Queued and running SGE jobs continue as if nothing happened.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3259:35,abort,abort,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3259,1,['abort'],['abort']
Safety,"With compliments to @mcovarr for the rebase base. Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1291:399,avoid,avoid,399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1291,1,['avoid'],['avoid']
Safety,Workflow Actor - Call Recover appropriately,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/664:22,Recover,Recover,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/664,1,['Recover'],['Recover']
Safety,Workflow abort fails to stop SGE jobs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3259:9,abort,abort,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3259,1,['abort'],['abort']
Safety,WorkflowActor should abort workflow's execution before escalating failure to WorkflowManagerActor when one of it's child actors crashed [BA-6071],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5253:21,abort,abort,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5253,1,['abort'],['abort']
Safety,WorkflowExecutionActor: Implement abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/762:34,abort,abort,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/762,1,['abort'],['abort']
Safety,"WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] JobStoreActor stopped; [2018-09-14 13:20:05,42] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,42] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] CallCacheWriteActor stopped; [2018-09-14 13:20:05,43] [info] DockerHashActor stopped; [2018-09-14 13:20:05,43] [info] IoProxy stopped; [2018-09-14 13:20:05,43] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] ServiceRegistryActor stopped; [2018-09-14 13:20:05,47] [info] Database closed; [2018-09-14 13:20:05,47] [info] Stream materializer shut down; [2018-09-14 13:20:05,48] [info] WDL HTTP import resolver closed; Workflow caab4283-a3d4-4966-85ba-56d0992c8f00 transitioned to state Failed; (p3cwl) [jeremiah@localhost ~]$ ; ```.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103:9552,Timeout,Timeout,9552,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103,2,['Timeout'],['Timeout']
Safety,Workflows occasionally fail due to timeouts on read_* functions,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057:35,timeout,timeouts,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057,1,['timeout'],['timeouts']
Safety,Wrap ask errors from WSCWA to avoid ClassCastException.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3826:30,avoid,avoid,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3826,2,['avoid'],['avoid']
Safety,"Write (and run) a bash script for sending the abort, wait, and check the ~~response~~ job has actually aborted.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2088:46,abort,abort,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088,2,['abort'],"['abort', 'aborted']"
Safety,"WriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleCl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:9867,abort,abort,9867,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"WriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.Abstrac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:12106,abort,abort,12106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"WriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:19170,abort,abort,19170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"WriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:21478,abort,abort,21478,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"Y_CACHEDIR variable is set. If not use a default; # based on the users home.; export SINGULARITY_CACHEDIR=/scratch/$USER/.singularity/cache; export SINGULARITY_LOCALCACHEDIR=/scratch/$USER/.singularity/localcache; export SINGULARITY_TMPDIR=/scratch/$USER/.singularity/tmp; mkdir -p $SINGULARITY_CACHEDIR; mkdir -p $SINGULARITY_LOCALCACHEDIR; mkdir -p $SINGULARITY_TMPDIR; export SINGULARITY_BINDPATH=input_data/hello,$EXECUTION_ROOT:/cromwell-executions,/usr/prog/nx/cromwell/test; # echo ""SINGULARITY_CACHEDIR: $SINGULARITY_CACHEDIR""; # echo ""SINGULARITY_BINDPATH: $SINGULARITY_BINDPATH""; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $SINGULARITY_CACHEDIR; LOCK_FILE=$SINGULARITY_CACHEDIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for; # for debugging, as is the echo command. These show up in stdout.submit.; flock --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://python@sha256:d03b690584424b88488e555e26820e458cc624e9d004e3fa0fe3ff99aa81b2b4 echo ""Success pulling docker!""; echo ""module load singularity/v3.5.2 && singularity exec --containall --bind cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello:/cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello \; \; docker://python@sha256:d03b690584424b88488e555e26820e458cc624e9d004e3fa0fe3ff99aa81b2b4 /bin/bash /cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello/execution/script"" | qsub \; -terse \; -b n \; -N cromwell_45d03417_say_hello \; -wd cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello \; -o cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello/execution/stdout \; -e cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello/execution/stderr \; \; -l m_mem_free=$(expr 4096 / 1)m \; -l h_rt=3600 \; -l s_rt=3600 \; \; \; \; -V; 2020-10-08 16:09:02,038 cromwell-system-akka.dispat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5925:8009,timeout,timeout,8009,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5925,1,['timeout'],['timeout']
Safety,"Yesterday there was a config change that targeted the /stats endpoint rather than /status to assess Cromwell vitality. Unfortunately this accidentally seems to have DOSed Cromwell and produced tons of messages like the following in the logs. Cromwell effectively locked up and needed a hard restart to recover. I don't think the rate at which /stats was called was excessively high. The counting mechanism is apparently sending messages around to the whole graph of execution actors when it seems like a more efficient means of answering the stats question should be possible. Even if turns out a more efficient calculation isn't possible, the current system doesn't appear to be taking sub workflow actors into account correctly and I don't even know how the MWDA and WIA got caught up in this. ```; WARN c.e.w.l.e.SubWorkflowExecutionActor - unhandled event JobCountQuery in state SubWorkflowRunningState; ```. ```; WARN c.e.w.l.m.MaterializeWorkflowDescriptorActor - MaterializeWorkflowDescriptorActor [UUID(XXXXX)]: received an unhandled message Event(JobCountQuery,()) in state MaterializingState; ```. ```; WARN c.e.w.l.i.WorkflowInitializationActor - WorkflowInitializationActor-XXXXX [UUID(XXXXX)]: received an unhandled message: Event(JobCountQuery,WorkflowLifecycleActorData(Set(Actor[XXXXX]),List(),Map(),List())); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3683:302,recover,recover,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3683,1,['recover'],['recover']
Safety,[30_hotfix] Abort sub workflows,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3261:12,Abort,Abort,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3261,1,['Abort'],['Abort']
Safety,"[40 hotfix] Safety net against long running ""log an event"" actions",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4954:12,Safe,Safety,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4954,1,['Safe'],['Safety']
Safety,[41 hotfix] Customizable PAPI request timeouts,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4993:38,timeout,timeouts,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4993,1,['timeout'],['timeouts']
Safety,[51 Hotfix] .trim() Docker image names to avoid pathological re behavior [BA-6478],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5547:42,avoid,avoid,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5547,1,['avoid'],['avoid']
Safety,[53 hotfix] GCS IO safety and test bucket size calls [BW-411],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5997:19,safe,safety,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5997,1,['safe'],['safety']
Safety,[53_hotfix] Call-by-name to avoid instantiating default commands if unused [BW-416],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5990:28,avoid,avoid,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5990,1,['avoid'],['avoid']
Safety,[55 Hotfix] Add Trivy github action and remediate [BW-493],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6163:40,remediat,remediate,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6163,1,['remediat'],['remediate']
Safety,[BT-597] use StorageException.getReason to detect requester pays,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6726:43,detect,detect,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6726,1,['detect'],['detect']
Safety,[HtCondor] Add recovery functionality,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1249:15,recover,recovery,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1249,1,['recover'],['recovery']
Safety,"[Jira issue](https://broadworkbench.atlassian.net/browse/BA-5943). Using Cromwell 44 and PAPI v2, occasionally machines in GCE are preempted but not handled as such. Metadata snippet:. ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task test_combine.combine:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation""; }; ],; ""jobId"": ""projects/finngen-refinery-dev/operations/18318369325465658337"",; ""backend"": ""PAPIv2"",; ""end"": ""2019-08-20T14:54:37.214Z"",; ```. Stackdriver log snippet:. ```; {; ""insertId"": ""15cmu2qg1chkm09"",; ""jsonPayload"": {; ""event_timestamp_us"": ""1566312261571808"",; ""actor"": {; ""user"": ""system""; },; ""resource"": {; ""name"": ""google-pipelines-worker-6eba778d59d69dcfe9189620b91117c5"",; ""type"": ""instance"",; ""zone"": ""europe-west1-b"",; ""id"": ""1966470788939888666""; },; ""trace_id"": ""systemevent-1566312254625-5908d7d8b55f5-68011c08-d6e13e66"",; ""event_type"": ""GCE_OPERATION_DONE"",; ""operation"": {; ""id"": ""1400679280860576170"",; ""name"": ""systemevent-1566312254625-5908d7d8b55f5-68011c08-d6e13e66"",; ""type"": ""operation"",; ""zone"": ""europe-west1-b""; },; ""event_subtype"": ""compute.instances.preempted"",; ""info"": [; {; ""code"": ""STATUS_MESSAGE"",; ""detail_message"": ""Instance was preempted.""; }; ],; ""version"": ""1.2""; },; ```. Although this happens rarely, it causes large workflows to fail and we'd like to avoid rerunning such workflows because at scale other issues may arise with e.g. call caching timeouts, and things are more manageable without otherwise unnecessary reruns. Any help would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5136:1404,avoid,avoid,1404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5136,2,"['avoid', 'timeout']","['avoid', 'timeouts']"
Safety,"[Per @mbookman]; This pull request is an initial update to address:. CROM-6718: FR: Add flag for minimizing chance of GCP cross-region network egress charges being incurred. This PR specifically focuses on the risks of egress charges incurred due to call caching. The framing of the approach here, which is a bit broader than originally noted in CROM-6718, is:; Make call caching location-aware, prioritizing copies that minimize egress charges.; Add a workflow option enabling control of what egress charges can be incurred for call cache copying.; The new workflow option would be:. call_cache_egress: [none, continental, global]. where the values affect whether call cache copies can incur egress charges:; none: only within-region copies are allowed, which generate no egress charges; continental: within content copies are allowed; within-content copies have reduced costs, such as $0.01 / GB in the US; global: copies across all regions are allowed. Cross-content egress charges can be much higher (ranging from $0.08 / GB up to $0.23 / GB). ### CURRENT STATUS OF PR:; These first few commits are a WIP/request for feedback. I would love discussion on what the best approach would be. The idea for this initial approach is to raise an exception right before copying cached outputs if the bucket locations would cause an egress charge (depending on workflow option). The CallCacheJobActor continue attempting to copy outputs until it finds one that doesn't cause an egress charge (depending on workflow option), or until it determines cache miss. . If the above approach is reasonable then I would need coding advice on:; 1) How do I properly use GcsBatchCommandBuilder.locationCommand (or something similar) in the CacheHitCopyingActor?; 2) How do I properly get the WorkflowOption CallCacheEgress in the CacheHitCopyingActor?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6324:210,risk,risks,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6324,1,['risk'],['risks']
Safety,"[The ticket](https://broadworkbench.atlassian.net/browse/DDO-2190) has a ton more info and I talked with @aednichols about this--the short version is:; - Whatever is doing the HTTP request to Cromwell (Akka?) does not set the Host header ([per MDN](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Host)) if it has been customized beforehand; - We've accidentally been doing that by copying the _request's_ headers that came into CromIAM, which obviously include the Host header correlating to CromIAM itself; - Thus CromIAM sends requests to Cromwell with an incorrect Host header; - This hasn't mattered before because Cromwell's Layer 4 load balancer doesn't care and Cromwell's Apache proxy didn't actually need to do host-based routing because it just forwards everything to one app, Cromwell; - This suddenly matters a lot now because BEEs use an Nginx controller for ingress instead of a GCP Layer 4 load balancer, and _it_ needs to use host-based routing; - TL;DR: CromIAM is proxying to Cromwell wrong-ish and it very much does not work in BEEs. Solution: strip out the Host header just like CromIAM already does for Timeout-Access, and everything is happy. The impact to live environments should be zero because they clearly didn't care about the header before. If this fails anywhere, Argo sees the failure immediately like it did for BEEs, and it sees it in a way that any deployment or promotion would be halted because CromIAM would fail to come online from Argo's perspective.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6803:1135,Timeout,Timeout-Access,1135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6803,1,['Timeout'],['Timeout-Access']
Safety,[WX-1301] Increase default timeout from 7 to 14 days,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7226:27,timeout,timeout,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7226,1,['timeout'],['timeout']
Safety,[WX-1448] Add verbose logging and timeout for getm,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7416:34,timeout,timeout,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7416,1,['timeout'],['timeout']
Safety,[develop edition] GCS IO safety and test bucket size calls [BW-411],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5993:25,safe,safety,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5993,1,['safe'],['safety']
Safety,"[develop edition] Update job abort handling for PAPIv2 ""Catherine"" [BW-408]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5995:29,abort,abort,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5995,1,['abort'],['abort']
Safety,[develop] Call-by-name to avoid instantiating default commands if unused [BW-416],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5991:26,avoid,avoid,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5991,1,['avoid'],['avoid']
Safety,"\""type\"": [\n \""null\"",\n \""float\""\n ],\n \""doc\"": \""Proportion of somatic deviation to include in fitted purity score. Default 1.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-somatic_penalty_weight\""\n },\n \""default\"": 1,\n \""id\"": \""#purple-2.44.cwl/somatic_penalty_weight\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional location of somatic variant vcf to assist fitting in highly-diploid samples.\\nSample name must match tumor parameter. GZ files supported.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-somatic_vcf\""\n },\n \""secondaryFiles\"": [\n \"".tbi\""\n ],\n \""id\"": \""#purple-2.44.cwl/somatic_vcf\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional location of structural variant vcf for more accurate segmentation.\\nGZ files supported.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-structural_vcf\""\n },\n \""secondaryFiles\"": [\n \"".tbi\""\n ],\n \""id\"": \""#purple-2.44.cwl/structural_vcf\""\n },\n {\n \""type\"": \""File\"",\n \""doc\"": \""Optional location of failing structural variants that may be recovered.\\nGZ files supported.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-sv_recovery_vcf\""\n },\n \""secondaryFiles\"": [\n \"".tbi\""\n ],\n \""id\"": \""#purple-2.44.cwl/sv_recovery_vcf\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-threads\""\n },\n \""default\"": 2,\n \""id\"": \""#purple-2.44.cwl/threads\""\n },\n {\n \""type\"": \""string\"",\n \""doc\"": \""Name of the tumor sample. This should correspond to the value used in AMBER and COBALT.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-tumor\""\n },\n \""id\"": \""#purple-2.44.cwl/tumor\""\n },\n {\n \""type\"": [\n \""null\"",\n \""boolean\""\n ],\n \""doc\"": \""Tumor only mode. Disables somatic fitting.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-tumor_only\""\n },\n \""default\"": false,\n \""id\"": \""#purple-2.44.cwl/tumor_only\""\n }\n ],\n \""outputs\"": [\n {\n \""type\"": \""Directory\"",\n \""outputBinding\"": {\n \""glob\""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:101937,recover,recovered,101937,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['recover'],['recovered']
Safety,"`JesCallPaths` extends `JesWorkflowPaths` and appears to create a lot of workflow-level data redundantly. Besides wasting time and possibly money, this causes unnecessary calls to Google APIs which count against our QPS limits. During 9/16 JG testing we saw an error creating `storage` in `JesWorkflowPaths` (which should have been treated as a transient error per #1436), but it seems that storage should have been created as part of the `JesBackendInitializationData` by the `JesInitializationActor` and then simply passed to the job actors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1437:93,redund,redundantly,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1437,1,['redund'],['redundantly']
Safety,"```$anon$1 was thrown during property evaluation. (SharedFileSystemJobExecutionActorSpec.scala:119)&#010; Message: A timeout occurred waiting for a future to complete. Queried 21 times, sleeping 500 milliseconds between each query.```. ```A timeout occurred waiting for a future to complete. Queried 21 times, sleeping 500 milliseconds between each query.```. https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/165/. If one needs more info please talk to @ndbolliger",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4222:117,timeout,timeout,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4222,2,['timeout'],['timeout']
Safety,```; [ERROR] [01/27/2017 13:33:05.570] [cromwell-system-akka.dispatchers.engine-dispatcher-30] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow 0fcd7ba4-f1e0-4e16-9b67-c83ad2378f44 failed (during ExecutingWorkflowState): Could not evaluate no_address.out = read_string(stdout()); java.lang.RuntimeException: Could not evaluate no_address.out = read_string(stdout()); 	at wdl4s.Task$$anonfun$11$$anonfun$4.applyOrElse(Task.scala:182); 	at wdl4s.Task$$anonfun$11$$anonfun$4.applyOrElse(Task.scala:181); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at wdl4s.Task$$anonfun$11.apply(Task.scala:181); 	at wdl4s.Task$$anonfun$11.apply(Task.scala:174); 	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); 	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104); 	at wdl4s.Task.evaluateOutputs(Task.scala:174); 	at cromwell.backend.wdl.OutputEvaluator$.evaluateOutputs(OutputEvaluator.scala:15); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.postProcess(JesAsyncBackendJobExecutionActor.scala:366); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionSuccess(JesAsyncBackendJobExecutionActor.scala:391); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionSuccess(JesAsyncBackendJobExecutionActor.scala:46); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1923:656,recover,recoverWith,656,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1923,1,['recover'],['recoverWith']
Safety,```; java -Dconfig.file=/root/cromwell-application.conf -jar /root/cromwell/cromwell-62.jar run hello.wdl; ```. ```; WorkflowManagerActor: Workflow 19f1873b-7e61-4683-9585-747bf842a261 failed (during ExecutingWorkflowState): java.lang.Exception: Job id job-0000000060963A8300007BD2000D7072 failed: 'cluster(job-0000000060963A8300007BD2000D7072_cromwell_0) error: InvalidArgument'; 	at cromwell.backend.impl.bcs.BcsAsyncBackendJobExecutionActor.handleExecutionFailure(BcsAsyncBackendJobExecutionActor.scala:281); 	at cromwell.backend.impl.bcs.BcsAsyncBackendJobExecutionActor.handleExecutionFailure(BcsAsyncBackendJobExecutionActor.scala:32); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1315); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1311); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6352:969,recover,recoverWith,969,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352,1,['recover'],['recoverWith']
Safety,a.util.Try$.apply(Try.scala:209); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive$(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:1870,recover,recoverAsync,1870,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recoverAsync']
Safety,"a/cromwell/engine/io/IoActor.scala#L119) and processed subsequently in an S3 specific manner, like it's currently done for GCS, to be useful. Because this isn't the case now, the S3 code in `S3BatchIoCommand` is effectively never called.; - It works because there is an implementation of java nio for S3. The `S3BatchIoCommand` ends up in the [NioFlow](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala) which uses the methods of the nio interface to execute the commands.; **A big issue is that the nio interface does not have a ""hash"" method**. To work around that, the `NioFlow` [streams down the content and md5s it](https://github.com/broadinstitute/cromwell/blob/ec67d653c58c9c5b4b25609731a8082f3b540fe6/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L111), [unless told otherwise](https://github.com/broadinstitute/cromwell/blob/ec67d653c58c9c5b4b25609731a8082f3b540fe6/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L109). This is what's currently happening to S3 files. As a final twist, it turns out the [pattern match on GcsPath](https://github.com/broadinstitute/cromwell/blob/ec67d653c58c9c5b4b25609731a8082f3b540fe6/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L109) in the hash method is actually just a fail safe but is not really needed. That is because some `IoCommand`s, including the `IoHashCommand`, are subclassed as `GcsBatchIoCommand`s and are processed through the [GcsBatchFlow](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/io/gcs/GcsBatchFlow.scala). The main goal of this ""flow"" is to batch requests to google together to increase throughput but by doing so also allows to interact with the GCS API directly and get the [crc32 from there](https://github.com/broadinstitute/cromwell/blob/ec67d653c58c9c5b4b25609731a8082f3b540fe6/filesystems/gcs/src/main/scala/cromwell/filesystems/gcs/batch/GcsBatchIoCommand.scala#L129).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4463:1801,safe,safe,1801,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4463,1,['safe'],['safe']
Safety,a18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051:2019,unsafe,unsafeToFuture,2019,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051,1,['unsafe'],['unsafeToFuture']
Safety,"a:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:9936,abort,abort,9936,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"a:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:12175,abort,abort,12175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"a:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleCl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:19239,abort,abort,19239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"a:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparse",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:21547,abort,abort,21547,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"abel-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # batch-timeout = 7 days. genomics {; auth = ""cromwell-service-account""; location: ""${region}""; compute-service-account = ""${compute_service_account}"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; auth = ""cromwell-service-account"". # For billing; project = ""${billing_project}"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }. }; http {}; }. # Important!! Some of the workflows take an excessive amount of time to run; batch-timeout = 28 days. default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2 GB""; bootDiskSizeGb: 10; # Allowed to be a String, or a list of Strings; disks: ""local-disk 10 SSD""; noAddress: true; preemptible: 0; docker: ""ubuntu:latest""; }. virtual-private-cloud {; network-name = ""${private_network}""; subnetwork-name = ""${private_subnet}""; }; }; }; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238:3593,timeout,timeout,3593,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238,1,['timeout'],['timeout']
Safety,abort-all-workflows-on-terminate does not clean up the WorkflowStore,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2492:0,abort,abort-all-workflows-on-terminate,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2492,1,['abort'],['abort-all-workflows-on-terminate']
Safety,add timeout to DB operations,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4739:4,timeout,timeout,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4739,1,['timeout'],['timeout']
Safety,"aft-2""; }; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; metadata {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql:<dburl>?rewriteBatchedStatements=true""; driver = ""com.mysql.cj.jdbc.Driver""; user = ""<user>""; password = ""<pass>"" ; connectionTimeout = 5000; }; }; }. call-caching; {; enabled = true; invalidate-bad-cache-result = true; }. docker {; hash-lookup {; enabled = true; }; }. backend {; default = sge; providers {. ; sge {; 	actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; #concurrent-job-limit = 5. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. runtime-attributes = """"""; String time = ""11:00:00""; Int cpu = 4; Float? memory_gb; String sge_queue = ""hammer.q""; String? sge_project; String? docker; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". submit-docker = """""" ; #location for .sif files and other apptainer tmp, plus lockfile; 	 export APPTAINER_CACHEDIR=<path>; export APPTAINER_PULLFOLDER=<path>; export APPTAINER_TMPDIR=<path>; export LOCK_FILE=""$APPTAINER_CACHEDIR/lockfile""; export IMAGE=$(echo ${docker} | tr '/:' '_').sif; if [ -z $APPTAINER_CACHEDIR ]; then; exit 1; fi; CACHE_DIR=$APPTAINER_CACHEDIR; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR; # downloads sifs only one a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:2752,timeout,timeout-seconds,2752,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,1,['timeout'],['timeout-seconds']
Safety,age.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.create(HttpStorageRpc.java:308); 	at com.google.cloud.storage.StorageImpl$3.call(StorageImpl.java:213); 	at com.google.cloud.storage.StorageImpl$3.call(StorageImpl.java:210); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); 	at com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at com.google.cloud.storage.StorageImpl.internalCreate(StorageImpl.java:209); 	at com.google.cloud.storage.StorageImpl.create(StorageImpl.java:171); 	at cromwell.filesystems.gcs.GcsPath.request$1(GcsPathBuilder.scala:196); 	at cromwell.filesystems.gcs.GcsPath.$anonfun$writeContent$2(GcsPathBuilder.scala:203); 	at cromwell.filesystems.gcs.GcsPath.$anonfun$writeContent$2$adapted(GcsPathBuilder.scala:203); 	at cromwell.filesystems.gcs.GcsEnhancedRequest$.$anonfun$recoverFromProjectNotProvided$3(GcsEnhancedRequest.scala:18); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:87); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:355); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:376); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:316); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseExce,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594:2183,recover,recoverFromProjectNotProvided,2183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594,2,['recover'],['recoverFromProjectNotProvided']
Safety,"aises the following error:; ```; Starting defuse command:; /usr/local/bin/gmap -D defuse-data/gmap -d cdna -f psl #<1 > #>1; Reasons:; /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakp; oints.split.001.fa.cdna.psl missing; Failure for defuse command:; /usr/local/bin/gmap -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a; -ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa > /cromwell-executions/detectFusions/96242; 9bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa.cdna.psl.tmp; Reason:; Job command with nonzero return code; Return codes: 139; Job output:; Running on 2ecb3961d54d; Note: /usr/local/bin/gmap.avx2 does not exist. For faster speed, may want to compile package on an AVX2 machine; GMAP version 2018-07-04 called with args: /usr/local/bin/gmap.sse42 -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa; Checking compiler assumptions for SSE2: 6B8B4567 327B23C6 xor=59F066A1; Checking compiler assumptions for SSE4.1: -103 -58 max=198 => compiler zero extends; Checking compiler options for SSE4.2: 6B8B4567 __builtin_clz=1 __builtin_ctz=0 _mm_popcnt_u32=17 __builtin_popcount=17 ; Finished checking compiler assumptions; Pre-loading compressed genome (oligos)......done (78,222,840 bytes, 19098 pages, 0.00 sec); Pre-loading compressed genome (bits)......done (78,222,864 bytes, 19098 pages, 0.02 sec); Looking for index files in directory defuse-data/gmap/cdna; Pointers file is cdna.ref153offsets64meta; Offsets file is cdna.ref153offsets64strm; Positions file is cdna.ref153positions; Offsets compression type: bitpack64; Allocating memory for ref offset pointers, kmer 15, interval 3...Attached existing memory (2 attached) for defuse-data/gmap/cdna/cdna.ref153offsets64meta...done (134,217,744 bytes, 0.00 ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465:1223,detect,detectFusions,1223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465,1,['detect'],['detectFusions']
Safety,"akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:12313,abort,abort,12313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"ala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:239); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:237); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```; The Cromwell configuration is:; ```; system {; workflow-restart = true; }; call-caching {; enabled = true; }. database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:/n/groups/bcbio/cwl/test_bcbio_cwl/somatic/cromwell_work/persist/metadata;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 20000; }; }. backend {; providers {; Local {; config {; runtime-attributes = """"""; Int? cpu; Int? memory_mb; """"""; submit-docker: """". filesystems {; local {; caching {; duplication-strategy: [""soft-link""]; hashing-strategy: ""path""; }; }; }. }; }. }; }; ```; Does this provide enough information to identify what might be happening? Thanks for any thoughts or clues about avoid this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3607:6618,avoid,avoid,6618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607,1,['avoid'],['avoid']
Safety,"allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; system {; 	job-rate-control {; 	 jobs = 100; 	 per = 1 second; 	}; input-read-limits {; lines = 128000000; bool = 7; int = 19; float = 50; string = 1280000; json = 12800000; tsv = 1280000000; map = 128000000; object = 128000000; }. # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; 	graceful-server-shutdown = true; max-concurrent-workflows = 5000. io {; throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; number-of-requests = 100000; per = 100 seconds; }; }; }. akka {; # Optionally set / override any akka settings; http {; server {; # Increasing these timeouts allow rest api responses for very large jobs; # to be returned to the user. When the timeout is reached the server would respond; # `The server was not able to produce a timely response to your request.`; # https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows; request-timeout = 600s; idle-timeout = 600s; }; }; }. services {; MetadataService {; #class = ""cromwell.services.metadata.impl.MetadataServiceActor""; config {; metadata-read-row-number-safety-threshold = 2000000; # # For normal usage the default value of 200 should be fine but for larger/production environments we recommend a; # # value of at least 500. There'll be no one size fits all number here so we recommend benchmarking performance and; # # tuning the value to match your environment.; db-batch-size = 700; }; }; }. google {. application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. docker {; hash-lookup {; met",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:8537,timeout,timeouts,8537,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['timeout'],['timeouts']
Safety,"arsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:2360,unsafe,unsafeToFuture,2360,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['unsafe'],['unsafeToFuture']
Safety,"at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.Abstra",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:12244,abort,abort,12244,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:19308,abort,abort,19308,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive$(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:2025,recover,recoverAsync,2025,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recoverAsync']
Safety,"atcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:4018,Abort,Abort,4018,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"atcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:37:14,145 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(60ec6228)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:23,720 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(a442dc1c)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:37:31,421 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17bed42e)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:40,098 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(e9851ba1)]: Abor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:4664,Abort,Abort,4664,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"atcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:37:14,145 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(60ec6228)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:23,720 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(a442dc1c)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:37:31,421 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17bed42e)]: Abort",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:4503,Abort,Abort,4503,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"ate_mafs_workflow/1641cccf-e6f1-42fd-9f17-9fd7d355ce3c/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-PK-A5H8-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/1641cccf-e6f1-42fd-9f17-9fd7d355ce3c/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-PK-A5H9-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/1641cccf-e6f1-42fd-9f17-9fd7d355ce3c/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-PK-A5HA-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/1641cccf-e6f1-42fd-9f17-9fd7d355ce3c/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-PK-A5HB-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/1641cccf-e6f1-42fd-9f17-9fd7d355ce3c/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-PK-A5HC-01A-11D-A30A-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; ```. I should also mention that this was in v21, when I attempted in v24, I got a more spectacular failure - `java.lang.UnsupportedOperationException: Could not evaluate expression: ""--maf2 "" + write_lines(maf2)`, and then what appeared to be an infinite loop of the exact same stack trace over and over, which after a standard `ctrl-c` kill became `Waiting for 1 workflows to abort...` over and over again, which finally necessitated a `kill -9`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875:16302,abort,abort,16302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875,1,['abort'],['abort']
Safety,"ave two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3032:860,detect,detect-adapter,860,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032,1,['detect'],['detect-adapter']
Safety,avoid fragile reflection,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/592:0,avoid,avoid,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/592,1,['avoid'],['avoid']
Safety,batch abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3753:6,abort,abort,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3753,1,['abort'],['abort']
Safety,"borting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3712,Abort,Aborting,3712,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"borting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:37:14,145 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(60ec6228)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:23,720 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(a442dc1c)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:37:31,421 cromwell-system-akka.d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:4358,Abort,Aborting,4358,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"borting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2904,Abort,Aborting,2904,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"borting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3227,Abort,Aborting,3227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"borting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2742,Abort,Aborting,2742,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"borting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3065,Abort,Aborting,3065,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"bstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:12,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:13,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:14,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:15,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:16,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:17,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:18,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:19,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:20,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:21,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:22,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:23,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:24,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:25,41] [error] Exception not convertible into handled response; ...snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:23415,abort,abort,23415,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,13,['abort'],['abort']
Safety,"c-7c69317ba0a2/call-PairedFastQsToUnmappedBAM/inputs/-2135135022/S000021_S7367Nr1.2.fastq.gz --OUTPUT S7367Nr1.unmapped.bam --READ_GROUP_NAME S7367Nr1 --SAMPLE_NAME S4431Nr1 --LIBRARY_NAME TwistCore+RefSeq+Mito-Panel --PLATFORM_UNIT platform_unit --PLATFORM Illumina --SEQUENCING_CENTER CeGaT --RUN_DATE 2021-10-10T06:00:00+0000 --USE_SEQUENTIAL_FASTQS false --SORT_ORDER queryname --MIN_Q 0 --MAX_Q 93 --STRIP_UNPAIRED_MATE_NUMBER false --ALLOW_AND_IGNORE_EMPTY_LINES false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; 2023-02-03 12:38:34 [Fri Feb 03 09:38:34 GMT 2023] Executing as root@d65fc5b7d470 on Linux 5.15.49-linuxkit amd64; OpenJDK 64-Bit Server VM 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.3.0.0; 2023-02-03 12:38:35 INFO 2023-02-03 09:38:35 FastqToSam Auto-detected quality format as: Standard.; 2023-02-03 12:39:08 INFO 2023-02-03 09:39:08 FastqToSam Processed 1,000,000 records. Elapsed time: 00:00:32s. Time for last 1,000,000: 32s. Last read position: */*`. I tried via Java 18.0.1.1 JDK and also later with 1.8.0_202 JDK. I also tried with the conda installation where Java dependency of OpenJDK 11.0.15 is automatically installed. I also tried combinations with Cromwell 69, 80 and 84. None of them works. They all have the same problem. It only works if I use Cromwell version 55 along with Java 1.8.0_202 JDK. It would be amazing if you look into this, as we would love to use the latest Cromwell versions and benefit from the conda environment. Thanks!. Machine info: `Darwin Ibrahims-MacBook-Pro.local 22.2.0 Darwin Kernel Version 22.2.0: Fri Nov 11 02:04:44 PST 2022; root:xnu-8792.61.2~4/RELEASE_ARM64_T8103 arm64`. MacOS = Ventura 13.1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6998:2783,detect,detected,2783,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6998,1,['detect'],['detected']
Safety,cJobExecutionActor.scala:196); 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive$(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:1754,recover,recover,1754,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recover']
Safety,"c_min_purity_spread_purple\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Minimum number of somatic variants required to assist highly diploid fits. Default 300.\\n\"",\n \""id\"": \""#somatic_min_total_purple\""\n },\n {\n \""type\"": [\n \""null\"",\n \""float\""\n ],\n \""doc\"": \""Proportion of somatic deviation to include in fitted purity score. Default 1.\\n\"",\n \""id\"": \""#somatic_penalty_weight_purple\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional location of somatic variant vcf to assist fitting in highly-diploid samples.\\nSample name must match tumor parameter. GZ files supported.\\n\"",\n \""secondaryFiles\"": [\n \"".tbi\""\n ],\n \""id\"": \""#somatic_vcf_purple\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional location of structural variant vcf for more accurate segmentation.\\nGZ files supported.\\n\"",\n \""secondaryFiles\"": [\n \"".tbi\""\n ],\n \""id\"": \""#structural_vcf_purple\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional location of failing structural variants that may be recovered.\\nGZ files supported.\\n\"",\n \""secondaryFiles\"": [\n \"".tbi\""\n ],\n \""id\"": \""#sv_recovery_vcf_purple\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads used for amber step\\n\"",\n \""id\"": \""#threads_amber\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads to run cobalt command\\n\"",\n \""id\"": \""#threads_cobalt\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads to use - set to 8 by default\"",\n \""id\"": \""#threads_gridss\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads\\n\"",\n \""id\"": \""#threads_purple\""\n },\n {\n \""type\"": \""File\"",\n \""doc\"": \""tumour BAM file\\n\"",\n \""secondaryFiles\"": [\n \"".bai\""\n ],\n \""id\"": \""#tumor_bam\""\n },\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""sample name of tumor. Must match the somatic snvvcf sample name. (Defa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:114090,recover,recovered,114090,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['recover'],['recovered']
Safety,cala:191); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65);,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:2136,recover,recoverAsync,2136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recoverAsync']
Safety,"cala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-02-02 11:55:36,711 cromwell-system-akka.dispatchers.engine-dispatcher-19 ERROR - WorkflowManagerActor Workflow 5fdb357a-3f1d-45b7-a85b-c22caa755c36 failed (during ExecutingWorkflowState): java.lang.IllegalArgumentException; cromwell.core.CromwellFatalException: java.lang.IllegalArgumentException; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.fork",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1944:6814,recover,recoverWith,6814,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1944,1,['recover'],['recoverWith']
Safety,"cessMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$GoogleJsonException: Request contains an invalid argument.; ... 21 more. [2021-08-13 10:45:10,13] [info] WorkflowManagerActor: Workflow actor for a15c46b7-5f93-46d6-94a2-28f656914866 completed with status 'Failed'. The workflow will be removed from the workflow store.; [2021-08-13 10:45:13,98] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2021-08-13 10:45:15,05] [info] Workflow polling stopped; [2021-08-13 10:45:15,07] [info] 0 workflows released by cromid-de31b6d; [2021-08-13 10:45:15,07] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; ...; ```. Contents of hello.wdl:; ```; task hello {; String addressee; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. Contents of hello.inputs:; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```; Contents of cromwell.BROADexamples.v4.conf:; ```; # This is a ""default"" Cromwell example that is intended for you you to start with; # and edit for your needs. Specifically, you will be interested to customize; # the configuration based on your preferred backend (see the backends section; # below in the file). For backend-specific examples for you to copy paste here,; # please see the cromwell.backend.examples folder in the repository. The files; # there also include links to online doc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:8434,Timeout,Timeout,8434,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Timeout'],['Timeout']
Safety,"ch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-02-02 11:55:36,711 cromwell-system-akka.dispatchers.engine-dispatcher-19 ERROR - WorkflowManagerActor Workflow 5fdb357a-3f1d-45b7-a85b-c22caa755c36 failed (during ExecutingWorkflowState): java.lang.IllegalArgumentException; cromwell.core.CromwellFatalException: java.lang.IllegalArgumentException; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1944:6890,recover,recoverWith,6890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1944,1,['recover'],['recoverWith']
Safety,changed gcloud alpine to slim to avoid bug w gsutil,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4114:33,avoid,avoid,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4114,1,['avoid'],['avoid']
Safety,"cher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Ab",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2241,Abort,Abort,2241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"cher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2079,Abort,Abort,2079,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"cher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3211,Abort,Abort,3211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"cher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2888,Abort,Abort,2888,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"ciated jobs are still present in JOB_STORE_ENTRY. . There are no errors in the logs: ; `; 2016-12-12 18:22:26,139 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(8a965a5e)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:29:42,727 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(73be7f27)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1432,Abort,Abort,1432,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"ckgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: job id: 9836; [2017-12-01 20:01:04,92] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: Status change from - to WaitingForReturnCodeFile; [2017-12-01 20:01:06,50] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2017-12-01 20:01:06,61] [error] WorkflowManagerActor Workflow 132d7527-a0af-4f08-8291-d935e7cd5632 failed (during ExecutingWorkflowState): Could not evaluate t1.out = if select_first([flag1,false]) then glob(""test1.txt"")[0] else glob(""test2.txt"")[0]; java.lang.RuntimeException: Could not evaluate t1.out = if select_first([flag1,false]) then glob(""test1.txt"")[0] else glob(""test2.txt"")[0]; at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:189); at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:188); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at scala.util.Failure.recoverWith(Try.scala:232); at wdl4s.wdl.WdlTask.$anonfun$evaluateOutputs$2(WdlTask.scala:188); at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:157); at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:157); at scala.collection.Iterator.foreach(Iterator.scala:929); at scala.collection.Iterator.foreach$(Iterator.scala:929); at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:157); at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:155); at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104); at wdl4s.wdl.WdlTask.evaluateOutputs(WdlTask.scala:181); at cromwell.backend.wdl.OutputEvaluator$.evaluateOutputs(OutputEvaluator.scala:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2972:3831,recover,recoverWith,3831,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2972,1,['recover'],['recoverWith']
Safety,"com/cloudsdktool/cloud-sdk:276.0.0-slim""; pullStopped:; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; timestamp: '2021-08-03T15:23:12.246251722Z'; - description: Started pulling ""gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim""; pullStarted:; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; timestamp: '2021-08-03T15:22:42.922496298Z'; - description: Worker ""google-pipelines-worker-xxxxxx""; assigned in ""us-central1-b"" on a ""custom-1-2048"" machine; timestamp: '2021-08-03T15:22:07.789742627Z'; workerAssigned:; instance: google-pipelines-worker-xxxxxx; machineType: custom-1-2048; zone: us-central1-b; labels:; cromwell-workflow-id: cromwell-xxxxxx; wdl-task-name: hello; pipeline:; actions:; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Starting\ container\ setup.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: ContainerSetup; timeout: 300s; - commands:; - -c; - mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: ContainerSetup; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Done\ container\ setup.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: ContainerSetup; timeout: 300s; - commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Background; runInBackground: true; - commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; outputName: stder",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:15897,timeout,timeout,15897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['timeout'],['timeout']
Safety,"cond; 	}; input-read-limits {; lines = 128000000; bool = 7; int = 19; float = 50; string = 1280000; json = 12800000; tsv = 1280000000; map = 128000000; object = 128000000; }. # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; 	graceful-server-shutdown = true; max-concurrent-workflows = 5000. io {; throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; number-of-requests = 100000; per = 100 seconds; }; }; }. akka {; # Optionally set / override any akka settings; http {; server {; # Increasing these timeouts allow rest api responses for very large jobs; # to be returned to the user. When the timeout is reached the server would respond; # `The server was not able to produce a timely response to your request.`; # https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows; request-timeout = 600s; idle-timeout = 600s; }; }; }. services {; MetadataService {; #class = ""cromwell.services.metadata.impl.MetadataServiceActor""; config {; metadata-read-row-number-safety-threshold = 2000000; # # For normal usage the default value of 200 should be fine but for larger/production environments we recommend a; # # value of at least 500. There'll be no one size fits all number here so we recommend benchmarking performance and; # # tuning the value to match your environment.; db-batch-size = 700; }; }; }. google {. application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. docker {; hash-lookup {; method = ""remote""; }; }. engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }. call-caching {; enabled = true; }. backend {; default = GCP",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:8631,timeout,timeout,8631,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['timeout'],['timeout']
Safety,"configuration file used is as follows (edited to remove the main script):; ```; include required(classpath(""application"")); backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 300; runtime-attributes = """"""; Int cpu; Int memory_mb; String? lsf_queue; String? lsf_project; String? docker; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; module load tools/singularity/3.8.3; SINGULARITY_MOUNTS='<redacted>'; export SINGULARITY_CACHEDIR=$HOME/.singularity/cache; LOCK_FILE=$SINGULARITY_CACHEDIR/singularity_pull_flock. export SINGULARITY_DOCKER_USERNAME=<redacted>; export SINGULARITY_DOCKER_PASSWORD=<redacted>. flock --exclusive --timeout 900 $LOCK_FILE \; singularity exec docker://${docker} \; echo ""Sucessfully pulled ${docker}"". bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; singularity exec --containall $SINGULARITY_MOUNTS --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """""". job-id-regex = ""Job <(\\d+)>.*""; kill = ""bkill ${job_id}""; kill-docker = ""bkill ${job_id}""; check-alive = ""bjobs -w ${job_id} |& egrep -qvw 'not found|EXIT|JOBID'"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path+modtime""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=fa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7203:1663,timeout,timeout,1663,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7203,1,['timeout'],['timeout']
Safety,cromwell server start error with MySQL: Error searching for abort requests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:60,abort,abort,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,1,['abort'],['abort']
Safety,"ctDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleCl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:16931,abort,abort,16931,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"ctor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. To a novice user it is not clear whether that this can safely be ignored. This is a problem in particular if this first use actually fails for some other reason. The user will spend time trying to figure out if the problem is caused by a credential issue. I tried to see if I could easily suppress this by setting up ""backends"" to only include ""local"". I pulled down the [application.conf](https://github.com/broadinstitute/cromwell/blob/9f759a54a0b8873f1338f36391f985477d83475a/engine/src/main/resources/application.conf) which sets:. ```; backend {; // Either ""jes"", ""local"", or ""sge"" (case insensitive); defaultBackend = ""local""; // List of backends which this Cromwell supports. Be sure to include the defaultBackend!; backendsAllowed = [ ""local"" ]; ```. and then passed the file as:. `$ java -Dconfig.file=application.conf -jar cromwell.jar run hello.wdl hello.json; `; But the exception and warning were still raised. Should the ADC check be occurring even when backendsAllowed does not include JES?. Thanks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/705:5161,safe,safely,5161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/705,1,['safe'],['safely']
Safety,"ctorFactory""; config {; runtime-attributes = """"""; String userid; String partitions; String memory_per_node; Int nodes; Int cores; String time; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). exit-code-timeout-seconds = 600. submit = """"""; chmod 770 -R ${cwd}; sudo change-files.sh ${userid} ${cwd}; phoenix_home_cwd=""/home/${userid}""; phoenix_home_out=""/home/${userid}/stdout""; phoenix_home_err=""/home/${userid}/stderr"". phoenix_script=${script}_phonix; cat ${script} | sed -s ""s@#\!/bin/bash@#\!/bin/bash\nsource '/etc/profile' @g"" > $phoenix_script. sbatch --uid=${userid} --gid=${userid} \; -J ${job_name} \; -p ${partitions} \; -N ${nodes} \; -n ${cores} \; --mem=${memory_per_node} \; --time=${time} \; -D $phoenix_home_cwd \; -o $phoenix_home_out \; -e $phoenix_home_err \; $phoenix_script; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*"". root = ""/fast/gdr/uat/cromwell-executions""; }; }. } # providers. } # backend. # https://gatkforums.broadinstitute.org/wdl/discussion/9536/how-do-i-set-up-a-mysql-database-for-cromwell; # http://slick.lightbend.com/doc/3.2.0/api/index.html#slick.jdbc.JdbcBackend$DatabaseFactoryDef@forConfig(String,Config,Driver):Database. database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://gdr-cromwell-uat-URL/uat_gdr_cromwell?rewriteBatchedStatements=true""; user = ""uat_gdr_cromwell""; password = ""<<cromwell_mysql_password>>""; connectionTimeout = 15000; }; }. system {; abort-jobs-on-terminate = false; max-concurrent-workflows = 1000; new-workflow-poll-rate = 2; max-workflow-launch-count = 50; }. # helpful links; # https://devhub.io/repos/broadinstitute-cromwell",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4404:3326,abort,abort-jobs-on-terminate,3326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4404,1,['abort'],['abort-jobs-on-terminate']
Safety,d:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:2721,unsafe,unsafeRunAsync,2721,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['unsafe'],['unsafeRunAsync']
Safety,d:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); akka,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051:2226,unsafe,unsafeRunAsync,2226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051,1,['unsafe'],['unsafeRunAsync']
Safety,"d; [2022-12-15 21:28:53,46] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2022-12-15 21:28:53,46] [info] Aborting all running workflows.; [2022-12-15 21:28:53,46] [info] 0 workflows released by cromid-b254006; [2022-12-15 21:28:53,47] [info] WorkflowStoreActor stopped; [2022-12-15 21:28:53,47] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2022-12-15 21:28:53,47] [info] WorkflowLogCopyRouter stopped; [2022-12-15 21:28:53,47] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2022-12-15 21:28:53,47] [info] JobExecutionTokenDispenser stopped; [2022-12-15 21:28:53,47] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2022-12-15 21:28:53,47] [info] WorkflowManagerActor: All workflows finished; [2022-12-15 21:28:53,47] [info] WorkflowManagerActor stopped; [2022-12-15 21:28:53,71] [info] Connection pools shut down; [2022-12-15 21:28:53,71] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2022-12-15 21:28:53,72] [info] SubWorkflowStoreActor stopped; [2022-12-15 21:28:53,72] [info] JobStoreActor stopped; [2022-12-15 21:28:53,72] [info] CallCacheWriteActor stopped; [2022-12-15 21:28:53,72] [info] IoProxy stopped; [2022-12-15 21:28:53,74] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2022-12-15 21:28:53,74] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:49454,Timeout,Timeout,49454,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,9,['Timeout'],['Timeout']
Safety,"ding=h.iqo65dknl60s). . Briefly, the intention is to move the ""rendering"" process inside the `ServiceRegistryActor` so that in the future calls to the ServiceRegistry return JSON rather than event lists. This allows the ""pre-rendered JSON"" metadata service to fulfil the same service interface as the ""database-event driven"" metadata service. ### PR Review Guidance. Most of the PR is noise but the ""signal"" is very important to get right!. Things to consider when reviewing this (perhaps otherwise unwieldy) PR:. - Does the actor structure in the diagrams below make sense?; - ... and does it match reality as implemented in this PR?; - Have the newly introduced actors been implemented well? (ie please review these as though they were brand new actors); - `ReadMetadataRegulatorActor`; - `MetadataBuilderActor`; - `ReadDatabaseMetadataWorkerActor`; - Have the responsibilities of the replaced actors been taken care of appropriately?; - Has the API of Cromwell changed inappropriately?; - I had to refactor the `CallCacheDiffActor` because it was using the metadata service directly. Did I do a good job? And are its new tests appropriately equivalent to its old ones?; - Are there sufficient tests between unit, CI and ""perf"" to make you feel good about me merging this PR?; - Am I forgetting anything?. ### Structure before the changes:. ![Before BA-5842_ Metadata Service Actor (3)](https://user-images.githubusercontent.com/13006282/64040517-426d4380-cb2b-11e9-8a40-fa11edd33b58.png). ### Structure after the changes:. ![After BA-5842_ Metadata Service Actor](https://user-images.githubusercontent.com/13006282/64040066-24531380-cb2a-11e9-8a74-98d7c976e6ec.png). ### Concerns. This feels slightly more risky than normal because the refactor was pretty fiddly and I was ""test driven"" for a significant portion of the refactor - mainly because actors are not typed and thus it was very tricky to make sure I found everywhere using the old object set which should now be using the new object set.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5150:1871,risk,risky,1871,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150,1,['risk'],['risky']
Safety,"dk/core/credentials/gce_read.py\"", line 50, in ReadNoProxy\n request, timeout=timeout_property).read()\n File \""/usr/lib/python2.7/urllib2.py\"", line 401, in open\n response = self._open(req, data)\n File \""/usr/lib/python2.7/urllib2.py\"", line 419, in _open\n '_open', req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 379, in _call_chain\n result = func(*args)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1211, in http_open\n return self.do_open(httplib.HTTPConnection, req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1184, in do_open\n r = h.getresponse(buffering=True)\n File \""/usr/lib/python2.7/httplib.py\"", line 1072, in getresponse\n response.begin()\n File \""/usr/lib/python2.7/httplib.py\"", line 408, in begin\n version, status, reason = self._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n)""; java.lang.Exception: Task m2.Mutect2.M2:1:1 failed. JES error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam -> /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam (cp failed: gsutil -q -m cp gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam, command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:9071,timeout,timeout,9071,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['timeout'],['timeout']
Safety,"dk:276.0.0-slim; labels:; logging: UserAction; timeout: 300s; - commands:; - /cromwell_root/script; entrypoint: /bin/bash; imageUri: ubuntu@sha256:1e48201ccc2ab83afc435394b3bf70af0fa0055215c1e26a5da9b50a1ae367c9; labels:; tag: UserAction; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Starting\ delocalization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Delocalization; timeout: 300s; - commands:; - -c; - /bin/bash /cromwell_root/gcs_delocalization.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Delocalization; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Done\ delocalization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Delocalization; timeout: 300s; - alwaysRun: true; commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/1xxxxxx.sh && chmod u+x /tmp/1xxxxxx.sh; && sh /tmp/1xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Delocalization; - alwaysRun: true; commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Delocalization; environment:; MEM_SIZE: '2.0'; MEM_UNIT: GB; resources:; virtualMachine:; bootDiskSizeGb: 12; bootImage: projects/cos-cloud/global/images/family/cos-stable; disks:; - name: local-disk; sizeGb: 10; type: pd-ssd; labels:; cromwell-workflow-id: xxxxxx; goog-pipelines-worker: 'true'; wdl-task-name: hello; machineType: custom-1-2048; network: {}; nvidiaDriverVersion: 450.51.06; serviceAccount:; email: default; scopes:; - https://www.googl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:20352,timeout,timeout,20352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['timeout'],['timeout']
Safety,"doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:12451,abort,abort,12451,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"e the error, but so far I can only get it from this workflow, possibly because of running time, IO, perl, seqware, I don't know. After much headdesking, I am nearly certain I have fixed the issue by changing the way cromwell executes a call. As you know, Cromwell generates a script.submit file which looks like this (in this case):. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow:/root/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow -i delly-docker-root /bin/bash < /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow/execution/script; ```. By removing input redirection into bash (i.e. removing the ""<"" character and changing the paths) I can get this workflow to run consistently without error. The new script.submit looks like:. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data-alex/cromwell-executions/PcawgDelly/bdc567a9-b1f7-4530-853a-5263b890ca6a/call-SeqwareWorkflow:/root/PcawgDelly/bdc567a9-b1f7-4530-853a-5263b890ca6a/call-SeqwareWorkflow -i delly-docker-root /bin/bash /root/PcawgDelly/bdc567a9-b1f7-4530-853a-5263b890ca6a/call-SeqwareWorkflow/execution/script; ```. Basically, it runs the script from inside the docker container. I cannot, unfortunately, describe why the bug happened in this seemingly rare case, other than pointing at the dangers of shell commands being interpreted from pipes/stdin and waving my hands a lot. But, I do think avoiding input redirection of commands into bash is probably a good thing, in this case. Am I missing some case where it's necessary?. Luckily I think the fix is simple, and probably just involves updating the core conf. here:; core/src/main/resources/reference.conf. I was able to provide my own config file overrides for now as a workaround. Thanks! Sorry for the super long bug report.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1556:2706,avoid,avoiding,2706,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556,1,['avoid'],['avoiding']
Safety,e we trying to upload an auth file when running in application default auth mode for both genomics and filesystems?. ```; [ERROR] [01/27/2017 14:39:36.100] [cromwell-system-akka.dispatchers.engine-dispatcher-5] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow 732474fd-88b0-4a5e-ad19-5ee5cd71d141 failed (during InitializingWorkflowState): Failed to upload authentication file; java.io.IOException: Failed to upload authentication file; 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$1.applyOrElse(JesInitializationActor.scala:81); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$1.applyOrElse(JesInitializationActor.scala:80); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1924:1033,recover,recoverWith,1033,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1924,1,['recover'],['recoverWith']
Safety,e$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:2766,unsafe,unsafeToFuture,2766,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['unsafe'],['unsafeToFuture']
Safety,e$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051:2271,unsafe,unsafeToFuture,2271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051,1,['unsafe'],['unsafeToFuture']
Safety,"e, @ruchim, @benjamincarlin, @gsaksena, @abaumann, @kshakir, @geoffjentry, and others at the Broad retreat and DSP holiday hackathon, we're putting a proposal for a new feature that reports task call resource utilization metrics to Stackdriver Monitoring API. This serves 2 important goals:. 1) Users can easily plot real-time resource usage statistics across all tasks in a workflow, or for a single task call across many workflow runs, etc. This can be very powerful to quickly determine outlier tasks that could use optimization, without the need for any configuration or code (or any changes to the workflow). It's also much easier than the current state-of-the-art, i.e. parsing task-level monitoring logs. 2) Scripts can easily get aggregate statistics on resource utilization and could produce suggestions based on those. This could provide a path towards automatic runtime configuration based on the models trained with historical data. One could also detect situations like out-of-memory calls and automatically adjust resources according to those. It would also be pretty easy to add logic for estimation of task call-level cost based on the pricing of associated resources. This could provide a long-sought feature of real-time cost monitoring/control (thanks to @TimothyTickle for the suggestion). Monitoring is done using the new ""monitoring action"" for PAPIv2, which currently uses the hard-coded [quay.io/broadinstitute/cromwell-monitor](https://quay.io/repository/broadinstitute/cromwell-monitor) image, built from https://github.com/broadinstitute/cromwell-monitor (I wasn't sure if that code belonged here or in a separate repo). This is advantageous to just using it as a _monitoring_script_, because it removes all assumptions on the ""user"" Docker image (for the task itself). For example, we don't have to assume a particular distribution or presence of Python and its libraries. So it should work exactly the same for any task. Per @geoffjentry's suggestion, we've [consulted](ht",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510:1000,detect,detect,1000,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510,1,['detect'],['detect']
Safety,"eg something like:; ```; $ cromwell quickstart; ## BACKEND ##; Would you like to run tasks [local], [sge] or [papi] (Google cloud)?; > [local] | papi; Installing gcloud......; Done! Would you like me to run [gcloud auth] for you to get default credentials set up?; > [yes] |; Running gcloud auth.......; Done!; Backend setup complete!. ## DATABASE ##; Would you like to keep a database of past runs (eg so that you can call-cache?); > [yes] | ; Would you like to use [mysql] or [hsqldb] file?; > [mysql] | ; MySQL detected. Will not reinstall.; Please enter a MySQL username:; > [root] | chris; Please enter the MySQL password:; > [] |; Database setup complete!. ## SETTING UP YOUR SYSTEM ##; Writing start/stop script to /usr/local/bin/cromwell.server...; Writing configuration file to /etc/cromwell/cromwell.conf... To run Cromwell you can now run:; # cromwell.server start; # cromwell submit <wdl> -i <inputs.json>. Good luck!; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2624:514,detect,detected,514,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2624,1,['detect'],['detected']
Safety,"ement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:527); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:56:36,88] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:17898,Timeout,TimeoutException,17898,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['Timeout'],['TimeoutException']
Safety,emented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:1497,recover,recoverAsync,1497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,2,['recover'],['recoverAsync']
Safety,empting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:1342,recover,recoverAsync,1342,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recoverAsync']
Safety,"entral1-b"" on a ""custom-1-2048"" machine; timestamp: '2021-08-03T15:22:07.789742627Z'; workerAssigned:; instance: google-pipelines-worker-xxxxxx; machineType: custom-1-2048; zone: us-central1-b; labels:; cromwell-workflow-id: cromwell-xxxxxx; wdl-task-name: hello; pipeline:; actions:; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Starting\ container\ setup.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: ContainerSetup; timeout: 300s; - commands:; - -c; - mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: ContainerSetup; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Done\ container\ setup.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: ContainerSetup; timeout: 300s; - commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Background; runInBackground: true; - commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; outputName: stderr; tag: Background; mounts:; - disk: local-disk; path: /cromwell_root; runInBackground: true; - commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; outputName: stdout; tag: Background; mounts:; - disk: local-disk; path: /cromwell_root; runInBackground: true;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:16375,timeout,timeout,16375,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['timeout'],['timeout']
Safety,"equest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:7697,abort,abort,7697,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"equest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:9798,abort,abort,9798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"equest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:12037,abort,abort,12037,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"equest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientReque",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:16584,abort,abort,16584,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"equest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.Abstra",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:19101,abort,abort,19101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"equest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.exec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:21409,abort,abort,21409,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"er-1282 INFO - WorkflowExecutionActor [UUID(8a965a5e)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:29:42,727 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(73be7f27)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1594,Abort,Abort,1594,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"ervices.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:5170,abort,abort,5170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"estarted the server as well. Can't find any other config/cache file where it has saved old address. Sometime workflows are fine pointing to new root but sometime not. <!-- Which backend are you running? -->; SLURM on cromwell 36. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. backend {; # Override the default backend.; default = ""PhoenixSLURM"". # The list of providers.; providers {. PhoenixSLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String userid; String partitions; String memory_per_node; Int nodes; Int cores; String time; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). exit-code-timeout-seconds = 600. submit = """"""; chmod 770 -R ${cwd}; sudo change-files.sh ${userid} ${cwd}; phoenix_home_cwd=""/home/${userid}""; phoenix_home_out=""/home/${userid}/stdout""; phoenix_home_err=""/home/${userid}/stderr"". phoenix_script=${script}_phonix; cat ${script} | sed -s ""s@#\!/bin/bash@#\!/bin/bash\nsource '/etc/profile' @g"" > $phoenix_script. sbatch --uid=${userid} --gid=${userid} \; -J ${job_name} \; -p ${partitions} \; -N ${nodes} \; -n ${cores} \; --mem=${memory_per_node} \; --time=${time} \; -D $phoenix_home_cwd \; -o $phoenix_home_out \; -e $phoenix_home_err \; $phoenix_script; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*"". root = ""/fast/gdr/uat/cromwell-executions""; }; }. } # providers. } # backend. # https://gatkforums.broadinstitute.org/wdl/discussion/9536/how-do-i-set-up-a-mysql-database-for-cromwell; # http://slick.lightbend.com/doc/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4404:1979,timeout,timeout,1979,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4404,1,['timeout'],['timeout']
Safety,"etProperty\n value = _GetPropertyWithoutD; efault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n; File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/googl; e/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n File \""/usr/local/share/google/google-cloud-sdk/lib/g; ooglecloudsdk/core/util/retry.py\"", line 155, in TryFunc\n return func(*args, **kwargs), None\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.p; y\"", line 41, in _ReadNoProxyWithCleanFailures\n return gce_read.ReadNoProxy(uri)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py\"", li; ne 50, in ReadNoProxy\n request, timeout=timeout_property).read()\n File \""/usr/lib/python2.7/urllib2.py\"", line 401, in open\n response = self._open(req, data)\n File \""/usr/lib/py; thon2.7/urllib2.py\"", line 419, in _open\n '_open', req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 379, in _call_chain\n result = func(*args)\n File \""/usr/lib/python2.7/urllib; 2.py\"", line 1211, in http_open\n return self.do_open(httplib.HTTPConnection, req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1184, in do_open\n r = h.getresponse(buffering=True); \n File \""/usr/lib/python2.7/httplib.py\"", line 1072, in getresponse\n response.begin()\n File \""/usr/lib/python2.7/httplib.py\"", line 408, in begin\n version, status, reason = self; ._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data; = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n); gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:4952,timeout,timeout,4952,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['timeout'],['timeout']
Safety,"etect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_1dec3320bf1ad48ec05404d0a505d12b.tmp \; --adapters gs://atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_d3da014369f27e577cdffc1919be7d8e.tmp \; \; --auto-detect-adapter \; \; \; --nth 2; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3032:2324,detect,detect-adapter,2324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032,1,['detect'],['detect-adapter']
Safety,"exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-18 13:22:35,882 cromwell-system-akka.dispatchers.engine-dispatcher-42 ERROR - WorkflowManagerActor Workflow 4057b0c6-0019-4a00-b8af-e392fbf89697 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.core.CromwellFatalException$.apply(core.scala:22); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:39); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run$$$capture(Promise.scala:60); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:4390,recover,recoverWith,4390,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recoverWith']
Safety,"f possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; We are using cromwell through broadinstitute/cromwell:87-ecd44b6 image.; cromwell configuration:; ```; include required(classpath(""application"")). system.new-workflow-poll-rate=1. // increase timeout for http requests..... getting meta-data can timeout for large workflows.; akka.http.server.request-timeout=600s. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; system {; 	job-rate-control {; 	 jobs = 100; 	 per = 1 second; 	}; input-read-limits {; lines = 128000000; bool = 7; int = 19; float = 50; string = 1280000; json = 12800000; tsv = 1280000000; map = 128000000; object = 128000000; }. # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; 	graceful-server-shutdown = true; max-concurrent-workflows = 5000. io {; throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; number-of-requests = 100000; per = 100 seconds; }; }; }. akka {; # Optionally set / override any akka settings; http {; server {; # Increasing these timeouts allow rest api responses for very large jobs; # to be returned to the user. When the timeout is reached the server would respond; # `The server was not able to produce a timely response to your request.`; # https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows; request-timeout = 600s; idle-timeout = 600s; }; }; }. services {; MetadataService {; #class = ""cromwell.services.metadata.impl.MetadataServiceActor""; config {; metadata-read-row-number-safety-threshold = 2000000; # # For normal usage the default",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:8135,timeout,timeout,8135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['timeout'],['timeout']
Safety,flaky centaur TES restart/abort test,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3571:26,abort,abort,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3571,1,['abort'],['abort']
Safety,"ge from Running to Success; 2023-04-18 22:00:18,464 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:106:1]: Status change from Running to Success; 2023-04-18 22:01:20,604 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:111:1]: Status change from Running to Success; 2023-04-18 22:14:47,728 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: Aborting workflow; 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:262:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/9178938377659283430); 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:112:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/8559201934542591362); 2023-04-18 22:14:48,295 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Successfully requested cancellation of projects/16371921765/locations/us-central1/operations/9178938377659283430; 2023-04-18 22:15:56,564 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: Status change from Running to Success; 2023-04-18 22:16:44,505 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Status change from Running to Cancelled; 2023-04-18 22:16:44,539 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: WorkflowExecutionActor [UUID(10fa31a8)] aborted: myco.pull:262:1; 2023-04-18 22:16:45,159 INFO - $f [UUID(10fa31a8)]: Copying workflow logs from /cromwell-workflow-logs/workflow.10fa31a8-acbe-4ab7-a96a-6550ec08df12.log to gs://fc-caa84e5a-8ef7-434e-af9c-feaf6366a042/submissions/93bf6971-bfa1-4cb8-bb22-c8a753f58c49/workflow.logs/workflow.10fa31a8-acbe-4ab7-a96a-6550ec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7121:5219,Abort,Aborted,5219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7121,1,['Abort'],['Aborted']
Safety,"google.com/storage/browser/broad-dsde-methods/cromwell-execution-34/PindelSmallVariants/?project=broad-dsde-methods&organizationId=548622027621) using the DSDE-methods Cromwell server V.34. The errors I'm experiencing is described below. Resubmitting the job fixes the problem. ## failure for job ID ; * `2ebeed9a-8f42-418b-8569-8d80f5654d50` (shard-40), ; * `1cc79dda-9c6e-41b4-ac99-e1a422258039` (shard-20). ```; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; /bin/bash: /cromwell_root/script: No such file or directory; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; ```. ------------. ## failure for job ID ; * `4c5c8530-b79d-465b-8050-f7ba7368c057` (shard-26), ; * `5cbb2124-c526-4ca0-978e-4154ff4501cd` (shard-0). ```; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; Initializing parameters...; Pindel version 0.2.5b8, 20151210.; Loading reference genome ...; Loading reference genome done.; Initializing parameters done.; Please use samtools to index your reference file.; .fai is missing. sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; ```. ------------. ## failure for job ID ; * `7f219a29-69c1-4116-9c54-5d4656ee0124`. ```; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; Initializing parameters...; Pindel version 0.2.5b8, 20151210.; Loading reference genome ...; Error: fasta line starts with  instead of '>'. Aborting.; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4148:1966,Abort,Aborting,1966,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4148,1,['Abort'],['Aborting']
Safety,"he slurm one); #; workflow-options; {; workflow-log-dir: ""cromwell-workflow-logs""; workflow-log-temporary: false; workflow-failure-mode: ""ContinueWhilePossible""; default; {; workflow-type: WDL; workflow-type-version: ""draft-2""; }; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; metadata {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql:<dburl>?rewriteBatchedStatements=true""; driver = ""com.mysql.cj.jdbc.Driver""; user = ""<user>""; password = ""<pass>"" ; connectionTimeout = 5000; }; }; }. call-caching; {; enabled = true; invalidate-bad-cache-result = true; }. docker {; hash-lookup {; enabled = true; }; }. backend {; default = sge; providers {. ; sge {; 	actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; #concurrent-job-limit = 5. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. runtime-attributes = """"""; String time = ""11:00:00""; Int cpu = 4; Float? memory_gb; String sge_queue = ""hammer.q""; String? sge_project; String? docker; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". submit-docker = """""" ; #location for .sif files and other apptainer tmp, plus lockfile; 	 export APPTAINER_CACHEDIR=<path>; export APPTAINER_PULLFOLDER=<path>; export APPTAINER_TMPDIR=<path>; export LOCK_FILE=""$APPTAINER_CACHEDIR/lockfile""; export IMAGE=$(echo ${do",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:2437,timeout,timeout-seconds,2437,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,1,['timeout'],['timeout-seconds']
Safety,"her-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Ab",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1918,Abort,Abort,1918,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"her-120 INFO - WorkflowExecutionActor [UUID(73be7f27)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1756,Abort,Abort,1756,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"her-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2565,Abort,Abort,2565,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"her-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2403,Abort,Abort,2403,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"hing; ```; <!-- Which backend are you running? -->; Used backend: ; GCPBATCH. Callcaching works with PAPIv2, not on GCPBATCH.; <!-- Paste/Attach your workflow if possible: -->; workflow used for testing:; ```; workflow myworkflow {; call mytask; }. task mytask {; String str = ""!""; command <<<; echo ""hello world ${str}""; >>>; output {; String out = read_string(stdout()); }. runtime{; docker: ""eu.gcr.io/project/image_name:tag""; cpu: ""1""; memory: ""500 MB""; disks: ""local-disk 5 HDD""; zones: ""europe-west1-b europe-west1-c europe-west1-d""; preemptible: 2; noAddress: true; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; We are using cromwell through broadinstitute/cromwell:87-ecd44b6 image.; cromwell configuration:; ```; include required(classpath(""application"")). system.new-workflow-poll-rate=1. // increase timeout for http requests..... getting meta-data can timeout for large workflows.; akka.http.server.request-timeout=600s. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; system {; 	job-rate-control {; 	 jobs = 100; 	 per = 1 second; 	}; input-read-limits {; lines = 128000000; bool = 7; int = 19; float = 50; string = 1280000; json = 12800000; tsv = 1280000000; map = 128000000; object = 128000000; }. # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; 	graceful-server-shutdown = true; max-concurrent-workflows = 5000. io {; throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; number-of-requests = 100000; per = 100 seconds; }; }; }. akka {; # Optionally set / override any",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:7481,timeout,timeout,7481,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['timeout'],['timeout']
Safety,"https://broadworkbench.atlassian.net/browse/WX-765. This compiles and passes tests, though I have a sense that the work done in `YamlUtils.scala` may now be duplicating remediations that are now built in to SnakeYAML. I feel like this PR disables built-in safety checks that we then reimplement. Another possible solution may be to stop disabling the safety checks and adjust the test expectations to match how SnakeYAML reports errors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6927:169,remediat,remediations,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6927,3,"['remediat', 'safe']","['remediations', 'safety']"
Safety,"https://cromwell.gotc-int.broadinstitute.org/swagger/index.html?url=/swagger/cromwell.yaml#!/Workflows/post_workflows_version_id_abort. When going through swagger, the abort takes a long time and then gives an error: ; Response Code 500; ""status"": ""error"",; ""message"": ""The server was not able to produce a timely response to your request."". The workflow is removed from WORKFLOW_STORE_ENTRY but the associated jobs are still present in JOB_STORE_ENTRY. . There are no errors in the logs: ; `; 2016-12-12 18:22:26,139 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(8a965a5e)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:29:42,727 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(73be7f27)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.disp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:168,abort,abort,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,7,"['Abort', 'abort']","['Abort', 'Aborting', 'abort']"
Safety,https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1025/. java.util.concurrent.TimeoutException: Futures timed out after [1 second] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:255) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:259) at scala.concurrent.Await$.$anonfun$result$1(package.scala:215) at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53) at scala.concurrent.Await$.result(package.scala:142) at akka.http.scaladsl.testkit.RouteTest.responseAs(RouteTest.scala:70) at akka.http.scaladsl.testkit.RouteTest.responseAs$(RouteTest.scala:68) at cromiam.webservice.SwaggerServiceSpec.responseAs(SwaggerServiceSpec.scala:17) at cromiam.webservice.SwaggerServiceSpec.$anonfun$new$2(SwaggerServiceSpec.scala:30) at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58) at akka.http.scaladsl.testkit.RouteTest.$anonfun$check$1(RouteTest.scala:56) at akka.http.scaladsl.testkit.RouteTestResultComponent$RouteTestResult.$tilde$greater(RouteTestResultComponent.scala:50) at cromiam.webservice.SwaggerServiceSpec.$anonfun$new$1(SwaggerServiceSpec.scala:27) at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85) at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83) at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) at org.scalatest.Transformer.apply(Transformer.scala:22) at org.scalatest.Transformer.apply(Transformer.scala:20) at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682) at org.scalatest.TestSuite.withFixture(TestSuite.scala:196) at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195) at org.scalatest.FlatSpec.withFixture(FlatSpec.scala:1685) at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680) at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692) at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289) at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692) at org.scalatest.FlatSpecLike.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4357:103,Timeout,TimeoutException,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4357,1,['Timeout'],['TimeoutException']
Safety,"https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/793/consoleFull. 14:08:29 cromwell-test_1 | [info] - should execute calls with input files and localize them appropriately *** FAILED *** (13 seconds, 25 milliseconds); 14:08:29 cromwell-test_1 | [info] $anon$1 was thrown during property evaluation. (SharedFileSystemJobExecutionActorSpec.scala:119); 14:08:29 cromwell-test_1 | [info] Message: A timeout occurred waiting for a future to complete. Queried 21 times, sleeping 500 milliseconds between each query.; 14:08:29 cromwell-test_1 | [info] Location: (SharedFileSystemJobExecutionActorSpec.scala:137); 14:08:29 cromwell-test_1 | [info] Occurred at table row 2 (zero based, not counting headings), which had values (; 14:08:29 cromwell-test_1 | [info] conf = BackendConfigurationDescriptor(Config(SimpleConfigObject({""default-runtime-attributes"":{""continueOnReturnCode"":0,""cpu"":1,""failOnStderr"":false},""filesystems"":{""local"":{""localization"":[""soft-link""]}},""root"":""local-cromwell-executions""})),Config(SimpleConfigObject({}))),; 14:08:29 cromwell-test_1 | [info] isSymLink = true; 14:08:29 cromwell-test_1 | [info] ); 14:08:29 cromwell-test_1 | [info] org.scalatest.exceptions.TableDrivenPropertyCheckFailedException:; 14:08:29 cromwell-test_1 | [info] ...; 14:08:29 cromwell-test_1 | [info] at cromwell.backend.sfs.SharedFileSystemJobExecutionActorSpec.localizationSpec(SharedFileSystemJobExecutionActorSpec.scala:119); 14:08:29 cromwell-test_1 | [info] at cromwell.backend.sfs.SharedFileSystemJobExecutionActorSpec.$anonfun$new$4(SharedFileSystemJobExecutionActorSpec.scala:156); 14:08:29 cromwell-test_1 | [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); 14:08:29 cromwell-test_1 | [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); 14:08:29 cromwell-test_1 | [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); 14:08:29 cromwell-test_1 | [info] at org.scalatest.Transformer.apply(Transformer.scala:22); 14:08:29 cromwell-t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4319:421,timeout,timeout,421,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4319,1,['timeout'],['timeout']
Safety,"https://github.com/broadinstitute/cromwell/blob/215cca97e6efafa7406fd89b38e21a39ce2d0d4e/cromwell.examples.conf#L440. It seems like the default configuration sends the stdout and stderr to the same filenames twice. In the config above, qsub is called with -o EG:; ```e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stdout```. In the script that will be executed on the node, there is some shenanigans:; ```; (; cd .../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution. bla bla bla my command invocation; ) > >(tee '.../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stdout') 2> >(tee '.../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stderr' >&2); ```. Those 'tees' are generated by cromwell somewhere, I do not know if the config has control of that. I don't know the details of what will happen, but I do not think it will be healthy. I smell race conditions. I came across this trying to debug missing log data when SGE aborts a job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3705:965,abort,aborts,965,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3705,1,['abort'],['aborts']
Safety,"ication/octet-stream]...; / [0/1 files][ 0.0 B/ 3.7 MiB] 0% Done ; BadRequestException: 400 The maximum object length is 1024 characters, but got a name with 1055 characters: ''gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-...''; CommandException: 1 file/object could not be transferred.; Copying file:///cromwell_root/bcbiotest/gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-calculate_sv_coverage/shard-0/cromwell_root/bcbiotest/gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-calculate_sv_bins/cromwell_root/bcbiotest/gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-postprocess_alignment/shard-0/cromwell_root/bcbiotest/gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-postprocess_alignment_to_rec/cromwell_root/bcbiotest/gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-alignment/shard-0/wf-alignment.cwl/94c8f53c-dad4-4c48-9e09-f6927356f352/call-merge_split_alignments/cromwell_root/bcbiotest/gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-alignment/shard-0/wf-alignment.cwl/94c8f53c-dad4-4c48-9e09-f6927356f352/call-process_alignment/shard-0/cromwell_root/align/Test2/Test2-sort.bam.bai [Content-Type=application/octet-stream]...; / [0/1 files][ 0.0 B/ 184.0 B] 0% Done ; BadRequestException: 400 The maximum object length is 1024 characters, but got a name with 1059 characters: ''gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-...''; CommandException: 1 file/object could not be transferred.; ```; We could avoid in this case by shortening the folder name we're using for cromwell storage but I'm worried about hitting this later since it will be dependent on sample names (the `Test2` in these paths) and users correctly setting a short root path. Do you know where this limit originates from? Is there any way to work around it? Thanks for any suggestions.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4471:3072,avoid,avoid,3072,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4471,1,['avoid'],['avoid']
Safety,"idss-purple.packed.json"" -F ""workflowInputs=@gridss-purple.packed.input.json"" -F ""workflowOptions=@/opt/cromwell/configs/options.json"" -F ""workflowType=CWL"" -F ""workflowTypeVersion=v1.0"" -F ""workflowRoot=main""; ```. # Inputs. ## gridss-purple.packed.json. <details>. <summary> Click to expand! </summary>. ```; {; ""$graph"": [; {; ""class"": ""CommandLineTool"",; ""doc"": ""AMBER is designed to generate a tumor BAF file for use in PURPLE from a provided VCF of likely heterozygous SNP sites.\n\nWhen using paired reference/tumor bams,\nAMBER confirms these sites as heterozygous in the reference sample bam then calculates the\nallelic frequency of corresponding sites in the tumor bam.\nIn tumor only mode, all provided sites are examined in the tumor with additional filtering then applied.\n\nThe Bioconductor copy number package is then used to generate pcf segments from the BAF file.\n\nWhen using paired reference/tumor data, AMBER is also able to:\n1. detect evidence of contamination in the tumor from homozygous sites in the reference; and\n2. facilitate sample matching by recording SNPs in the germline\n"",; ""requirements"": [; {; ""dockerPull"": ""quay.io/biocontainers/hmftools-amber:3.3--0"",; ""class"": ""DockerRequirement""; },; {; ""expressionLib"": [; ""var get_start_memory = function(){ /* Start with 2 Gb */ return 2000; }"",; ""var get_max_memory_from_runtime_memory = function(max_ram){ /* Get Max memory and subtract heap memory */ return max_ram - get_start_memory(); }""; ],; ""class"": ""InlineJavascriptRequirement""; },; {; ""coresMin"": 16,; ""ramMin"": 32000,; ""class"": ""ResourceRequirement""; },; {; ""class"": ""ShellCommandRequirement""; }; ],; ""baseCommand"": [; ""AMBER""; ],; ""arguments"": [; {; ""prefix"": ""-Xms"",; ""separate"": false,; ""valueFrom"": ""$(get_start_memory())m"",; ""position"": -2; },; {; ""prefix"": ""-Xmx"",; ""separate"": false,; ""valueFrom"": ""$(get_max_memory_from_runtime_memory(runtime.ram))m"",; ""position"": -1; }; ],; ""inputs"": [; {; ""type"": ""File"",; ""doc"": ""Path to vcf file containing l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:1113,detect,detect,1113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['detect'],['detect']
Safety,"if a job is aborted before it is started, cancel it efficiently",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2966:12,abort,aborted,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2966,1,['abort'],['aborted']
Safety,"il.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:990); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3562); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3462); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3905); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2530); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2683); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2491); 	at com.mysql.jdbc.ConnectionImpl.setAutoCommit(ConnectionImpl.java:4807); 	at com.zaxxer.hikari.pool.ProxyConnection.setAutoCommit(ProxyConnection.java:379); 	at com.zaxxer.hikari.pool.HikariProxyConnection.setAutoCommit(HikariProxyConnection.java); 	at slick.jdbc.JdbcBackend$BaseSession.startInTransaction(JdbcBackend.scala:470); 	at slick.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:39); 	at slick.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:36); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.io.EOFException: Can not read response from server. Expected to read 4 bytes, read 0 bytes before connection was unexpectedly lost.; 	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3014); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3472); 	... 16 common frames omitted. ```. The `Lock wait timeout exceeded; try restarting transaction submit failed.` one keeps happening over and over. Workflows that have been submitted do not start (the only log entry is the ""workflow submitted"" one).; The workflow store entry table is empty, which makes sense in terms of nothing happening.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4360:8140,timeout,timeout,8140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4360,1,['timeout'],['timeout']
Safety,"ill; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 10000. # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; # account = """"; # token = """"; }. # Number of workers to assign to PAPI requests; request-workers = 3. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""service-account""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # pipeline-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""service-account"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""europe-west4"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:12207,timeout,timeout,12207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,2,['timeout'],['timeout']
Safety,in `when(WorkflowExecutionAbortingState)` there's a handler for `AbortedResponse` which appears to be doing some reasonable stuff but I can't find anything sending that message.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1376:65,Abort,AbortedResponse,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1376,1,['Abort'],['AbortedResponse']
Safety,"in the workbench chat room they were tracking something where rawls was spamming cromwell with abort requests. The root cause was that a workbench submission was requested to abort which causes rawls to continuously send abort requests to the submission's workflows until it is aborted, which sometimes never happens. they're not fans of this behavior. Tagging @helgridly and @jmthibault79 as people who can provide more details.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1976:95,abort,abort,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1976,4,['abort'],"['abort', 'aborted']"
Safety,increase test timeout on akka http routes for slow CI envs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4250:14,timeout,timeout,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4250,1,['timeout'],['timeout']
Safety,"ine 50, in ReadNoProxy\n request, timeout=timeout_property).read()\n File \""/usr/lib/python2.7/urllib2.py\"", line 401, in open\n response = self._open(req, data)\n File \""/usr/lib/python2.7/urllib2.py\"", line 419, in _open\n '_open', req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 379, in _call_chain\n result = func(*args)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1211, in http_open\n return self.do_open(httplib.HTTPConnection, req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1184, in do_open\n r = h.getresponse(buffering=True)\n File \""/usr/lib/python2.7/httplib.py\"", line 1072, in getresponse\n response.begin()\n File \""/usr/lib/python2.7/httplib.py\"", line 408, in begin\n version, status, reason = self._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n); gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam -> /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam (cp failed: gsutil -q -m cp gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam, command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205, in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False)\n Fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:12404,timeout,timeout,12404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['timeout'],['timeout']
Safety,"ing a success. ```; 2023-04-18 21:59:54,599 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:108:1]: Status change from Running to Success; 2023-04-18 22:00:09,060 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:107:1]: Status change from Running to Success; 2023-04-18 22:00:18,464 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:106:1]: Status change from Running to Success; 2023-04-18 22:01:20,604 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:111:1]: Status change from Running to Success; 2023-04-18 22:14:47,728 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: Aborting workflow; 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:262:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/9178938377659283430); 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:112:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/8559201934542591362); 2023-04-18 22:14:48,295 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Successfully requested cancellation of projects/16371921765/locations/us-central1/operations/9178938377659283430; 2023-04-18 22:15:56,564 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: Status change from Running to Success; 2023-04-18 22:16:44,505 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Status change from Running to Cancelled; 2023-04-18 22:16:44,539 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: WorkflowExecutionActor [UUID(10fa31a8)] aborted: myco.pull:262:1; 2023-04-18 22:16:45,1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7121:4937,Abort,Aborted,4937,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7121,1,['Abort'],['Aborted']
Safety,"instead of registering the workflow id as the collection, as currently happens, register the collection name. if a collection was not specified (it's optional) create a collection name, `USERNAME_caas_collection`. . If the collection name specified already exists in sam and the user does not have write access to it it will return an error (or at least that's the belief of the author of this ticket). Make sure to detect that and return an appropriate error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2837:416,detect,detect,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2837,1,['detect'],['detect']
Safety,"io=5 os_prio=31 tid=0x00007fb76f4b6000 nid=0x7b03 waiting on condition [0x000000012b643000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""db-1"" #26 daemon prio=5 os_prio=31 tid=0x00007fb76b442000 nid=0x7903 waiting on condition [0x000000012d905000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""Hikari Housekeeping Timer (pool db)"" #24 daemon prio=5 os_prio=31 tid=0x00007fb76d88f000 nid=0x7503 waiting on condition ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:40437,Unsafe,Unsafe,40437,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"ion settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2019-02-11 10:13:27,65] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Workflow test1 complete. Final Outputs:; {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; }; [2019-02-11 10:13:27,69] [info] WorkflowManagerActor WorkflowActor-52999e15-953f-44d6-aaae-1774c74d2910 is in a terminal state: WorkflowSucceededState; [2019-02-11 10:13:35,97] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; },; ""id"": ""52999e15-953f-44d6-aaae-1774c74d2910""; }; [2019-02-11 10:13:36,30] [info] Workflow polling stopped; [2019-02-11 10:13:36,32] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-11 10:13:36,33] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stoppe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:15287,Timeout,Timeout,15287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,3,"['Abort', 'Timeout']","['Aborting', 'Timeout']"
Safety,"ion.WorkflowExecutionActor.akka$actor$Timers$$super$aroundReceive(WorkflowExecutionActor.scala:57); 	at akka.actor.Timers.aroundReceive(Timers.scala:51); 	at akka.actor.Timers.aroundReceive$(Timers.scala:40); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:57); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); 	at akka.actor.ActorCell.invoke(ActorCell.scala:583); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2024-03-12 20:24:51 cromwell-system-akka.actor.default-dispatcher-4 INFO - Message [cromwell.engine.workflow.lifecycle.EngineLifecycleActorAbortCommand$] from Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-a06a4c5e-fbf7-4c1d-ac71-b036aaf48fbc#2096097107] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-a06a4c5e-fbf7-4c1d-ac71-b036aaf48fbc/WorkflowExecutionActor-a06a4c5e-fbf7-4c1d-ac71-b036aaf48fbc#659989485] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-a06a4c5e-fbf7-4c1d-ac71-b036aaf48fbc/WorkflowExecutionActor-a06a4c5e-fbf7-4c1d-ac71-b036aaf48fbc#659989485]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; ```; {; 	""status"": ""Aborting"",; 	""id"": ""a06a4c5e-fbf7-4c1d-ac71-b036aaf48fbc""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7386:9284,Abort,Aborting,9284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7386,1,['Abort'],['Aborting']
Safety,"ipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; # Google project which will be billed for the requests; project = ""xxxxx-xxxxx-xxxxx"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:4518,timeout,timeouts,4518,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,2,"['Timeout', 'timeout']","['Timeout', 'timeouts']"
Safety,"ipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""service-account""; # Google project which will be billed for the requests; project = ""***-***"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:14164,timeout,timeouts,14164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,2,"['Timeout', 'timeout']","['Timeout', 'timeouts']"
Safety,"is.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleCl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:7628,abort,abort,7628,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"is.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.Abstra",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:9729,abort,abort,9729,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"is.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogle",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:11968,abort,abort,11968,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"is.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:14483,abort,abort,14483,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"is.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoog",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:16515,abort,abort,16515,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"is.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:19032,abort,abort,19032,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"is.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.g",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:21340,abort,abort,21340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,ispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:8069,recover,recover,8069,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recover']
Safety,"isting workflow for Mutect2 available here: https://app.terra.bio/#workspaces/terra-outreach/CHIP-Detection-Mutect2 to run on SLURM with Singularity configuration. There are multiple steps similar to Mutect2 public workflow available here: https://github.com/broadinstitute/gatk/blob/master/scripts/mutect2_wdl/mutect2.wdl , but still attaching the modified WDL with additional steps. . So when we run this with the given configuration using the following; export SINGULARITY_CACHEDIR=$PWD/singularity_cache; export SINGULARITY_TMPDIR=$PWD/tmpdir; module load singularity; rm -rf nohup.out && nohup java -Dconfig.file=$PWD/cromwell_singularity.conf -jar $PWD/cromwell-84.jar run $PWD/mutect2_modified.wdl --inputs $PWD/inputs.json &. The issue is that the first step of splitting intervals runs fine, but as it starts mutect2, it starts copying of the complete execution directory making here is the directory structure. cromwell-executions/; └── Mutect2; └── e5769b79-5e02-44a5-a4f8-38745e152beb; ├── call-M2; │ └── shard-0; │ ├── execution; │ └── inputs; │ ├── -1816294717; │ ├── 1855713868; │ │ └── run_cromwell_only.tmp; │ │ └── cromwell-executions; │ │ └── Mutect2; │ │ └── e5769b79-5e02-44a5-a4f8-38745e152beb; │ ├── 2035192126; │ └── 891763929; └── call-SplitIntervals; ├── execution; │ ├── glob-0fc990c5ca95eebc97c4c204e3e303e1; │ └── interval-files; ├── inputs; │ └── -1816294717; └── tmp.c9d96672. As you can see that run_cromwell_only.tmp is being made and that happens to fall in an endless loop and eventually, it errors stating the file name is too long to copy. Can you help me how to avoid this behavior of making circular paths when copying files for execution? Also, note it does not happen in the first step of SplitIntervals but happens in the Mutect2 call. [mutect2_gatk.wdl.txt](https://github.com/broadinstitute/cromwell/files/9813528/mutect2_gatk.wdl.txt); [cromwell_singularity.conf.txt](https://github.com/broadinstitute/cromwell/files/9813529/cromwell_singularity.conf.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6934:1633,avoid,avoid,1633,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6934,1,['avoid'],['avoid']
Safety,"ka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:16862,abort,abort,16862,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"l.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-2"" #43 prio=5 os_prio=31 tid=0x00007fb76e8ee000 nid=0x3f0b waiting on condition [0x000000012ee35000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""_jprofiler_control_sampler"" #34 daemon prio=9 os_prio=31 tid=0x00007fb771044800 nid=0x6307 waiting on condition [0x000000012ab3a000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at com.jprofiler.agent.probe.y.run(ejt:1030). ""_jprofiler_native_sample",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:34559,Unsafe,Unsafe,34559,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"l.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-3"" #57 prio=5 os_prio=31 tid=0x00007fb76e95e000 nid=0x9b03 waiting on condition [0x00000001322d6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-17"" #56 daemon prio=5 os_prio=31 tid=0x00007fb76b6b6000 nid=0x9903 waiting on condition [0x0000000131db9000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$C",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:19698,Unsafe,Unsafe,19698,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"l.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-4"" #60 prio=5 os_prio=31 tid=0x00007fb76d42b000 nid=0xa103 waiting on condition [0x0000000132168000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-19"" #59 daemon prio=5 os_prio=31 tid=0x00007fb770631000 nid=0x9f03 waiting on condition [0x00000001324dc000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$C",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:16668,Unsafe,Unsafe,16668,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"le {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; # #number-of-requests = 100000; # #per = 100 seconds; # }. # Number of times an I/O operation should be attempted before giving up and failing it.; #number-of-attempts = 5; }. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; #lines = 128000; #bool = 7; #int = 19; #float = 50; #string = 128000; #json = 128000; #tsv = 128000; #map = 128000; #object = 128000; }. abort {; # These are the default values in Cromwell, in most circumstances there should not be a need to change them. # How frequently Cromwell should scan for aborts.; scan-frequency: 30 seconds. # The cache of in-progress aborts. Cromwell will add entries to this cache once a WorkflowActor has been messaged to abort.; # If on the next scan an 'Aborting' status is found for a workflow that has an entry in this cache, Cromwell will not ask; # the associated WorkflowActor to abort again.; cache {; enabled: true; # Guava cache concurrency.; concurrency: 1; # How long entries in the cache should live from the time they are added to the cache.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 100000; }; }. # Cromwell reads this value into the JVM's `networkaddress.cache.ttl` setting to control DNS cache expiration; dns-cache-ttl: 3 minutes; }. docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:3467,Abort,Aborting,3467,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,2,"['Abort', 'abort']","['Aborting', 'abort']"
Safety,licate-pairs-cloud.tsv. ```. ```; Could not localize gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam -> /home/lichtens/test_onco_m2/cromwell-executions/Mutect2ReplicateValidation/bf7e55a8-033b-4b36-9aa6-eeb2d77579d8/call-Mutect2/shard-11/Mutect2/0802e0bb-3231-4e14-a627-1ed839b213ae/call-CollectSequencingArtifactMetrics/inputs/broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam:; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam doesn't exists; null; 500 Internal Server Error; Backend Error; 500 Internal Server Error; Backend Error; at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1$$anonfun$apply$1.applyOrElse(StandardAsyncExecutionActor.scala:106); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1$$anonfun$apply$1.applyOrElse(StandardAsyncExecutionActor.scala:105); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at scala.util.Failure.recoverWith(Try.scala:203); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1.apply(StandardAsyncExecutionActor.scala:105); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1.apply(StandardAsyncExecutionActor.scala:105); at cromwell.backend.wdl.Command$.instantiate(Command.scala:27); at cromwell.backend.standard.StandardAsyncExecutionActor$class.instantiatedCommand(StandardAsyncExecutionActor.scala:198); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.standard.StandardAsyncExecutionActor$class.commandScriptContents(StandardAsyncExecutionActor.scala:170); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2011:2325,recover,recoverWith,2325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2011,1,['recover'],['recoverWith']
Safety,llo:NA:1]: Error attempting to Recover(StandardAsyncJob(projects/broad-dsde-cromwell-dev/locations/us-central1/jobs/job-7ce25791-3731-4a69-97f1-b7b65ac8ff71)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:3060,recover,recover,3060,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recover']
Safety,logs show integrity constraint violation in hsqldb when running job is aborted,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/869:71,abort,aborted,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/869,1,['abort'],['aborted']
Safety,"low-options.workflow-log-dir: ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/workflow-logs"". # Allows re-use of existing results for jobs you've already run; call-caching.enabled: true. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; # set the root directory to the run; root = ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/execution""; filesystems.local {; ## do not allow copy (too huge files); ## prefer hard-links, to don't remove data and kept analysis intact; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. The log shows the following stack-trace:. ```; [2018-03-09 15:31:16,47] [error] Failed to properly flush metadata to database; java.sql.SQLException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActio",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:1209,timeout,timeout,1209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['timeout'],['timeout']
Safety,"m Google from https://partnerissuetracker.corp.google.com/issues/71697449:; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #1 Jan 8, 2018 09:25AM ; > Reported Issue; > I don't have specific numbers at this time, but over the past several weeks our production operations staff started noticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Mes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:1026,ABORT,ABORTED,1026,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['ABORT'],['ABORTED']
Safety,"m; labels:; tag: Background; runInBackground: true; - commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; outputName: stderr; tag: Background; mounts:; - disk: local-disk; path: /cromwell_root; runInBackground: true; - commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; outputName: stdout; tag: Background; mounts:; - disk: local-disk; path: /cromwell_root; runInBackground: true; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Starting\ localization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Localization; timeout: 300s; - commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Localization; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Localization; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - /bin/bash /cromwell_root/gcs_localization.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Localization; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:17608,timeout,timeout,17608,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['timeout'],['timeout']
Safety,"mber-of-workflow-log-copy-workers = 10. # Default number of cache read workers; #number-of-cache-read-workers = 25. io {; # throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; # #number-of-requests = 100000; # #per = 100 seconds; # }. # Number of times an I/O operation should be attempted before giving up and failing it.; #number-of-attempts = 5; }. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; #lines = 128000; #bool = 7; #int = 19; #float = 50; #string = 128000; #json = 128000; #tsv = 128000; #map = 128000; #object = 128000; }. abort {; # These are the default values in Cromwell, in most circumstances there should not be a need to change them. # How frequently Cromwell should scan for aborts.; scan-frequency: 30 seconds. # The cache of in-progress aborts. Cromwell will add entries to this cache once a WorkflowActor has been messaged to abort.; # If on the next scan an 'Aborting' status is found for a workflow that has an entry in this cache, Cromwell will not ask; # the associated WorkflowActor to abort again.; cache {; enabled: true; # Guava cache concurrency.; concurrency: 1; # How long entries in the cache should live from the time they are added to the cache.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 100000; }; }. # Cromwell reads this value into the JVM's `networkaddress.cache.ttl` setting to control DNS cache expiration; dns-cache-ttl: 3 minutes; }. docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:3433,abort,abort,3433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['abort'],['abort']
Safety,"me (oligos)......done (78,222,840 bytes, 19098 pages, 0.00 sec); Pre-loading compressed genome (bits)......done (78,222,864 bytes, 19098 pages, 0.02 sec); Looking for index files in directory defuse-data/gmap/cdna; Pointers file is cdna.ref153offsets64meta; Offsets file is cdna.ref153offsets64strm; Positions file is cdna.ref153positions; Offsets compression type: bitpack64; Allocating memory for ref offset pointers, kmer 15, interval 3...Attached existing memory (2 attached) for defuse-data/gmap/cdna/cdna.ref153offsets64meta...done (134,217,744 bytes, 0.00 sec); Allocating memory for ref offsets, kmer 15, interval 3...Attached new memory for defuse-data/gmap/cdna/cdna.ref153offsets64strm...done (234,475,312 bytes, 0.23 sec); Pre-loading ref positions, kmer 15, interval 3......done (276,173,052 bytes, 0.05 sec); Starting alignment; Failed attempt to alloc 18446744073709550532 bytes; Exception: Allocation Failed raised at indexdb.c:2885; /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/log/defuse.12.sh: line 6: 7481 Segmentation fault (core dumped) /usr/local/bin/gmap -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa > /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa.cdna.psl.tmp; ; real 0m1.262s; user 0m0.046s; sys 0m0.564s. ```. Run within the docker container but not using Cromwell, the output of that command looks like this:; ```; Starting defuse command:; /usr/local/bin/gmap -D Program_required_data/deFuse/defuse-data/gmap -d cdna -f psl #<1 > #>1; Reasons:; /mnt/Workflow_runs/2_fusion_detection_tools/BT474/BT474_deFuse_0.8.1/jobs/breakpoints.split.001.fa.cdna.psl m; issing; Success for defuse command:; /usr/local/bin/gmap -D Program_required_data/deFuse/defuse-data/gmap -d est4 -f psl /mnt/Workflow_runs/2_fus; ion_detection_to",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465:2647,detect,detectFusions,2647,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465,1,['detect'],['detectFusions']
Safety,"minute delays for processing cache hits. With multiple steps in serial, this means that nothing in my pipeline starts running till 14 minutes after I start the run. Can you help me fix that?. Thank you for the help!. Happy to provide any more info than the below if that's helpful. I'm running with cromwell 84. Here's the command I'm running `java -Dconfig.file=workflow/cromwell.conf -jar utilities/cromwell-84.jar run workflow/expanse_workflow.wdl`. Here's my configuration (ignore the SLURM part, I'm not using it yet). Potentially important bits:; * I'm running with the local backend.; * I'm using symlink caching so that should be fast, with path+timestamp hash codes so the whole file doesn't need to be read; * I'm using the file based Hsql database. I don't see why that should matter, but maybe it does.; ```; # See https://cromwell.readthedocs.io/en/stable/Configuring/; # only use double quotes!; include required(classpath(""application"")). system {; abort-jobs-on-terminate = true; io {; number-of-requests = 30; per = 1 second; }; }. ## file based persistent database; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 120000; numThreads = 1; }; }. call-caching {; enabled = true; }. backend {; default = ""Local""; providers { ; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10; run-in-background = true; submit = ""/usr/bin/env bash ${script}""; root = ""cromwell-executions""; filesystems {; local {; localization: [""soft-link""]; caching {; duplication-strategy: [""soft-link""]; hasing-strategy: [""path+modtime""]; }; }; }; }; }; SLURM {; actor-factory = ""cromwell.ba",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:1199,abort,abort-jobs-on-terminate,1199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['abort'],['abort-jobs-on-terminate']
Safety,"mit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". submit-docker = """""" ; #location for .sif files and other apptainer tmp, plus lockfile; 	 export APPTAINER_CACHEDIR=<path>; export APPTAINER_PULLFOLDER=<path>; export APPTAINER_TMPDIR=<path>; export LOCK_FILE=""$APPTAINER_CACHEDIR/lockfile""; export IMAGE=$(echo ${docker} | tr '/:' '_').sif; if [ -z $APPTAINER_CACHEDIR ]; then; exit 1; fi; CACHE_DIR=$APPTAINER_CACHEDIR; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR; # downloads sifs only one at a time; apptainer sif db doesn't handle concurrency well; out=$(flock --exclusive --timeout 1800 $LOCK_FILE apptainer pull $IMAGE docker://${docker} 2>&1); ret=$?; if [[ $ret == 0 ]]; then; echo ""Successfully pulled ${docker}!""; else; if [[ $(echo $out | grep ""exists"" ) ]]; then; echo ""Image file already exists, ${docker}!""; else; echo ""Failed to pull ${docker}"" >> /dev/stderr; exit $ret; fi; fi; #full path to sif for qsub command; IMAGE=""$APPTAINER_PULLFOLDER/$IMAGE""; qsub \; -terse \; -V \; -b y \; -N ""${job_name}"" \; -wd ""${cwd}"" \; -o ""${out}.qsub"" \; -e ""${err}.qsub"" \; -pe smp ""${cpu}"" \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; apptainer exec --cleanenv --bind ""${cwd}:${docker_cwd},<path>"" ""$IMAGE"" ""${job_shell}"" ""${docker_script}""; """""". default-runtime-attributes; {; failOnStderr: false; continueOnReturnCode: 0; }; }; }. sge_docker {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. runtime-attributes = """"""; String time = ""11:00:00""; Int cpu = 4; Float? memory_gb; String sge_queue = ""hammer.q""; String? sge_project; String? docker; """""". submit = """"""; qsub \",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:3843,timeout,timeout,3843,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,1,['timeout'],['timeout']
Safety,"mktemp -d /tmp/tmp.XXXXXX)"". runtime-attributes = """"""; Int runtime_minutes = 60; Int cpu = 1; Int memory_mb = 3900; String? docker; """""". submit = """""" \; 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; -p batch,scavenger \; -c ${cpu} \; --mem $(( (${memory_mb} >= ${cpu} * 3900) ? ${memory_mb} : $(( ${cpu} * 3900 )) )) \; -N 1 \; --exclusive \; --wrap ""/bin/bash ${script}""'; """""". submit-docker = """""" \. # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; module load apptainer; if [ -z $APPTAINER_CACHEDIR ];; then CACHE_DIR=$HOME/.apptainer/cache; else CACHE_DIR=$APPTAINER_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/apptainer_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --exclusive --timeout 900 $LOCK_FILE \; apptainer exec --containall /mainfs/wrgl/broadinstitute_warp_development/warp/images/${docker}.sif \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM. 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; -p batch,scavenger \; -c ${cpu} \; --mem $(( (${memory_mb} >= ${cpu} * 3900) ? ${memory_mb} : $(( ${cpu} * 3900 )) )) \; -N 1 \; --exclusive \; --wrap \; ""module load apptainer; apptainer exec \; --containall \; --bind /mainfs/wrgl/reference_files/reference_genome/gcp-public-data--broad-references:/mainfs/wrgl/reference_files/reference_genome/gcp-public-data--broad-references \; --bind ${cwd}:${docker_cwd} \; --bind /tmp:/tmp \; /mainfs/wrgl/broadinstitute_warp_development/warp/images/${docker}.sif \; ${job_shell} \; ${docker_script}""'; """""". kill = ""'scancel ${job_id}'"". check-alive = ""'squeue -j ${job_id}'"". job-id-regex = ""Submitted batch job (\\d+).*"". }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7086:2199,timeout,timeout,2199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7086,1,['timeout'],['timeout']
Safety,mwell-system-akka.dispatchers.engine-dispatcher-47 INFO - Assigned new job execution tokens to the following groups: 119e11a5: 1; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - BT-322 119e11a5:wf_hello.hello:-1:1 is eligible for call caching with read = true and write = true; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-43 INFO - BT-322 119e11a5:wf_hello.hello:-1:1 cache hit copying nomatch: could not find a suitable cache hit.; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-43 INFO - 119e11a5-b981-4510-a6d9-b5c26dfbb4e3-EngineJobExecutionActor-wf_hello.hello:NA:1 [UUID(119e11a5)]: Could not copy a suitable cache hit for 119e11a5:wf_hello.hello:-1:1. No copy attempts were made.; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.backend-dispatcher-84 ERROR - GcpBatchAsyncBackendJobExecutionActor [UUID(119e11a5)wf_hello.hello:NA:1]: Error attempting to Recover(StandardAsyncJob(projects/broad-dsde-cromwell-dev/locations/us-central1/jobs/job-7ce25791-3731-4a69-97f1-b7b65ac8ff71)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:2114,Recover,Recover,2114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['Recover'],['Recover']
Safety,mwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor` exception when it tries to recover a running job. Stacktrace:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyn,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:1101,recover,recover,1101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Safety,"mwell.yaml#!/Workflows/post_workflows_version_id_abort. When going through swagger, the abort takes a long time and then gives an error: ; Response Code 500; ""status"": ""error"",; ""message"": ""The server was not able to produce a timely response to your request."". The workflow is removed from WORKFLOW_STORE_ENTRY but the associated jobs are still present in JOB_STORE_ENTRY. . There are no errors in the logs: ; `; 2016-12-12 18:22:26,139 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(8a965a5e)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:29:42,727 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(73be7f27)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Ab",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1108,Abort,Abort,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"n ReadNoProxy\n request, timeout=timeout_property).read()\n File \""/usr/lib/python2.7/urllib2.py\"", line 401, in open\n response = self._open(req, data)\n File \""/usr/lib/py; thon2.7/urllib2.py\"", line 419, in _open\n '_open', req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 379, in _call_chain\n result = func(*args)\n File \""/usr/lib/python2.7/urllib; 2.py\"", line 1211, in http_open\n return self.do_open(httplib.HTTPConnection, req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1184, in do_open\n r = h.getresponse(buffering=True); \n File \""/usr/lib/python2.7/httplib.py\"", line 1072, in getresponse\n response.begin()\n File \""/usr/lib/python2.7/httplib.py\"", line 408, in begin\n version, status, reason = self; ._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data; = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n); gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam -> /mnt/loc; al-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam (cp failed: gsutil -q -m cp gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga; /STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam,; command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/s; hare/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205, in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:5877,timeout,timeout,5877,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['timeout'],['timeout']
Safety,"n this issue. In all cases, if Cromwell fails to retrieve the docker hash for a task, for any reason, the corresponding call(s) will NOT be eligible for call caching, neither read nor write, regardless of the call caching configuration in effect. **When to get the hashes and what to do with them:**. 1. Cromwell will lookup the hashes corresponding to docker tags, for all docker attributes in all tasks in a workflow and its subworkflows, at Materialization time.; If the runtime attribute value can't be determined, the task in question will be ineligible for call caching. The only case when that should be true is if the attribute is an expression with variables depending on previous tasks being run. 2. If the hash lookup succeed, Cromwell will use that hash to perform any call cache read / write according to the call caching configuration in effect. It will also provide that hash, along with the original floating tag, to the backend when the job gets dispatched. 3. Backends will choose wether to use the hash or the floating tag. They will report to the engine which one they used, so that the engine can send this information to the metadata. **How to get the hash:**. 1. How to get the hash depends on the backend. Which means, at this time, that only workflows for which the backend is known statically at workflow submission time will be supported. 2. If the task is expected to run on the **Local Backend**, Cromwell will attempt to find the hash corresponding to the tag on the machine where it's being run. This first attempt must be done without executing a `pull` to avoid overriding the current local image, if it exits, with the remote repository version.; If the image is not present locally, cromwell will attempt to `pull` the image locally, and use the hash from the newly retrieved image. 3. If the task is expected to run on a **Non Local Backend**, cromwell will attempt to retrieve the image from a remote repository. The `DockerHashActor` can be used for that effect.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2048:1927,avoid,avoid,1927,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048,1,['avoid'],['avoid']
Safety,"n trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3876:3257,detect,detect-adapter,3257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876,1,['detect'],['detect-adapter']
Safety,n(ProcessRunner.scala:20); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$isAlive$1(SharedFileSystemAsyncJobExecutionActor.scala:196); 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive$(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecove,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:1618,recover,recover,1618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recover']
Safety,"ne 408, in begin\n version, status, reason = self._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n)""; at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$.StandardException(JesAsyncBackendJobExecutionActor.scala:63); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:411); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:67); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$1.applyOrElse(StandardAsyncExecutionActor.scala:666); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$1.applyOrElse(StandardAsyncExecutionActor.scala:663); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:16319,recover,recoverWith,16319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['recover'],['recoverWith']
Safety,ne.wrap(SSLEngine.java:564); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.wrap(SslHandler.java:1041); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.wrapNonAppData(SslHandler.java:927); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1409); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.unwrapNonAppData(SslHandler.java:1327); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.access$1800(SslHandler.java:169); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner.resumeOnEventExecutor(SslHandler.java:1718); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner.access$2000(SslHandler.java:1609); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner$2.run(SslHandler.java:1770); at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174); at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470); at io.grpc.netty.shaded.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:403); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997); at io.grpc.netty.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74); at io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30); ... 1 common frames omitted; Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target; at java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:388); at java.base/sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:271); at java.base/sun.securit,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7551:6232,safe,safeExecute,6232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7551,1,['safe'],['safeExecute']
Safety,"need, but I decided to err on providing more info over less. The dashboard view has panels of grouped information, categorized by labels on the jobs. Imagine a user had an owner label and a project label on all of their jobs. The dashboard panels would be pivoted by project and owner, and show probably the first ~5-10 labels that have the most running jobs with that label. These panels would be populated with a header that is the key of the cromwell label, a list of values that match that key that the users have access to, and then a summary of their statuses. . The dashboard will be filterable by other labels, but maybe not at first. A use case example there is filtering the image above by a label `key:value` of `flag:archived`. There is a concept of flagging jobs as archived so you don't see them anymore, as a way to get your failures list down to ""inbox 0"" and say ""I've addressed those jobs, I don't want to see them anymore"". So it's possible a user could want to filter those jobs out of their dashboard view as well. v1 will not have this chart pictured and will not have the left panel of server information. ### Ticket Prioritization Suggestions; 1. I would like to start with a spike/design doc and scoping out the amount of effort it would take to support this in Cromwell before end of Q1. ; 2. This ticket can also represent the implementation if Cromwell wants, which we need by end of May to be able to do the front end work before end of Q2. . ### Current Status; Currently, I think this view would require many pings to the cromwell query endpoint with different queries to get back all of the numbers and results. . ### Risks I know of; I don't think this is blocked by the labels endpoint update in #3233, but wanted to mention it in case it is a risk. ### ACs; - The Job Manager Dashboard can be quickly filled in; - The user can choose which labels they want to get this summary information on (i.e. it's not only a fixed set of labels that are supported by Cromwell)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3348:2136,Risk,Risks,2136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3348,2,"['Risk', 'risk']","['Risks', 'risk']"
Safety,netty.request.NettyRequestSender.sendRequest(NettyRequestSender.java:111); at org.asynchttpclient.DefaultAsyncHttpClient.execute(DefaultAsyncHttpClient.java:240); at org.asynchttpclient.DefaultAsyncHttpClient.executeRequest(DefaultAsyncHttpClient.java:209); at org.asynchttpclient.BoundRequestBuilder.execute(BoundRequestBuilder.java:35); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1(AsyncHttpClientBackend.scala:53); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1$adapted(AsyncHttpClientBackend.scala:42); at cats.effect.IO$.$anonfun$async$1(IO.scala:1042); at cats.effect.IO$.$anonfun$async$1$adapted(IO.scala:1040); at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:329); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:118); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); at cats.effect.IO.unsafeRunAsync(IO.scala:269); at cats.effect.IO.unsafeToFuture(IO.scala:341); at cromwell.languages.util.ImportResolver$.$anonfun$httpResolverWithHeaders$1(ImportResolver.scala:92); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$2(package.scala:25); at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); at scala.collection.immutable.List.foldLeft(List.scala:86); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$1(package.scala:22); at cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.wdl$draft3$transforms$wdlom2wom$FileElementToWomBundle$$importWomBundle(FileElementToWomBundle.scala:101); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$11(FileElementToWomBundle.scala:74); at cats.instances.VectorInstances$$anon$1.$anonfun$traverse$2(vector.scala:77); at cats.instances.VectorInstances$$anon$1.loop$2(vector.s,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3977:2702,unsafe,unsafeToFuture,2702,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977,1,['unsafe'],['unsafeToFuture']
Safety,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:7227,Timeout,Timeout,7227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,3,['Timeout'],['Timeout']
Safety,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkflowStoreActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4062:5869,Timeout,Timeout,5869,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062,3,['Timeout'],['Timeout']
Safety,"ng the singularity method of task execution; ```; cromwell-system-akka.dispatchers.engine-dispatcher-27 WARN - BackendPreparationActor_for_bcfd9d26:UnmappedBamToAlignedBam.SamToFastqAndBwaMemAndMba:14:1 [UUID(bcfd9d26)]: Docker lookup failed; cala:35); ```. How do I set it up to enable caching calls?. ------------------------------------------------------------------------------------------; running file; ```; java -jar -Ddocker.hash-lookup.method=local -Ddocker.hash-lookup.enabled=true -Dwebservice.port=8088 -Dwebservice.interface=0.0.0.0 -Dconfig.file=/work/share/ac7m4df1o5/bin/cromwell/3_config/cromwellslurmsingularitynew.conf ./cromwell-84.jar server. ```; config ; ```; # This line is required. It pulls in default overrides from the embedded cromwell; # `reference.conf` (in core/src/main/resources) needed for proper performance of cromwell.; include required(classpath(""application"")). # Cromwell HTTP server settings; webservice {; #port = 8000; #interface = 0.0.0.0; #binding-timeout = 5s; #instance.name = ""reference""; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }. # Cromwell ""system"" settings; system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; #abort-jobs-on-terminate = false. # this tells Cromwell to retry the task with Nx memory when it sees either OutOfMemoryError or Killed in the stderr file.; memory-retry-error-keys = [""OutOfMemory"", ""Out Of Memory"",""Out of memory""]; # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; #graceful-server-shutdown = true. # Cromwell will cap the number of running workflows at N; #max-concurrent-workflows = 5000. # Cromwell will launch up to N submitted workflows at a time",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:1134,timeout,timeout,1134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['timeout'],['timeout']
Safety,ngine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; Caused by: java.io.IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoin,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4542:1652,recover,recoverWith,1652,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542,1,['recover'],['recoverWith']
Safety,no more ??? on abort. Closes #1109. Closes #1110. Closes #1111. Closes #1112,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1122:15,abort,abort,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1122,1,['abort'],['abort']
Safety,"ns since in this case there is no need for a ""?"" in the ```y``` nor the ```x``` or the invokation of ```select_first```; however I have to say that I don't see why this ""coversion"" would be invalid but I'm not much of a wdl or scala expert. Now the for-sure issue here is that instead of failing indicating what is going on the workflow was still running and the offending task(s) were reported as ""Starting"" in the metadata and the timing and they stayed that way forever. . In order to find out what was going on I needed to install and run a locally v36 server (I usually use dsde-method's community cromwell servers). The logs show first the causing wdl bug like so:. ```; [ERROR] [03/19/2019 09:52:14.444] [cromwell-system-akka.dispatchers.engine-dispatcher-47] [akka://cromwell-system ... Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomAnyType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([""gs:// .... 70.tsv.gz""])), []); ```; Notice that Skipped most of the message text showing (what I think are) the important bits . . This meesage is follow for java exception directly dump into the log output file. java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(W....z""])), []); at wom.values.WomArray$.apply(WomArray.scala:34); at wom.values.WomArray$.apply(WomArray.scala:38); at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:108); at cats.data.Validated.map(Validated.scala:194); ... After this exception there is a log [ERROR] entry appears reporting the exception and exception stack trace. The timing diagram show the tasks hanging in the ""Starting"" state forever and the metadata does not report anything apart than these tasks are ""Starting"". So the error is silenced and the only recurse left is to abort. A re-submit of the workflow would just get stuck in the same place.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4755:2473,abort,abort,2473,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755,1,['abort'],['abort']
Safety,"nt or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; # Google project which will be billed for the requests; project = ""xxxxx-xxxxx-xxxxx"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:4617,Timeout,Timeout,4617,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['Timeout'],['Timeout']
Safety,"nt or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""service-account""; # Google project which will be billed for the requests; project = ""***-***"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; dupli",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:14263,Timeout,Timeout,14263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Timeout'],['Timeout']
Safety,"o SUCCEEDED for job projects/392615380452/locations/us-south1/jobs/job-ba81bad8-82e9-4d95-8fc0-04dfbbd746da.; taskExecution.exitCode=0; ```. What we define as execution events:. ```; ExecutionEvent(Job state is set from QUEUED to SCHEDULED for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:10:01.704137839Z,None); ExecutionEvent(Job state is set from SCHEDULED to RUNNING for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:11:30.631264449Z,None); ExecutionEvent(Job state is set from RUNNING to SUCCEEDED for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:12:16.898798407Z,None); ```. </details>; </details>. ## Load test results. We have executed many load tests, this is the latest one involving 14k jobs. Data / Backend | Batch with Mysql | PAPIv2 with Mysql; ------------- | -------------|---------; Jobs | 14400 | 14400; Execution time | 20936 seconds | 24451 seconds. Overall, all our tests indicate that Batch finishes executing the jobs faster than PAPIv2. <details>; <summary>Load tests settings</summary>. We have ran Cromwell in server mode with the following settings:. - request-timeout: 10m; - idle-timeout: 10m; - job-rate-control: jobs = 20, per = 10 seconds; - max-workflow-launch-count: 50; - new-workflow-poll-rate: 1; - database: MySQL; - virtual-private-cloud setup; - maximum-polling-interval: 600s; - localization-attempts: 3; - google.auth: service account; - request-workers: 3; - concurrent-job-limit: 14400. JVM Options:; - `-Xms512m -Xmx64g`. **NOTE**: Initially we found a bottleneck on Batch but Google enabled an experimental settings to schedule many jobs concurrently which reduced the total execution time. Server capacity (from Google Cloud):; - VM Machine Type: n2-standard-16; - Virtual CPUs: 16; - Memory: 64G; - Architecture: x86/64; - CPU Platform: Intel Cascade Lake. </details>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412:5834,timeout,timeout,5834,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412,2,['timeout'],['timeout']
Safety,"ocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:5596,abort,abort,5596,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"oinTask.java); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(Redefined); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java). ""cromwell-system-scheduler-1"" #14 prio=5 os_prio=31 tid=0x00007fb76aa14800 nid=0x6103 runnable [0x00000001295b3000]; java.lang.Thread.State: RUNNABLE; at com.jprofiler.agent.InstrumentationCallee.exitFilteredMethod(Native Method); at com.jprofiler.agent.InstrumentationCallee.__ejt_filter_exitMethod(ejt:86); at akka.actor.LightArrayRevolverScheduler.clock(Scheduler.scala:213); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Redefined); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Redefined); at java.lang.Thread.run(Redefined). ""cromwell-system-akka.actor.default-dispatcher-4"" #13 prio=5 os_prio=31 tid=0x00007fb76b38c000 nid=0x5f03 waiting on condition [0x000000012ac3d000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c002f9e0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(Redefined); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(Redefined); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java). ""cromwell-system-scheduler-1"" #10 prio=5 os_prio=31 tid=0x00007fb76b20f000 nid=0x5a07 runnable [0x000000012a0e1000]; java.lang.Thread.State: RUNNABLE; at com.jprofiler.agent.InstrumentationCallee.exitFilteredMethod(Native Method); at com.jprofiler.agent.InstrumentationCallee.__ejt_filter_exitMethod(ejt:86); at akka.dispatch.AbstractNodeQueue$Node.next(AbstractNodeQueue.java:124); at akka.dispatch.AbstractNodeQueue.pollNode(AbstractNodeQueue.java:86); at akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:411); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Redefined); at akka.actor.LightArrayRevolverScheduler$$anon$8",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:47349,Unsafe,Unsafe,47349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"oint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Localization; timeout: 300s; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Running\ user\ action:\; docker\ run\ -v\ /mnt/local-disk:/cromwell_root\ --entrypoint\=/bin/bash\; ubuntu@sha256:1e48201ccc2ab83afc435394b3bf70af0fa0055215c1e26a5da9b50a1ae367c9\; /cromwell_root/script; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: UserAction; timeout: 300s; - commands:; - /cromwell_root/script; entrypoint: /bin/bash; imageUri: ubuntu@sha256:1e48201ccc2ab83afc435394b3bf70af0fa0055215c1e26a5da9b50a1ae367c9; labels:; tag: UserAction; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Starting\ delocalization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Delocalization; timeout: 300s; - commands:; - -c; - /bin/bash /cromwell_root/gcs_delocalization.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Delocalization; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Done\ delocalization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Delocalization; timeout: 300s; - alwaysRun: true; commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/1xxxxxx.sh && chmod u+x /tmp/1xxxxxx.sh; && sh /tmp/1xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Delocalization; - alwaysRun: true; commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:19886,timeout,timeout,19886,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['timeout'],['timeout']
Safety,"old root which was /g/cromwell/cromwell-executions. . Note I'm running cromwell in server mode with mariadb. I've cleaned and deleted all tables from mariadb. restarted the server as well. Can't find any other config/cache file where it has saved old address. Sometime workflows are fine pointing to new root but sometime not. <!-- Which backend are you running? -->; SLURM on cromwell 36. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. backend {; # Override the default backend.; default = ""PhoenixSLURM"". # The list of providers.; providers {. PhoenixSLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String userid; String partitions; String memory_per_node; Int nodes; Int cores; String time; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). exit-code-timeout-seconds = 600. submit = """"""; chmod 770 -R ${cwd}; sudo change-files.sh ${userid} ${cwd}; phoenix_home_cwd=""/home/${userid}""; phoenix_home_out=""/home/${userid}/stdout""; phoenix_home_err=""/home/${userid}/stderr"". phoenix_script=${script}_phonix; cat ${script} | sed -s ""s@#\!/bin/bash@#\!/bin/bash\nsource '/etc/profile' @g"" > $phoenix_script. sbatch --uid=${userid} --gid=${userid} \; -J ${job_name} \; -p ${partitions} \; -N ${nodes} \; -n ${cores} \; --mem=${memory_per_node} \; --time=${time} \; -D $phoenix_home_cwd \; -o $phoenix_home_out \; -e $phoenix_home_err \; $phoenix_script; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*"". root = ""/fast/gdr/uat/cromwell-executions""; }; }. } # prov",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4404:1688,timeout,timeout-seconds,1688,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4404,2,['timeout'],"['timeout', 'timeout-seconds']"
Safety,"om.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.Ab",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:5385,abort,abort,5385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,on.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804:2394,recover,recoverWith,2394,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804,1,['recover'],['recoverWith']
Safety,on.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:803); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:815); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:812); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:95); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1340); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1336); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:14668,recover,recoverWith,14668,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['recover'],['recoverWith']
Safety,"on.WorkflowExecutionActor.akka$actor$Timers$$super$aroundReceive(WorkflowExecutionActor.scala:57); 	at akka.actor.Timers.aroundReceive(Timers.scala:51); 	at akka.actor.Timers.aroundReceive$(Timers.scala:40); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:57); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); 	at akka.actor.ActorCell.invoke(ActorCell.scala:583); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2024-03-12 18:49:15 cromwell-system-akka.actor.default-dispatcher-3 INFO - Message [cromwell.engine.workflow.lifecycle.EngineLifecycleActorAbortCommand$] from Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-262d278a-cc62-4458-9150-f31976c2c554#401797350] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-262d278a-cc62-4458-9150-f31976c2c554/WorkflowExecutionActor-262d278a-cc62-4458-9150-f31976c2c554#-742739735] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-262d278a-cc62-4458-9150-f31976c2c554/WorkflowExecutionActor-262d278a-cc62-4458-9150-f31976c2c554#-742739735]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; ```; {; 	""status"": ""Aborting"",; 	""id"": ""262d278a-cc62-4458-9150-f31976c2c554""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7385:7522,Abort,Aborting,7522,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385,1,['Abort'],['Aborting']
Safety,onActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:1730,recover,recoverAsync,1730,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,2,['recover'],['recoverAsync']
Safety,"oncurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoog",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:5523,abort,abort,5523,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,oogle.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:46); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:62); 	at cromwell.backend.async.AsyncBacke,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:3340,recover,recoverAsync,3340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,2,['recover'],['recoverAsync']
Safety,"or.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); 	at akka.actor.ActorCell.invoke(ActorCell.scala:583); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2023-11-07 14:51:17,39] [info] Message [cromwell.engine.workflow.lifecycle.EngineLifecycleActorAbortCommand$] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-4e522458-e360-45e8-be15-2fc99652d692#-686070856] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-4e522458-e360-45e8-be15-2fc99652d692/WorkflowExecutionActor-4e522458-e360-45e8-be15-2fc99652d692#-1420206102] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-4e522458-e360-45e8-be15-2fc99652d692/WorkflowExecutionActor-4e522458-e360-45e8-be15-2fc99652d692#-1420206102]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; In fact, the program becomes unresponsive to even a Ctrl+C kill command and I have to close the terminal entirely to stop it. . The WDL passes `womtool validate` (version 84) and was run using Cromwell version 84. . When run in Terra, the workflow just immediate goes into an aborting state without any helpful error message. It would be great to incorporate this type of support for `None` inside struct fields.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7249:7121,abort,aborting,7121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7249,1,['abort'],['aborting']
Safety,"ore/credentials/gce_read.py\"", line 50, in ReadNoProxy\n request, timeout=timeout_property).read()\n File \""/usr/lib/python2.7/urllib2.py\"", line 401, in open\n response = self._open(req, data)\n File \""/usr/lib/python2.7/urllib2.py\"", line 419, in _open\n '_open', req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 379, in _call_chain\n result = func(*args)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1211, in http_open\n return self.do_open(httplib.HTTPConnection, req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1184, in do_open\n r = h.getresponse(buffering=True)\n File \""/usr/lib/python2.7/httplib.py\"", line 1072, in getresponse\n response.begin()\n File \""/usr/lib/python2.7/httplib.py\"", line 408, in begin\n version, status, reason = self._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n)""; at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$.StandardException(JesAsyncBackendJobExecutionActor.scala:63); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:411); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:67); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$1.applyOrElse(StandardAsyncExecutionActor.scala:666); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$1.applyOrElse(StandardAsyncExecutionActor.scala:663); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.Batch",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:15591,timeout,timeout,15591,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['timeout'],['timeout']
Safety,orkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:6340,recover,recover,6340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Safety,orkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:6464,recover,recoverAsync,6464,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recoverAsync']
Safety,"orting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2095,Abort,Aborting,2095,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"orting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2581,Abort,Aborting,2581,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"orting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2419,Abort,Aborting,2419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"orting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2257,Abort,Aborting,2257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,ot copy a suitable cache hit for 119e11a5:wf_hello.hello:-1:1. No copy attempts were made.; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.backend-dispatcher-84 ERROR - GcpBatchAsyncBackendJobExecutionActor [UUID(119e11a5)wf_hello.hello:NA:1]: Error attempting to Recover(StandardAsyncJob(projects/broad-dsde-cromwell-dev/locations/us-central1/jobs/job-7ce25791-3731-4a69-97f1-b7b65ac8ff71)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(Stand,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:2826,recover,recover,2826,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recover']
Safety,"ould not read from /home/devarea/karl/PathoCromwell/cromwell-executions/Agilent_Exome_Single/3f4aa8de-d1a9-419a-b9b4-10f9ed0a9d53/call-SomaticVariants/SomaticVariantcalling/0c6cefa3-4c84-497f-8003-b0d7221bedbc/call-mutect2/Mutect2/fb66e417-4c06-4f15-8607-da8261e16448/call-scatterList/execution/stdout: /home/devarea/karl/PathoCromwell/cromwell-executions/Agilent_Exome_Single/3f4aa8de-d1a9-419a-b9b4-10f9ed0a9d53/call-SomaticVariants/SomaticVariantcalling/0c6cefa3-4c84-497f-8003-b0d7221bedbc/call-mutect2/Mutect2/fb66e417-4c06-4f15-8607-da8261e16448/call-scatterList/execution/stdout; cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:973); cromwell_1 | at scala.util.Success.$anonfun$map$1(Try.scala:255) cromwell_1 | at scala.util.Success.map(Try.scala:213); cromwell_1 | at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ; cromwell_1 | at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) cromwell_1 | at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) cromwell_1 | at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) cromwell_1 | at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92) ; `. Basically cromwell fails to read a stdout file written by a job.; When we check the file it exists and contains data so i suspect some kind of IO problem. All Data is on NFS shares which are usually quite stable and we see no errors in any of the filesystem/nfs backends. It seems to mostly happen in this scatter step, so i suspect some race condition or timeout somewhere in there, however, this job was creating a scatter to 12 bed files, so its really not that big . Just re-running the job usually works, but its extremely annoying. Call caching is disabled:; `call-caching {; enabled = false; }`. Any idea what to do ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094:2693,timeout,timeout,2693,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094,1,['timeout'],['timeout']
Safety,"patchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Job bccmdfdd6o377kru9q6g is complete; 2018-06-07 13:13:04,883 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Status change from Running to Complete; 2018-06-07 13:13:06,346 cromwell-system-akka.dispatchers.engine-dispatcher-59 ERROR - WorkflowManagerActor Workflow af282f7a-1e95-4390-8cf7-c3bbd93b10b2 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: /Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3743:6967,recover,recoverWith,6967,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743,1,['recover'],['recoverWith']
Safety,"patchers.engine-dispatcher-38 INFO - BT-322 0845428a:myworkflow.mytask:-1:1 is not eligible for call caching; ```; <!-- Which backend are you running? -->; Used backend: ; GCPBATCH. Callcaching works with PAPIv2, not on GCPBATCH.; <!-- Paste/Attach your workflow if possible: -->; workflow used for testing:; ```; workflow myworkflow {; call mytask; }. task mytask {; String str = ""!""; command <<<; echo ""hello world ${str}""; >>>; output {; String out = read_string(stdout()); }. runtime{; docker: ""eu.gcr.io/project/image_name:tag""; cpu: ""1""; memory: ""500 MB""; disks: ""local-disk 5 HDD""; zones: ""europe-west1-b europe-west1-c europe-west1-d""; preemptible: 2; noAddress: true; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; We are using cromwell through broadinstitute/cromwell:87-ecd44b6 image.; cromwell configuration:; ```; include required(classpath(""application"")). system.new-workflow-poll-rate=1. // increase timeout for http requests..... getting meta-data can timeout for large workflows.; akka.http.server.request-timeout=600s. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; system {; 	job-rate-control {; 	 jobs = 100; 	 per = 1 second; 	}; input-read-limits {; lines = 128000000; bool = 7; int = 19; float = 50; string = 1280000; json = 12800000; tsv = 1280000000; map = 128000000; object = 128000000; }. # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; 	graceful-server-shutdown = true; max-concurrent-workflows = 5000. io {; throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the G",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:7373,timeout,timeout,7373,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['timeout'],['timeout']
Safety,"pertyCheckFailedException:; 14:08:29 cromwell-test_1 | [info] ...; 14:08:29 cromwell-test_1 | [info] at cromwell.backend.sfs.SharedFileSystemJobExecutionActorSpec.localizationSpec(SharedFileSystemJobExecutionActorSpec.scala:119); 14:08:29 cromwell-test_1 | [info] at cromwell.backend.sfs.SharedFileSystemJobExecutionActorSpec.$anonfun$new$4(SharedFileSystemJobExecutionActorSpec.scala:156); 14:08:29 cromwell-test_1 | [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); 14:08:29 cromwell-test_1 | [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); 14:08:29 cromwell-test_1 | [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); 14:08:29 cromwell-test_1 | [info] at org.scalatest.Transformer.apply(Transformer.scala:22); 14:08:29 cromwell-test_1 | [info] at org.scalatest.Transformer.apply(Transformer.scala:20); 14:08:29 cromwell-test_1 | [info] ...; 14:08:29 cromwell-test_1 | [info] Cause: org.scalatest.concurrent.Futures$FutureConcept$$anon$1: A timeout occurred waiting for a future to complete. Queried 21 times, sleeping 500 milliseconds between each query.; 14:08:29 cromwell-test_1 | [info] ...; 14:08:29 cromwell-test_1 | [info] at cromwell.backend.sfs.SharedFileSystemJobExecutionActorSpec.$anonfun$localizationSpec$1(SharedFileSystemJobExecutionActorSpec.scala:137); 14:08:29 cromwell-test_1 | [info] at cromwell.backend.sfs.SharedFileSystemJobExecutionActorSpec.$anonfun$localizationSpec$1$adapted(SharedFileSystemJobExecutionActorSpec.scala:119); 14:08:30 cromwell-test_1 | [info] at org.scalatest.enablers.UnitTableAsserting$TableAssertingImpl.$anonfun$forAll$7(TableAsserting.scala:505); 14:08:30 cromwell-test_1 | [info] at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:789); 14:08:30 cromwell-test_1 | [info] at scala.collection.immutable.List.foreach(List.scala:389); 14:08:30 cromwell-test_1 | [info] at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:788); 14:08:30 cromwel",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4319:2212,timeout,timeout,2212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4319,1,['timeout'],['timeout']
Safety,"ponse to your request."". The workflow is removed from WORKFLOW_STORE_ENTRY but the associated jobs are still present in JOB_STORE_ENTRY. . There are no errors in the logs: ; `; 2016-12-12 18:22:26,139 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(8a965a5e)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:29:42,727 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(73be7f27)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-ak",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1286,Abort,Aborting,1286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"pu: 1; memory: ""3.75 GB""; }; }; workflow test {; call hello. output {; File response = hello.response; }; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```hoco; backend {; default = ""batch""; providers {; batch {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {. # The Project To execute in; project = ""${compute_project}"". # The bucket where outputs will be written to; root = ""gs://${bucket}"". # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # batch-timeout = 7 days. genomics {; auth = ""cromwell-service-account""; location: ""${region}""; compute-service-account = ""${compute_service_account}"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; auth = ""cromwell-service-account"". # For billing; project = ""${billing_project}"". caching {; # When a cache hit is found, the following duplication ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238:2069,timeout,timeout,2069,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238,2,['timeout'],['timeout']
Safety,"ractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleCl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:12382,abort,abort,12382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,radL/bar; cromwell.backend.sfs.SharedFileSystem$$anonfun$localizeInputs$1$$anon$1: Failures during localization:; Could not localize /mnt/lustre/home/conradL/foo/baz -> /mnt/lustre/home/conradL/cromwell-executions/symLinkTest/6cf6c785-dc48-4409-bbd1-6f1411211f42/call-referenceTheLink/inputs/mnt/lustre/home/conradL/foo/baz:; 	/mnt/lustre/home/conradL/bar doesn't exists; 	File not found /mnt/lustre/home/conradL/cromwell-executions/symLinkTest/6cf6c785-dc48-4409-bbd1-6f1411211f42/call-referenceTheLink/inputs/mnt/lustre/home/conradL/foo/baz -> /mnt/lustre/home/conradL/bar; 	File not found /mnt/lustre/home/conradL/bar; 	File not found /mnt/lustre/home/conradL/bar; 	at cromwell.backend.sfs.SharedFileSystem$$anonfun$localizeInputs$1.applyOrElse(SharedFileSystem.scala:200); 	at cromwell.backend.sfs.SharedFileSystem$$anonfun$localizeInputs$1.applyOrElse(SharedFileSystem.scala:199); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.backend.sfs.SharedFileSystem$class.localizeInputs(SharedFileSystem.scala:199); 	at cromwell.backend.sfs.SharedFileSystemJobCachingActorHelper$$anon$1.localizeInputs(SharedFileSystemJobCachingActorHelper.scala:40); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLinePreProcessor$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLinePreProcessor$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at cromwell.backend.wdl.Command$.instantiate(Command.scala:27); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.instantiatedCommand(StandardAsyncExecutionActor.scala:83); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsy,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1950:2386,recover,recoverWith,2386,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1950,1,['recover'],['recoverWith']
Safety,"ration.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:5454,abort,abort,5454,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"rectory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560:2172,timeout,timeout-seconds,2172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560,4,['timeout'],"['timeout', 'timeout-seconds']"
Safety,remove create PAPI requests from queue when aborting,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2752:44,abort,aborting,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2752,1,['abort'],['aborting']
Safety,"rent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""Hikari Housekeeping Timer (pool db)"" #24 daemon prio=5 os_prio=31 tid=0x00007fb76d88f000 nid=0x7503 waiting on condition [0x000000012cebb000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0623fc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""cromwell-system-akka.io.pinned-dispatcher-7"" #23 prio=5 os_prio=31 tid=0x00007fb76b450800 nid=0x7303 runnable [0x000000012cdb8000]; java.lang.Thread.State: RUNNABLE;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:41524,Unsafe,Unsafe,41524,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"ression type: bitpack64; Allocating memory for ref offset pointers, kmer 15, interval 3...Attached existing memory (2 attached) for defuse-data/gmap/cdna/cdna.ref153offsets64meta...done (134,217,744 bytes, 0.00 sec); Allocating memory for ref offsets, kmer 15, interval 3...Attached new memory for defuse-data/gmap/cdna/cdna.ref153offsets64strm...done (234,475,312 bytes, 0.23 sec); Pre-loading ref positions, kmer 15, interval 3......done (276,173,052 bytes, 0.05 sec); Starting alignment; Failed attempt to alloc 18446744073709550532 bytes; Exception: Allocation Failed raised at indexdb.c:2885; /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/log/defuse.12.sh: line 6: 7481 Segmentation fault (core dumped) /usr/local/bin/gmap -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa > /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa.cdna.psl.tmp; ; real 0m1.262s; user 0m0.046s; sys 0m0.564s. ```. Run within the docker container but not using Cromwell, the output of that command looks like this:; ```; Starting defuse command:; /usr/local/bin/gmap -D Program_required_data/deFuse/defuse-data/gmap -d cdna -f psl #<1 > #>1; Reasons:; /mnt/Workflow_runs/2_fusion_detection_tools/BT474/BT474_deFuse_0.8.1/jobs/breakpoints.split.001.fa.cdna.psl m; issing; Success for defuse command:; /usr/local/bin/gmap -D Program_required_data/deFuse/defuse-data/gmap -d est4 -f psl /mnt/Workflow_runs/2_fus; ion_detection_tools/BT474/BT474_deFuse_0.8.1/jobs/breakpoints.split.001.fa > /mnt/Workflow_runs/2_fusion_detection_to; ols/BT474/BT474_deFuse_0.8.1/jobs/breakpoints.split.001.fa.est.4.psl.tmp; Return codes: 0; Job output:; Running on a4980b348957; GMAP version 2018-07-04 called with args: /usr/local/bin/gmap.avx2 -D Program_required_data/deFuse/defuse-dat; a/gmap -",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465:2994,detect,detectFusions,2994,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465,1,['detect'],['detectFusions']
Safety,"rides from the embedded cromwell; # `reference.conf` (in core/src/main/resources) needed for proper performance of cromwell.; include required(classpath(""application"")). # Cromwell HTTP server settings; webservice {; #port = 8000; #interface = 0.0.0.0; #binding-timeout = 5s; #instance.name = ""reference""; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }. # Cromwell ""system"" settings; system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; #abort-jobs-on-terminate = false. # this tells Cromwell to retry the task with Nx memory when it sees either OutOfMemoryError or Killed in the stderr file.; memory-retry-error-keys = [""OutOfMemory"", ""Out Of Memory"",""Out of memory""]; # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; #graceful-server-shutdown = true. # Cromwell will cap the number of running workflows at N; #max-concurrent-workflows = 5000. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; #max-workflow-launch-count = 50. # Number of seconds between workflow launches; #new-workflow-poll-rate = 20. # Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; #number-of-workflow-log-copy-workers = 10. # Default number of cache read workers; #number-of-cache-read-workers = 25. io {; # throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; # #number-of-requests = 100000; # #per = 100 seconds; # }. # Number of times an I/O operation should be attempted before giving up and failing it.; #number-of-attempts = 5; }. # Maximum number of input file bytes allowe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:1909,timeout,timeout,1909,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['timeout'],['timeout']
Safety,"riteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.Abstra",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:16655,abort,abort,16655,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,rkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Ret,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:8194,recover,recoverAsync,8194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recoverAsync']
Safety,"rmance and; # # tuning the value to match your environment.; db-batch-size = 700; }; }; }. google {. application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. docker {; hash-lookup {; method = ""remote""; }; }. engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }. call-caching {; enabled = true; }. backend {; default = GCPBATCH; providers {; GCPBATCH {; // life sciences; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; ## Google project; project = ""$PROJECT"". ## Base bucket for workflow executions; root = ""$BUCKET""; name-for-call-caching-purposes: PAPI; #60000/min in google; ##genomics-api-queries-per-100-seconds = 90000; virtual-private-cloud {; network-name = ""$NET""; subnetwork-name = ""$SUBNET""; }; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; 	 request-workers = 4; batch-timeout = 7 days; 	 # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in PAPI.; 	 slow-job-warning-time: 24 hours; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; compute-service-account = ""default""; # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false; ## Location; location = ""europe-west1"". ; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""$PROJECT""; caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:10306,timeout,timeout,10306,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['timeout'],['timeout']
Safety,"rowse/CROM-6734. <!-- Which backend are you running? -->; AWSBatch. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. ```; include required(classpath(""application"")). backend {; default = ""aws""; providers {; aws {; config {; default-runtime-attributes {; scriptBucketName = ""caper4-04-20-2021""; queueArn = ""arn:aws:batch:us-east-1:618537831167:job-queue/default-caper5""; }; filesystems {; s3 {; caching {; duplication-strategy = ""reference""; }; auth = ""default""; }; }; concurrent-job-limit = 1000; numSubmitAttempts = 6; numCreateDefinitionAttempts = 6; auth = ""default""; root = ""s3://caper4-04-20-2021/out""; }; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; }; }; }. system {; job-rate-control {; jobs = 1; per = ""2 seconds""; }; abort-jobs-on-terminate = true; graceful-server-shutdown = true; max-concurrent-workflows = 40; }; call-caching {; invalidate-bad-cache-results = true; enabled = true; }; database {; db {; connectionTimeout = 30000; numThreads = 1; url = ""jdbc:hsqldb:file:/opt/caper/default_file_db;shutdown=false;hsqldb.tx=mvcc;hsqldb.lob_compressed=true;hsqldb.default_table_type=cached;hsqldb.result_max_memory_rows=10000;hsqldb.large_data=true;hsqldb.applog=1;hsqldb.script_format=3""; }; }; aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""us-east-1""; }; engine {; filesystems {; s3 {; auth = ""default""; }; }; }; ```. Even with the `reference` strategy, Cromwell still make a `cacheCopy` of previous outputs. Here is the `metadata.json` from a pipeline run with `reference` strategy. Cromwell still makes a `cacheCopy` directory.; ```; {; ""calls"": {; ""atac.bam2ta"": [; {; ""executionStatus"": ""Done"",; ""stdout"": ""s3://caper4-04-20-2021/out/atac/b59c0d05-5210-4341-b4f0-dcbf5b9e74c1/call-bam2ta/shard-0/bam2ta-0-stdout.log"",; ""compressedDockerSize"": 963995760,; ""shardIndex"": 0,;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6327:1941,abort,abort-jobs-on-terminate,1941,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6327,1,['abort'],['abort-jobs-on-terminate']
Safety,"rting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1934,Abort,Aborting,1934,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"rting 11 EJEAs; 2016-12-12 18:29:42,727 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(73be7f27)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1610,Abort,Aborting,1610,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"rting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1772,Abort,Aborting,1772,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,"s the error more precisely in `handleGoogleError`. I tested this by deliberately breaking the request so it always gets a `400` back. With the existing code, the log looks just like what we see in prod:. ```; 2023-11-01 19:54:12 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - PAPI request worker had 2 failures making 5 requests: ; 400 Bad Request; POST https://lifesciences.googleapis.com/v2beta/projects/1005074806481/locations/us-central1/operations/6175597626605185257:cancel; {; ""code"": 400,; ""errors"": [; {; ""domain"": ""global"",; ""message"": ""Invalid JSON payload received. Unexpected token.\nasdf\n^ Payload appears to be compressed. It may either be corrupt or uncompressed data may be too large for the server to handle."",; ""reason"": ""parseError""; }; ],; ""message"": ""Invalid JSON payload received. Unexpected token.\nasdf\n^ Payload appears to be compressed. It may either be corrupt or uncompressed data may be too large for the server to handle."",; ""status"": ""INVALID_ARGUMENT""; }; ```. <img width=""1238"" alt=""Screenshot 2023-11-01 at 15 43 59"" src=""https://github.com/broadinstitute/cromwell/assets/1087943/63e4e788-517f-4f1e-a4ff-4075cec3e6d3"">. ---. Changing `throwExceptionOnExecuteError` to `false`, we see that we no longer throw an exception and we get the expected ""no longer running"" message in the log!. ```; 2023-11-01 19:57:48 cromwell-system-akka.dispatchers.backend-dispatcher-162 INFO - PAPI declined to abort job projects/1005074806481/locations/us-central1/operations/5250112889402522122 in workflow b70eafc9-66a7-4b22-b9bc-621c22b5a4ed, most likely because it is no longer running. Marking as finished. Message: Invalid JSON payload received. Unexpected token.; asdf; ^ Payload appears to be compressed. It may either be corrupt or uncompressed data may be too large for the server to handle.; ```. <img width=""1239"" alt=""Screenshot 2023-11-01 at 15 45 48"" src=""https://github.com/broadinstitute/cromwell/assets/1087943/5ce424c4-c3e0-4ffc-b078-f46e065da586"">",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7245:1558,abort,abort,1558,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7245,1,['abort'],['abort']
Safety,"s were finished which Cromwell didn't detect. The context: ; - Trying to run jprofiler to get a profile of the run described in #820. Full stack dump:. ```; Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode):. ""cromwell-system-akka.actor.default-dispatcher-27"" #115 prio=5 os_prio=31 tid=0x00007fb76d052800 nid=0xf503 waiting on condition [0x0000000135d74000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0021d00> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""ForkJoinPool-3-worker-5"" #82 daemon prio=5 os_prio=31 tid=0x00007fb76cc73800 nid=0xcd03 waiting on condition [0x0000000134661000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0041f30> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""pool-1-thread-20"" #81 prio=5 os_prio=31 tid=0x00007fb76cc5b800 nid=0xcb03 waiting on condition [0x000000013455e000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:1011,Unsafe,Unsafe,1011,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"s://genomics.googleapis.com/"",; ""googleProject"": ""broad-dsde-alpha""; },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""ubuntu:latest"",; ""maxRetries"": ""0"",; ""cpu"": ""1"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zones"": ""us-central1-b"",; ""memoryMin"": ""2.048 GB"",; ""memory"": ""2.048 GB""; },; ""callCaching"": {; ""allowResultReuse"": false,; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ""inputs"": {; ""addressee"": ""World""; },; ""backendLabels"": {; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663"",; ""wdl-task-name"": ""hello""; },; ""labels"": {; ""wdl-task-name"": ""hello"",; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663""; },; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""message"": ""java.lang.IllegalArgumentException: Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""backend"": ""JES"",; ""end"": ""2018-12-11T16:07:04.207Z"",; ""stderr"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello-stderr.log"",; ""callRoot"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello"",; ""attempt"": 1,; ""executionEvents"": [; {; ""startTime"": ""2018-12-11T16:07:02.746Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2018-12-11T16:07:03.606Z""; },; {; ""startTime"": ""2018-12-11T16:07:03.648Z"",; ""description"": ""RunningJob"",; ""endTime"": ""2018-12-11T16:07:04.116Z""; },; {; ""startTime"": ""2018-12-11T16:07:04.116Z"",; ""description"": ""UpdatingJobStore"",; ""endTime"": ""2018-12-11T16:07:04.207Z""; },; {; ""startTime"": ""2018-12-11T16:07:02.746Z"",; ""description"": ""Pending"",; ""endTime"": ""2018-12-11T16:07:02.746Z""; },; {; ""startTime"": ""2018-12-11T16:07:03.606Z"",; ""description"": ""WaitingForValueStore"",; ""endTime"": ""2018-12-11T16:07:03.607Z""; },; {; ""startTime"": ""201",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4484:1706,Abort,AbortedExecutionHandle,1706,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4484,1,['Abort'],['AbortedExecutionHandle']
Safety,"sage"": ""Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""message"": ""java.lang.IllegalArgumentException: Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""backend"": ""JES"",; ""end"": ""2018-12-11T16:07:04.207Z"",; ""stderr"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello-stderr.log"",; ""callRoot"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello"",; ""attempt"": 1,; ""executionEvents"": [; {; ""startTime"": ""2018-12-11T16:07:02.746Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2018-12-11T16:07:03.606Z""; },; {; ""startTime"": ""2018-12-11T16:07:03.648Z"",; ""description"": ""RunningJob"",; ""endTime"": ""2018-12-11T16:07:04.116Z""; },; {; ""startTime"": ""2018-12-11T16:07:04.116Z"",; ""description"": ""UpdatingJobStore"",; ""endTime"": ""2018-12-11T16:07:04.207Z""; },; {; ""startTime"": ""2018-12-11T16:07:02.746Z"",; ""description"": ""Pending"",; ""endTime"": ""2018-12-11T16:07:02.746Z""; },; {; ""startTime"": ""2018-12-11T16:07:03.606Z"",; ""description"": ""WaitingForValueStore"",; ""endTime"": ""2018-12-11T16:07:03.607Z""; },; {; ""startTime"": ""2018-12-11T16:07:03.607Z"",; ""description"": ""PreparingJob"",; ""endTime"": ""2018-12-11T16:07:03.648Z""; }; ],; ""backendLogs"": {; ""log"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello.log""; },; ""start"": ""2018-12-11T16:07:02.746Z""; }; ]; },; ""outputs"": {},; ""workflowRoot"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/"",; ""actualWorkflowLanguage"": ""WDL"",; ""id"": ""9cc9b141-b2fb-4277-94bd-80ad87a49663"",; ""inputs"": {; ""wf_hello.hello.addressee"": ""World""; },; ""labels"": {; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663"",; ""caas-collection-name"": ""107540717239837386549""; },; ""submission"": ""2018-12-11T16:06:49.563Z"",; ""status"": ""Aborted"",; ""end"": ""2018-12-11T16:07:06.544Z"",; ""start"": ""2018-12-11T16:07:00.970Z""; } ; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4484:3469,Abort,Aborted,3469,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4484,1,['Abort'],['Aborted']
Safety,"scala:135); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2018-09-14 13:20:00,08] [info] WorkflowManagerActor WorkflowActor-caab4283-a3d4-4966-85ba-56d0992c8f00 is in a terminal state: WorkflowFailedState; [2018-09-14 13:20:00,92] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2018-09-14 13:20:05,34] [info] Workflow polling stopped; [2018-09-14 13:20:05,36] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-09-14 13:20:05,36] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-09-14 13:20:05,37] [info] Aborting all running workflows.; [2018-09-14 13:20:05,39] [info] WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103:8304,Timeout,Timeout,8304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103,3,"['Abort', 'Timeout']","['Aborting', 'Timeout']"
Safety,"sdk:276.0.0-slim; labels:; tag: Localization; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - /bin/bash /cromwell_root/gcs_localization.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Localization; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Localization; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Done\ localization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Localization; timeout: 300s; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Running\ user\ action:\; docker\ run\ -v\ /mnt/local-disk:/cromwell_root\ --entrypoint\=/bin/bash\; ubuntu@sha256:1e48201ccc2ab83afc435394b3bf70af0fa0055215c1e26a5da9b50a1ae367c9\; /cromwell_root/script; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: UserAction; timeout: 300s; - commands:; - /cromwell_root/script; entrypoint: /bin/bash; imageUri: ubuntu@sha256:1e48201ccc2ab83afc435394b3bf70af0fa0055215c1e26a5da9b50a1ae367c9; labels:; tag: UserAction; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Starting\ delocalization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Delocalization; timeout: 300s; - commands:; - -c; - /bin/bash /cromwell_root/gcs_delocalization.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Delocalization; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - pr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:19017,timeout,timeout,19017,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['timeout'],['timeout']
Safety,"sh ${script}"". # We're asking bash-within-singularity to run the script, but the script's location on the machine; # is different then the location its mounted to in the container, so need to change the path with sed; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} bash \; ""$(echo ${script} | sed -e 's@.*cromwell-executions@/cromwell-executions@')""; """"""; filesystems {; local {; localization: [""hard-link""]; caching {; duplication-strategy: [""hard-link""]; hasing-strategy: ""fingerprint""; check-sibling-md5: true; fingerprint-size: 1048576 # 1 MB ; }; }; }; }; }; # For running jobs by submitting them from an interactive node to the cluster; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; root = ""cromwell-executions""; dockerRoot = ""/cromwell-executions"". runtime-attributes = """"""; Int cpus = 1; String mem = ""2g""; String dx_timeout; String? docker; """"""; check-alive = ""squeue -j ${job_id}""; exit-code-timeout-seconds = 500; job-id-regex = ""Submitted batch job (\\d+).*"". submit = """"""; sbatch \; --partition ind-shared \; --nodes 1 \; --job-name=${job_name} \; -o ${out} -e ${err} \; --ntasks-per-node=${cpus} \; --mem=${mem} \; -c ${cpus} \; --time=$(echo ${dx_timeout} | sed -e 's/ //g' -e 's/\([0-9]\+\)h\([0-9]\+\)m/\1:\2:00/' -e 's/\([0-9]\+\)h/\1:00:00/' -e 's/\([0-9]\+\)m/\1:00/') \; --chdir ${cwd} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}"". # We're asking bash-within-singularity to run the script, but the script's location on the machine; # is different then the location its mounted to in the container, so need to change the path with sed; submit-docker = """"""; sbatch \; --partition ind-shared \; --nodes 1 \; --job-name=${job_name} \; -o ${out} -e ${err} \; --ntasks-per-node=${cpus} \; --mem=${mem} \; -c ${cpus} \; --time=$(echo ${dx_timeout} | sed -e 's/ //g' -e 's/\([0-9]\+\)h\([0-9]\+\)m/\1:\2:00/' -e 's/\([0-9]\+\)h/\",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:5426,timeout,timeout-seconds,5426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['timeout'],['timeout-seconds']
Safety,"sk.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2023-02-04 08:55:08,24] [info] WorkflowManagerActor WorkflowActor-48f62f22-25fe-4f0f-b5fe-21191f035abd is in a terminal state: WorkflowFailedState; [2023-02-04 08:55:08,24] [info] $a [[38;5;2m48f62f22[0m]: Copying workflow logs from /mnt/g/ELM-WES-pipeline/cromwell-workflow-logs/workflow.48f62f22-25fe-4f0f-b5fe-21191f035abd.log to /mnt/g/ELM-WES-pipeline/cromwell_wf_logs/workflow.48f62f22-25fe-4f0f-b5fe-21191f035abd.log; [2023-02-04 08:55:15,88] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2023-02-04 08:55:17,27] [info] Workflow polling stopped; [2023-02-04 08:55:17,29] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2023-02-04 08:55:17,30] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2023-02-04 08:55:17,31] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2023-02-04 08:55:17,31] [info] Aborting all running workflows.; [2023-02-04 08:55:17,31] [info] JobExecutionTokenDispenser stopped; [2023-02-04 08:55:17,31] [info] WorkflowStoreActor stopped; [2023-02-04 08:55:17,32] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2023-02-04 08:55:17,32] [info] WorkflowLogCopyRouter stopped; [2023-02-04 08:55:17,32] [info] WorkflowManagerActor All workflows finished; [2023-02-04 08:55:17,32] [info] WorkflowManagerActor stopped; [2023-02-04 08:55:17,32] [info] Connection pools shut down; [2023-02-04 08:55:17,33] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] SubWorkflowStoreActor stopped; [2023-02-04 08:55:17,33] [info] Shutting down CallCacheWriteActor - Timeo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999:16042,Timeout,Timeout,16042,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999,4,"['Abort', 'Timeout']","['Aborting', 'Timeout']"
Safety,spatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:6117,recover,recover,6117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Safety,spatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockCon,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:2458,unsafe,unsafeToFuture,2458,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['unsafe'],['unsafeToFuture']
Safety,"swagger, the abort takes a long time and then gives an error: ; Response Code 500; ""status"": ""error"",; ""message"": ""The server was not able to produce a timely response to your request."". The workflow is removed from WORKFLOW_STORE_ENTRY but the associated jobs are still present in JOB_STORE_ENTRY. . There are no errors in the logs: ; `; 2016-12-12 18:22:26,139 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(8a965a5e)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:29:42,727 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(73be7f27)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1124,Abort,Aborting,1124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,swap running jobs to aborting in the workflow store if abort-on-termi…,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2819:21,abort,aborting,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2819,2,['abort'],"['abort-on-termi', 'aborting']"
Safety,t #1. Retrying after 596 milliseconds; org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false); at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90); at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); at java.base/java.lang.Thread.run(Thread.java:834); at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144); at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139); at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119); at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119); at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83); at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:172); Caused by: java.net.SocketTimeoutException: An attempt to establish connection with quay.io/50.17.122.58:443 timed out after 10 seconds.; at org.http4s.blaze.channel.nio2.ClientChannelFactory$$anon$1.run(ClientChannelFactory.scala:66); at org.http4s.blaze.util.Execution$$anon$3.execute(Execution.scala:80); at org.http4s.blaze.util.TickWheelExecutor$Node.run(TickWheelExecutor.scala:271); at org.http4s.blaze.util.TickWheelExecutor$Bucket.checkNext$1(TickWheelExecutor.scala:207); at org.http4s.blaze.util.TickWheelExecutor$Bucket.prune(TickWheelExecutor.scala:213); at org.http4s.blaze.util.TickWheelExecutor.go$3(TickWheelExecutor.scala:168); at org.http4s.blaze.util.TickWheelExecutor.org$http4s$blaze$util$TickWheelExecutor$$cycle(TickWheelExecutor.scala:171); at org.http4s.blaze.util.TickWheelExecutor$$anon$1.run(TickWheelExecutor.scala:68). To confirm https (port 443) access to the quay.io/50.17.122.58 in this ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7136:1508,unsafe,unsafeRunSync,1508,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7136,1,['unsafe'],['unsafeRunSync']
Safety,t cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43) ; at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.IOBracket$BracketStart.apply(IOBracket.scala:60); at cats.effect.internals.IOBracket$BracketStart.apply(IOBracket.scala:41); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:134); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); at cats.effect.internals.IOBracket$.$anonfun$apply$1(IOBracket.scala:36); at cats.effect.internals.IOBracket$.$anonfun$apply$1$adapted(IOBracket.scala:33); at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:328); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:117); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); at cats.effect.IO.unsafeRunAsync(IO.scala:258); at cats.effect.IO.unsafeToFuture(IO.scala:345); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeAsync(AwsBatchAsyncBackendJobExecutionActor.scala:342); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:943); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backen,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:6004,unsafe,unsafeToFuture,6004,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['unsafe'],['unsafeToFuture']
Safety,"t has saved old address. Sometime workflows are fine pointing to new root but sometime not. <!-- Which backend are you running? -->; SLURM on cromwell 36. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. backend {; # Override the default backend.; default = ""PhoenixSLURM"". # The list of providers.; providers {. PhoenixSLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String userid; String partitions; String memory_per_node; Int nodes; Int cores; String time; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). exit-code-timeout-seconds = 600. submit = """"""; chmod 770 -R ${cwd}; sudo change-files.sh ${userid} ${cwd}; phoenix_home_cwd=""/home/${userid}""; phoenix_home_out=""/home/${userid}/stdout""; phoenix_home_err=""/home/${userid}/stderr"". phoenix_script=${script}_phonix; cat ${script} | sed -s ""s@#\!/bin/bash@#\!/bin/bash\nsource '/etc/profile' @g"" > $phoenix_script. sbatch --uid=${userid} --gid=${userid} \; -J ${job_name} \; -p ${partitions} \; -N ${nodes} \; -n ${cores} \; --mem=${memory_per_node} \; --time=${time} \; -D $phoenix_home_cwd \; -o $phoenix_home_out \; -e $phoenix_home_err \; $phoenix_script; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*"". root = ""/fast/gdr/uat/cromwell-executions""; }; }. } # providers. } # backend. # https://gatkforums.broadinstitute.org/wdl/discussion/9536/how-do-i-set-up-a-mysql-database-for-cromwell; # http://slick.lightbend.com/doc/3.2.0/api/index.html#slick.jdbc.JdbcBackend$DatabaseFactoryDef@forConfig(Str",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4404:1999,timeout,timeout-seconds,1999,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4404,1,['timeout'],['timeout-seconds']
Safety,"t or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861:1855,recover,recoverWith,1855,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861,1,['recover'],['recoverWith']
Safety,"t$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:527). 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:38:57,90] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-03-09 15:42:11,14] [info] Using noop to send events.; [2018-03-09 15:49:48,68] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:8898,Timeout,TimeoutException,8898,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['Timeout'],['TimeoutException']
Safety,"t.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.Abstra",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:7490,abort,abort,7490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"t.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.Abstrac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:9591,abort,abort,9591,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"t.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.jso",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:11830,abort,abort,11830,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"t.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:14345,abort,abort,14345,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"t.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.j",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:16377,abort,abort,16377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"t.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:18894,abort,abort,18894,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"t.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:21202,abort,abort,21202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,t.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); 	at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); 	at cats.effect.internals.ForwardCancelable.loop$1(ForwardCancelable.scala:46); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1(ForwardCancelable.scala:52); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1$adapted(ForwardCancelable.scala:52); 	at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:337); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:119); 	at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); 	at cats.effect.IO.unsafeRunAsync(IO.scala:258); 	at cats.effect.internals.IORace$.onSuccess$1(IORace.scala:40); 	at cats.effect.internals.IORace$.$anonfun$simple$4(IORace.scala:79); 	at cats.effect.internals.IORace$.$anonfun$simple$4$adapted(IORace.scala:77); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:136); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:372); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:312); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5178:3700,unsafe,unsafeRunAsync,3700,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178,1,['unsafe'],['unsafeRunAsync']
Safety,t.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); 	at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); 	at cats.effect.internals.ForwardCancelable.loop$1(ForwardCancelable.scala:46); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1(ForwardCancelable.scala:52); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1$adapted(ForwardCancelable.scala:52); 	at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:341); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:119); 	at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); 	at cats.effect.IO.unsafeRunAsync(IO.scala:257); 	at cats.effect.internals.IORace$.onSuccess$1(IORace.scala:40); 	at cats.effect.internals.IORace$.$anonfun$simple$4(IORace.scala:79); 	at cats.effect.internals.IORace$.$anonfun$simple$4$adapted(IORace.scala:77); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:136); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:355); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:376); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:316); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5925:3016,unsafe,unsafeRunAsync,3016,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5925,1,['unsafe'],['unsafeRunAsync']
Safety,"tch.ForkJoinExecutorConfigurator$AkkaForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""ForkJoinPool-3-worker-5"" #82 daemon prio=5 os_prio=31 tid=0x00007fb76cc73800 nid=0xcd03 waiting on condition [0x0000000134661000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0041f30> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""pool-1-thread-20"" #81 prio=5 os_prio=31 tid=0x00007fb76cc5b800 nid=0xcb03 waiting on condition [0x000000013455e000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #80 prio=5 os_prio=31 tid=0x00007fb76cc59800 nid=0xc903 waiting on condition [0x0000000134093000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c1476660> (a java.util.concurrent.locks.AbstractQueuedSynchroniz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:1534,Unsafe,Unsafe,1534,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"tcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3696,Abort,Abort,3696,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"tcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:37:14,145 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(60ec6228)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:23,720 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(a442dc1c)]: Abort",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:4342,Abort,Abort,4342,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"tcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:2726,Abort,Abort,2726,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"tcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3049,Abort,Abort,3049,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"tcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3534,Abort,Abort,3534,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"tcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:37:14,145 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(60ec6228)]: Abort",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:4180,Abort,Abort,4180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"tcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3857,Abort,Abort,3857,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"tcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(44318e55)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:34:16,741 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(28d5b663)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:28,999 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(7c8b62e6)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:34:40,026 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(3206b2df)]: Abort received. Aborting 1 EJEAs; 2016-12-12 18:35:03,235 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(63064089)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:13,509 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(812336d3)]: Abort received. Aborting 14 EJEAs; 2016-12-12 18:35:23,568 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(197caaf2)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:35:36,241 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(98068011)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:3372,Abort,Abort,3372,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,te)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.fo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5001:1864,recover,recoverWith,1864,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001,1,['recover'],['recoverWith']
Safety,"teUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:7421,abort,abort,7421,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"teUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:9522,abort,abort,9522,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"teUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:11761,abort,abort,11761,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"teUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:14276,abort,abort,14276,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"teUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:16308,abort,abort,16308,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"teUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractG",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:18825,abort,abort,18825,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"teUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.new",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:21133,abort,abort,21133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"tead, Cromwell seems to think it is defined, and it has a length. This can cause all sorts of issues, such as breaking downstream tasks that are only supposed to run if the optional upstream task has run, and some very odd error messages. Simple example:; ```; # task_a and task_b are mutually exclusive scattered tasks; Array[File?] vcfs = select_first([task_a.vcf_out, task_b.vcf_out]); ```; Due to this bug, vcfs will yield an empty array if task_a did not run, even though task_b did run. This gets quite messy if you need to process the output of mutually exclusive tasks later. More involved example: ; ```; # variant_call_after_earlyQC_filtering is an optional task, so variant_call_after_earlyQC_filtering.errorcode is an optional type; if(defined(variant_call_after_earlyQC_filtering.errorcode)) {. # variant_call_after_earlyQC_filtering is a scattered task, so variant_call_after_earlyQC_filtering.errorcode is an array; # this length check should be redundant with the defined check earlier, but neither of them seem to work properly; if(length(variant_call_after_earlyQC_filtering.errorcode) > 0) {; 	; # get the first (0th) value and coerce it into type String; 	String coerced_vc_filtered_errorcode = select_first([variant_call_after_earlyQC_filtering.errorcode[0], ""FALLBACK""]); 	call echo as echo_a {input: integer=length(variant_call_after_earlyQC_filtering.errorcode), string=variant_call_after_earlyQC_filtering.errorcode[0]}; 	call echo as echo_b {input: string=coerced_vc_filtered_errorcode}; call echo_array as echo_c {input: strings=variant_call_after_earlyQC_filtering.errorcode}; }; }; ```. Output:; * echo_a will echo ""1"" for input _integer_ and an empty string for input _string_; * echo_b will echo ""FALLBACK"" for input _string_; * echo_c will cause an error ; * `""message"":""Cannot interpolate Array[String?] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,Some( ))]""`; * This error occurs even if echo_array takes in non-optional Array[Str",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7201:1049,redund,redundant,1049,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7201,1,['redund'],['redundant']
Safety,"ter is initialized in code, this is the number of workers; #number-of-workflow-log-copy-workers = 10. # Default number of cache read workers; #number-of-cache-read-workers = 25. io {; # throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; # #number-of-requests = 100000; # #per = 100 seconds; # }. # Number of times an I/O operation should be attempted before giving up and failing it.; #number-of-attempts = 5; }. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; #lines = 128000; #bool = 7; #int = 19; #float = 50; #string = 128000; #json = 128000; #tsv = 128000; #map = 128000; #object = 128000; }. abort {; # These are the default values in Cromwell, in most circumstances there should not be a need to change them. # How frequently Cromwell should scan for aborts.; scan-frequency: 30 seconds. # The cache of in-progress aborts. Cromwell will add entries to this cache once a WorkflowActor has been messaged to abort.; # If on the next scan an 'Aborting' status is found for a workflow that has an entry in this cache, Cromwell will not ask; # the associated WorkflowActor to abort again.; cache {; enabled: true; # Guava cache concurrency.; concurrency: 1; # How long entries in the cache should live from the time they are added to the cache.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 100000; }; }. # Cromwell reads this value into the JVM's `networkaddress.cache.ttl` setting to control DNS cache expiration; dns-cache-ttl: 3 minutes; }. docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:3343,abort,aborts,3343,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['abort'],['aborts']
Safety,"testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The rerun uses this:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The second argument changes from the intended path to a path inside of the execution folder. It looks like the output from the preceding mapping job gets linked to in the execution folder after restarting the workflow. Which is used as output for that job(?). This output `File` is used to determine what the name for the output of the indexing call is. Which is now different because it now points at the link in the execution folder, rather than the actual output the mapping job produced. As such the expected output doesn't exist and the job gets rerun. ; Am I correct in these statements? If so, is there a way this can be avoided? (ie. Is there a way the original output path can be remembered between restarts?)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3717:2909,avoid,avoided,2909,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717,1,['avoid'],['avoided']
Safety,text$.withBlockContext(BlockContext.scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43) ; at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.IOBracket$BracketStart.apply(IOBracket.scala:60); at cats.effect.internals.IOBracket$BracketStart.apply(IOBracket.scala:41); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:134); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); at cats.effect.internals.IOBracket$.$anonfun$apply$1(IOBracket.scala:36); at cats.effect.internals.IOBracket$.$anonfun$apply$1$adapted(IOBracket.scala:33); at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:328); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:117); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); at cats.effect.IO.unsafeRunAsync(IO.scala:258); at cats.effect.IO.unsafeToFuture(IO.scala:345); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeAsync(AwsBatchAsyncBackendJobExecutionActor.scala:342); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:943); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBacken,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:5956,unsafe,unsafeRunAsync,5956,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['unsafe'],['unsafeRunAsync']
Safety,"the Cromwell metadata as described by [the paragraph about metadata in the Cromwell docs](https://cromwell.readthedocs.io/en/stable/SubWorkflows/). When executing a workflow written in WDL and executed with Cromwell (the scientific workflow engine) one can extract metadata out of the Cromwell database. Within this metadata, the following ""executionEvents"" are available for each ""workflow.task"" in the ""calls"" objects. Pending; Requesting ExecutionToken; WaitingFor ValueStore; PreparingJob; CallCache Reading; RunningJob; Updating CallCache; Updating JobStore. From the documentation:; [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) allows Cromwell to detect when a job has been run in the past so that it doesn't have to re-compute results, saving both time and money. The main purpose of the [Job Store table](https://cromwell.readthedocs.io/en/stable/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores/#job-store-job_store_entry) is to support resuming execution of a workflow when Cromwell is restarted by recovering the outputs of completed jobs. I couldn't find a description of the Execution Token nor of the [Value Store](https://cromwell.readthedocs.io/en/stable/developers/bitesize/workflowExecution/jobKeyValueStore/) in [the docs](https://cromwell.readthedocs.io/en/develop/developers/Arch). My questions are the following:. What is the engine waiting on when a task/job is ""Pending""?; Is Requesting an Execution Token something that happens for every task because of security reasons, or does it have to do with the allowed capacity for Cromwell? What types of token are we talking about?; What happens during Value Store, where are which values stored and why are we waiting on it rather than doing it?; is this, for example, collecting default environment variables that should be set before running the workflow; or; is it collecting the values of variables that are used in the workflow, provided with the `inputs.json`?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5579:1638,recover,recovering,1638,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5579,1,['recover'],['recovering']
Safety,"the following sample error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""503 Service Unavailable\nBackend Error""; }; ],; ""message"": ""Could not read from gs://broad-epi-cromwell/workflows/ChipSeq/ce6a5671-baf6-4734-a32b-abf3d9138e9b/call-epitope_classifier/memory_retry_rc: 503 Service Unavailable\nBackend Error""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://broad-epi-cromwell/workflows/ChipSeq/ce6a5671-baf6-4734-a32b-abf3d9138e9b/call-epitope_classifier/memory_retry_rc: 503 Service Unavailable\nBackend Error""; }; ]; ```. In https://github.com/broadinstitute/cromwell/issues/6154 @freeseek reports that Cromwell is unexpectedly failing to retry 504s and provides the following sample error:; ```; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ```; Our regexes did not allow for the `\n` in the Google errors. I believe this bug came about when copy-pasting to create the test cases.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6155:1387,Timeout,Timeout,1387,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6155,2,['Timeout'],['Timeout']
Safety,"the logs: ; `; 2016-12-12 18:22:26,139 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(8a965a5e)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:29:42,727 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(73be7f27)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:42,564 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(f6bef34c)]: Abort received. Aborting 9 EJEAs; 2016-12-12 18:34:05,494 cromwell-system-ak",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1448,Abort,Aborting,1448,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Aborting']
Safety,ther.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); languages.wdl.draft3.WdlDraft3LanguageFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:128); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:9139,unsafe,unsafeToFuture,9139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['unsafe'],['unsafeToFuture']
Safety,ther.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); languages.wdl.draft3.WdlDraft3LanguageFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:138); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999:11963,unsafe,unsafeToFuture,11963,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999,1,['unsafe'],['unsafeToFuture']
Safety,tionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:46); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:62); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutio,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:3584,recover,recoverAsync,3584,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,2,['recover'],['recoverAsync']
Safety,"tor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: job id: 115; [2018-08-30 17:53:31,10] [info] BackgroundConfigAsyncJobExecutionActor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: Status change from - to Done; [2018-08-30 17:53:33,13] [info] WorkflowExecutionActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc [4dbd7d1c]: Workflow HelloWorld complete. Final Outputs:; {; ""HelloWorld.WriteGreeting.outfile"": ""/gatk/wsb/cromwell-executions/HelloWorld/4dbd7d1c-e7e8-4f83-9750-5c638d1567bc/call-WriteGreeting/execution/stdout""; }; [2018-08-30 17:53:33,18] [info] WorkflowManagerActor WorkflowActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc is in a terminal state: WorkflowSucceededState; [2018-08-30 17:53:36,13] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""HelloWorld.WriteGreeting.outfile"": ""/gatk/wsb/cromwell-executions/HelloWorld/4dbd7d1c-e7e8-4f83-9750-5c638d1567bc/call-WriteGreeting/execution/stdout""; },; ""id"": ""4dbd7d1c-e7e8-4f83-9750-5c638d1567bc""; }; [2018-08-30 17:53:41,12] [info] Workflow polling stopped; [2018-08-30 17:53:41,13] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-08-30 17:53:41,14] [info] Aborting all running workflows.; [2018-08-30 17:53:41,15] [info] WorkflowStoreActor stopped; [2018-08-30 17:53:41,15] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4062:4551,Timeout,Timeout,4551,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062,2,"['Abort', 'Timeout']","['Aborting', 'Timeout']"
Safety,tor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:46); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:62); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:3451,recover,recoverAsync,3451,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,2,['recover'],['recoverAsync']
Safety,"tractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:7559,abort,abort,7559,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"tractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:9660,abort,abort,9660,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"tractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:11899,abort,abort,11899,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"tractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleCl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:14414,abort,abort,14414,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"tractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:16446,abort,abort,16446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"tractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.Abstrac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:18963,abort,abort,18963,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"tractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.n",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:21271,abort,abort,21271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['abort'],['abort']
Safety,"ts the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encry",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:84381,timeout,timeout,84381,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,3,['timeout'],['timeout']
Safety,turn abort request into GET request,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2318:5,abort,abort,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2318,1,['abort'],['abort']
Safety,"tus"": ""error"",; ""message"": ""The server was not able to produce a timely response to your request."". The workflow is removed from WORKFLOW_STORE_ENTRY but the associated jobs are still present in JOB_STORE_ENTRY. . There are no errors in the logs: ; `; 2016-12-12 18:22:26,139 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(8a965a5e)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:29:42,727 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(73be7f27)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(c2caa0f6)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:30,399 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17a61aa5)]: Ab",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:1270,Abort,Abort,1270,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['Abort'],['Abort']
Safety,"ty \; exec \; gemBS.simg \; /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script &; echo $?; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: job id: 0; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: Status change from - to Done; [2018-08-27 02:04:19,50] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Workflow wgbs complete. Final Outputs:; {. }; [2018-08-27 02:04:19,53] [info] WorkflowManagerActor WorkflowActor-967af8b6-0d68-44c4-b04e-204674333468 is in a terminal state: WorkflowSucceededState; [2018-08-27 02:04:22,18] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {. },; ""id"": ""967af8b6-0d68-44c4-b04e-204674333468""; }; [2018-08-27 02:04:26,91] [info] Workflow polling stopped; [2018-08-27 02:04:26,91] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-08-27 02:04:26,92] [info] Aborting all running workflows.; [2018-08-27 02:04:26,92] [info] WorkflowStoreActor stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:6033,Timeout,Timeout,6033,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,2,"['Abort', 'Timeout']","['Aborting', 'Timeout']"
Safety,"t}""; """""". submit-docker = """"""; # SINGULARITY_CACHEDIR needs to point to a directory accessible by; # the jobs (i.e. not lscratch). Might want to use a workflow local; # cache dir like in run.sh; source /work/share/ac7m4df1o5/bin/cromwell/set_singularity_cachedir.sh; SINGULARITY_CACHEDIR=/work/share/ac7m4df1o5/bin/cromwell/singularity-cache; source /work/share/ac7m4df1o5/bin/cromwell/test.sh ${docker}; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; if [ -z $SINGULARITY_CACHEDIR ]; then; CACHE_DIR=$HOME/.singularity; else; CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; LOCK_FILE=$CACHE_DIR/singularity_pull_flock. # we want to avoid all the cromwell tasks hammering each other trying; # to pull the container into the cache for the first time. flock works; # on GPFS, netapp, and vast (of course only for processes on the same; # machine which is the case here since we're pulling it in the master; # process before submitting).; #flock --exclusive --timeout 1200 $LOCK_FILE \; # singularity exec --containall docker://${docker} \; # echo ""successfully pulled ${docker}!"" &> /dev/null. # Ensure singularity is loaded if it's installed as a module; module load apps/singularity/3.7.3. # Build the Docker image into a singularity image; #IMAGE=$(echo $SINGULARITY_CACHEDIR/pull/${docker}.sif|sed ""s#:#_#g""); #singularity build $IMAGE docker://${docker}. # Submit the script to SLURM; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${cwd}/execution/stdout \; --error=${cwd}/execution/stderr \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $SINGULARITY_CACHEDIR/pull/$docker_image.sif ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }. }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:8713,timeout,timeout,8713,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['timeout'],['timeout']
Safety,"unts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Done\ delocalization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Delocalization; timeout: 300s; - alwaysRun: true; commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/1xxxxxx.sh && chmod u+x /tmp/1xxxxxx.sh; && sh /tmp/1xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Delocalization; - alwaysRun: true; commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Delocalization; environment:; MEM_SIZE: '2.0'; MEM_UNIT: GB; resources:; virtualMachine:; bootDiskSizeGb: 12; bootImage: projects/cos-cloud/global/images/family/cos-stable; disks:; - name: local-disk; sizeGb: 10; type: pd-ssd; labels:; cromwell-workflow-id: xxxxxx; goog-pipelines-worker: 'true'; wdl-task-name: hello; machineType: custom-1-2048; network: {}; nvidiaDriverVersion: 450.51.06; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/compute; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/cloudkms; - https://www.googleapis.com/auth/userinfo.email; - https://www.googleapis.com/auth/userinfo.profile; - https://www.googleapis.com/auth/monitoring.write; - https://www.googleapis.com/auth/bigquery; - https://www.googleapis.com/auth/cloud-platform; volumes:; - persistentDisk:; sizeGb: 10; type: pd-ssd; volume: local-disk; zones:; - us-central1-a; - us-central1-b; timeout: 604800s; startTime: '2021-08-03T15:22:07.789742627Z'; name: projects/xxxxxx/locations/us-central1/operations/xxxxxx; response:; '@type': type.googleapis.com/cloud.lifesciences.pipelines.RunPipelineResponse. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:21869,timeout,timeout,21869,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['timeout'],['timeout']
Safety,"use QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 25000. # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; # account = """"; # token = """"; }. #docker-image-cache-manifest-file = ""gs://xxxxx-xxxxx/xxxxx.json"". # Number of workers to assign to PAPI requests; request-workers = 3. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # pipeline-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""us-central1"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localizati",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:2558,timeout,timeout,2558,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,2,['timeout'],['timeout']
Safety,"use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; # Google project which will be billed for the requests; project = ""xxxxx-xxxxx-xxxxx"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }. default-runtime-attributes {; cpu: 4; failOnStderr: false; continueOnReturnCode: 0; memory: ""2 ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:4851,abort,abort,4851,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['abort'],['abort']
Safety,"use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""service-account""; # Google project which will be billed for the requests; project = ""***-***"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }. default-runtime-attributes {; cpu: 2; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:14497,abort,abort,14497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['abort'],['abort']
Safety,"usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n)""; at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$.StandardException(JesAsyncBackendJobExecutionActor.scala:63); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:411); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:67); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$1.applyOrElse(StandardAsyncExecutionActor.scala:666); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$1.applyOrElse(StandardAsyncExecutionActor.scala:663); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:16394,recover,recoverWith,16394,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['recover'],['recoverWith']
Safety,"util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-10"" #49 daemon prio=5 os_prio=31 tid=0x00007fb7720cd800 nid=0x8b03 waiting on condition [0x00000001316a4000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-9"" #48 daemon prio=5 os_prio=31 tid=0x00007fb76b529800 nid=0x8903 waiting on condition [0x000",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:28105,Unsafe,Unsafe,28105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-11"" #50 daemon prio=5 os_prio=31 tid=0x00007fb7720ce800 nid=0x8d03 waiting on condition [0x00000001317a7000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-10"" #49 daemon prio=5 os_prio=31 tid=0x00007fb7720cd800 nid=0x8b03 waiting on condition [0x00",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:27029,Unsafe,Unsafe,27029,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-12"" #51 daemon prio=5 os_prio=31 tid=0x00007fb7720cf000 nid=0x8f03 waiting on condition [0x00000001318aa000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-11"" #50 daemon prio=5 os_prio=31 tid=0x00007fb7720ce800 nid=0x8d03 waiting on condition [0x00",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:25953,Unsafe,Unsafe,25953,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-13"" #52 daemon prio=5 os_prio=31 tid=0x00007fb76e96e000 nid=0x9103 waiting on condition [0x00000001319ad000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-12"" #51 daemon prio=5 os_prio=31 tid=0x00007fb7720cf000 nid=0x8f03 waiting on condition [0x00",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:24877,Unsafe,Unsafe,24877,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-14"" #53 daemon prio=5 os_prio=31 tid=0x00007fb77061e000 nid=0x9303 waiting on condition [0x0000000131ab0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-13"" #52 daemon prio=5 os_prio=31 tid=0x00007fb76e96e000 nid=0x9103 waiting on condition [0x00",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:23801,Unsafe,Unsafe,23801,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-15"" #54 daemon prio=5 os_prio=31 tid=0x00007fb76b6b4000 nid=0x9503 waiting on condition [0x0000000131bb3000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-14"" #53 daemon prio=5 os_prio=31 tid=0x00007fb77061e000 nid=0x9303 waiting on condition [0x00",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:22725,Unsafe,Unsafe,22725,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-16"" #55 daemon prio=5 os_prio=31 tid=0x00007fb76e92f000 nid=0x9703 waiting on condition [0x0000000131cb6000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-15"" #54 daemon prio=5 os_prio=31 tid=0x00007fb76b6b4000 nid=0x9503 waiting on condition [0x00",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:21649,Unsafe,Unsafe,21649,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""db-18"" #58 daemon prio=5 os_prio=31 tid=0x00007fb770630800 nid=0x9d03 waiting on condition [0x00000001323d9000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0751828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-3"" #57 prio=5 os_prio=31 tid=0x00007fb76e95e000 nid=0x9b03 waiting on condition [0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:18619,Unsafe,Unsafe,18619,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Unsafe'],['Unsafe']
Safety,"via REST API end points. However, we are working on HPC cluster where we don't have admin privileges to start server and submit requests to api. Backend: `slurm`; Workflow: [Link](https://github.com/biowdl/RNA-seq/blob/develop/RNA-seq.wdl). <details>; <summary>Config</summary>. ```; backend {. default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int time_minutes = 600; Int cpu = 4; #Int memory = 500; String queue = ""short""; String map_path = ""/shared/rna-seq""; String partition = ""compute""; String root = ""/shared/rna-seq/cromwell-executions""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. submit = """"""; task=`echo ${job_name}|cut -d'_' -f3`; echo $task; image=`grep ""\b$task\b"" ${map_path}/map.txt |cut -d',' -f2`; echo $PWD; echo $image; if [ ! -z $image ]; then \; echo ""Inside Singularity exec""; \; echo ""CPU count: "" ${cpu}; \; echo ""time_minutes: "" ${time_minutes}; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""singularity exec -B /shared/rna-seq:/shared/rna-seq $image /bin/bash ${script}""; else \; echo ""No Singularity""; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""/bin/bash ${script}""; fi;; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. </details>. <details>; <summary>Error stack trace</summary>. ```; [2021-03-08 11:53:28,10] [ESC[38;5;1merrorESC[0m] Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is no",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6208:1642,timeout,timeout-seconds,1642,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6208,1,['timeout'],['timeout-seconds']
Safety,"w slots exist; #max-workflow-launch-count = 50. # Number of seconds between workflow launches; #new-workflow-poll-rate = 20. # Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; #number-of-workflow-log-copy-workers = 10. # Default number of cache read workers; #number-of-cache-read-workers = 25. io {; # throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; # #number-of-requests = 100000; # #per = 100 seconds; # }. # Number of times an I/O operation should be attempted before giving up and failing it.; #number-of-attempts = 5; }. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; #lines = 128000; #bool = 7; #int = 19; #float = 50; #string = 128000; #json = 128000; #tsv = 128000; #map = 128000; #object = 128000; }. abort {; # These are the default values in Cromwell, in most circumstances there should not be a need to change them. # How frequently Cromwell should scan for aborts.; scan-frequency: 30 seconds. # The cache of in-progress aborts. Cromwell will add entries to this cache once a WorkflowActor has been messaged to abort.; # If on the next scan an 'Aborting' status is found for a workflow that has an entry in this cache, Cromwell will not ask; # the associated WorkflowActor to abort again.; cache {; enabled: true; # Guava cache concurrency.; concurrency: 1; # How long entries in the cache should live from the time they are added to the cache.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 100000; }; }. # Cromwell reads this value into the JVM's `networkaddress.cache.ttl` setting to control DNS cache expiration; dns-cache-ttl: 3 minutes; }. docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:3119,abort,abort,3119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['abort'],['abort']
Safety,"wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${out} \; --error=${err} \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # SINGULARITY_CACHEDIR needs to point to a directory accessible by; # the jobs (i.e. not lscratch). Might want to use a workflow local; # cache dir like in run.sh; source /work/share/ac7m4df1o5/bin/cromwell/set_singularity_cachedir.sh; SINGULARITY_CACHEDIR=/work/share/ac7m4df1o5/bin/cromwell/singularity-cache; source /work/share/ac7m4df1o5/bin/cromwell/test.sh ${docker}; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; if [ -z $SINGULARITY_CACHEDIR ]; then; CACHE_DIR=$HOME/.singularity; else; CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; LOCK_FILE=$CACHE_DIR/singularity_pull_flock. # we want to avoid all the cromwell tasks hammering each other trying; # to pull the container into the cache for the first time. flock works; # on GPFS, netapp, and vast (of course only for processes on the same; # machine which is the case here since we're pulling it in the master; # process before submitting).; #flock --exclusive --timeout 1200 $LOCK_FILE \; # singularity exec --containall docker://${docker} \; # echo ""successfully pulled ${docker}!"" &> /dev/null. # Ensure singularity is loaded if it's installed as a module; module load apps/singularity/3.7.3. # Build the Docker image into a singularity image; #IMAGE=$(echo $SINGULARITY_CACHEDIR/pull/${docker}.sif|sed ""s#:#_#g""); #singularity build $IMAGE docker://${docker}. # Submit the script to SLURM; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${cwd}/execution/stdout \; --error=${cwd}/execution/stderr \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""singularity exec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:8389,avoid,avoid,8389,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['avoid'],['avoid']
Safety,"well-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,157 cromwell-system-akka.dispatchers.backend-dispatcher-37 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,233 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PAPI request worker batch interval is 33333 milliseconds; ```. but then it immediately starts printing these errors:; ```; 2019-07-21 23:34:40,010 cromwell-system-akka.actor.default-dispatcher-32 ERROR - Error searching for abort requests; java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""WORKFLOW_STORE_ENTRY"" where (""WORKFLOW_STATE"" = cast('Aborting' as varchar(1677' at line 1; 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120); 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97); 	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:970); 	at com.mysql.cj.jdbc.ClientPreparedStatement.execute(ClientPreparedStatement.java:387); 	at com.zaxxer.hikari.pool.ProxyPreparedStatement.execute(ProxyPreparedStatement.java:44); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java); 	at slick.jdbc.StatementInvoker.results(StatementInvoker.scala:38); 	at slick.jdbc.StatementInvoker.iteratorTo(StatementInvoker.scala:21); 	at slick.jdbc.Invoker.foreach(Invoker.scala:47); 	at slick.jdbc.Invoker.foreach$(Invoker.scala:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:3943,Abort,Aborting,3943,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,1,['Abort'],['Aborting']
Safety,womtool does not detect missing inputs for sub-workflow,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3230:17,detect,detect,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3230,1,['detect'],['detect']
Safety,workflow stuck in submitted after abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1885:34,abort,abort,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1885,1,['abort'],['abort']
Safety,ws `java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor` exception when it tries to recover a running job. Stacktrace:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:995,recover,recover,995,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Safety,"xt task. The next task takes in a file of files. ; The python inside the wdl is very counter-intuitive, prone to error, and unnecessary in other execution managers. See my real example below... ``` wdl; workflow crsp_validation_workflow {. ....snip....; Array[Array[File]] triplet_file_array = read_tsv(input_triplet_file_list); Float ploidy=""2"". scatter (triplet in triplet_file_array) {; ....snip.... call run_sensitivity_precision {; input:; entity_id=triplet[0],; oncotated_target_seg_gt_file = oncotate.oncotated_target_seg_gt_file,; ploidy=ploidy; }; }. call run_plot_purity_series {; input:; output_dir=""plots/"",; amp_sens_prec=run_sensitivity_precision.amp_sens_prec_file,; del_sens_prec=run_sensitivity_precision.del_sens_prec_file,; small_sens=run_sensitivity_precision.small_sens_file; }; }; ....snip....; task run_sensitivity_precision {; File oncotated_target_seg_gt_file; Float ploidy; String entity_id. command {; # Ignore chromosome 2, since the normal has this event and HCC1143T does not, so ground truth may be off, since; # detection of deletions could be reduced. Chromosome 6 may have a similar issue.; run_sensitivity_precision -i ""[2]"" ${oncotated_target_seg_gt_file} ${ploidy} ${entity_id}.sens_prec; }. output {; File amp_sens_prec_file = ""${entity_id}.sens_prec.amp.tsv""; File del_sens_prec_file = ""${entity_id}.sens_prec.del.tsv""; File small_sens_file = ""${entity_id}.sens_prec.small_segs.tsv""; File gene_segs_sens_prec_file = ""${entity_id}.sens_prec.gene_seg""; }. runtime {; docker: ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; memory: ""2GB""; }; }. task run_plot_purity_series {; String output_dir; Array[File] amp_sens_prec; Array[File] del_sens_prec; Array[File] small_sens. command {; ################# HERE; python <<CODE; files = ""${sep="","" amp_sens_prec}"".split("",""); files.extend(""${sep="","" del_sens_prec}"".split("","")); with open(""sens_prec_aggregate.txt"", ""w"") as fp:; fp.write('\n'.join(files)); CODE; wc -l sens_prec_aggregate.txt. python <<CODE;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1263:1180,detect,detection,1180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1263,1,['detect'],['detection']
Safety,"…. Closes #940 . Note that the tests for aborting were already commented out, and as this would be tricky to test anyways I took that as a sign from above. I've manually verified it, however. @abaumann - this will not clean up previously stuck workflows, I'm going to provide a script or something like that to clean up the old crap, fixing those from within Cromwell was building up too many tendrils for a hack fix to something which is going to be replaced in the near future.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/986:41,abort,aborting,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/986,1,['abort'],['aborting']
Safety,…lues which caused the jobs to fail once cromwell recovered after a migration. For Develop,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2242:50,recover,recovered,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2242,1,['recover'],['recovered']
Safety,"…nate is true. It doesn't matter in run mode since it apparently uses an in-memory workflow store, but in the unlikely case in which someone sets `abort-on-terminate` to `true` in server mode, we want to set the status of all running jobs in the workflow store to `Aborting` so that when we restart the server we don't keep running those jobs. It also fixes a bug which was preventing run mode to exit properly.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2819:147,abort,abort-on-terminate,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2819,2,"['Abort', 'abort']","['Aborting', 'abort-on-terminate']"
Security,"	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-05-02 15:22:54,71] [[38;5;1merror[0m] bc4644da:batch_for_variantcall:-1:1: Hash error, disabling call caching for this job.; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceiv",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584:6792,hash,hash,6792,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584,1,['hash'],['hash']
Security,	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataK,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:5997,validat,validation,5997,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,1,['validat'],['validation']
Security,	at java.nio.channels.Channels$1.close(Channels.java:178); 	at java.nio.file.Files.write(Files.java:3300); 	at better.files.File.writeByteArray(File.scala:269); 	at better.files.File.write(File.scala:279); 	at cromwell.core.path.BetterFileMethods$class.write(BetterFileMethods.scala:179); 	at cromwell.filesystems.gcs.GcsPath.write(GcsPathBuilder.scala:101); 	at cromwell.engine.io.nio.NioFlow$$anonfun$write$1.apply$mcV$sp(NioFlow.scala:53); 	at cromwell.engine.io.nio.NioFlow$$anonfun$write$1.apply(NioFlow.scala:52); 	at cromwell.engine.io.nio.NioFlow$$anonfun$write$1.apply(NioFlow.scala:52); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); 	... 6 more; Caused by: javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Broken pipe; 	at sun.security.ssl.SSLSocketImpl.checkEOF(SSLSocketImpl.java:1541); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:95); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); 	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); 	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); 	at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.jav,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183:4222,secur,security,4222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183,1,['secur'],['security']
Security,	at wdl4s.WdlNamespace$$anonfun$17.apply(WdlNamespace.scala:208); 	at wdl4s.WdlNamespace$$anonfun$17.apply(WdlNamespace.scala:207); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.immutable.List.map(List.scala:285); 	at wdl4s.WdlNamespace$.apply(WdlNamespace.scala:207); 	at wdl4s.WdlNamespace$.wdl4s$WdlNamespace$$load(WdlNamespace.scala:177); 	at wdl4s.WdlNamespace$.loadUsingSource(WdlNamespace.scala:173); 	at wdl4s.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:542); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$validateNamespaceWithImports$1.apply(MaterializeWorkflowDescriptorActor.scala:363); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$validateNamespaceWithImports$1.apply(MaterializeWorkflowDescriptorActor.scala:356); 	at lenthall.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:17); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.validateNamespaceWithImports(MaterializeWorkflowDescriptorActor.scala:356); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.validateNamespace(MaterializeWorkflowDescriptorActor.scala:372); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:172); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:132); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:130); 	at scala.runtime.Abstra,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1958:2095,validat,validateNamespaceWithImports,2095,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1958,1,['validat'],['validateNamespaceWithImports']
Security, 	at scala.collection.TraversableOnce.sum(TraversableOnce.scala:216); 	at scala.collection.TraversableOnce.sum$(TraversableOnce.scala:216); 	at scala.collection.AbstractIterator.sum(Iterator.scala:1417); 	at better.files.File.size(File.scala:502); 	at cromwell.core.path.BetterFileMethods.size(BetterFileMethods.scala:323); 	at cromwell.core.path.BetterFileMethods.size$(BetterFileMethods.scala:323); 	at cromwell.filesystems.gcs.GcsPath.size(GcsPathBuilder.scala:179); 	at cromwell.backend.wdl.ReadLikeFunctions.$anonfun$fileSize$2(ReadLikeFunctions.scala:18); 	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.wdl.ReadLikeFunctions.$anonfun$fileSize$1(ReadLikeFunctions.scala:18); 	at cromwell.backend.wdl.ReadLikeFunctions.$anonfun$validateFileSizeIsWithinLimits$1(ReadLikeFunctions.scala:54); 	at scala.util.Success.flatMap(Try.scala:247); 	at cromwell.backend.wdl.ReadLikeFunctions.validateFileSizeIsWithinLimits(ReadLikeFunctions.scala:53); 	at cromwell.backend.wdl.ReadLikeFunctions.validateFileSizeIsWithinLimits$(ReadLikeFunctions.scala:51); 	at cromwell.backend.standard.StandardExpressionFunctions.validateFileSizeIsWithinLimits(StandardExpressionFunctions.scala:22); 	at cromwell.backend.wdl.ReadLikeFunctions.read_string(ReadLikeFunctions.scala:83); 	at cromwell.backend.wdl.ReadLikeFunctions.read_string$(ReadLikeFunctions.scala:81); 	at cromwell.backend.standard.StandardExpressionFunctions.read_string(StandardExpressionFunctions.scala:22); 	at sun.reflect.GeneratedMethodAccessor360.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at wdl4s.wdl.expression.WdlFunctions.$anonfun$getFunction$1(WdlFunctions.scala:11); 	at wdl4s.wdl.expression.ValueEvaluator.evaluate(ValueEvaluator.scala:190); 	at wdl4s.wdl.expression.ValueEvaluator.evaluate(ValueEvaluator.scala:56); 	at wdl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2576:6434,validat,validateFileSizeIsWithinLimits,6434,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2576,1,['validat'],['validateFileSizeIsWithinLimits']
Security, 	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); 	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); 	at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at com.google.cloud.storage.spi.DefaultStorageRpc.write(DefaultStorageRpc.java:564); 	... 24 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Broken pipe; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppOutputStream.write(AppOutputStream.java:128); 	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82); 	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140); 	at java.io.PrintStream.flush(PrintStream.java:338); 	at java.io.FilterOutputStream.flush(FilterOutputStream.java:140); 	at com.google.api.client.http.AbstractInputStreamContent.writeTo(AbstractInputStreamContent.java:73); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:79); 	... 26 more; Caused by: java.net.SocketException: Broken pipe; 	at java.net.SocketOutputStream.socketWrite0(Native Method); 	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109); 	at java.net.SocketOutputStream.write(SocketOutputStream.java:153),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183:5550,secur,security,5550,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183,1,['secur'],['security']
Security," # Funcotator inputs; Boolean? run_funcotator; String? sequencing_center; String? sequence_source; String? funco_reference_version; String? funco_output_format; Boolean? funco_compress; Boolean? funco_use_gnomad_AF; File? funco_data_sources_tar_gz; String? funco_transcript_selection_mode; File? funco_transcript_selection_list; Array[String]? funco_annotation_defaults; Array[String]? funco_annotation_overrides; Array[String]? funcotator_excluded_fields; Boolean? funco_filter_funcotations; String? funcotator_extra_args. String funco_default_output_format = ""MAF""; ; # Use as a last resort to increase the disk given to every task in case of ill behaving data; Int? emergency_extra_disk; }. Int contig_size = select_first([min_contig_size, 1000000]); Int preemptible_or_default = select_first([preemptible, 2]); Int max_retries_or_default = select_first([max_retries, 2]). Runtime standard_runtime = {""gatk_docker"": gatk_docker, ""gatk_override"": gatk_override,; ""max_retries"": max_retries_or_default, ""preemptible"": preemptible_or_default, ""cpu"": small_task_cpu,; ""machine_mem"": small_task_mem * 1000, ""command_mem"": small_task_mem * 1000 - 500,; ""disk"": small_task_disk, ""boot_disk_size"": boot_disk_size}. scatter (normal_bam in zip(normal_bams, normal_bais)) {; call m2.Mutect2 {; input:; intervals = intervals,; ref_fasta = ref_fasta,; ref_fai = ref_fai,; ref_dict = ref_dict,; tumor_reads = normal_bam.left,; tumor_reads_index = normal_bam.right,; scatter_count = scatter_count,; m2_extra_args = select_first([m2_extra_args, """"]) + ""--max-mnp-distance 0"",; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible = preemptible,; max_retries = max_retries,; pon = pon,; pon_idx = pon_idx,; gnomad = gnomad,; gnomad_idx = gnomad_idx; }; }. output {; Array[File] normal_calls = Mutect2.filtered_vcf; Array[File] normal_calls_idx = Mutect2.filtered_vcf_idx. }; }. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5347:6524,PASSWORD,PASSWORDS,6524,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347,1,['PASSWORD'],['PASSWORDS']
Security," ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ; Jobs which required gpuType: ""nvidia-tesla-t4"", nvidiaDriverVersion: ""418.40.04"", failed. Our pipeline backend is Google : genomics.googleapis.com; ""jes"": {; ""endpointUrl"": ""https://genomics.googleapis.com/"",; ""zone"": ""us-central1-f"",; ....; },. job runtimeAttributes:; ...; ""preemptible"": ""1"",; ""gpuCount"": ""1"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""70"",; ""disks"": ""local-disk 70 SSD"",; ""continueOnReturnCode"": ""0"",; ""gpuType"": ""nvidia-tesla-t4"",; ""nvidiaDriverVersion"": ""418.40.04"",; ""maxRetries"": ""0"",; ""cpu"": ""8"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zone"": ""us-central1-f"",; ""memoryMin"": ""2 GB"",; ""memory"": ""64 GB"". Jobs failed with following message:; ""Task wf_quip_lymphocyte_segmentation_incep_v01052021.quip_lymphocyte_segmentation:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: generic::unknown: installing drivers: container exited with unexpected exit code 1: + COS_KERNEL_INFO_FILENAME=kernel_info\n+ C",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6195:1108,PASSWORD,PASSWORDS,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195,1,['PASSWORD'],['PASSWORDS']
Security," - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-01-31 18:29:35,282 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(Thread",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:2857,validat,validate,2857,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['validat'],['validate']
Security," . If I can get this working, I'll happily document and update the CallCaching documentation page with what I've found. ## Background information. Version: Cromwell-47. Documentation:; - https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346:1601,hash,hash,1601,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346,1,['hash'],['hash']
Security," 16:15:32 GMT; Server: Docker Registry; X-XSS-Protection: 1; mode=block; X-Frame-Options: SAMEORIGIN; Alt-Svc: quic="":443""; ma=2592000; v=""41,39,38,37,35"". {; ""name"": ""unused"",; ""tag"": ""unused"",; ""architecture"": ""amd64"",; ""fsLayers"": [; {; ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""; },; {; ""blobSum"": ""sha256:1c4816548d6a2a08f89c304bf09503e791a338c4be90629610152124c7285d3f""; }; ],; ""history"": [; {; ""v1Compatibility"": ""{\""architecture\"":\""amd64\"",\""config\"":{\""ArgsEscaped\"":true,\""AttachStderr\"":false,\""AttachStdin\"":false,\""AttachStdout\"":false,\""Cmd\"":[\""/bin/bash\""],\""Domainname\"":\""\"",\""Entrypoint\"":[],\""Env\"":[\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""ExposedPorts\"":{},\""Hostname\"":\""6f82340aefbb\"",\""Image\"":\""sha256:7e927fe855b39870a9d03f4c3f8ae4b764d5e6847cdd0b7cee4be942e1ccc871\"",\""Labels\"":{},\""MacAddress\"":\""\"",\""NetworkDisabled\"":false,\""OnBuild\"":[],\""OpenStdin\"":false,\""Shell\"":[],\""StdinOnce\"":false,\""StopSignal\"":\""\"",\""Tty\"":false,\""User\"":\""\"",\""Volumes\"":{},\""WorkingDir\"":\""\""},\""container\"":\""be8ce157bc5ce90906f21220f2fd1442baa95c7284eead432626d6f1b4ac182e\"",\""container_config\"":{\""ArgsEscaped\"":true,\""AttachStderr\"":false,\""AttachStdin\"":false,\""AttachStdout\"":false,\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) \"",\""CMD [\\\""/bin/bash\\\""]\""],\""Domainname\"":\""\"",\""Entrypoint\"":[],\""Env\"":[\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""ExposedPorts\"":{},\""Hostname\"":\""6f82340aefbb\"",\""Image\"":\""sha256:7e927fe855b39870a9d03f4c3f8ae4b764d5e6847cdd0b7cee4be942e1ccc871\"",\""Labels\"":{},\""MacAddress\"":\""\"",\""NetworkDisabled\"":false,\""OnBuild\"":[],\""OpenStdin\"":false,\""Shell\"":[],\""StdinOnce\"":false,\""StopSignal\"":\""\"",\""Tty\"":false,\""User\"":\""\"",\""Volumes\"":{},\""WorkingDir\"":\""\""},\""created\"":\""2017-08-07T23:50:27.564116691Z\"",\""docker_version\"":\""17.03.1-ce\"",\""id\"":\""0a8d1e311b7797cd62611f599600a2e4633e4a7d1df9c1119141bd99c4842beb\"",\""os\"":\""linux\"",\""parent\"":\""63",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2826:2465,Expose,ExposedPorts,2465,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2826,2,['Expose'],['ExposedPorts']
Security," ; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.final.json \; default_runtimes \; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.metadata.json; ```. Error message:. ```; [2016-09-21 17:51:25,15] [error] Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); java.lang.RuntimeException: Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); at cromwell.backend.validation.RuntimeAttributesValidation$class.validateOptionalExpression(RuntimeAttributesValidation.scala:319); at cromwell.backend.validation.RuntimeAttributesValidation$$anon$1.validateOptionalExpression(RuntimeAttributesValidation.scala:90); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$badRuntimeAttrsForTask$1$2.apply(BackendWorkflowInitializationActor.scala:141); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$badRuntimeAttrsForTask$1$2.apply(BackendWorkflowInitializationActor.scala:139); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.TraversableLike$$anonfun$map$1.a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1465:1560,validat,validation,1560,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465,1,['validat'],['validation']
Security," = GCPBATCH; providers {; GCPBATCH {; // life sciences; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; ## Google project; project = ""$PROJECT"". ## Base bucket for workflow executions; root = ""$BUCKET""; name-for-call-caching-purposes: PAPI; #60000/min in google; ##genomics-api-queries-per-100-seconds = 90000; virtual-private-cloud {; network-name = ""$NET""; subnetwork-name = ""$SUBNET""; }; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; 	 request-workers = 4; batch-timeout = 7 days; 	 # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in PAPI.; 	 slow-job-warning-time: 24 hours; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; compute-service-account = ""default""; # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false; ## Location; location = ""europe-west1"". ; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""$PROJECT""; caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""reference""; }; }; }. default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2 GB""; bootDiskS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:10708,access,access,10708,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['access'],['access']
Security, Call Caching: cache miss; Uncaught error from thread [cromwell-system-akka.actor.default-dispatcher-2676] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]. **java.lang.OutOfMemoryError: Java heap space**; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.lang.StringBuilder.toString(StringBuilder.java:407); at scala.StringContext.standardInterpolator(StringContext.scala:128); at scala.StringContext.s(StringContext.scala:95); at cromwell.engine.workflow.WorkflowActor$$anonfun$34.apply(WorkflowActor.scala:885); at cromwell.engine.workflow.WorkflowActor$$anonfun$34.apply(WorkflowActor.scala:881); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:155); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:155); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:155); at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104); at cromwell.engine.workflow.WorkflowActor.cromwell$engine$workflow$WorkflowActor$$startRunnableCalls(WorkflowActor.scala:881); at cromwell.engine.workflow.WorkflowActor$$anonfun$2.applyOrElse(WorkflowActor.scala:532); at cromwell.engine.workflow.WorkflowActor$$anonfun$2.applyOrElse(WorkflowActor.scala:467); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:604); at cromwell.engine.workflow.WorkflowActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowActor.scala:236); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:734); at cromwell.engine.workflow.WorkflowActo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/695:10690,Hash,HashMap,10690,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/695,2,['Hash'],"['HashMap', 'HashTrieMap']"
Security," Docker); - Initial localisation strategy: `[hard-link, cached-copy]`; - Local SFS environment; - My input files can be fairly large (~250GB per Bam with up to 16 Bams). . If I can get this working, I'll happily document and update the CallCaching documentation page with what I've found. ## Background information. Version: Cromwell-47. Documentation:; - https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~Wha",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346:1447,hash,hash,1447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346,1,['hash'],['hash']
Security," \""type\"": \""File\"",\n \""doc\"": \""Path to tumor bam file\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-tumor_bam\""\n },\n \""secondaryFiles\"": [\n \"".bai\""\n ],\n \""id\"": \""#amber-3.3.cwl/tumor_bam\""\n },\n {\n \""type\"": \""boolean\"",\n \""doc\"": \""Flag to put AMBER into tumor only mode\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-tumor_only\""\n },\n \""default\"": false,\n \""id\"": \""#amber-3.3.cwl/tumor_only\""\n },\n {\n \""type\"": \""int\"",\n \""doc\"": \""Min VAF in ref and alt in tumor only mode\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-tumor_only_min_support\""\n },\n \""default\"": 2,\n \""id\"": \""#amber-3.3.cwl/tumor_only_min_support\""\n },\n {\n \""type\"": [\n \""null\"",\n \""float\""\n ],\n \""doc\"": \""Min support in ref and alt in tumor only mode\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-tumor_only_min_vaf\""\n },\n \""default\"": 0.05,\n \""id\"": \""#amber-3.3.cwl/tumor_only_min_vaf\""\n },\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""SAM validation strategy: STRICT, SILENT, LENIENT [STRICT]\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-validation_stringency\""\n },\n \""default\"": \""STRICT\"",\n \""id\"": \""#amber-3.3.cwl/validation_stringency\""\n }\n ],\n \""outputs\"": [\n {\n \""type\"": \""Directory\"",\n \""outputBinding\"": {\n \""glob\"": \""$(inputs.output_dir)\""\n },\n \""id\"": \""#amber-3.3.cwl/outdir\""\n }\n ],\n \""id\"": \""#amber-3.3.cwl\""\n },\n {\n \""class\"": \""CommandLineTool\"",\n \""doc\"": \""Count bam lines determines the read depth ratios of the supplied tumor and reference genomes.\\n\\nCOBALT starts with the raw read counts per 1,000 base window\\nfor both normal and tumor samples by counting the number of alignment starts in the\\nrespective bam files with a mapping quality score of at least 10\\nthat is neither unmapped, duplicated, secondary, nor supplementary.\\nWindows with a GC content less than 0.2 or greater than 0.6 or with an average mappability below 0.85\\nare excluded from further analysis.\\n\\nNext we apply a GC normalization to calculate the rea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:66782,validat,validation,66782,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['validat'],['validation']
Security," `v2+json` hash available will run, but will always fail call caching. We currently request a ""v2+json"" Docker-Content-Digest for all GCR images. Recently, GCR [""correctly""](https://enterprise.google.com/supportcenter/managecases#Case/0016000000MNGDy/U-13919143) returns 404 when it doesn't actually have ""v2+json"" for the images. There does appear to be an alternative ""v1+prettyjws"" Docker-Content-Digest available for GCR images. (FYI there are a [number of search hits](https://www.google.com/search?q=Docker-Content-Digest) re: ""Docker-Content-Digest"", especially v1 vs. v2, etc.) It's likely that Google's ""correction"" mentioned above was to fix the server from returning the wrong ""v1"" hash when ""v2"" was requested-but-not-available. While cromwell only uses the headers via HTTP HEAD, the hashes in the headers and the contents of an HTTP GET _are_ different between v1 and v2. Google tech support have also mentioned that clients may request either hash type, submitting both `Accept` headers, and that one can read the `Content-Type` from the server to determine which `Docker-Content-Digest` was returned. Bodies reformatted by jq for readability:; ```bash; $ curl -i -s -H 'Accept: application/vnd.docker.distribution.manifest.v1+prettyjws' https://gcr.io/v2/google-containers/ubuntu-slim/manifests/0.14; HTTP/1.1 200 OK; Docker-Distribution-API-Version: registry/2.0; Content-Type: application/vnd.docker.distribution.manifest.v1+prettyjws; Content-Length: 2647; Docker-Content-Digest: sha256:781290a693dc805993b19b7b4c5be40f7688f595312646b926abe2baae2fa9ff; Date: Mon, 06 Nov 2017 16:15:32 GMT; Server: Docker Registry; X-XSS-Protection: 1; mode=block; X-Frame-Options: SAMEORIGIN; Alt-Svc: quic="":443""; ma=2592000; v=""41,39,38,37,35"". {; ""name"": ""unused"",; ""tag"": ""unused"",; ""architecture"": ""amd64"",; ""fsLayers"": [; {; ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""; },; {; ""blobSum"": ""sha256:1c4816548d6a2a08f89c304bf09503e791a338c4be9062961015212",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2826:1093,hash,hash,1093,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2826,1,['hash'],['hash']
Security," `womtool validate` (and it validated fine on Terra with the automatic validation they do). But the job would run about halfway and then automatically switch to ""Aborting"" status with no explanation or error message. The workflow would eventually fail after a huge delay (about 22 hours), and there would be no real error message. All tasks that ran were successful (but not all tasks ran). # Minimal WDL example. Here is a working example:. ```wdl; version 1.0. workflow my_workflow {; call my_task; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. And here is a non-working example that still validates fine using `womtool validate`:. ```wdl; version 1.0. workflow my_workflow {; input {; Boolean run_task; }. if (run_task) {; call my_task; }. output {; File out = select_first([my_task.out, stdout()]); }; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. The above gives; ```console; (cromwell) [sfleming@laptop:~/cromwell]$ womtool validate test.wdl ; Success!; ```. # The problem. The problem is that the non-working WDL example above should not validate successfully, as it is NOT a valid WDL. The `stdout()` built-in inside the `select_first()` in the `output` block of the `workflow` is not actually allowed. It will cause a very bizarre error when this WDL is run. # What am I asking for?. 1. Fix `womtool validate` to catch these kinds of errors. Also happens with `stderr()`.; 2. Provide an actionable error message when this kind of edge case ends up being run by Cromwell. Right now it automatically moves to ""Aborting"" status with no error message at all. Very hard to diagnose!. # Other information. I found this error using `miniwdl check`, which correctly identified the error, just FYI. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6976:1803,validat,validate,1803,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6976,3,"['PASSWORD', 'validat']","['PASSWORDS', 'validate']"
Security," a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; Our backend: ; GCP PAPIv2 ; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory"". endpoint-url = ""https://genomics.googleapis.com/"". <!-- Paste/Attach your workflow if possible: -->; workflow runtime; runtime {; docker: ""us.gcr.io/cloudypipelines-com/til_segmentation:1.5""; bootDiskSizeGb: 70; disks: ""local-disk 70 SSD""; memory: ""52 GB""; cpu: ""8""; maxRetries: 1; gpuCount: 1; zones: ""us-east1-d us-east1-c us-central1-a us-central1-c us-west1-a us-west1-b""; ##gpuType: ""nvidia-tesla-k80""; gpuType: ""nvidia-tesla-t4""; nvidiaDriverVersion: ""418.40.04""; ##nvidiaDriverVersion: ""418.87.00""; ; }. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; #### Recently, Our All workflows with GPU failed under the same configurations which most of workflows used to work on Cromwell 48, we updated to the latest Cromwell 52, still had the same errors, see belowL. 2020-08-04 23:44:00,228 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - WorkflowManagerActor Workflow f1dca11c-ea29-48b1-9691-9f30c9e59154 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_quip_lymphocyte_segmentation_v03232020.quip_lymphocyte_segmentation:NA:2 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: generic::unknown: installing drivers: container exited with unexpected exit code 1: + COS_DOWNLOAD_GCS=https://storage.googleapis.com/cos-tools; + COS_KERNEL_SRC_GIT=https://chromium.googlesource.com/chromiumos/third_party/kernel; + COS_KERNEL_SRC_ARCHIVE=kernel-src.tar.gz; + TOOLCHAIN_URL_FILENAME=toolchain_url; + TOOLCHAIN_ARCHIVE=toolchain.tar.xz; + TOOLCHAIN_ENV_FILENAME=toolchain_env; + CHROMIUMOS_SDK_GCS=https://storage.googleapis.com/chrom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:1693,PASSWORD,PASSWORDS,1693,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,1,['PASSWORD'],['PASSWORDS']
Security," akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Optional value was not set and no 'default' attribute was provided; Optional value was not set and no 'default' attribute was provided; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:534); ... 39 common frames omitted. ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3927:5345,Validat,Validation,5345,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927,2,['Validat'],"['Validation', 'ValidationTry']"
Security," at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs(ConfigA",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:1840,validat,validation,1840,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,1,['validat'],['validation']
Security," backend configuration, since whether this is something you want to do or not will depend on the infrastucture your workflow is running on. One potential way to configure this, might be to add multipliers for certain runtime attributes in the backend configuration:; ```; [...]; config {; runtime-attributes = """"""; Int? cpu = 1; Int? memory = 4; """"""; runtime-attribute-retry-multipliers = {; memory: 1.5; }; [...]; }; [...]; ```. This would, for example, cause the memory attribute to be multiplied by `1.5` with each retry. For the first attempt it would be `4`, for the the second `6`, the third `9` etc. Another option might be that the values here indicate a fraction of the original attribute which it should be increased it by each retry, so in the above example it would be: `4` -> `10` -> `16` etc. You would probably want set a lower value in that case, though. Another option would be to supply a list of numbers with each indicating a multiplier for a certain attempt. A value of `[1.5, 2]` (or maybe `[1, 1.5, 2]`) would cause the value to be multiplied by `1.5` on the second attempt and `2` on the third, repeating the last multiplier if neccesary. (ie. `4` -> `6` -> `8` -> `8`). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4346:2280,PASSWORD,PASSWORDS,2280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4346,1,['PASSWORD'],['PASSWORDS']
Security," but really having problems with the `sepFunctionEvaluator`, it's a two value function so I tried to use the `processTwoValidatedValues` from `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioF",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:1370,validat,validation,1370,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['validat'],['validation']
Security," causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143:1634,validat,validateWdlNamespace,1634,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143,1,['validat'],['validateWdlNamespace']
Security," creating a `CallableTaskDefinition` structure. This structure has an empty meta section. When rewriting `foo` in draft-2, this works as expected. . ```wdl; version 1.0. task foo {; input {; String buf ; }; command {}; output {; String s = buf; } ; meta {; type : ""native"",; id: ""applet-xxxx""; }; }; ```. This is the code used to parse the task: ; ```scala; // Extract the only task from a namespace ; def getMainTask(bundle: WomBundle) : CallableTaskDefinition = {; // check if the primary is nonempty ; val task: Option[CallableTaskDefinition] = bundle.primaryCallable match {; case Some(task : CallableTaskDefinition) => Some(task); case Some(exec : ExecutableTaskDefinition) => Some(exec.callableTaskDefinition); case _ => None; }; task match {; case Some(x) => x; case None =>; // primary is empty, check the allCallables map ; if (bundle.allCallables.size != 1); throw new Exception(""WDL file must contains exactly one task""); val (_, task) = bundle.allCallables.head; task match {; case task : CallableTaskDefinition => task; case exec : ExecutableTaskDefinition => exec.callableTaskDefinition; case _ => throw new Exception(""Cannot find task inside WDL file""); 		}; }; }. def parseWdlTask(wfSource: String) : CallableTaskDefinition = {; val languageFactory =; if (wfSource.startsWith(""version 1.0"") ||; wfSource.startsWith(""version draft-3"")) {; new WdlDraft3LanguageFactory(ConfigFactory.empty()); } else {; new WdlDraft2LanguageFactory(ConfigFactory.empty()); }. val bundleChk: Checked[WomBundle] =; languageFactory.getWomBundle(wfSource, ""{}"", List.empty, List(languageFactory)); val womBundle = bundleChk match {; case Left(errors) => throw new Exception(s""""""|WOM validation errors: ; | ${errors} ; |"""""".stripMargin); case Right(bundle) => bundle; }; val task: Option[CallableTaskDefinition] = bundle.primaryCallable match {; case Some(task : CallableTaskDefinition) => Some(task); case Some(exec : ExecutableTaskDefinition) => Some(exec.callableTaskDefinition); case _ => None; }. }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4709:1758,validat,validation,1758,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4709,1,['validat'],['validation']
Security," first task cannot be 'linked' or 'copied'. This cause the workflow to fail. The interesting part is that in the input folder of the second task there are two subfolders: 1 is empty named as `13016223` and the other is not accessible `-1976550098`. The workflow to run needs installed:; `cutadapt` and the script named `moveBarcodeToID.pl` that can be downloaded from here:. https://drive.google.com/open?id=1AizxTwjOEhL5XA7rsx-wbY97p0duB1nw. input fastq files can be retrieved here (they are small ~10000 reads each):. https://drive.google.com/file/d/1-c14Tja4zY3lyr6icFWT06stznR_-Zqr/view?usp=sharing; https://drive.google.com/file/d/1oJd_U9MjTllL0_kpNivw8I_LtSyvqpXH/view?usp=sharing. How can I solve this issue and make the workflow running smoothly?. ### Which backend are you running? ; I am running locally the workflow for now (because I am in the first phase of the development). ### Workflow is this:; ```; #workflow validated before running with: wdltool validate example.wdl and womtool validate scMeth_v2.wdl.sh -i scMeth_input_3.json. workflow scMeth {; # information for trimming the cell barcode; File command; Int bases; File input_fastq1; File input_fastq2; String sampleName. # information for trimming the adapters and low quality reads; File file_format; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; call trimCellBarcode {; input:; sampleName=sampleName,; bases=bases,; input_fastq1=input_fastq1,; input_fastq2=input_fastq2,; command=command; }; call trimAdapters {; input:; file_format=file_format,; input_r1 = trimCellBarcode.fastq_debarcoded_R1,; input_r2 = trimCellBarcode.fastq_debarcoded_R2,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; TAG=TAG; }; }. t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:1263,validat,validate,1263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['validat'],['validate']
Security," for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default""; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""project-test1""; }; }; }; }; }; }; ```. I created the service account from https://cloud.google.com/docs/authentication/getting-started and give the role: Project -> Owner. I've downloaded Google Cloud SDK and run these; ```; gcloud auth login juha.wilppu@gmail.com; gcloud auth application-default login; gcloud config set project project-test1; gsutil ls gs://project-test1 // This command works, so authentication is successful.; ```; **project-test1-59b66448c3ab.json**; ```; {; ""type"": ""service_account"",; ""project_id"": ""project-test1"",; ""private_key_id"": ""59b66448c3ab730097135e1dba83b375a6b57ea3"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\n(Omitted)\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""project-service@project-test1.iam.gserviceaccount.com"",; ""client_id"": ""104927211954691424974"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/project-service%40project-test1.iam.gserviceaccount.com""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690:5842,authenticat,authentication,5842,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690,1,['authenticat'],['authentication']
Security," for workflows that contain branches that rejoin (and most do). . ---. @danbills commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334536133). I think this comment is more appropriate here:. Equals is supposed to be bitwise/value-based equality, whereas `eq` indicates reference equality. I don't recall the exact motivation behind using `eq` in the test, maybe @cjllanwarne can comment when he returns on Tuesday. ---. @curoli commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334555964). If you don't like this fix, what do you suggest we do instead? How about we accept this fix now and you can rework the whole thing later, if you want?. This is a show-stopper once you want to create larger (aka real-life) workflows. . For example: if node a has two outputs, which are inputs of b, then b.hashCode calls a.hasCode twice. If b produces two outputs which are inputsof c, then c.hashCode calls b.hashCode twice, and a.hashCode is called four times. If c produces two outputs which are inputs of d, d.hashCode calls c.hashCode twice, b.hashCode four times and a.hashCode eight times, etc. . ---. @curoli commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334557027). `eq` is object equality. `equals` and `==` are the same, and there is no general rule that they need to be component-based (""bitwise""). On the contrary, `equals`/`==` should not be component-based if an object is mutable. Your typical GraphNode is mutable, because it indirectly contains `GraphNodeSetter`. ---. @danbills commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334575875). I suggest we leave this as-is with the understanding that it could be a performance issue down the road. . >rework the whole thing later. This is a specific anti-goal. As I suggested, I would like to discuss w/ Chris when he gets back next week as we introduced the re",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2694:1713,hash,hashCode,1713,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2694,2,['hash'],['hashCode']
Security, greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); 	at cromwell.backend.impl.aws.AwsBatchAsyncBac,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:1608,validat,validatedRuntimeAttributes,1608,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,1,['validat'],['validatedRuntimeAttributes']
Security," if you have any questions; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions"". concurrent-job-limit = 10; # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; ## Warning: If set, Cromwell will run 'check-alive' for every job at this interval; exit-code-timeout-seconds = 360; filesystems {; local {; localization: [; # soft link does not work for docker with --contain. Hard links won't work; # across file systems; ""copy"", ""hard-link"", ""soft-link""; ]; caching {; duplication-strategy: [""copy"", ""hard-link"", ""soft-link""]; hashing-strategy: ""file""; }; }; }. #; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 3; Int requested_memory_mb_per_core = 8000; Int memory_mb = 40000; String? docker; String? partition; String? account; String? IMAGE; """""". submit = """"""; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${out} \; --error=${err} \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # SINGULARITY_CACHEDIR needs to point to a directory accessible by; # the jobs (i.e. not lscratch). Might want to use a workflow local; # cache dir like in run.sh; source /work/share/ac7m4df1o5/bin/cromwell/set_singularity_cachedir.sh; SINGULARITY_CACHEDIR=/work/share/ac7m4df1o5/bin/cromwell/singularity-cache; source /work/share/ac7m4df1o5/bin/cromwell/test.sh ${docker}; echo ""SINGULARITY_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:7173,hash,hashing-strategy,7173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['hash'],['hashing-strategy']
Security," latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there is no `.circici/config.yml` will probably just yell at you for having ""Version 1.0"" or not finding a config.; - You will want to turn on building forked pull requests in the settings; - Under environment variables, define the following:; - `DOCKER_USER` should be the user to authenticate pushing; - `DOCKER_PASS` password for that user (**important** do not turn on also testing of forked pull requests on their branch (different setting from above) as this could compromise these credentials.; - `CONTAINER_NAME` should be something like `broadinstiutute/cromwell-dev`. - The tag will always build the commit id, and then latest. If you want to change this behavior, define `DOCKER_TAG`.; - ensure the branch logic (when things are triggered) is to your liking.; - update the repo badge to be cromwell here and not on vsoch (after you connect the two!). I noticed that there is no sbt version set (in some config file) - would this make sense to do?. ```bash; [warn] No sbt.version set in project/build.properties, base directory: /; [info] Set current project to root (in build file:/); [info] 1.2.1; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015:2277,authenticat,authenticate,2277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015,2,"['authenticat', 'password']","['authenticate', 'password']"
Security," md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers wi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346:2240,hash,hashing,2240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346,2,['hash'],"['hash', 'hashing']"
Security," message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` strategy is implemented using the 64-bit xxhash algorithm. (I didn't make the xxh32 algorithm available. Is there any Cromwell server still running on 32-bit?) This can be set in the call caching configuration.; + A new `fingerprint` strategy suggested by @illusional, which takes the modtime, size and a xxh64 hash of the first 10 mb of the file to create a virtually unique fingerprint.; + The `file` strategy get's a new alias `md5` which is more clear. Although `file` will still work in the config for backwards compatibility. . I feel we should move to xxh64 as default after it has proven itself in a few releases. The speed-up is an order of magnitude.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450:2147,hash,hashtest,2147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450,2,['hash'],"['hash', 'hashtest']"
Security," output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-shell"": ""/bin/sh"",; ""cromwell_id"": ""cromwell-fdcce1"",; ""cromwell_id_random_suffix"": false; }; database: {; ""db"": {; ""driver"": ""com.mysql.cj.jdbc.Driver"",; ""url"": ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false&serverTimezone=UTC"",; ""user"": ""root"",; ""connectionTimeout"": 5000; },; ""profile"": ""slick.jdbc.MySQLProfile$""; }; backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/franklinmichael/janis/cache_test/20200110_090106_f8ee04/janis/execution"",; ""filesystems"": {; ""local"": {; ""caching"": {; ""hashing-strategy"": ""path+modtime""; }; }; }; }; }; }; }; call-caching: {; ""enabled"": true; }; ```; </p></details>. Thanks in advance for your help!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346:4891,hash,hashing-strategy,4891,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346,1,['hash'],['hashing-strategy']
Security, preemptible: true; zones:; - us-central1-b; - us-central1-c; pipelineArgs:; clientId: ''; inputs:; exec: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/exec.sh; input_bam-0: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-SortAndFixReadGroupBam/shard-22/HTC3GCCXX.8.Pond-536132.sorted.bam; ref_dict-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict; ref_fasta-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; ref_fasta_index-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai; logging:; gcsPath: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22.log; outputs:; HTC3GCCXX.8.Pond-536132.validation_report: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/HTC3GCCXX.8.Pond-536132.validation_report; ValidateReadGroupSamFile-22-rc.txt: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22-rc.txt; projectId: broad-gotc-int; resources:; bootDiskSizeGb: 0; disks: []; minimumCpuCores: 0; minimumRamGb: 0; noAddress: false; preemptible: false; zones: []; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/compute; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/compute; runtimeMetadata:; '@type': type.googleapis.com/google.genomics.v1alpha2.RuntimeMetadata; computeEngine:; diskNames:; - local-disk-12146155240789544851; instanceName: ggp-12146155240789544851; machineType: us-central1-b/n1-standard-2;,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1501:4201,Validat,ValidateReadGroupSamFile,4201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501,1,['Validat'],['ValidateReadGroupSamFile']
Security, scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:37); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:47); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendLifecycleActor$class.performActionThenRespond(BackendLifecycleActor.scala:44); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.performActionThenRespond(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.applyOrElse(BackendWorkflowInitializationActor.scala:163); 	at akka.actor.Actor$class.aroundReceiv,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1737:2736,validat,validateRuntimeAttributes,2736,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737,1,['validat'],['validateRuntimeAttributes']
Security, strings should be of the format 'local-disk' or '/mount/point' but got: 'local-disk 100 HDD'; at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4274:1282,validat,validatedRuntimeAttributes,1282,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4274,1,['validat'],['validatedRuntimeAttributes']
Security, such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*R?.fq.gz': No such file or directory; ; 04:26:11; /bin/bash: line 44: /cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa': No such file or directory; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': Permission denied; ; 04:26:11; /bin/bash: line 52: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*.qcstats': No such file or directory; ; 04:26:11; /bin/bash: line 58: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': No such file or directory; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': Permission denied; ; 04:26:11; /bin/bash: line 66: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/cwl.output.json': No such file or directory; ; 04:26:11; /bin/bash: line 72: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': No such file or directory; ; 04:26:11; mv: cannot stat '/cromwell_root/bbmap-rc.txt.tmp': No such file or directory; ; 04:26:11; MIME-Version: 1.0; ; 04:26:11; Content-Type: multipart/alternative; boundary=278185423cec5467d351ab751807c36a; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Content-Type: text/plain; ; 04:26:11; Content-Disposition: attachment; filename=rc.txt; ; 04:26:11; cat: /cromwell_root/bbmap-rc.txt: No such file or directory; ; 04:26:11; --278185423cec5467d351ab751807c36a; ; 04:26:11; Conte,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4542:5954,access,access,5954,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542,2,['access'],['access']
Security," tests using the suggested edits to aws.conf. I then try to specify the output directories using the following `options.json` file. ```; {; ""final_workflow_outputs_dir"": ""s3://bucket/cromwell/outputs"",; ""final_call_logs_dir"": ""s3:/bucket/cromwell/call_logs"",; ""final_workflow_log_dir"": ""s3://bucket/cromwell/wf_logs""; }; ```. When running cromwell : `java -Dconfig.file=awsbatch/aws.conf -jar cromwell-36.jar run awsbatch/hello.wdl -i awsbatch/hello.inputs -o options.json`. it results with the error:. ```; [2019-01-12 00:31:03,94] [info] $a [866d19d0]: Copying workflow logs from /rcecloud/kmavrommatis/workspace/Workflows/cromwell/cromwell-workflow-logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log to s3://celgene-rnd-riku-researchanalytics/cromwell/wf_logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log; [2019-01-12 00:31:04,03] [error] Key cannot be empty; java.lang.IllegalArgumentException: Key cannot be empty; 	at software.amazon.awssdk.core.util.ValidationUtils.assertStringNotEmpty(ValidationUtils.java:111); 	at software.amazon.awssdk.core.runtime.transform.PathMarshallers$GreedyPathMarshaller.marshall(PathMarshallers.java:109); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:87); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:31); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:88); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.headObject(DefaultS3Client.java:1628); 	at org.lerch.s3fs.util.S3Utils.getS3ObjectSummary(S3Utils.java:47); 	at org.lerch.s3fs.S3FileSystemProvider.exists(S3FileSystemProvider.java:624); 	at org.lerch.s3fs.S3FileSystemProvider.checkAccess(S3FileSystemP",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4541:1074,Validat,ValidationUtils,1074,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4541,1,['Validat'],['ValidationUtils']
Security," will be able to utilize results from all calls that are in that database.""_. Secondly, if this can. **Question 2.**. Can call caching be initiated if a scatter, wraps a workflow, which then wraps tools.; Or will the entire workflow need to be in one script? (I have attached an example as zip); And, the options file.; [DsTrim - Broken.zip](https://github.com/broadinstitute/cromwell/files/3842334/DsTrim.-.Broken.zip). **Question 3.**. What exactly triggers callcaching to change from ""CallCachingOff"" to on, in the following result?; `; ""callCaching"": {; ""effectiveCallCachingMode"": ""CallCachingOff"",; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss""; },`. **If the in-memory is the issue, then please close and we will set-up a UAT correctly.; If not any additional assistance or comments will be most apprecitated.** . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5280:2894,PASSWORD,PASSWORDS,2894,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280,1,['PASSWORD'],['PASSWORDS']
Security," |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:File R2Fastq|""9f1cf8859a902eb75202a5c048cd43aa-388""|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:File R1Fastq|""62396abd6b589747ee16034888c9a0b5-381""|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:File R1Fastq|""62396abd6b589747ee16034888c9a0b5-381""|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input count|9BF31C7FF062936A96D3C8BD1F8F2FF3|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input count|9BF31C7FF062936A96D3C8BD1F8F2FF3|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:command template|7BCEDB02C5FC300FF83F07417B49229E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:command template|7BCEDB02C5FC300FF83F07417B49229E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:backend name|2267EF43AEF6BB551F414FEC2390F68A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:backend name|2267EF43AEF6BB551F414FEC2390F68A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:effectiveCallCachingMode|ReadAndWriteCache|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:effectiveCallCachingMode|ReadAndWriteCache|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:allowResultReuse|true|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:allowResultReuse|true|. <!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:8067,PASSWORD,PASSWORDS,8067,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['PASSWORD'],['PASSWORDS']
Security, |WORKFLOW_EXECUTION_UUID|METADATA_KEY|METADATA_VALUE|; |-----------------------|------------|--------------|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:result|Cache Miss|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:result|Cache Miss|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hit|false|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hit|false|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:failOnStderr|68934A3E9455FA72420237EB05902327|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:runtime attribute:failOnStderr|68934A3E9455FA72420237EB05902327|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:docker|4AD3C387725244C1348F252B031B956D|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCachin,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:1640,hash,hashes,1640,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,5,['hash'],['hashes']
Security," },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads used for amber step\\n\"",\n \""id\"": \""#threads_amber\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads to run cobalt command\\n\"",\n \""id\"": \""#threads_cobalt\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads to use - set to 8 by default\"",\n \""id\"": \""#threads_gridss\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads\\n\"",\n \""id\"": \""#threads_purple\""\n },\n {\n \""type\"": \""File\"",\n \""doc\"": \""tumour BAM file\\n\"",\n \""secondaryFiles\"": [\n \"".bai\""\n ],\n \""id\"": \""#tumor_bam\""\n },\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""sample name of tumor. Must match the somatic snvvcf sample name. (Default: \\\\${sample}_T)\\n\"",\n \""id\"": \""#tumor_sample\""\n },\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""htsjdk SAM/BAM validation level (STRICT (default), LENIENT, or SILENT)\\n\"",\n \""default\"": \""STRICT\"",\n \""id\"": \""#validation_stringency\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""optional - list of known viral hosts - Refseq_id,Virus_name = viral_host_ref.csv\\n\"",\n \""id\"": \""#viral_hosts_file_linx\""\n },\n {\n \""type\"": [\n \""null\"",\n \""boolean\""\n ],\n \""doc\"": \""Write output to for generation of Circos clustering and chaining plots\\n\"",\n \""id\"": \""#write_vis_data_linx\""\n }\n ],\n \""steps\"": [\n {\n \""in\"": [\n {\n \""source\"": \""#bafsnps_amber\"",\n \""id\"": \""#amber_step/loci\""\n },\n {\n \""source\"": \""#max_depth_percent_amber\"",\n \""id\"": \""#amber_step/max_depth_percent\""\n },\n {\n \""source\"": \""#max_het_af_percent_amber\"",\n \""id\"": \""#amber_step/max_het_af_percent\""\n },\n {\n \""source\"": \""#min_base_quality_amber\"",\n \""id\"": \""#amber_step/min_base_quality\""\n },\n {\n \""source\"": \""#min_depth_percent_amber\"",\n \""id\"": \""#amber_step/min_depth_percent\""\n },\n {\n \""source\"": \""#min_het_af_percent_amber\"",\n \""id\"": \""#amber",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:115146,validat,validation,115146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['validat'],['validation']
Security," }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""service-account""; # Google project which will be billed for the requests; project = ""***-***"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }. default-runtime-attributes {; cpu: 2; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; # Allowed to be a String, or a list of Strings; disks: ""local-disk 10 SSD""; noAddress: false; preemptible: 0; zones: [""eu-west4-a"",""eu-west4-b"",""eu-west4-c""]; }. include ""papi_v2_reference_image_manifest.conf""; }; }; }; }; ```. Other info:; Debian GNU/Linux 10 (buster); openjdk version ""11.0.9.1-internal"" 2020-11-04 (through MiniConda, also tried with openjdk version ""11.0.12"" 2021-07-20, no difference to failure message). Permissions for service-account (quite liberal); ![image](https://user-images.githubusercontent.com/36060453/129350599-b68eee59-f08b-458f-b164-c48210b140de.png)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:15282,access,accessible,15282,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['access'],['accessible']
Security,"! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->; [2022-03-03 19:26:59,66] [info] WorkflowManagerActor: Workflow 496206d8-8854-48c1-abed-3717510ceb4e failed (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://mys3-cloudformation/cromwell-execution/wf_hello/496206d8-8854-48c1-abed-3717510ceb4e/call-hello/hello-rc.txt: s3://s3.amazonaws.com/mys3-cloudformation/cromwell-execution/wf_hello/496206d8-8854-48c1-abed-3717510ceb4e/call-hello/hello-rc.txt; Caused by: java.io.IOException: Could not read from s3://mys3-cloudformation/cromwell-execution/wf_hello/496206d8-8854-48c1-abed-3717510ceb4e/call-hello/hello-rc.txt: s3://s3.amazonaws.com/mys3-cloudformation/cromwell-execution/wf_hello/496206d8-8854-48c1-abed-3717510ceb4e/call-hello/hello-rc.txt. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->; ![image](https://user-images.githubusercontent.com/96741804/156643007-76a24c99-509c-4480-8484-df1c6f7b9c72.png). <!-- Which backend are you running? -->. AWS Batch. <!-- Paste/Attach your workflow if possible: -->. I have see this as an issue previously reported ; I am trying to set up a genomics work flow using AWS batch and Cromwell . How to solve this issue; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6695:2183,PASSWORD,PASSWORDS,2183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6695,1,['PASSWORD'],['PASSWORDS']
Security,""" HDD\""\n memory: \""3500 MB\""\n }\n output {\n File report = \""${report_filename}\""\n }\n}\n\nworkflow BamToUnmappedBams {\n File input_bam\n String dir_pattern = \""gs://.*/\""\n #String dir_pattern = \""/.*/\""\n Int revert_sam_disk_size = 400\n Int sort_sam_disk_size = 400\n Int validate_sam_file_disk_size = 200\n\n call RevertSam {\n input:\n input_bam = input_bam,\n revert_bam_name = sub(sub(input_bam, dir_pattern, \""\""), \"".bam$\"", \""\"") + \"".unmapped.bam\"",\n disk_size = revert_sam_disk_size\n }\n\n# call SortSam {\n# input:\n# input_bam = RevertSam.unmapped_bam,\n# sorted_bam_name = sub(sub(RevertSam.unmapped_bam, dir_pattern, \""\""), \"".bam$\"", \""\"") + \"".sorted.bam\"",\n# disk_size = sort_sam_disk_size\n# }\n\n call ValidateSamFile {\n input:\n input_bam = RevertSam.unmapped_bam,\n report_filename = sub(sub(RevertSam.unmapped_bam, dir_pattern, \""\""), \"".unmapped.bam$\"", \""\"") + \"".validation_report\"",\n disk_size = validate_sam_file_disk_size\n }\n\n output {\n RevertSam.*\n ValidateSamFile.*\n }\n}"",; ""options"": ""{\n \""default_runtime_attributes\"": {\n \""zones\"": \""us-central1-b us-central1-c us-central1-f\""\n },\n \""google_project\"": \""engle-macarthur-ccdd\"",\n \""auth_bucket\"": \""gs://cromwell-auth-engle-macarthur-ccdd\"",\n \""refresh_token\"": \""cleared\"",\n \""final_workflow_log_dir\"": \""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/c7af7e06-a435-44ec-8466-124ad8e1bcaf/workflow.logs\"",\n \""account_name\"": \""kcibul@broadinstitute.org\"",\n \""jes_gcs_root\"": \""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/c7af7e06-a435-44ec-8466-124ad8e1bcaf\""\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""a714b11b-0162-4585-afa5-abbd7433af51"",; ""inputs"": {; ""BamToUnmappedBams.input_bam"": ""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/batch04/S64-2_Illumina.bam""; },; ""submission"": ""2017-01-19T18:17:12.188Z"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Google credentials are invalid: connect timed out""; }],; ""workflowLog"": ""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/c7af7e0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1886:3396,Validat,ValidateSamFile,3396,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886,1,['Validat'],['ValidateSamFile']
Security,""")). # Note: If you spot a mistake in this configuration sample, please let us know by making an issue at:; # https://github.com/broadinstitute/cromwell/issues. call-caching {; enabled = false; }. backend {; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = true; runtime-attributes = ""String? docker Int? max_runtime = 2""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". # Root directory where Cromwell writes job results. This directory must be; # visible and writeable by the Cromwell process as well as the jobs that Cromwell; # launches.; root: ""cromwell-executions"". filesystems {; local {; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]. caching {; duplication-strategy: [; ""soft-link""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; hashing-strategy: ""path"". # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.; # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.; check-sibling-md5: false; }; }; }; }; }; }; }. database {; db.url = ""jdbc:mysql://mysql-db/cromwell_db?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true""; db.user = ""cromwell""; db.password = ""cromwell""; db.driver = ""com.mysql.cj.jdbc.Driver""; profile = ""slick.jdbc.MySQLProfile$""; db.connectionTimeout = 15000; }; ```. and here is my cormwell dockerfile:. ```; FROM broadinstitute/cromwell:develop. RUN git clone https://github.com/vishnubob/wait-for-it.git; RUN mkdir cromwell-working-dir; WORKDIR cromwell-working-dir. COP",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7006:1827,hash,hash,1827,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7006,1,['hash'],['hash']
Security,"""; ## funcotator_excluded_fields: Annotations that should not appear in the output (VCF or MAF). Specified as <ANNOTATION>. For example: ""ClinVar_ALLELEID""; ## funco_filter_funcotations: If true, will only annotate variants that have passed filtering (. or PASS value in the FILTER column). If false, will annotate all variants in the input file. Default: true; ## funcotator_extra_args: Any additional arguments to pass to Funcotator. Default: """"; ##; ## Outputs :; ## - One VCF file and its index with primary filtering applied; secondary filtering and functional annotation if requested; a bamout.bam; ## file of reassembled reads if requested; ##; ## Cromwell version support; ## - Successfully tested on v34; ##; ## LICENSING :; ## This script is released under the WDL source code license (BSD-3) (see LICENSE in; ## https://github.com/broadinstitute/wdl). Note however that the programs it calls may; ## be subject to different licenses. Users are responsible for checking that they are; ## authorized to run all programs before running this script. Please see the docker; ## pages at https://hub.docker.com/r/broadinstitute/* for detailed licensing information; ## pertaining to the included programs. struct Runtime {; String gatk_docker; File? gatk_override; Int max_retries; Int preemptible; Int cpu; Int machine_mem; Int command_mem; Int disk; Int boot_disk_size; }. workflow Mutect2 {; input {; # Mutect2 inputs; File? intervals; File ref_fasta; File ref_fai; File ref_dict; File file_tumor_reads; Array[File] all_tumor_reads = read_lines(file_tumor_reads); File file_tumor_reads_indexes; Array[File] all_tumor_reads_indexes = read_lines(file_tumor_reads_indexes); Array[Pair[File,File]] tumor_reads_and_indexes = zip(all_tumor_reads,all_tumor_reads_indexes); File? normal_reads; File? normal_reads_index; File? pon; File? pon_idx; Int scatter_count; File? gnomad; File? gnomad_idx; File? variants_for_contamination; File? variants_for_contamination_idx; File? realignment_index_bundle; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:6266,authoriz,authorized,6266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,1,['authoriz'],['authorized']
Security,"""Pipeline"" Scopes are added only for ""Pipeline"" Credentials.; Otherwise scopes must be requested when asking for credentials.; `Credential` generator (vs. `Credentials`, the former an older API) still returns an unscoped Credential.; Renamed methods returning Credentials from `credential` to `credentials`.; Now also validating USA Credentials before returning.; Credentials lookups from workflow options are only done for ""Pipeline"" creds and tests.; Removed a `validate(WorkflowOptions)` that wasn't in use since commit 6fbeadc.; Removed scope declarations no longer in use.; Using scope-constants as-much-as-possible from the Google SDKs.; Added an `unsafe` to replace `toTry.get`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4054:318,validat,validating,318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4054,2,['validat'],"['validate', 'validating']"
Security,"""Requester pays bucket access requires authentication"" when deploy cloud function",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5311:23,access,access,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5311,2,"['access', 'authenticat']","['access', 'authentication']"
Security,"# Introduction. The essence of a presigned URL is that it gives you privileged access to data (via HTTP verbs, usually `GET`) for a finite amount of time. Some metadata can be obtained via the `HEAD` verb. DOS URI's can be resolved to presigned URLs, and it's not immediately obvious how to provide the info Cromwell needs to do its job. Hence this document. The essence of this question is how do we leverage HTTP. # Information Needed for Cromwell to work. 1. The data itself, i.e. the file to which the URL refers.; 1. Size Metadata; 1. Hash Metadata; 1. Byte-level access (needed for things like WDL's `read_lines`). ## Information Provided by OpenDJ / Martha as of 6/25/18. * Size ; * MD5 Hash; * Presigned URL . ## Information provided by HTTP (in theory). * Metadata/ETag via `HEAD`; * Byte-level access via `RANGE` header on GET; * Full data of resource. ## Information *Not* Provided by OpenDJ/Martha as of 6/25/18. * Byte-level access; * CRC32 Hash. # Outstanding questions (please comment if you have info). 1. What metadata can be obtained via `HEAD`?; 1. Is the `HEAD` metadata a standard, and do all clouds implement that standard? (I think ETag is common name for this info.); 1. How does call-caching work with an expiration date on the URL?; 1. Byte-level access: HTTP request to the data can be limited to a range via [`Range` header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests). Do clouds support this feature? Are there other ways of achieving this requirement?; 1. Write access: WDL supports `write_lines`, which AFAIK is only possible via `PATCH` ; 1. Can Cromwell use any hash besides CRC32? If not how do we obtain CRC32 reliably?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3817:79,access,access,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817,10,"['Hash', 'access', 'hash']","['Hash', 'access', 'hash']"
Security,"# Proxy Support in Cromwell. Are proxies supported in Cromwell? . Cromwell fails to use remote hashing behind my corporate proxy. However, I can get this to work outside the proxy. While behind the proxy, Cromwell can still pull Docker images and local hashing works as these rely on the Docker daemon which does utilize my proxies. ## Exceptions. These are the exceptions that Cromwell is raising behind the proxy:; ```; Request method=GET uri=https://auth.docker.io/token?service=registry.docker.io&scope=repository%3Alibrary/debian%3Apull headers= threw an exception on attempt #4. Giving up.; java.net.ConnectException: Failed to connect to endpoint: RequestKey(Scheme(https),auth.docker.io); ```; ```; Docker lookup failed; java.lang.Exception: Failed to get docker hash for debian@sha256:75f7d0590b45561bfa443abad0b3e0f86e2811b1fc176f786cd30eb078d1846f Failed to connect to endpoint: RequestKey(Scheme(https),auth.docker.io); ```. ## Troubleshooting. I set my JVM proxies using the bash command ; ``` ; export _JAVA_OPTIONS='-Dhttp.proxyHost=proxy.myproxy.com -Dhttp.proxyPort=myport -Dhttps.proxyHost=proxy.myproxy.com -Dhttps.proxyPort=myport'; ```. And at the top of my cromwell logs I can see ; ```; Picked up _JAVA_OPTIONS: -Dhttp.proxyHost=proxy.myproxy.com -Dhttp.proxyPort=myport -Dhttps.proxyHost=proxy.myproxy.com -Dhttps.proxyPort=myport; ```. Of note, I have tried with and without the digest tag. Both are unsuccessful. . ## Cromwell Version. I am using cromwell 41.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5006:95,hash,hashing,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5006,3,['hash'],"['hash', 'hashing']"
Security,"## About this PR; 📦 Updates ; * [ch.qos.logback:logback-access](https://github.com/qos-ch/logback); * [ch.qos.logback:logback-classic](https://github.com/qos-ch/logback); * [ch.qos.logback:logback-core](https://github.com/qos-ch/logback). from `1.2.11` to `1.2.12`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""ch.qos.logback"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""ch.qos.logback"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7260:56,access,access,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7260,1,['access'],['access']
Security,"## About this PR; 📦 Updates [com.eed3si9n:sbt-assembly](https://github.com/sbt/sbt-assembly) from `1.1.1` to `2.1.5` ⚠. 📜 [GitHub Release Notes](https://github.com/sbt/sbt-assembly/releases/tag/v2.1.5) - [Version Diff](https://github.com/sbt/sbt-assembly/compare/v1.1.1...v2.1.5). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (1.1.1).; You might want to review and update them manually.; ```; womtool/src/test/resources/validate/wdl_draft3/valid/arrays_v1/arrays_v1.inputs.json; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" }; }]; ```; </details>. <sup>; labels: sbt-plugin-update, early-semver-major, semver-spec-major, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7271:1024,validat,validate,1024,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7271,1,['validat'],['validate']
Security,"## About this PR; 📦 Updates [org.glassfish.jersey.inject:jersey-hk2](https://github.com/eclipse-ee4j/jersey) from `2.32` to `2.41`. 📜 [GitHub Release Notes](https://github.com/eclipse-ee4j/jersey/releases/tag/2.41) - [Version Diff](https://github.com/eclipse-ee4j/jersey/compare/2.32...2.41). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (2.32).; You might want to review and update them manually.; ```; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.glassfish.jersey.inject"", artifactId = ""jersey-hk2"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.glassfish.jersey.inject"", artifactId = ""jersey-hk2"" }; }]; ```; </details>. <sup>; labels: library-update, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7308:50,inject,inject,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7308,3,['inject'],['inject']
Security,"## Call-caching problems with path+modtime; I have been doing some call-caching benchmarking on the [BioWDL RNA-seq](https://github.com/biowdl/RNA-seq) pipeline and it turns out any `path` or `path+modtime` strategies do not work with containers. As is reported in these issues: #5405, #5370, #5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450:587,hash,hash,587,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450,3,"['hash', 'integrity']","['hash', 'hashes', 'integrity']"
Security,"## Discussion \#1; ```; bshifaw [3:59 PM]; Hi Chris, ; The featured joint calling method is using NIO.; https://portal.firecloud.org/#methods/gatk/joint-discovery-gatk4/9/wdl; Is this the method you are referencing? (edited). bshifaw [4:28 PM]; @vdauwera, just confirmed with @jsoto. The wdl isn’t using NIO when importing the GVCFs. Due to a change in the wdl we decide to implement to best leverage the FC data model (using an array of input files instead of a sample name map file). (edited). Collapse; cwhelan [9:48 PM]; right, that’s the method i was using. vdauwera [11:22 PM]; oooh that’s an interesting case that would benefit from the flexible data models work — this would be great to show @andreah; ```. ## Discussion \#2. ```; cwhelan [11:17 AM]; ie it’s trying to localize each gvcf to each shard instance. tjeandet [11:17 AM]; do you have an idea of how many input files each shard has ?. Collapse; cwhelan [11:17 AM]; 555 samples; ```. # Takeaways. Run https://portal.firecloud.org/#methods/gatk/joint-discovery-gatk4/9/wdl in a non-production environment w/ 555 samples and try to reproduce issue w/ hashing timeouts. We predict they will not occur as cromwell production was seeing elevated CPU usage due to it's /stats endpoint being hit repeatedly.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3712:1116,hash,hashing,1116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3712,1,['hash'],['hashing']
Security,"## Motivation. A significant limitation of using the Google Backend on Cromwell today is that [only N* machine types](https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/util/GcpBatchMachineConstraints.scala#L29-L32) can be used on GCP. In particular, N* machine types do not provide access to modern GPUs and users interested in running workflows on GPUs are limited to older NVIDIA T4 or V100 accelerators. ## Proposal. Add support for standard machine types, which would allow running workflows on a much broader range of machine types on GCP, including those configured with modern GPUs (e.g.: NVIDIA A100, H100). Related: https://github.com/broadinstitute/cromwell/issues/6558",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7535:364,access,access,364,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7535,1,['access'],['access']
Security,"## Today. Cromwell workers are deployed as a Terraform `instance,` most likely w/ a `count` of 1.*. ## What we want. ### # of instances. The `instance` stanza should reflect a minimum # of instances = `1`, max=`4`. The 1 represents the normal case where we want 1 Cromwell running, and only scales up when necessary. . ### Autoscaling. Should be set to `CPU Usage` where target == `50%`. Add an additional `group` stanza that creates a GCP instance group and refers to the Cromwell workers.*. *Raph Luckom made this presumption, the author does not have access to verify.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4798:554,access,access,554,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4798,1,['access'],['access']
Security,"## basic issue; `womtool validate` does not catch all scenarios where you are defining a new variable based on an optional variable. Instead, Cromwell fails at runtime -- even if it is actually impossible for that optional variable to be undefined. [A working example is available](https://github.com/aofarrel/myco/commit/e7f9ba6951d1b0fe5b3c1a650835312dd2b6e68f), but it is a complex WDL, so a more basic example is listed below. ## background; WDL doesn't really have a proper understanding of mutual exclusivity, so it doesn't realize that anything under a ""is optional variable X defined?"" block can only happen if optional variable X is defined. In other words, if variant_caller.errorcode has type Array[String?], the following code block is invalid, and womtool correctly flags it as such:. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = variant_caller.errorcode; }; ```. > Failed to process declaration 'Array[String] varcall_error_if_earlyQC_filtered = variant_call_after_earlyQC_filtering.errorcode' (reason 1 of 1): Cannot coerce expression of type 'Array[String?]' to 'Array[String]'. The normal workaround for this is to use select_first() with a bogus fallback value, since the `defined` check means that fallback value will never be selected. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = select_first([variant_caller.errorcode, [""according to all known laws of aviation""]]); }; ```. The same holds true if I only care about the first (index 0) variable in the array. That's the case for me, since the actual workflow I'm working on will be run on Terra data tables, eg each instance of the workflow only gets one sample but dozens of instances of the workflow will be created. For compatibility reasons I cannot convert the variant caller into a non-scattered task, so its error code will still have type Array[String]? even though that array will only have one value. ```; if(defined(variant_caller.er",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7194:25,validat,validate,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194,1,['validat'],['validate']
Security,"### Description. <!-- What is the purpose of this change? What should reviewers know? -->; This change adds AWS ECR remote hashing support, which helps fix call caching for jobs using AWS ECR hosted containers. This change supports both private and public ECR registies. There were a couple non-standard behaviours from ECR which have been mitigated in this PR:; - Private ECR requires Basic authentication. The ability to override the authorization scheme for a particular registry was added.; - ECR does not return a `Docker-Content-Digest` header, so a fallback to calculate the image digest from the response body has been added.; - ECR supports images with no repository e.g. `123456790.dkr.ecr.eu-west-2.amazonaws.com/foo`. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7444:123,hash,hashing,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7444,3,"['authenticat', 'authoriz', 'hash']","['authentication', 'authorization', 'hashing']"
Security,"### Description. As part of preparing for the fall 2024 audit, we were asked to fix the permissions on the various healthcheck buckets such as `gs://cromwell-ping-me-dev`. It would have taken some tinkering to make sure the permissions are secure enough _and_ the healthcheck still works, so I decided to drop the healthcheck. I am not aware of any times it's helped us and doesn't pass the ""would we add this today"" test. The only notable bucket we'd want to make sure Cromwell itself has permissions on is the workflow archiver - and it uses [a separate service account from the rest of Cromwell](https://github.com/broadinstitute/terra-helmfile/blob/master/charts/cromwell/templates/config/_cromwell.conf.tpl#L267-L271), so it's not a valid test. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7533:56,audit,audit,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7533,2,"['audit', 'secur']","['audit', 'secure']"
Security,"### Description. Fixes CI failures caused by errors like:; ```; $ docker pull quay.io/broadinstitute/cromwell-docker-test:centaur; centaur: Pulling from broadinstitute/cromwell-docker-test; [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of quay.io/broadinstitute/cromwell-docker-test:centaur to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/; ```. The very old images need to be updated to get around this. For Python, we can use a newer version. For the Quay image we manage, using a different one because I fear push access to the current one has been lost in the mists of time. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [X] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [X] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7450:755,access,access,755,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7450,1,['access'],['access']
Security,"### Description. UPDATE: issues with special characters in passwords appear to be resolved. PR to demo broken private Docker repo support in GCP Batch. There are actually multiple existing PAPI v2 Centaur tests in this vein; the one test enabled here for GCP Batch seems to be the simplest and demonstrates the issues clearly enough. The crux of this test is that the Docker image that is specified for the task is in a private repo to which the Centaur service account has been granted access. This test passes on PAPI v2 but on GCP Batch jobs fail with messages like the following visible in `gcloud batch jobs describe`:. ```; Job state is set from RUNNING to FAILED for job projects/1005074806481/locations/us-central1/jobs/job-27607753-d2d5-404d-89af-a786da8ad383.Job; failed due to task failure. Specifically, task with index 0 failed due to the; following task event: ""Task state is updated from RUNNING to FAILED on zones/us-central1-b/instances/8098872438472929780; with exit code 125."". ```. Exit code 125 being a typical ""[something's wrong with that Docker invocation](https://stackoverflow.com/questions/53640424/exit-code-125-from-docker-when-trying-to-run-container-programmatically)"" error. in Cloud Logging I see the following, including what looks like a plaintext password which I have x'd out below:. ```; Executing runnable container:{image_uri:""broadinstitute/cloud-cromwell@sha256:0d51f90e1dd6a449d4587004c945e43f2a7bbf615151308cff40c15998cc3ad4"" commands:""/mnt/disks/cromwell_root/script"" entrypoint:""/bin/bash"" volumes:""/mnt/disks/cromwell_root:/mnt/disks/cromwell_root"" username:""firecloud"" password:""xxxxx""} labels:{key:""tag"" value:""UserRunnable""} for Task task/job-27607753-d2d5-132dc052-df92-4db100-group0-0/0/0 in TaskGroup group0 of Job job-27607753-d2d5-132dc052-df92-4db100.; ```. So it looks like the GCP Batch backend has acquired and plumbed through the required Docker credentials, but the login to Docker Hub doesn't seem to have happened. ### Release Notes Confi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515:59,password,passwords,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515,2,"['access', 'password']","['access', 'passwords']"
Security,"### Description. We need to propagate the Google credentials while pulling metadata from private GCR repositories. This is likely fixes #7356. Before this change, we'd get a log error when cromwell tries pulling the metadata, this occurs because `GoogleRegistry` implementation does not have a valid auth token:. ```; [2024-06-28 01:14:19,56] [info] Assigned new job execution tokens to the following groups: 5fe16e0e: 1; [2024-06-28 01:14:20,38] [warn] BackendPreparationActor_for_5fe16e0e:myWorkflow.myTask:-1:1 [5fe16e0e]: Docker lookup failed; java.lang.Exception: Failed to get docker hash for gcr.io/<REDACTED>/debian:latest Request failed with status 403 and body {""errors"":[{""code"":""DENIED"",""message"":""Unauthenticated request. Unauthenticated requests do not have permission \""artifactregistry.repositories.downloadArtifacts\"" on resource \""projects/<REDACTED>/locations/us/repositories/gcr.io\"" (or it may not exist)""}]}; ```. <details>; <summary>An example Workflow.wdl to test this</summary>. ```; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }. runtime {; docker: ""gcr.io/<REDACTED>/debian:latest""; bootDiskSizeGb: 50; preemptible: 0; }; }; ```. </details>. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7464:590,hash,hash,590,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7464,1,['hash'],['hash']
Security,"### Motivation; The Job Manager UI is hoping to support a dashboard view, pictured below. We've heard from multiple users that this view would be very helpful to them. In addition, this capability is of especially high priority for our partners in this, Verily. Verily also needs to make changes to dsub to support this. ![image](https://user-images.githubusercontent.com/9449764/36913061-ae9bb686-1e16-11e8-9d8e-0f30eebc8dd6.png). ### What is it; This may be more information than you need, but I decided to err on providing more info over less. The dashboard view has panels of grouped information, categorized by labels on the jobs. Imagine a user had an owner label and a project label on all of their jobs. The dashboard panels would be pivoted by project and owner, and show probably the first ~5-10 labels that have the most running jobs with that label. These panels would be populated with a header that is the key of the cromwell label, a list of values that match that key that the users have access to, and then a summary of their statuses. . The dashboard will be filterable by other labels, but maybe not at first. A use case example there is filtering the image above by a label `key:value` of `flag:archived`. There is a concept of flagging jobs as archived so you don't see them anymore, as a way to get your failures list down to ""inbox 0"" and say ""I've addressed those jobs, I don't want to see them anymore"". So it's possible a user could want to filter those jobs out of their dashboard view as well. v1 will not have this chart pictured and will not have the left panel of server information. ### Ticket Prioritization Suggestions; 1. I would like to start with a spike/design doc and scoping out the amount of effort it would take to support this in Cromwell before end of Q1. ; 2. This ticket can also represent the implementation if Cromwell wants, which we need by end of May to be able to do the front end work before end of Q2. . ### Current Status; Currently, I think this",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3348:1004,access,access,1004,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3348,1,['access'],['access']
Security,"### What happened. On 10/10/2018, around 11:15 AM, there was a spike in backpressure and 403 copy failures. It was discovered that a user had submitted workflows attempting to access buckets it did not have access to. . ![image](https://user-images.githubusercontent.com/16748522/46764755-59087300-ccab-11e8-9163-afd953710adf.png); Purple line- backpressure; Light green line- 403 copy failures. ### What was done to fix it. The situation was discussed with the user, and once he aborted all his workflows, Cromwell slowly returned to its normal state. The issue was resolved around 1:50 PM. ### Potential causes. The user had reused a WDL from another user, but he didn't have access to their Google Cloud buckets. This workflow contained job that ran 5000 split intervals against dataset of approx 1300 samples. Each of the 5000 outputs would be copied, per workflow, per sample. Depending on the number of samples the other user had previously run, each interval-output-for-each-sample tried call caching to other user's workspace. This resulted in a lot of attempts to copy files and then failures.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4229:176,access,access,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4229,3,['access'],['access']
Security,"#### Background. For background information see [this design doc](https://docs.google.com/document/d/1QqXuURg1HlwAymaQkwCL6v9HA5NWsp1pbribvGafNt0/edit?ts=5c6c875c#heading=h.56f418pyjwq8). #### Concept. * Allow Cromwell to spin up PAPIv2 jobs on high security networks ; * At project-creation time, scripts may create a high-security network for that project and record the network name in the project metadata.; * If these fields exist, Cromwell should honor them. #### Proposal. * Use a key in Cromwell's configuration to locate the appropriate project metadata; * For example, perhaps: ; ```; backend {; providers {; PAPIv2 {; config {; backend.providers.PAPIv2.config.vpc {; name: ""terra-network""; subnetwork: ""terra-subnetwork""; }; }; }; }; }; ```; * When about to submit a job to PAPI, see whether the `name` field exists in the configuration.; * If so, check whether the specified label exists in the Google project, eg:; ```; $ gcloud projects describe my-fc-project. createTime: '2017-07-07T17:07:10.345Z'; labels:; terra-network: firecloud; terra-subnetwork: firecloud; lifecycleState: ACTIVE; name: my-fc-project; ```; * If so, deduce a `NETWORK_PATH` by combining the `GOOGLE_PROJECT_ID` (from workflow options) and `NETWORK_NAME` (the label value) as: `projects/GOOGLE_PROJECT_ID/global/networks/NETWORK_NAME`; * Include this in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; }; ```; * If both the `name` and `subnetwork` fields are defined in configuration, and both exist as project labels:; * The `SUBNETWORK` is the raw value of the `subnetwork` label in the google project; * We additionally provide the subnetwork in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; subnetwork: SUBNETWORK; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4806:250,secur,security,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4806,2,['secur'],['security']
Security,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. ![Screen Shot 2021-12-01 at 4 39 47 PM](https://user-images.githubusercontent.com/4966343/144191887-75590326-1edb-442d-b2eb-ffb04968a964.png); <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; Local. <!-- Paste/Attach your workflow if possible: -->; WholeGenomeGermlineSingleSample_develop 3.0.0; https://github.com/broadinstitute/warp/releases/tag/WholeGenomeGermlineSingleSample_develop. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; [cromwell.conf.zip](https://github.com/broadinstitute/cromwell/files/7631795/cromwell.conf.zip). $ java -jar -Dconfig.file=cromwell.conf cromwell-71.jar server; $ curl -X POST --header ""Accept: application/json"" -v ""0.0.0.0:8000/api/workflows/v1"" -F ""workflowSource=@WholeGenomeGermlineSingleSample_develop.wdl"" -F ""workflowInputs=@WholeGenomeGermlineSingleSample_develop.inputs.local.json"" -F ""workflowDependencies=@WholeGenomeGermlineSingleSample_develop.zip""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6582:1398,PASSWORD,PASSWORDS,1398,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6582,1,['PASSWORD'],['PASSWORDS']
Security,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--. Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; We want to submit workflow pipelines on GCP on specified Custom machine type, such as E2, N1, N2, n1-standard-8, etc. can you please support that?; Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6217:1108,PASSWORD,PASSWORDS,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6217,1,['PASSWORD'],['PASSWORDS']
Security,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6459:1108,PASSWORD,PASSWORDS,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6459,5,['PASSWORD'],['PASSWORDS']
Security,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hi, ; Can I ask a question? I think it's related with ; https://github.com/broadinstitute/cromwell/issues/4212. I found the `job_name` is the UUID and it's assigned with subworkflow's UUID if there is a subworflow. What I would like to ask is if there is any other system variables that store the main UUID. As we have our own backend implementation, we need to pass the main UUID to the backend. . Thanks, ; Seung",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6005:1108,PASSWORD,PASSWORDS,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6005,1,['PASSWORD'],['PASSWORDS']
Security,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. hello, ; Support for wdl step-by-step?; After executing a step, wait for the command to execute the next step.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5537:1108,PASSWORD,PASSWORDS,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5537,1,['PASSWORD'],['PASSWORDS']
Security,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Cromwell version is 55; We had submitted workflow pipelines on GCP PAPIv2 (https://genomics.googleapis.com/), in WDL file, set cpu: ""4"", memory: ""48 GB"", the actual VM created as custom (8 vCPUs, 48 GB memory), for ""memory"": ""64 GB"" ""cpu"": ""8"", the actual VM created as (10 vCPU, 64 GB), Cromwell did not created VM as configured in WDL. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6216:1108,PASSWORD,PASSWORDS,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6216,1,['PASSWORD'],['PASSWORDS']
Security,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. endpoint-url = ""https://genomics.googleapis.com/""; Cromwell version 55. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; All job submissions stopped working today with errors:; Unable to complete PAPI request due to system or connection error (PipelinesApiRequestHandler actor termination caught by manager)"". Error messages from Cromwell logs:; cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anon$1: A batch of PAPI status requests failed. The request manager will retry automatically up to 10 times. The error was: 404 Not Found; POST https://genomics.googleapis.com/batch; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 404 (Not Found)!!1</title>; ...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6203:1180,PASSWORD,PASSWORDS,1180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6203,1,['PASSWORD'],['PASSWORDS']
Security,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; The backend the workflow pipelines is https://genomics.googleapis.com/. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Error message: ; The job was stopped before the command finished. PAPI error code 14. Execution failed: worker was terminated. The job was running on non-preemptible VM, with one instance of nvidia-tesla-t4 attached, nvidiaDriverVersion: 418.40.04. . What does ""PAPI error code 14"" mean? Can you suggest what we should do with it?. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6306:1180,PASSWORD,PASSWORDS,1180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6306,1,['PASSWORD'],['PASSWORDS']
Security,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. Cromwell lacks support for Shared VPC setup in GCP. Shared VPC model is quite common to large enterprises. Searching for the support I came across the pull request https://github.com/broadinstitute/cromwell/pull/6225 The code changes in this pull request seems to address the shared vpc support. What are the plans to get this on the roadmap for upcoming versions?. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. GCP. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6443:1479,PASSWORD,PASSWORDS,1479,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6443,1,['PASSWORD'],['PASSWORDS']
Security,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. Hi there,; I'm running Cromwell on a SLURM compute node, so I have enough RAM for the workflow database. Cromwell is used to coordinate this workflow: https://github.com/gatk-workflows/gatk4-somatic-snvs-indels/blob/master/mutect2.wdl on google cloud. When I scancel the SLURM job the google api continues to create VM instances even though the Cromwell job has been killed. How can this be prevented?. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5380:1511,PASSWORD,PASSWORDS,1511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380,1,['PASSWORD'],['PASSWORDS']
Security,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###; The early release of Cromwell had a script that included some steps of executions and the docker run commands. Currently we are using Cromwell release 52, that script or similar script is not found, for reproducible purpose, our users want to know the actual commands, for example, if three runtime attributes are supplied: gpuType, gpuCount and nvidiaDriverVersion, what is the command line of docker run after NVIDIA driver installed?. Thanks!. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5792:1555,PASSWORD,PASSWORDS,1555,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5792,1,['PASSWORD'],['PASSWORDS']
Security,#3984 restored streaming logs. A/C for this ticket would add a regression test for the functionality. One possible centaur implementation for a task:; - Writes to local stdout & stderr; - Calculates the gcloud stdout & stderr path; - Sleep a bit to allow streaming; - Use gsutil to download the stdout & stderr from gcloud; - Validate that the stdout & stderr are as expected,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4187:326,Validat,Validate,326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4187,1,['Validat'],['Validate']
Security,$1.apply(MaterializeWorkflowDescriptorActor.scala:87); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:86); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1(MaterializeWorkflowDescriptorActor.scala:86); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:110); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:109); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:109); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:103); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(Materia,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1406:4277,Validat,ValidationFlatMap,4277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406,1,['Validat'],['ValidationFlatMap']
Security,$1.apply(MaterializeWorkflowDescriptorActor.scala:89); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:86); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1(MaterializeWorkflowDescriptorActor.scala:86); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:110); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:109); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:109); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:103); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(Materia,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/705:3410,Validat,ValidationFlatMap,3410,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/705,2,['Validat'],['ValidationFlatMap']
Security,$1.apply(WorkflowActor.scala:982) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$fetchLocallyQualifiedInputs$1.apply(WorkflowActor.scala:962) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor.fetchLocallyQualifiedInputs(WorkflowActor.scala:962) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor.cromwell$engine$workflow$WorkflowActor$$processRunnableCall(WorkflowActor.scala:1254) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$33.apply(WorkflowActor.scala:868) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$33.apply(WorkflowActor.scala:867) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor.cromwell$engine$workflow$WorkflowActor$$startRunnableCalls(WorkflowActor.scala:867) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$2.applyOrElse(WorkflowActor.scala:532) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$2.applyOrElse(WorkflowActor.scala:467) ~[cromwell.jar:0.19]; at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[cromwell.jar:0.19]; at ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/695:5698,Hash,HashMap,5698,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/695,1,['Hash'],['HashMap']
Security,$PromiseCompletingRunnable.run(Future.scala:24); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.net.SocketTimeoutException: Read timed out; 	at java.net.SocketInputStream.socketRead0(Native Method); 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); 	at java.net.SocketInputStream.read(SocketInputStream.java:171); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	at sun.security.ssl.InputRecord.read(InputRecord.java:503); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); 	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1569); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1474); 	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); 	at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); 	at co,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2229:3736,secur,security,3736,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229,1,['secur'],['security']
Security,'/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:1621,validat,validatedRuntimeAttributes,1621,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,1,['validat'],['validatedRuntimeAttributes']
Security,(HttpURLConnection.java:1441); 	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); 	at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at com.google.cloud.storage.spi.DefaultStorageRpc.write(DefaultStorageRpc.java:564); 	... 24 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Broken pipe; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppOutputStream.write(AppOutputStream.java:128); 	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82); 	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140); 	at java.io.PrintStream.flush(PrintStream.java:338); 	at java.io.FilterOutputStream.flush(FilterOutputStream.java:140); 	at com.google.api.client.http.AbstractInputStreamContent.writeTo(AbstractInputStreamContent.java:73); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:79); 	... 26 more; Caused by: java.net.SocketException: Broken pipe; 	at java.net.SocketOutputStream.socketWrite0(Native Method); 	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109); 	at java.net.SocketOutputStream.write(SocketOutputStream.java:153); 	at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431); 	at sun.security.ssl.OutputRecord.write(OutputRecord.java:417); 	at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876); 	at s,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183:5771,secur,security,5771,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183,1,['secur'],['security']
Security,"(Probably related to #4081). . Using cromwell v36. I had the following code block in my wdl:. ```; call MergePileupSummaries as MergeTumorPileups {; input:; input_tables = TumorPileups.pileups,; output_name = , // <--- forgot to specify this input; ref_dict = ref_dict,; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible_attempts = preemptible_attempts,; max_retries = max_retries,; disk_space = ceil(SumSubVcfs.total_size * large_input_to_output_multiplier) + disk_pad; }; ```. Cromwell error didn't give me anything, and `womtool validate` threw an exception:. ```; tsato@gsa5:fc-read-orientation: java -jar womtool-36.jar validate mutect2.wdl; Exception in thread ""main"" scala.MatchError: null; 	at wdl.draft2.model.WdlExpression$.toString(WdlExpression.scala:117); 	at wdl.draft2.model.WdlExpression.toString(WdlExpression.scala:200); 	at wdl.draft2.model.WdlExpression.toWomString(WdlExpression.scala:203); 	at wom.values.WomValue.valueString(WomValue.scala:50); 	at wom.values.WomValue.valueString$(WomValue.scala:50); 	at wdl.draft2.model.WdlExpression.valueString(WdlExpression.scala:185); 	at wdl.draft2.model.WdlWomExpression.sourceString(WdlExpression.scala:219); 	at wom.graph.expression.ExpressionNode$.$anonfun$buildFromConstructor$4(ExpressionNode.scala:66); 	at cats.data.NonEmptyList.map(NonEmptyList.scala:76); 	at wom.graph.expression.ExpressionNode$.$anonfun$buildFromConstructor$3(ExpressionNode.scala:66); 	at cats.data.Validated.leftMap(Validated.scala:203); 	at wom.graph.expression.ExpressionNode$.buildFromConstructor(ExpressionNode.scala:66); 	at wom.graph.expression.AnonymousExpressionNode$.fromInputMapping(AnonymousExpressionNode.scala:17); 	at wdl.draft2.model.WdlWomExpression$.$anonfun$toAnonymousExpressionNode$1(WdlExpression.scala:293); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flat; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4570:554,validat,validate,554,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4570,5,"['Validat', 'validat']","['Validated', 'validate', 'validation']"
Security,"(cromwell version 35-5f86a05-SNAP). call caching version from previous run . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4330:831,PASSWORD,PASSWORDS,831,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4330,1,['PASSWORD'],['PASSWORDS']
Security,") is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346:2511,hash,hash,2511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346,1,['hash'],['hash']
Security,"), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3986:2375,PASSWORD,PASSWORDS,2375,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986,1,['PASSWORD'],['PASSWORDS']
Security,"){; call centrifugeDownload {; input:; centrifugeOutput= centrifuge.outputDir,; domain=centrifuge.domain,; database=if defined(centrifuge.database) then centrifuge.database else ""refseq""; }; }; }; ```; The `centrifugeList` is a list of dictionaries. The resulting `centrifuge` `object` may or may not have a key database. . ## Expected behaviour:; `database` defaults to `""refseq""` if no `database` key is present in the dictionary. It will use the database key if it exists. ## Observed behaviour:; ```; java.lang.RuntimeException: Evaluating if defined(centrifuge.database) then centrifuge.database else ""refseq"" failed: Could not find key database in WdlObject; at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2(ExpressionKey.scala:36); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2$adapted(ExpressionKey.scala:31); at scala.Function1.$anonfun$andThen$1(Function1.scala:52); at cats.data.Validated.fold(Validated.scala:14); at cats.data.Validated.bimap(Validated.scala:109); at cats.data.Validated.map(Validated.scala:152); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:31); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$4(WorkflowExecutionActor.scala:452); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$2(WorkflowExecutionActor.scala:449); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$1(WorkflowExecutionActor.scala:448); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processRunnableTaskCallInputExpression(WorkflowExecutionActor.scala:447); at cromwell.engin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3093:1074,Validat,Validated,1074,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3093,1,['Validat'],['Validated']
Security,* Makes Cromwell validate that inputs provided were actually wanted by the workflow.; * Adds a validation option to womtool that allows users to validate their inputs json without needing to submit to Cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3473:17,validat,validate,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3473,3,['validat'],"['validate', 'validation']"
Security,* Re-pin conformance test hash to the current HEAD of master.; * Allow workflow inputs to be recycled back as outputs (part 1/2 fixing conformance 20).; * Allow arrays to be coerced to Anys (part 2/2 fixing conformance 20).; * Fix prefix handling with empty arrays (fix conformance 125).; * Adjust expected conformance failures for all of the above.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3467:26,hash,hash,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3467,1,['hash'],['hash']
Security,"**As a first step -- please confirm that this issue still exists**. It seems that some inputs are being copied over twice to the same path for all tasks. I'm not exactly sure when this started happening but the cromwell git hash we are currently using is https://github.com/broadinstitute/cromwell/tree/c66eabc3582085e28b197b667cb82b241ab6d1dd . Logs in comment. In this example the input_bam and interval_list are listed twice as inputs.; jes operations ID for the following call : operations/EI3Pz-W1KhiNs_ydkKOavrwBIJ-ikOmeDSoPcHJvZHVjdGlvblF1ZXVl; 295959:2016-03-09 18:46:47,375 cromwell-system-akka.actor.default-dispatcher-12 INFO - JesBackend UUID(a7884b2d):HaplotypeCaller:12: Starting call with pre-emptible VM; 295960:2016-03-09 18:46:47,375 cromwell-system-akka.actor.default-dispatcher-12 INFO - JES Pipeline UUID(a7884b2d):HaplotypeCaller:12: Inputs:; 295961: input_bam-0 -> disk:local-disk relpath:broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/a7884b2d-e859-4ef3-bb74-b820623755b4/call-GatherBamFiles/M1132.bam; 295964: 2eb61371-0 -> disk:local-disk relpath:broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/a7884b2d-e859-4ef3-bb74-b820623755b4/call-GatherBamFiles/M1132.bam; 295965: e1220deb-0 -> disk:local-disk relpath:broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/a7884b2d-e859-4ef3-bb74-b820623755b4/call-ScatterIntervalList/glob-cb4648beeaff920acb03de7603c06f98/20scattered.interval_list; 295966: interval_list-0 -> disk:local-disk relpath:broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/a7884b2d-e859-4ef3-bb74-b820623755b4/call-ScatterIntervalList/glob-cb4648beeaff920acb03de7603c06f98/20scattered.interval_list; 295968: input_bam_index-0 -> disk:local-disk relpath:broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/a7884b2d-e859-4ef3-bb74-b820623755b4/call-GatherBamFiles/M1132.bai",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/576:224,hash,hash,224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/576,1,['hash'],['hash']
Security,"**Backend:** AWS. **Workflow:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; **First input json:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-parameters.json; **Second input json is LIKE this one, but refers to a batch of 100 input datasets:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-batchofOne.json. **Config:** ; Installed the cromwell version in PR #4790. . **Error:**; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hitFailures"": [; {; ""dd860da7-bed8-4e70-812c-227f4e6fead8:Panel_BWA_GATK4_Samtools_Var_Annotate_Split.SamToFastq:0"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ],; ""message"": ""[Attempted 1 time(s)] - S3Exception: The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ```. This version of Cromwell does seem to successfully access and copy a cached file from a previous workflow at least on the first task in a shard. This workflow is essentially a batch in which each row of a batch file is passed to a shard and then the tasks run independently on each input dataset and they never gather. However, when the files get larger than the single test data set it seems it can't get to the previous file in order to determine if there's a hit.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4805:1384,access,access,1384,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4805,1,['access'],['access']
Security,"**Not working with ""gzip"":**; ```shell; $ curl \; -X GET \; -H ""Authorization: Bearer $(gcloud auth application-default print-access-token)"" \; -H 'Accept-Encoding: gzip' \; ""https://${CAAS_PROD}/api/workflows/v1/d7923869-af12-4a09-9a5d-fa1aaef84e90/metadata?expandSubWorkflows=false"" \; | wc; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:00:05 --:--:-- 0; curl: (18) transfer closed with outstanding read data remaining; 0 0 0; $ ; ```. **Working with ""identity"":**; ```shell; $ curl \; -X GET \; -H ""Authorization: Bearer $(gcloud auth application-default print-access-token)"" \; -H 'Accept-Encoding: identity' \; ""https://${CAAS_PROD}/api/workflows/v1/d7923869-af12-4a09-9a5d-fa1aaef84e90/metadata?expandSubWorkflows=false"" \; | wc; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 16.0M 100 16.0M 0 0 740k 0 0:00:22 0:00:22 --:--:-- 1100k; 0 902345 16872427; $ ; ```. NOTE:; This perhaps may only occur on larger (possibly chunked?) payloads. The above json is 16MB decompressed. If required Rex should be able to give you access to this workflow via FC if you want to reproduce this exact case. A/C:; - curl works with gzip encoding for a 16MB+ json response; - regression test to ensure this doesn't re-occur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4708:64,Authoriz,Authorization,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4708,5,"['Authoriz', 'access']","['Authorization', 'access', 'access-token']"
Security,"**PR**: https://github.com/broadinstitute/cromwell/pull/7224. **Idea:** Add an auth mode for GCP that allows Cromwell to impersonate a service account and use [short-lived credentials](https://cloud.google.com/iam/docs/create-short-lived-credentials-direct) for calls to GCP. . **Context:** Service account impersonation is now the [defacto recommendation](https://cloud.google.com/docs/authentication#auth-decision-tree) when choosing an authentication mechanism that relies on service accounts. Some teams (such as Verily) might prefer this approach over the current option to rely on downloaded service account keys. **How it works**; Users would specify an auth block in the .conf file with one of the following formats. The first specifies . - Application default credentials used for **source service account**; ```; {; name = ""user-service-account""; scheme = ""user_service_account_impersonation""; }; ```. - A specified JSON file used for **source service account**; ```; {; name = ""user-service-account""; scheme = ""user_service_account_impersonation""; json-file= ""path/to/file.json""; }; ```. Users would then add the following option when making workflow requests:; ```; {; ""user_service_account_email"": ""someemail@domain.com; }; ```. Cromwell would use the **source service account** to impersonate the **target service account** from the workflowOptions. It would then mint and refresh access tokens from this target service account for all GCP requests. . In order for this to work, the source service account would need the IAM role roles/iam.serviceAccountTokenCreator. **If the team would prefer a smaller PR**; We could change this to only ever use applicationDefaultCredentials, in which case we could remove parts of the code that are altering ServiceAccountMode",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7223:387,authenticat,authentication,387,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7223,3,"['access', 'authenticat']","['access', 'authentication']"
Security,"*See comment below on how to fix/address*. - cromwell-27-c89c83f-SNAP.jar; - JES backend; - server mode; - local mysql. I have a database block that looks exactly like the one in the example (from the error message), yet I still get the error message. I tried a diff on the database blocks, between the example and my database block, so I am sure that they match. Is this just a mistake in the error message itself? . This is blocking me. The error:. ```; Caused by: java.lang.Exception:; *******************************; ***** DEPRECATION MESSAGE *****; *******************************. Use of configuration path 'database.driver' has been deprecated. Replace with a ""profile"" element instead, e.g:. database {; #driver = ""slick.driver.MySQLDriver$"" #old; profile = ""slick.jdbc.MySQLProfile$"" #new; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?rewriteBatchedStatements=true""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; }. Cromwell thanks you. at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:70); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 22 more. ```. My conf file for database:; ```; database {; #driver = ""slick.driver.MySQLDriver$""; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell_24?useSSL=false&rewriteBatchedStatements=true""; user = ""root""; password = ""blahblah""; connectionTimeout = 5000; }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2217:921,password,password,921,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2217,2,['password'],['password']
Security,", ""command_mem"": small_task_mem * 1000 - 500,; ""disk"": small_task_disk, ""boot_disk_size"": boot_disk_size}. scatter (normal_bam in zip(normal_bams, normal_bais)) {; call m2.Mutect2 {; input:; intervals = intervals,; ref_fasta = ref_fasta,; ref_fai = ref_fai,; ref_dict = ref_dict,; tumor_reads = normal_bam.left,; tumor_reads_index = normal_bam.right,; scatter_count = scatter_count,; m2_extra_args = select_first([m2_extra_args, """"]) + "" --max-mnp-distance 0"",; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible = preemptible,; max_retries = max_retries,; pon = pon,; pon_idx = pon_idx,; gnomad = gnomad,; gnomad_idx = gnomad_idx; }; }. output {; Array[File] normal_calls = Mutect2.filtered_vcf; Array[File] normal_calls_idx = Mutect2.filtered_vcf_idx. }; }. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5352:4925,PASSWORD,PASSWORDS,4925,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352,1,['PASSWORD'],['PASSWORDS']
Security,"- 0.24; - SGE backend. I accidentally gave an Int parameter a String value in the json. I would prefer an error specific to parameter type, rather than a generic invalid runtime attribute error message (below). Proposed solution: ; ``Task m1_task was given an invalid type for cpu = ""${cpu}"". A String was given, though parameter is an Int``. Current error message:; ```; [ERROR] [02/08/2017 10:38:57.225] [cromwell-system-akka.dispatchers.engine-dispatcher-8] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor Workflow 07a3f007-8c62-4cd4-8668-6ac034ff42f1 failed (during InitializingWorkflowState): Task m1_task has an invalid runtime attribute cpu = ""${cpu}""; java.lang.IllegalArgumentException: Task m1_task has an invalid runtime attribute cpu = ""${cpu}""; at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:156); at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1963:869,validat,validateRuntimeAttributes,869,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1963,1,['validat'],['validateRuntimeAttributes']
Security,"- 0.24; - single workflow mode; - JES backend. When I run the workflow, I get a localization permission error, but when I try again from the command line, there is no issue.; From cromwell:; ```; ....snip....; java.lang.RuntimeException: Task 773d051e-2e93-4248-bca4-e40292e0e59d:generate_true_positives failed: error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list -> /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list (cp failed: gsutil -q -m cp gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list, command failed: AccessDeniedException: 403 Caller does not have storage.objects.list access to bucket firecloud-tcga-open-access.\nCommandException: 1 file/object could not be transferred.\n)""; at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:489); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:61); ....snip....; ```; ; BUT I would think this next operation would fail and it does not:; ```; lichtens@lichtens-big:~/test_dl_oxoq/create_bs$ gsutil ls gs://firecloud-tcga-open-access/tutorial/reference/; gs://firecloud-tcga-open-access/tutorial/reference/CNV.hg19.bypos.111213.txt; gs://firecloud-tcga-open-access/tutorial/reference/Homo_sapiens_assembly19.dict; gs://firecloud-tcga-open-access/tutori",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1960:426,access,access,426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1960,4,['access'],['access']
Security,- Added `monitoring_image_script` localization after ssh access startup and before starting the `monitoring_image`; - Added test to make sure ssh access wasn't broken by change; - Ensure Genomics & CLS use the exact same order for actions; - Other de-dupes related to updating Genomics/CLS actions; - Entrypoint workarounds for BA-6406 / https://partnerissuetracker.corp.google.com/issues/155231711 in images other than the user's `runtime { docker: … }` such as `monitoring_image`; - More actions-logging-other-actions such as the `monitoring_image` and ssh access starting,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5766:57,access,access,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5766,3,['access'],['access']
Security,"- Adds a way to lookup docker hashes from local machine, thanks @kshakir !; - ~~Adds a config option to disable docker lookup entirely.~~; - Disable docker lookup if the backend does not support docker. ~~@LeeTL1220 this is slightly different from what we talked this morning but I think it should still enable your use case.; If you disable docker hash lookup on the SGE cromwell server, you'll still have call caching and there will be no lookup (so it won't fail..).~~",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2139:30,hash,hashes,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2139,2,['hash'],"['hash', 'hashes']"
Security,"- Better localization and delocalization of directories in PAPI2 using hidden files to cover for empty directories; - IWDR localization is not baked in the CWL code anymore but left to the backend. This allows for the PAPI backend to opt out of it since localization is done directly on the VM.; - ~~Use configurable `job-shell` instead of hardcoded `/bin/bash`~~ It fixes 117 but also makes a bunch of centaur tests fail, so leaving as is for now.; - Refactors Pipelines conversions in v2 (w/ typeclasses !); - Allow for lazy evaluation of file and directory literals so that they can be written when the backend and the appropriate IoFunctions are known. This only partially covers the possible cases. It needs a deeper tech talk discussion. This is orthogonal to the above and only here to avoid a later rebase (the files changed overlap with the refactoring mentioned).; - Partially replaces the custom `MemorySize` with [squants](https://github.com/typelevel/squants); - Turns the CPU runtime validation from an `Int` to a `Int Refined Positive`; - Automatically fits the resources requirements in the task to the [GCE constraints](https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type#specifications)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3697:998,validat,validation,998,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3697,1,['validat'],['validation']
Security,"- Commit 1: Creates 2 subdirectories in the call directory. ```; cromwell-executions/globbinginput/; └── 970c776f-3b9d-4c0e-a2ff-dbd76395b4ba; ├── call-globtask; │   ├── execution; │   │   ├── outputfile1.txt; │   │   ├── outputfile2.txt; │   │   ├── rc; │   │   ├── script; │   │   ├── script.background; │   │   ├── script.submit; │   │   ├── stderr; │   │   ├── stderr.background; │   │   ├── stdout; │   │   └── stdout.background; │   └── inputs; │   └── 90e8d77e2a99e99efa82c33e27365d71; │   └── somefile.txt; ```. This prevents globbing from matching input files as they're not in the same branch.; - Commit 2: Instead of recreating the whole directory structure underneath the call directory, hash the path and create a single directory named with this hash. ; - This reduces the depth of the call directory, and is hopefully a bit less confusing (people tend to be confused when they see the full path of their input file appended to the call directory); - The hash is only computed from the fullpath of the input's parent directory (not including the filename). This ensures that multiple files from the same directory will still end up in the same directory once localized). The hashing part is not necessary to fix this bug at all, and I remember the choice was made not to use hashes because it's unreadable and makes it hard to know where the inputs came from but I feel like recreating the full path underneath the call directory is even worse...; - Commit 3: Change the globbing pattern so it does NOT traverse recursively the hierarchy looking for matches by default. If that's the desired behaviour it should be reflected in the glob pattern in the wdl (with `**`). This is necessary to allow for this kind of usage https://github.com/broadinstitute/cromwell/issues/1245",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1415:700,hash,hash,700,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1415,5,['hash'],"['hash', 'hashes', 'hashing']"
Security,"- Cromwell does not support soft links for dockerized jobs by [design](https://github.com/broadinstitute/cromwell/blob/29_hotfix/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystem.scala#L112); - I guess that's because symlinks are a big challenge with Docker. They do not work inside Docker container unless both the directories are mounted (dir with symlink - Cromwell's tmp execution dir and original dir where input files are present), plus, mounts inside the container should have the same name/path as that of the host file system. Since Cromwell has information about original input files, they can be mounted along with the tmp execution dir (where symlinks will be created), to the Docker container; - This possesses a threat (of being modified inside a Docker container), to input files, which can be circumvented by using read only access to the input files (similar threat is also applicable to hard linked input files, in case of current behavior); - Also given the nature of hard-links, they can be completely eliminated . Benefits of using symlinks over hard-links:; - Hard links can't cross file systems; soft links can; - OS user which runs Cromwell requires to have a write access to input files in order for hard-links to work, this is not a requirement for soft links; - When hard-links do not work in case of Docker jobs (when necessary conditions - write access, same device requirement, aren't satisfied), Cromwell falls back to copy option, which takes a significant performance hit as the input files could be huge",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2620:751,threat,threat,751,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2620,5,"['access', 'threat']","['access', 'threat']"
Security,- Enables 121 on PAPIv2; - Refactors Pipelines conversions in v2 (w/ typeclasses !); - Partially replaces the custom `MemorySize` with [squants](https://github.com/typelevel/squants); - Turns the CPU runtime validation from an `Int` to a `Int Refined Positive`; - Automatically fits the resources requirements in the task to the [GCE constraints](https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type#specifications); - Allow for lazy evaluation of file and directory literals so that they can be written when the backend and the appropriate IoFunctions are known,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3694:208,validat,validation,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3694,1,['validat'],['validation']
Security,"- JES backend; - 0.24; - single workflow mode. When JES returns a 403 AccessDeniedException, should cromwell keep retrying? It delays the result getting back to the user and should have no way of recovering with retries. Proposed solution: When AccessDeniedException is seen from JES, simply end there, instead of initiating any retries...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961:70,Access,AccessDeniedException,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961,2,['Access'],['AccessDeniedException']
Security,- Moved memory validation from WDL4s to Backend validation.; - Removed unused validation functions from CromwellRuntimeAttributes.; - Added validation for negative values in memory and cpu.; - Moved CPU validation from JES backend to generic backend validation functions object.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/741:15,validat,validation,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/741,6,['validat'],['validation']
Security,"- Refactor things slightly to give the `typeEvaluators` access to the `typeAliases` map, which contains all the struct definitions ; - Make a `typeEvaluator` for StructLiterals",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7402:56,access,access,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7402,1,['access'],['access']
Security,- Remove the bit where supplied labels are injected into Google. ; - Remove the Google-related restrictions on label structure. These should just be allowed to be anything*; - Provide a new workflow option `google-labels` which **do** get injected into Google and enforces Google's restrictions; - Communicate that this changed to FC,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3233:43,inject,injected,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3233,2,['inject'],['injected']
Security,"- Removed `martha_v2` response parsing; - When traversing files using a mapper function then mapper does the exclusion; - Use serialized class instead of config string replace for Martha request generation; - Request partial responses from Martha; - Use JDK standard responses for missing file attributes (size=0, time=epoch, hash=None); - Copy `timeCreated` from DOS/DRS to file attributes; - Martha `read_string()` uses `gsUri` (gs://bucket/name) instead of `bucket` and `name`; - Martha localization uses safer file paths still based on the DOS/DRS URI; - Reading DOS/DRS content uses the config google auth type, not always Bond-or-USA; - Google config auth types support ADC=SA, passing in scopes to ADC; - Google auth type `UserMode` no longer requires config values that it was ignoring; - Allow skipping docker build-and-push by specifying the `CROMWELL_BUILD_PAPI_DOCKER_IMAGE_DRS`; - `papi-v2-usa` backend now ALWAYS uses the USA just like Terra/FC does",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5912:326,hash,hash,326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5912,1,['hash'],['hash']
Security,"- SGE backend (though I bet backend does not matter); - server mode; - cromwell 29. WDL takes in a list of filenames and scatters over a read_lines call. Each line is a file.; If the list file has DOS line endings, read_lines preserves the `\r` character in the file name. After running dos2unix, the issue disappeared. Here is the error message and you can even see the appended `\r`... ```; Could not localize /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r -> /dsde/working/lichtens/sge_cromwell/cromwell-executions/m2_validation/3055776a-c32a-4309-a426-87f5730454b4/call-m1_basic_validator/shard-1/inputs/seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r:\n\t/seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r doesn't exists\n\tFile not found /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r\n\tFile not found /dsde/working/lichtens/sge_cromwell/cromwell-executions/m2_validation/3055776a-c32a-4309-a426-87f5730454b4/call-m1_basic_validator/shard-1/inputs/seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r -> /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r\n\tFile not found /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r""; ```. Hash error:; ```; ""Cannot hash file /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r because it can't be found"". ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2632:1208,Hash,Hash,1208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2632,2,"['Hash', 'hash']","['Hash', 'hash']"
Security,"- There are *five* different authentication schemes (It looks like ""four"" is a typo); - Made each of the five options subheaders under ""Configuring Authentication"" for clarity",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5529:29,authenticat,authentication,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5529,2,"['Authenticat', 'authenticat']","['Authentication', 'authentication']"
Security,"- There's no migration to move hashes from previous workflows to metadata. Which means the Call Caching Diff endpoint won't work for those. Instead of returning an empty diff which could be mistaken for ""those calls are identical"", return 404.; @katevoss @bradtaylor If that's a behavior we want to change please say so (cf email sent earlier this week). - Reverts the code that was persisting failed jobs hashes to the hash table. We don't need it anymore because it's in the metadata and that's what the Call Caching endpoint is using. - Update changelog / README",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2372:31,hash,hashes,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2372,3,['hash'],"['hash', 'hashes']"
Security,- To the changelog:; - Added entry for already released 62; - Added placeholder for next release 63; - For homebrew:; - Remove extra slash added to generated URLs; - Changed default publishing instructions to include homebrew; - Added validation of brew style according to guidelines; - Fixed casing of 'cromwell' in PR name; - For the publishing GitHub token scopes:; - updated instructions; - updated validation; - gracefully error with helpful messages; - added an example image; - Removed attempt to publish from dbms tests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6338:235,validat,validation,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6338,2,['validat'],['validation']
Security,"- Update to WDL 1.0 syntax; - Automate PR to homebrew; - Can be used to release minor versions. The brew task supposes `brew` is installed on the machine where Cromwell runs. There is a [Brew Linux docker image](https://hub.docker.com/r/linuxbrew/linuxbrew/) that maybe could be used but the `git` commands don't seem to be authenticated properly in the container (probably because the ssh keys aren't there). I'm sure this can be worked around, maybe in the next iteration.. This can be fully tested with a different `organization` as long as it has a fork of Cromwell and homebrew-core.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3810:324,authenticat,authenticated,324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3810,1,['authenticat'],['authenticated']
Security,"- [x] Implement https://github.com/openwdl/wdl/pull/219 in the WDL Biscayne classes; - Implement all of the changes in the SPEC text, not just the issue description!; - [x] Add `womtool validate` tests to ensure they are still not allowed in `version 1.0`; - [x] Cycle back to the openWDL issue so that it can be *merged* once an implementation exists",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3871:186,validat,validate,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3871,1,['validat'],['validate']
Security,"- [x] Needs a rebase on `develop` after #3505 . Wires in and checks imported tasks, workflows, namespaces and aliases; NB: the test WDL appears twice - once in a `womtool validate` test and once as a centaur test",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3530:171,validat,validate,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3530,1,['validat'],['validate']
Security,"- [x] Needs https://github.com/broadinstitute/wdl4s/pull/47; - [x] Needs https://github.com/broadinstitute/centaur/pull/114; - [x] Needs WDL doc; - [x] Needs Cromwell doc. What it does in a nutshell:. - Enables sub workflows execution; - Sub workflow metadata can be queried separately or injected in the main workflow metadata; - Restarts work; - Aborts should work (work meaning what abort is doing in develop now). To be addressed:; - ~~Sub Workflow Store cleanup~~; - ~~Workflow outputs copying~~ -> https://github.com/broadinstitute/cromwell/issues/1684; - ~~Call logs copying~~; - ~~Provenance: More related to imports, but right now the actual WDL content of a sub workflow is unknown to cromwell (it's in the `WdlNamespace` as a scala object but the actual text is not available).~~; - ~~Stats Endpoint~~",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1682:289,inject,injected,289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1682,1,['inject'],['injected']
Security,- [x] README update; - [x] Changelog update; - [x] What should happen to `call-caching.lookup-docker-hash` conf entry ?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1969:101,hash,hash,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1969,1,['hash'],['hash']
Security,"- dev snapshot of cromwell pre-0.21; - local backend; - Specifying docker from options file; - Fails when running with sudo or without (same error); - `wdltool` validates successfully; - Being run on google cloud VM ; - And after error occurs, cromwell stays running. . I believe that this was working, as is, in cromwell 0.19. I believe that it is having trouble parsing the option file. Command:. ``` bash; java -Xmx4g -Dconfig.file=local_application.conf -jar \; /home/lichtens/test_eval/cromwell-0.20-028b74a-SNAPSHOT.jar run case_gatk_acnv_workflow.final.wdl \ ; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.final.json \; default_runtimes \; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.metadata.json; ```. Error message:. ```; [2016-09-21 17:51:25,15] [error] Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); java.lang.RuntimeException: Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); at cromwell.backend.validation.RuntimeAttributesValidation$class.validateOptionalExpression(RuntimeAttributesValidation.scala:319); at cromwell.backend.validation.RuntimeAttributesValidation$$anon$1.validateOptionalExpression(RuntimeAttributesValidation.scala:90); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.sfs.Shar",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1465:161,validat,validates,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465,1,['validat'],['validates']
Security,"- local backend with 16 cores and 104GB RAM; - cromwell-0.21-6da2d10-SNAPSHOT.jar (incl. file path hashing and local backend throttling). Whether the job is a cache hit or not, it seems that the cromwell final overhead takes 3-8 minutes, which is a long time. This happens even for jobs where the files generated (and input) are very small (and there are few files). We do not use `read_string` in the wdl. http://104.198.41.229:8080/api/workflows/v2/70a6e380-1dd7-473b-a852-4bd54b22ecdf/timing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1488:99,hash,hashing,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488,1,['hash'],['hashing']
Security,"- local backend. The slowness is probably driven by calculating a md5 hash on the bam input files. For jobs that do not have bam files as inputs, the cache determination is fast. Calculating a md5 on a bam file is too slow for use in call caching on a local backend. I can provide a time estimate, if necessary. Does SGE backend has the same issue?. Proposed solution:; - If md5 file is next to input file (e.g. sample1.bam is next to sample1.bam.md5) then read the md5 hash from the *.md5 file. Otherwise, use the file path. For bams, the file paths will tend to be static for most local backend environments anyway. ; - Make sure this convention is documented.; - Perhaps have a flag in the conf file so that users can choose which convention they prefer? IMHO, have the fast method (above) as the default.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1483:70,hash,hash,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1483,2,['hash'],['hash']
Security,"- single workflow; - JES. Been at this point for ~15 minutes. Please note that the workflow directory in the bucket does not exist. ```; ...snip....; [2016-11-02 13:27:06,88] [info] WorkflowManagerActor Successfully started WorkflowActor-35b2e3c9-0aba-4f96-a7cf-9f1fbf3ced16; [2016-11-02 13:27:06,88] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2016-11-02 13:27:07,21] [info] MaterializeWorkflowDescriptorActor [35b2e3c9]: Call-to-Backend assignments: case_gatk_acnv_workflow.PlotACNVResults -> JES, case_gatk_acnv_workflow.NormalNormalizeSomaticReadCounts -> JES, case_gatk_acnv_workflow.NormalWholeGenomeCoverage -> JES, case_gatk_acnv_workflow.TumorCaller -> JES, case_gatk_acnv_workflow.TumorCorrectGCBias -> JES, case_gatk_acnv_workflow.NormalPerformSeg -> JES, case_gatk_acnv_workflow.TumorWholeGenomeCoverage -> JES, case_gatk_acnv_workflow.CNLoHAndSplitsCaller -> JES, case_gatk_acnv_workflow.TumorCalculateTargetCoverage -> JES, case_gatk_acnv_workflow.NormalCaller -> JES, case_gatk_acnv_workflow.TumorNormalizeSomaticReadCounts -> JES, case_gatk_acnv_workflow.HetPulldown -> JES, case_gatk_acnv_workflow.PlotSegmentedCopyRatio -> JES, case_gatk_acnv_workflow.TumorAnnotateTargets -> JES, case_gatk_acnv_workflow.NormalCorrectGCBias -> JES, case_gatk_acnv_workflow.TumorPerformSeg -> JES, case_gatk_acnv_workflow.NormalCalculateTargetCoverage -> JES, case_gatk_acnv_workflow.PadTargets -> JES, case_gatk_acnv_workflow.NormalAnnotateTargets -> JES, case_gatk_acnv_workflow.AllelicCNV -> JES; [2016-11-02 13:27:07,49] [info] JES [35b2e3c9]: Creating authentication file for workflow 35b2e3c9-0aba-4f96-a7cf-9f1fbf3ced16 at; gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/case_gatk_acnv_workflow/35b2e3c9-0aba-4f96-a7cf-9f1fbf3ced16/35b2e3c9-0aba-4f96-a7cf-9f1fbf3ced16_auth.json; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1644:1573,authenticat,authentication,1573,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644,1,['authenticat'],['authentication']
Security,"-+---------+---------------+; | CUSTOM_LABEL_ENTRY | 0 | PRIMARY | 1 | CUSTOM_LABEL_ENTRY_ID | A | 531285 | NULL | NULL | | BTREE | | |; | CUSTOM_LABEL_ENTRY | 0 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1 | CUSTOM_LABEL_KEY | A | 31 | NULL | NULL | YES | BTREE | | |; | CUSTOM_LABEL_ENTRY | 0 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 2 | WORKFLOW_EXECUTION_UUID | A | 531285 | NULL | NULL | | BTREE | | |; | CUSTOM_LABEL_ENTRY | 1 | SYS_IDX_11226 | 1 | WORKFLOW_EXECUTION_UUID | A | 132821 | NULL | NULL | | BTREE | | |; +--------------------+------------+-------------------------------+--------------+-------------------------+-----------+-------------+----------+--------+------+------------+---------+---------------+; ```; So MySQL appears to be table scanning `WORKFLOW_METADATA_SUMMARY_ENTRY` and then finding the the at-most-one matching rows in `CUSTOM_LABEL_ENTRY` for each label parameter using the unique index on `WORKFLOW_EXECUTION_UUID` + `CUSTOM_LABEL_KEY`. So the labels table access should be fast but the summary table is table scanning. I experimented with adding a non-unique index on `CUSTOM_LABEL_ENTRY` for `CUSTOM_LABEL_KEY` + `CUSTOM_LABEL_VALUE` in the hope that MySQL could use that first and then join back to the summary table on workflow ID. However I haven't had any luck getting MySQL to use this index for even the simplest possible queries:. ```; mysql> create index IDX_KEY_VALUE on CUSTOM_LABEL_ENTRY (CUSTOM_LABEL_KEY, CUSTOM_LABEL_VALUE); ; .; .; .; mysql> explain select WORKFLOW_EXECUTION_UUID from CUSTOM_LABEL_ENTRY where CUSTOM_LABEL_KEY = 'caas-collection-name' AND CUSTOM_LABEL_VALUE = 'miguel-collection';; +----+-------------+--------------------+------------+------+---------------------------------------------+-------------------------------+---------+-------+------+----------+-------------+; | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |; +----+-------------+--------------------+------------+--",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4598:4064,access,access,4064,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4598,1,['access'],['access']
Security,"------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; https://security-tracker.debian.org/tracker/CVE-2017-17458; -----------------------------------------; CVE-2017-12562: [High] ; Found in: libsndfile [1.0.27-3]; Fixed By: ; Heap-based Buffer Overflow in the psf_binheader_writef function in common.c in libsndfile through 1.0.28 allows remote attackers to cause a denial of service (application crash) or possibly have unspecified other impact.; https://security-tracker.debian.org/tracker/CVE-2017-12562; -----------------------------------------; CVE-2018-1000001: [High] ; Found in: glibc [2.24-11+deb9u4]; Fixed By: ; In glibc 2.26 and earlier there is confusion in the usage of getcwd() by realpath() which can be used to write before the destination buffer leading to a buffer underflow and potential code execution.; https://security-tracker.debian.org/tracker/CVE-2018-1000001; -----------------------------------------; CVE-2016-2779: [High] ; Found in: util-linux [2.29.2-1+deb9u1]; Fixed By: ; runuser in util-linux allows local users to escape to the parent session via a crafted TIOCSTI ioctl call, which pushes characters to the terminal's input buffer.; https://security-tracker.debian.org/tracker/CVE-2016-2779; -----------------------------------------; CVE-2017-14062: [High] ; Found in: libidn [1.33-1]; Fixed By: ; Integer overflow in the decode_digit function in puny_decode.c in Libidn2 before 2.0.4 allows remote attackers to cause a denial of service or possibly have unspecified other impact.; https://security-tracker.debian.org/tracker/CVE-2017-14062; -----------------------------------------",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4979:2779,secur,security-tracker,2779,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979,4,"['attack', 'secur']","['attackers', 'security-tracker']"
Security,"-07-21 23:34:35,956 INFO - Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.service-dispatcher-13 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2019-07-21 23:34:38,594 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2019-07-21 23:34:38,667 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:2252,hash,hash-lookup,2252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,1,['hash'],['hash-lookup']
Security,"-12-15 21:22:59,16] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.sex_aneuploidy:-1:1-20000000003 [9e4f5894main.sex_aneuploidy:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,17] [info] BT-322 9e4f5894:main.sex_aneuploidy:-1:1 cache hit copying success with aggregated hashes: initial = 86896541F0DCB2C2B959EEF37F266B30, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,17] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.sex_aneuploidy:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,32] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.month_of_birth:-1:1-20000000024 [9e4f5894main.month_of_birth:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,32] [info] BT-322 9e4f5894:main.month_of_birth:-1:1 cache hit copying success with aggregated hashes: initial = 601F8C709AA96517AA171B340CCA88BF, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,32] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.month_of_birth:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,78] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.kinship_count' (scatter index: None, attempt 1); [2022-12-15 21:22:59,78] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.reported_sex' (scatter index: None, attempt 1); [2022-12-15 21:22:59,78] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.sex_aneuploidy' (scatter index: None, attempt 1); [2022-12-15 21:22:59,78] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:23566,hash,hashes,23566,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['hash'],['hashes']
Security,"-194b-4f53-a43d-d31f0b370f79 submitted; 2021-09-27 13:48:20,474 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - 1 new workflows fetched by cromid-69bdc1a: 075e0cf3-194b-4f53-a43d-d31f0b370f79; 2021-09-27 13:48:20,484 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowManagerActor: Starting workflow UUID(075e0cf3-194b-4f53-a43d-d31f0b370f79); 2021-09-27 13:48:20,511 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowManagerActor: Successfully started WorkflowActor-075e0cf3-194b-4f53-a43d-d31f0b370f79; 2021-09-27 13:48:20,511 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2021-09-27 13:48:20,547 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; Sep 27, 2021 1:48:20 PM com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 2021-09-27 13:48:21,326 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - MaterializeWorkflowDescriptorActor [UUID(075e0cf3)]: Parsing workflow as WDL draft-2; 2021-09-27 13:48:22,359 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - MaterializeWorkflowDescriptorActor [UUID(075e0cf3)]: Call-to-Backend assignments: wf_hello.hello -> PAPIv2; 2021-09-27 13:48:24,671 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowExecutionActor-075e0cf3-194b-4f53-a43d-d31f0b370f79 [UUID(075e0cf3)]: Starting wf_hello.hello; 2021-09-27 13:48:29,304 cromwell-system-akka.dispatchers.engin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:9647,authenticat,authenticated,9647,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['authenticat'],['authenticated']
Security,"-2e93-4248-bca4-e40292e0e59d:generate_true_positives failed: error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list -> /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list (cp failed: gsutil -q -m cp gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list, command failed: AccessDeniedException: 403 Caller does not have storage.objects.list access to bucket firecloud-tcga-open-access.\nCommandException: 1 file/object could not be transferred.\n)""; at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:489); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:61); ....snip....; ```; ; BUT I would think this next operation would fail and it does not:; ```; lichtens@lichtens-big:~/test_dl_oxoq/create_bs$ gsutil ls gs://firecloud-tcga-open-access/tutorial/reference/; gs://firecloud-tcga-open-access/tutorial/reference/CNV.hg19.bypos.111213.txt; gs://firecloud-tcga-open-access/tutorial/reference/Homo_sapiens_assembly19.dict; gs://firecloud-tcga-open-access/tutorial/reference/Homo_sapiens_assembly19.fasta; gs://firecloud-tcga-open-access/tutorial/reference/Homo_sapiens_assembly19.fasta.fai; ```. and: ; ```; lichtens@lichtens-big:~/test_dl_oxoq/create_bs$ gsutil ls gs://firecloud-tcga-open-access/; gs://fireclo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1960:1232,access,access,1232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1960,2,['access'],['access']
Security,"-team/junit4/issues/1671"">#1671</a>)</li>; <li><a href=""https://github.com/junit-team/junit4/commit/a5d205c7956dbed302b3bb5ecde5ba4299f0b646""><code>a5d205c</code></a> Fix GitHub link in FAQ (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1672"">#1672</a>)</li>; <li><a href=""https://github.com/junit-team/junit4/commit/3a5c6b4d08f408c8ca6a8e0bae71a9bc5a8f97e8""><code>3a5c6b4</code></a> Deprecated since jdk9 replacing constructor instance of Double and Float (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1660"">#1660</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/junit-team/junit4/compare/r4.13...r4.13.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=junit:junit&package-manager=maven&previous-version=4.13&new-version=4.13.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/configuring-github-dependabot-security-updates). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5941:4077,secur,security-vulnerabilities,4077,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5941,2,['secur'],"['security-updates', 'security-vulnerabilities']"
Security,". engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 3; numCreateDefinitionAttempts = 3; root = ""s3://mybucket/cromwell-execution""; 	auth = ""default""; concurrent-job-limit = 16; default-runtime-attributes { queueArn = ""arn:aws:batch:us-east-1:<account-id>:job-queue/MyHighPriorityQue-ae4256f76f07d96"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }. call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users.; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This is used to blacklist cache hit paths based on the; # prefixes of cache hit paths that Cromwell previously failed to copy for authorization reasons.; enabled: false; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 1000; }; }. database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://cromwell-db-rdscluster.cluster.us-east-1.rds.amazonaws.com/cromwell""; user = ""myuser""; password = ""my password""; connectionTimeout = 5000; }; }. my hello.wdl is:; task hello {; String name; command {; echo 'Hello ${name}!' > ""hello${name}.txt""; }; output {; File response = ""hello${name}.txt""; }; runtime {; docker: ""ubuntu:latest""; }; }. workflow test {; call hello; }. My hello_inputs.json is:; {; ""test.hello.name"": ""World"",; }. I ran java -Dconfig.file=aws.callcache.conf -jar ~/cromwell-35.jar run -i hello_inputs.json hello.wdl several times, each time there is a new job submitted to aws batch. Should it submit aws batch only once when first time ran it? . Your assistance will be highly appreciated. Thanks; Jing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412:1369,access,access,1369,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412,3,"['access', 'password']","['access', 'password']"
Security,.$anonfun$foldRight$2(vector.scala:41); cats.Eval$.advance(Eval.scala:272); cats.Eval$.loop$1(Eval.scala:354); cats.Eval$.cats$Eval$$evaluate(Eval.scala:372); cats.Eval$Defer.value(Eval.scala:258); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:76); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:7075,validat,validation,7075,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['validat'],['validation']
Security,....by using a Travis [encryption key](https://docs.travis-ci.com/user/encryption-keys/) rather than an environment variable. Results: https://travis-ci.com/broadinstitute/cromwell/builds/98563607,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4582:23,encrypt,encryption,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4582,2,['encrypt'],"['encryption', 'encryption-keys']"
Security,".3; SINGULARITY_MOUNTS='<redacted>'; export SINGULARITY_CACHEDIR=$HOME/.singularity/cache; LOCK_FILE=$SINGULARITY_CACHEDIR/singularity_pull_flock. export SINGULARITY_DOCKER_USERNAME=<redacted>; export SINGULARITY_DOCKER_PASSWORD=<redacted>. flock --exclusive --timeout 900 $LOCK_FILE \; singularity exec docker://${docker} \; echo ""Sucessfully pulled ${docker}"". bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; singularity exec --containall $SINGULARITY_MOUNTS --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """""". job-id-regex = ""Job <(\\d+)>.*""; kill = ""bkill ${job_id}""; kill-docker = ""bkill ${job_id}""; check-alive = ""bjobs -w ${job_id} |& egrep -qvw 'not found|EXIT|JOBID'"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path+modtime""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3;; hsqldb.log_size=0; """"""; connectionTimeout = 86400000; numThreads = 2; }; insert-batch-size = 2000; read-batch-size = 5000000; write-batch-size = 5000000; metadata {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-metadata-db/;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7203:2400,hash,hashing-strategy,2400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7203,1,['hash'],['hashing-strategy']
Security,".; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""service-account""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # pipeline-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""service-account"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""europe-west4"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration tu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:13000,access,access,13000,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['access'],['access']
Security,".DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Optional value was not set and no 'default' attribute was provided; Optional value was not set and no 'default' attribute was provided; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:534); ... 39 common frames omitted. ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3927:5431,Validat,Validation,5431,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927,2,['Validat'],"['Validation', 'ValidationTry']"
Security,.WorkflowActor.fetchLocallyQualifiedInputs(WorkflowActor.scala:962) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor.cromwell$engine$workflow$WorkflowActor$$processRunnableCall(WorkflowActor.scala:1254) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$33.apply(WorkflowActor.scala:868) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$33.apply(WorkflowActor.scala:867) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor.cromwell$engine$workflow$WorkflowActor$$startRunnableCalls(WorkflowActor.scala:867) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$2.applyOrElse(WorkflowActor.scala:532) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$2.applyOrElse(WorkflowActor.scala:467) ~[cromwell.jar:0.19]; at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[cromwell.jar:0.19]; at akka.actor.FSM$class.processEvent(FSM.scala:604) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowActor.scala:236) ~[cromwell.jar:0.19]; at akka.actor.LoggingFSM$class.processEvent(FSM.scala:734) ~[cromwell.jar:0.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/695:5967,Hash,HashMap,5967,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/695,2,['Hash'],"['HashMap', 'HashTrieMap']"
Security,".c in Mercurial before 4.6.1 mishandles integer addition and subtraction, aka OVE-20180430-0002.; https://security-tracker.debian.org/tracker/CVE-2018-13347; -----------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; https://security-tracker.debian.org/tracker/CVE-2017-17458; -----------------------------------------; CVE-2017-12562: [High] ; Found in: libsndfile [1.0.27-3]; Fixed By: ; Heap-based Buffer Overflow in the psf_binheader_writef function in common.c in libsndfile through 1.0.28 allows remote attackers to cause a denial of service (application crash) or possibly have unspecified other impact.; https://security-tracker.debian.org/tracker/CVE-2017-12562; -----------------------------------------; CVE-2018-1000001: [High] ; Found in: glibc [2.24-11+deb9u4]; Fixed By: ; In glibc 2.26 and earlier there is confusion in the usage of getcwd() by realpath() which can be used to write before the destination buffer leading to a buffer underflow and potential code execution.; https://security-tracker.debian.org/tracker/CVE-2018-1000001; -----------------------------------------; CVE-2016-2779: [High] ; Found in: util-linux [2.29.2-1+deb9u1]; Fixed By: ; runuser in util-linux allows local users to escape to the parent session via a crafted TIOCSTI ioctl call, which pushes characters to the terminal's input buffer.; https://security-tracker.debian.org/tracker/CVE-2016-2779; -----------------------------------------; CVE-2017-14062: [High] ; Found in: libidn [1.33-1]; Fixed By: ; Integer overflow in the decode_digit function in puny_decode.c in Libidn2 before 2.0.4 allows remote attackers to cause a d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4979:2401,secur,security-tracker,2401,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979,1,['secur'],['security-tracker']
Security,".dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/xxx/o?projection=full&userProject=xxx&uploadType=multipart; {; ""code"" : 403,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project."",; ""reason"" : ""forbidden""; } ],; ""message"" : ""xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.""; }; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:150); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:555); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:475); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:592); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.create(HttpStorageRpc.java:305); 	... 22 common frames omitted; [2020-07-27 18:34:01,11] [info] WorkflowManagerActor Workflow 3d2d7a27-7c37-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594:3637,access,access,3637,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594,1,['access'],['access']
Security,".dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/xxx/o?projection=full&userProject=xxx&uploadType=multipart; {; ""code"" : 403,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project."",; ""reason"" : ""forbidden""; } ],; ""message"" : ""xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.""; }; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:150); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:555); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:475); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:592); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.create(HttpStorageRpc.java:305); 	... 22 more; ```. I have no idea what `serviceusage.services.use` is and how to activate it. The tutorial ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594:7547,access,access,7547,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594,1,['access'],['access']
Security,.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); 	at cromwell.backe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:6567,validat,validatedRuntimeAttributes,6567,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,1,['validat'],['validatedRuntimeAttributes']
Security,.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470); at io.grpc.netty.shaded.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:403); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997); at io.grpc.netty.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74); at io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30); ... 1 common frames omitted; Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target; at java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:388); at java.base/sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:271); at java.base/sun.security.validator.Validator.validate(Validator.java:256); at java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:284); at java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:144); at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslClientContext$ExtendedTrustManagerVerifyCallback.verify(ReferenceCountedOpenSslClientContext.java:234); at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslContext$AbstractCertificateVerifier.verify(ReferenceCountedOpenSslContext.java:779); at io.grpc.netty.shaded.io.netty.internal.tcnative.CertificateVerifierTask.runTask(CertificateVerifierTask.java:36); at io.grpc.netty.shaded.io.netty.internal.tcnative.SSLTask.run(SSLTask.java:48); at io.grpc.netty.shaded.io.netty.internal.tcnative.SSLTask.run(SSLTask.java:42); at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOp,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7551:7161,validat,validator,7161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7551,1,['validat'],['validator']
Security,".handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:68); cromwell_1 | 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); cromwell_1 | 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); cromwell_1 | 	at software.amazon.awssdk.services.sts.DefaultStsClient.getCallerIdentity(DefaultStsClient.java:673); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:86); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:76); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:91); cromwell_1 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); cromwell_1 | 	at scala.util.Try$.apply(Try.scala:213); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:91); cromwell_1 | 	... 46 more; cromwell_1 | ; cromwell_1 | 2020-03-15 16:09:58,022 cromwell-system-akka.dispatchers.engine-dispatcher-59 INFO - WorkflowManagerActor WorkflowActor-c4ee3308-f9bf-41d2-acdb-70c02b6cc4b3 is in a terminal state: WorkflowFailedState`. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5452:4588,validat,validateCredential,4588,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452,1,['validat'],['validateCredential']
Security,".impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1.applyOrElse(FileHashingActor.scala:21); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.callcaching.FileHashingActor.aroundReceive(FileHashingActor.scala:16); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. Snippet from attached conf file:. ```; ...snip...; local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies used to make those links. They are ordered. If one fails; // The next one is tried:; //; // hard-link: attempt to create a hard-link to the file; // copy: copy the file; // soft-link: create a symbolic link to the file; //; // NOTE: soft-link will be skipped for Docker jobs; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; hashing {; strategy: ""path"" ; check-sibling-md5: true; } ; }; ...snip... ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1597:3872,hash,hashing,3872,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597,1,['hash'],['hashing']
Security,.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); 	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); 	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); 	at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at com.google.cloud.storage.spi.DefaultStorageRpc.write(DefaultStorageRpc.java:564); 	... 24 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Broken pipe; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppOutputStream.write(AppOutputStream.java:128); 	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82); 	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140); 	at java.io.PrintStream.flush(PrintStream.java:338); 	at java.io.FilterOutputStream.flush(FilterOutputStream.java:140); 	at com.google.api.client.http.AbstractInputStreamContent.writeTo(AbstractInputStreamContent.java:73); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:79); 	... 26 more; Caused by: java.net.SocketException: Broken pipe; 	at java.net.SocketOutputStream.socketWrite0(Native Method); 	at java.ne,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183:5421,secur,security,5421,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183,1,['secur'],['security']
Security,.lang.Thread.State: RUNNABLE; at sun.nio.ch.FileDispatcherImpl.read0(Native Method); at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46); at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223); at sun.nio.ch.IOUtil.read(IOUtil.java:197); at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); - locked <0x00000006c54b2e78> (a java.lang.Object); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.bac,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1597:1348,Hash,HashFileStrategy,1348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597,2,"['Hash', 'hash']","['HashFileStrategy', 'hash']"
Security,.netty.handler.ssl.SslHandler$SslTasksRunner.access$2000(SslHandler.java:1609); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner$2.run(SslHandler.java:1770); at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174); at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470); at io.grpc.netty.shaded.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:403); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997); at io.grpc.netty.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74); at io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30); ... 1 common frames omitted; Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target; at java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:388); at java.base/sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:271); at java.base/sun.security.validator.Validator.validate(Validator.java:256); at java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:284); at java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:144); at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslClientContext$ExtendedTrustManagerVerifyCallback.verify(ReferenceCountedOpenSslClientContext.java:234); at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslContext$AbstractCertificateVerifier.verify(ReferenceCountedOpenSslContext.java:779); at io.grpc.netty.shaded.io.netty.internal.tcna,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7551:6865,secur,security,6865,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7551,1,['secur'],['security']
Security,".py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-attributes= """"""; Int? cpu=1; Int? memory=4; String? disks; String? time; String? preemptible; """"""; submit = """"""; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe smp ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${out} \; -e ${err} \; ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". filesystems {; local {; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; }; }; }; engine{; filesystems{; local{; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; ```. I wonder if there is something wrong with my config files or if Cromwell's localization is at fault.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3876:4739,hash,hashing-strategy,4739,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876,2,['hash'],['hashing-strategy']
Security,.relevantAsUpstream$1(GraphPrint.scala:178); 	at wom.views.GraphPrint$.upstreamPortToRelevantNodes$1(GraphPrint.scala:187); 	at wom.views.GraphPrint$.$anonfun$upstreamLinks$1(GraphPrint.scala:190); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.Set$Set1.foreach(Set.scala:97); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at wom.views.GraphPrint$.upstreamLinksforNode$1(GraphPrint.scala:190); 	at wom.views.GraphPrint$.relevantAsUpstream$1(GraphPrint.scala:176); 	at wom.views.GraphPrint$.upstreamPortToRelevantNodes$1(GraphPrint.scala:187); 	at wom.views.GraphPrint$.$anonfun$upstreamLinks$1(GraphPrint.scala:190); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321); 	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at wom.views.GraphPrint$.upstreamLinksforNode$1(GraphPrint.scala:190); 	at wom.views.GraphPrint$.upstreamLinks(GraphPrint.scala:191); 	at wom.views.GraphPrint.$anonfun$listAllGraphNodes$2(GraphPrint.scala:33); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:160); 	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:158); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429); ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6744:3455,Hash,HashSet,3455,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6744,1,['Hash'],['HashSet']
Security,".toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. ## My takeaway. - I can't use a `softlink` cache duplication strategy as it's not allowed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346:2683,hash,hashes,2683,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346,1,['hash'],['hashes']
Security,.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); languages.wdl.draft3.WdlDraft3LanguageFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:128); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Prom,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:8501,validat,validateNamespace,8501,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['validat'],['validateNamespace']
Security,.traverse(vector.scala:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3La,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:7377,validat,validation,7377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['validat'],['validation']
Security,.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor.fetchLocallyQualifiedInputs(WorkflowActor.scala:962) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor.cromwell$engine$workflow$WorkflowActor$$processRunnableCall(WorkflowActor.scala:1254) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$33.apply(WorkflowActor.scala:868) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$33.apply(WorkflowActor.scala:867) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor.cromwell$engine$workflow$WorkflowActor$$startRunnableCalls(WorkflowActor.scala:867) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$2.applyOrElse(WorkflowActor.scala:532) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$2.applyOrElse(WorkflowActor.scala:467) ~[cromwell.jar:0.19]; at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[cromwell.jar:0.19]; at akka.actor.FSM$class.processEvent(FSM.scala:604) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowActor.scala:236) ~[cromwell.jar:0.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/695:5896,Hash,HashMap,5896,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/695,1,['Hash'],['HashMap']
Security,"/ Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47; //; // A value of 'true' means that call hashes will more accurately represent the; // Docker image that was used to run the call, but at a cost of having to make a; // request to an external service (DockerHub, GCR). If a call fails to lookup a; // Docker hash, it will fail.; lookup-docker-hash = false; }. google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh_token""; client-id = ""secret_id""; client-secret = ""secret_secret""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:86249,hash,hash,86249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['hash'],['hash']
Security,"/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam -> /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD; /DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam (cp failed: gsutil -q -m cp gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A2; 5E-08.2.bam /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam, command failed: Traceback (most recent call last):\n; File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.; py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205,; in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py; \"", line 1221, in Get\n required)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutD; efault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n; File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/googl; e/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n File \""/usr/local/share/google/google-cloud-sdk/lib/g; ooglecloudsdk/core/util/retry.py\"", line 155, in TryFunc\n return func(*args, **kwargs), None\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.p; y\"", line 41, ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:3697,validat,validate,3697,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['validat'],['validate']
Security,/describe endpoint validates workflows,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4467:19,validat,validates,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4467,1,['validat'],['validates']
Security,"/github.com/broadinstitute/wdl4s/issues/248#issuecomment-334536133). I think this comment is more appropriate here:. Equals is supposed to be bitwise/value-based equality, whereas `eq` indicates reference equality. I don't recall the exact motivation behind using `eq` in the test, maybe @cjllanwarne can comment when he returns on Tuesday. ---. @curoli commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334555964). If you don't like this fix, what do you suggest we do instead? How about we accept this fix now and you can rework the whole thing later, if you want?. This is a show-stopper once you want to create larger (aka real-life) workflows. . For example: if node a has two outputs, which are inputs of b, then b.hashCode calls a.hasCode twice. If b produces two outputs which are inputsof c, then c.hashCode calls b.hashCode twice, and a.hashCode is called four times. If c produces two outputs which are inputs of d, d.hashCode calls c.hashCode twice, b.hashCode four times and a.hashCode eight times, etc. . ---. @curoli commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334557027). `eq` is object equality. `equals` and `==` are the same, and there is no general rule that they need to be component-based (""bitwise""). On the contrary, `equals`/`==` should not be component-based if an object is mutable. Your typical GraphNode is mutable, because it indirectly contains `GraphNodeSetter`. ---. @danbills commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334575875). I suggest we leave this as-is with the understanding that it could be a performance issue down the road. . >rework the whole thing later. This is a specific anti-goal. As I suggested, I would like to discuss w/ Chris when he gets back next week as we introduced the reference equality in the first place and I'm not aware of his motivation to do so. ---. @curoli commented on [Thu Oct 05",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2694:1834,hash,hashCode,1834,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2694,2,['hash'],['hashCode']
Security,"0. workflow wdl_v1_tests {; scatter (x in [0]) {; scatter (y in [0]) {; call input_default_not_used; }; }; }. task input_default_not_used {; input { String greeting = ""hello"" }; command { echo ~{greeting} }; runtime { docker: ""bash"" }; }; ```. ```; [2018-06-08 01:30:26,49] [error] WorkflowManagerActor Workflow 58ccc276-40f7-447c-bbff-87a47aa7163e failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; key not found: wdl_v1_tests.input_default_not_used.greeting; scala.collection.immutable.Map$Map1.apply(Map.scala:111); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$8(ScatterElementToGraphNode.scala:103); scala.collection.immutable.List.map(List.scala:283); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$7(ScatterElementToGraphNode.scala:102); cats.data.Validated.map(Validated.scala:194); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertInnerScatter(ScatterElementToGraphNode.scala:99); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:31); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:1075,Validat,Validated,1075,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Validat'],['Validated']
Security,"0000 \n }\n runtime {\n docker: \""broadinstitute/genomes-in-the-cloud:2.2.3-1469027018\""\n disks: \""local-disk \"" + disk_size + \"" HDD\""\n memory: \""3500 MB\""\n }\n output {\n File unmapped_bam = \""${revert_bam_name}\""\n }\n}\n\ntask SortSam {\n File input_bam\n String sorted_bam_name\n Int disk_size\n\n # TODO: why not use samtools sort as it is multi-threaded?\n command {\n java -Xmx3000m -jar /usr/gitc/picard.jar \\\n SortSam \\\n INPUT=${input_bam} \\\n OUTPUT=${sorted_bam_name} \\\n SORT_ORDER=queryname \\\n MAX_RECORDS_IN_RAM=1000000\n }\n runtime {\n docker: \""broadinstitute/genomes-in-the-cloud:2.2.3-1469027018\""\n disks: \""local-disk \"" + disk_size + \"" HDD\""\n memory: \""3500 MB\""\n }\n output {\n File sorted_bam = \""${sorted_bam_name}\""\n }\n}\n\ntask ValidateSamFile {\n File input_bam\n String report_filename\n Int disk_size\n\n command {\n java -Xmx3000m -jar /usr/gitc/picard.jar \\\n ValidateSamFile \\\n INPUT=${input_bam} \\\n OUTPUT=${report_filename} \\\n MODE=VERBOSE \\\n IS_BISULFITE_SEQUENCED=false \n }\n runtime {\n docker: \""broadinstitute/genomes-in-the-cloud:2.2.3-1469027018\""\n disks: \""local-disk \"" + disk_size + \"" HDD\""\n memory: \""3500 MB\""\n }\n output {\n File report = \""${report_filename}\""\n }\n}\n\nworkflow BamToUnmappedBams {\n File input_bam\n String dir_pattern = \""gs://.*/\""\n #String dir_pattern = \""/.*/\""\n Int revert_sam_disk_size = 400\n Int sort_sam_disk_size = 400\n Int validate_sam_file_disk_size = 200\n\n call RevertSam {\n input:\n input_bam = input_bam,\n revert_bam_name = sub(sub(input_bam, dir_pattern, \""\""), \"".bam$\"", \""\"") + \"".unmapped.bam\"",\n disk_size = revert_sam_disk_size\n }\n\n# call SortSam {\n# input:\n# input_bam = RevertSam.unmapped_bam,\n# sorted_bam_name = sub(sub(RevertSam.unmapped_bam, dir_pattern, \""\""), \"".bam$\"", \""\"") + \"".sorted.bam\"",\n# disk_size = sort_sam_disk_size\n# }\n\n call ValidateSamFile {\n input:\n input_bam = RevertSam.unmapped_bam,\n report_filename = sub(sub(RevertSam.unmapped_ba",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1886:2155,Validat,ValidateSamFile,2155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886,1,['Validat'],['ValidateSamFile']
Security,"000m -jar /usr/gitc/picard.jar \\\n RevertSam \\\n INPUT=${input_bam} \\\n OUTPUT=${revert_bam_name} \\\n VALIDATION_STRINGENCY=LENIENT \\\n ATTRIBUTE_TO_CLEAR=FT \\\n ATTRIBUTE_TO_CLEAR=XS \\\n SORT_ORDER=queryname \\\n MAX_RECORDS_IN_RAM=1000000 \n }\n runtime {\n docker: \""broadinstitute/genomes-in-the-cloud:2.2.3-1469027018\""\n disks: \""local-disk \"" + disk_size + \"" HDD\""\n memory: \""3500 MB\""\n }\n output {\n File unmapped_bam = \""${revert_bam_name}\""\n }\n}\n\ntask SortSam {\n File input_bam\n String sorted_bam_name\n Int disk_size\n\n # TODO: why not use samtools sort as it is multi-threaded?\n command {\n java -Xmx3000m -jar /usr/gitc/picard.jar \\\n SortSam \\\n INPUT=${input_bam} \\\n OUTPUT=${sorted_bam_name} \\\n SORT_ORDER=queryname \\\n MAX_RECORDS_IN_RAM=1000000\n }\n runtime {\n docker: \""broadinstitute/genomes-in-the-cloud:2.2.3-1469027018\""\n disks: \""local-disk \"" + disk_size + \"" HDD\""\n memory: \""3500 MB\""\n }\n output {\n File sorted_bam = \""${sorted_bam_name}\""\n }\n}\n\ntask ValidateSamFile {\n File input_bam\n String report_filename\n Int disk_size\n\n command {\n java -Xmx3000m -jar /usr/gitc/picard.jar \\\n ValidateSamFile \\\n INPUT=${input_bam} \\\n OUTPUT=${report_filename} \\\n MODE=VERBOSE \\\n IS_BISULFITE_SEQUENCED=false \n }\n runtime {\n docker: \""broadinstitute/genomes-in-the-cloud:2.2.3-1469027018\""\n disks: \""local-disk \"" + disk_size + \"" HDD\""\n memory: \""3500 MB\""\n }\n output {\n File report = \""${report_filename}\""\n }\n}\n\nworkflow BamToUnmappedBams {\n File input_bam\n String dir_pattern = \""gs://.*/\""\n #String dir_pattern = \""/.*/\""\n Int revert_sam_disk_size = 400\n Int sort_sam_disk_size = 400\n Int validate_sam_file_disk_size = 200\n\n call RevertSam {\n input:\n input_bam = input_bam,\n revert_bam_name = sub(sub(input_bam, dir_pattern, \""\""), \"".bam$\"", \""\"") + \"".unmapped.bam\"",\n disk_size = revert_sam_disk_size\n }\n\n# call SortSam {\n# input:\n# input_bam = RevertSam.unmapped_bam,\n# sorted_bam_name = sub(sub",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1886:2017,Validat,ValidateSamFile,2017,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886,1,['Validat'],['ValidateSamFile']
Security,"010e4b|callCaching:hashes:input:Int runtime_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int runtime_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int runtime_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int runtime_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int numCPUs|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int numCPUs|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int default_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int default_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int default_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int default_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Float runtime_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Float runtime_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Float default_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Float default_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:File R2Fastq|""9f1cf8859a902eb75202a5c048cd43aa-388""|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:File R2Fastq|""9f1cf8859a902eb75202a5c048cd43aa-388""|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:File R1Fast",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:5306,hash,hashes,5306,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,0] [info] WorkflowManagerActor WorkflowActor-8fa7a9e4-f30d-4c19-b8cb-68be6442f317 is in a terminal state: WorkflowFailedState. ```. Looking at the cloudwatch logs it appears that the problem is with permission on the node. ```; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl': Permission denied; ; 04:26:11; chmod: cannot access '': No such file or directory; ; 04:26:11; mkfifo: cannot create fifo '/out.1': Permission denied; ; 04:26:11; mkfifo: cannot create fifo '/err.1': Permission denied; ; 04:26:11; /bin/bash: line 15: /out.1: No such file or directory; ; 04:26:11; /bin/bash: line 16: /err.1: No such file or directory; ; 04:26:11; /bin/bash: line 22: /out.1: Permission denied; ; 04:26:11; /bin/bash: line 23: /cromwell_root/bbmap-rc.txt.tmp: Permission denied; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa': Permission denied; ; 04:26:11; /bin/bash: line 38: /cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*R?.fq.gz': No such file or directory; ; 04:26:11; /bin/bash: line 44: /cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa': No such file or directory; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': Permission denied; ; 04:26:11; /bin/bash: line 52: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*.qcstats': No such file or directory; ; 04:26:11; /bin/bash: line 58: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': No such file or director,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4542:5183,access,access,5183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542,1,['access'],['access']
Security,"0e4b|callCaching:hashes:input:Int default_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int default_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Float runtime_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Float runtime_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Float default_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Float default_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:File R2Fastq|""9f1cf8859a902eb75202a5c048cd43aa-388""|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:File R2Fastq|""9f1cf8859a902eb75202a5c048cd43aa-388""|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:File R1Fastq|""62396abd6b589747ee16034888c9a0b5-381""|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:File R1Fastq|""62396abd6b589747ee16034888c9a0b5-381""|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input count|9BF31C7FF062936A96D3C8BD1F8F2FF3|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input count|9BF31C7FF062936A96D3C8BD1F8F2FF3|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:command template|7BCEDB02C5FC300FF83F07417B49229E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:command template|7BCEDB02C5FC300FF83F07417B49229E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:backend name|2267EF43AEF6BB551F414FEC2390F68A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:backend name|2267EF43AEF6BB551F414FEC2390F68A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:effectiveCallCachingMode|ReadAndWriteCache|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:effectiveCallCachingMode|ReadAndWriteCa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:6286,hash,hashes,6286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,0e4b|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String default_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String default_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String default_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String default_attr:docker|A5281F25296D4311ED0C46422719018E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int runtime_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int runtime_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int runtime_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int runtime_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCach,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:3824,hash,hashes,3824,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,"1-minute description of this: VariableReference had to become aware of where it was, because `p.left` might need a `Pair` called `p`'s `""left""` field, or it might need a `task` called `p`'s `""left""` output, depending on its scope, and the variable reference is different in each case (`p` and `p.left` respectively). Determining whether a reference is a member access or a task output reference is a bit inefficient right now, but I think it should be ok since it's only called during WDL instantiation (thereafter it's all just following the links in WOM objects)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2947:361,access,access,361,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2947,1,['access'],['access']
Security,"1. Find a scientific workflow and run it manually a couple of times to confirm that it is successfully caching as expected (write to cache set to true for all these runs).; 2. Try to invalidate some cache hits (by deleting certain outputs).; 3. Re-run the workflow and confirm that it's not using the invalidated cache hits and using alternate cache hits instead.; 4. Re-run with using floating tags vs hashing and confirm expected behavior.; ; AC: Try the alterations above and others that seem relevant. Keep logs of the workflow attempts or metadata, something to journal this testing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1986:403,hash,hashing,403,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1986,1,['hash'],['hashing']
Security,1.0: allow *file-expecting* function calls to work against index access expressions,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3660:65,access,access,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3660,1,['access'],['access']
Security,107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); 	at cromwell.backend.impl.aws.AwsBatchAsyncBac,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:6701,validat,validatedRuntimeAttributes,6701,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,1,['validat'],['validatedRuntimeAttributes']
Security,16]: at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:794); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:52); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileH,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3383:2135,Hash,HashFileStrategy,2135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383,1,['Hash'],['HashFileStrategy']
Security,1; mkfifo: cannot create fifo '/err.1': Permission denied; ; 04:26:11; /bin/bash: line 15: /out.1: No such file or directory; ; 04:26:11; /bin/bash: line 16: /err.1: No such file or directory; ; 04:26:11; /bin/bash: line 22: /out.1: Permission denied; ; 04:26:11; /bin/bash: line 23: /cromwell_root/bbmap-rc.txt.tmp: Permission denied; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa': Permission denied; ; 04:26:11; /bin/bash: line 38: /cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*R?.fq.gz': No such file or directory; ; 04:26:11; /bin/bash: line 44: /cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-2e18d4d3f934d19c17412db2b66b70fa': No such file or directory; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': Permission denied; ; 04:26:11; /bin/bash: line 52: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/*.qcstats': No such file or directory; ; 04:26:11; /bin/bash: line 58: /cromwell_root/glob-560912e697c3494360223c7ca65aa3e8.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/glob-560912e697c3494360223c7ca65aa3e8': No such file or directory; ; 04:26:11; mkdir: cannot create directory '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': Permission denied; ; 04:26:11; /bin/bash: line 66: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe/cromwell_glob_control_file: No such file or directory; ; 04:26:11; ln: failed to access '/cromwell_root/cwl.output.json': No such file or directory; ; 04:26:11; /bin/bash: line 72: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe.list: Permission denied; ; 04:26:11; ls: cannot access '/cromwell_root/,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4542:5380,access,access,5380,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542,2,['access'],['access']
Security,2); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:200); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq:; 	fastq doesn't exist; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	File not found fastq; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574). ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:15083,validat,validation,15083,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,8,"['Validat', 'validat']","['Validation', 'ValidationTry', 'validation']"
Security,2); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.net.SocketTimeoutException: Read timed out; 	at java.net.SocketInputStream.socketRead0(Native Method); 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); 	at java.net.SocketInputStream.read(SocketInputStream.java:171); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	at sun.security.ssl.InputRecord.read(InputRecord.java:503); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); 	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1569); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1474); 	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(H,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2229:3609,secur,security,3609,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229,1,['secur'],['security']
Security,"2-15 21:27:55,79] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.categorical_covariates:0:1-20000000027 [9e4f5894main.categorical_covariates:0:1]: Unrecognized runtime attribute keys: dx_t; imeout; [2022-12-15 21:27:55,79] [info] BT-322 9e4f5894:main.categorical_covariates:0:1 cache hit copying success with aggregated hashes: initial = C760DC2B9015D0B787EF7BEE7D21AA58, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:55,79] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.categorical_covariates:0:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:55,79] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.pcs:-1:1-20000000010 [9e4f5894main.pcs:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:27:55,79] [info] BT-322 9e4f5894:main.pcs:-1:1 cache hit copying success with aggregated hashes: initial = 58D108557F21E539CF9BE064A9528392, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:55,79] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.pcs:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:56,12] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.ethnicity_self_report:-1:1-20000000008 [9e4f5894main.ethnicity_self_report:NA:1]: Unrecognized runtime attribute keys: dx_t; imeout; [2022-12-15 21:27:56,12] [info] BT-322 9e4f5894:main.ethnicity_self_report:-1:1 cache hit copying success with aggregated hashes: initial = A32F403CF4C1AEE5AC6D327D9290D15E, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:56,12] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.ethnicity_self_report:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:56,51] [info] WorkflowExecutionAc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:31154,hash,hashes,31154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['hash'],['hashes']
Security,200 files scattered 200x fails to call cache due to GCS hash timeout,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4873:56,hash,hash,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4873,1,['hash'],['hash']
Security,"2016-06-02 00:18:27,540 cromwell-system-akka.actor.default-dispatcher-341 INFO - WorkflowActor [UUID(fa18fa5f)]: persisting status of RotateGVCFIndex to Running.; 2016-06-01T20:19:00.527-0400: 104257.028: [GC (Allocation Failure) [PSYoungGen: 1821155K->270322K(1864192K)] 4006446K->2894561K(7456768K), 0.1718483 secs] [Times: user=1.22 sys=0.01, real=0.17 secs] ; 2016-06-02 00:19:39,491 ForkJoinPool-3-worker-7 ERROR - Exception not convertible into handled response; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method) ~[na:1.8.0_72]; at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[na:1.8.0_72]; at java.net.SocketInputStream.read(SocketInputStream.java:170) ~[na:1.8.0_72]; at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[na:1.8.0_72]; at sun.security.ssl.InputRecord.readFully(InputRecord.java:465) ~[na:1.8.0_72]; at sun.security.ssl.InputRecord.read(InputRecord.java:503) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930) ~[na:1.8.0_72]; at sun.security.ssl.AppInputStream.read(AppInputStream.java:105) ~[na:1.8.0_72]; at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_72]; at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_72]; at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_72]; at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704) ~[na:1.8.0_72]; at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647) ~[na:1.8.0_72]; at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536) ~[na:1.8.0_72]; at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441) ~[na:1.8.0_72]; at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConne",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/932:2443,secur,security,2443,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/932,1,['secur'],['security']
Security,"20:19:00.527-0400: 104257.028: [GC (Allocation Failure) [PSYoungGen: 1821155K->270322K(1864192K)] 4006446K->2894561K(7456768K), 0.1718483 secs] [Times: user=1.22 sys=0.01, real=0.17 secs] ; 2016-06-02 00:19:39,491 ForkJoinPool-3-worker-7 ERROR - Exception not convertible into handled response; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method) ~[na:1.8.0_72]; at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[na:1.8.0_72]; at java.net.SocketInputStream.read(SocketInputStream.java:170) ~[na:1.8.0_72]; at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[na:1.8.0_72]; at sun.security.ssl.InputRecord.readFully(InputRecord.java:465) ~[na:1.8.0_72]; at sun.security.ssl.InputRecord.read(InputRecord.java:503) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930) ~[na:1.8.0_72]; at sun.security.ssl.AppInputStream.read(AppInputStream.java:105) ~[na:1.8.0_72]; at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_72]; at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_72]; at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_72]; at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704) ~[na:1.8.0_72]; at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647) ~[na:1.8.0_72]; at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536) ~[na:1.8.0_72]; at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441) ~[na:1.8.0_72]; at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37) ~[cromwell.ja",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/932:2617,secur,security,2617,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/932,1,['secur'],['security']
Security,234); 	at scala.collection.immutable.List.map(List.scala:285); 	at wdl4s.WdlNamespace$.apply(WdlNamespace.scala:207); 	at wdl4s.WdlNamespace$.wdl4s$WdlNamespace$$load(WdlNamespace.scala:177); 	at wdl4s.WdlNamespace$.loadUsingSource(WdlNamespace.scala:173); 	at wdl4s.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:542); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$validateNamespaceWithImports$1.apply(MaterializeWorkflowDescriptorActor.scala:363); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$validateNamespaceWithImports$1.apply(MaterializeWorkflowDescriptorActor.scala:356); 	at lenthall.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:17); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.validateNamespaceWithImports(MaterializeWorkflowDescriptorActor.scala:356); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.validateNamespace(MaterializeWorkflowDescriptorActor.scala:372); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:172); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:132); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:130); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:117); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.processEvent(Materializ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1958:2496,validat,validateNamespace,2496,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1958,1,['validat'],['validateNamespace']
Security,"255) NULL, DEPLOYMENT_ID VARCHAR(10) NULL); 2019-01-31 18:29:35,271 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,279 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-01-31 18:29:35,282 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBacke",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:2712,Validat,ValidatingVisitor,2712,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['Validat'],['ValidatingVisitor']
Security,"268581 container_name/cromwellazure_cromwell_1[2296]: #011at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:54); Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at akka.actor.ActorCell.invoke(ActorCell.scala:583); Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at akka.dispatch.Mailbox.run(Mailbox.scala:229); Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at akka.dispatch.Mailbox.exec(Mailbox.scala:241); Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Sep 1 16:44:51 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: 2022-09-01 16:44:51,173 cromwell-system-akka.dispatchers.engine-dispatcher-29033 INFO - WorkflowManagerActor: Workflow actor for 2cd0993c-94df-4663-923d-48bbce3feead completed with status 'Failed'. The workflow will be removed from the workflow store. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6904:6349,PASSWORD,PASSWORDS,6349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6904,1,['PASSWORD'],['PASSWORDS']
Security,"27 / 26 Docker hash consistency, develop edition.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2361:15,hash,hash,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2361,1,['hash'],['hash']
Security,"28000; #tsv = 128000; #map = 128000; #object = 128000; }. abort {; # These are the default values in Cromwell, in most circumstances there should not be a need to change them. # How frequently Cromwell should scan for aborts.; scan-frequency: 30 seconds. # The cache of in-progress aborts. Cromwell will add entries to this cache once a WorkflowActor has been messaged to abort.; # If on the next scan an 'Aborting' status is found for a workflow that has an entry in this cache, Cromwell will not ask; # the associated WorkflowActor to abort again.; cache {; enabled: true; # Guava cache concurrency.; concurrency: 1; # How long entries in the cache should live from the time they are added to the cache.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 100000; }; }. # Cromwell reads this value into the JVM's `networkaddress.cache.ttl` setting to control DNS cache expiration; dns-cache-ttl: 3 minutes; }. docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; enabled = ""false""; }; }. # Here is where you can define the backend providers that Cromwell understands.; # The default is a local provider.; # To add additional backend providers, you should copy paste additional backends; # of interest that you can find in the cromwell.example.backends folder; # folder at https://www.github.com/broadinstitute/cromwell; # Other backend providers include SGE, SLURM, Docker, udocker, ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:3998,hash,hash-lookup,3998,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['hash'],['hash-lookup']
Security,29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:runtime attribute:continueOnReturnCode|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:output expression:File sorted_bam|816FBD6EEA5D806309AA0664E2F2AB86|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output expression:File sorted_bam|816FBD6EEA5D806309AA0664E2F2AB86|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String default_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String default_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String default_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:3199,hash,hashes,3199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,2AB86|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output expression:File sorted_bam|816FBD6EEA5D806309AA0664E2F2AB86|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String default_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String default_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String default_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String default_attr:docker|A5281F25296D4311ED0C46422719018E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int runtime_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCach,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:3447,hash,hashes,3447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,"2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.white_brits' (scatter index: None, attempt 1); [2022-12-15 21:23:03,67] [info] Assigned new job execution tokens to the following groups: 9e4f5894: 3; [2022-12-15 21:23:03,69] [info] BT-322 9e4f5894:main.categorical_covariates:0:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:23:03,70] [info] BT-322 9e4f5894:main.ethnicity_self_report:-1:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:23:03,70] [info] BT-322 9e4f5894:main.pcs:-1:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:27:50,35] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.assessment_ages:-1:1-20000000002 [9e4f5894main.assessment_ages:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:27:50,35] [info] BT-322 9e4f5894:main.assessment_ages:-1:1 cache hit copying success with aggregated hashes: initial = EEC3507DAE39FE605FDE6F9F6FC0A5A8, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:50,35] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.assessment_ages:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:50,48] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.assessment_ages' (scatter index: None, attempt 1); [2022-12-15 21:27:50,82] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.genetic_sex:-1:1-20000000011 [9e4f5894main.genetic_sex:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:27:50,82] [info] BT-322 9e4f5894:main.genetic_sex:-1:1 cache hit copying success with aggregated hashes: initial = FD7DC79B974CF6706FC3376F067965B9, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:50,82] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecuti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:28837,hash,hashes,28837,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['hash'],['hashes']
Security,32); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:264); at com.typesafe.config.ConfigFactory$1.call(ConfigFactory.java:261); at com.typesafe.config.impl.ConfigImpl$LoaderCache.getOrElseUpdate(ConfigImpl.java:66); at com.typesafe.config.impl.ConfigImpl.computeCachedConfig(ConfigImpl.java:93); at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:261); at com.typesafe.config.ConfigFactory.load(ConfigFactory.java:237); at cromwell.languages.util.ImportResolver$HttpResolver$.apply(ImportResolver.scala:237); at womtool.input.WomGraphMaker$.importResolvers$lzycompute$1(WomGraphMaker.scala:28); at womtool.input.WomGraphMaker$.importResolvers$1(WomGraphMaker.scala:27); at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala:39); at scala.util.Either.flatMap(Either.scala:352); at womtool.input.WomGraphMaker$.getBundleAndFactory(WomGraphMaker.scala:30); at womtool.input.WomGraphMaker$.fromFiles(WomGraphMaker.scala:46); at womtool.validate.Validate$.validate(Validate.scala:26); at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:54); at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:161); at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:166); at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:27); at scala.Function0.apply$mcV$sp(Function0.scala:42); at scala.Function0.apply$mcV$sp$(Function0.scala:42); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); at scala.App.$anonfun$main$1(App.scala:98); at scala.App.$anonfun$main$1$adapted(App.scala:98); at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:575); at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:573); at scala.collection.AbstractIterable.foreach(Iterable.scala:933); at scala.App.main(App.scala:98); at scala.App.main$(App.scala:96); at womtool.WomtoolMain$.main(WomtoolMain.scala:27); at womtool.WomtoolMain.main(WomtoolMain.scala); Caused by: com.typesafe.config.Config,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:3620,Validat,Validate,3620,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['Validat'],['Validate']
Security,"32d67e-3e95-40c8-acbd-d42f75040f1b""; }; ```. Before:; ```; ERROR - Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; java.lang.UnsupportedOperationException: Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; 	at wom.values.WomMap.<init>(WomMap.scala:79); 	at wom.values.WomMap$.apply(WomMap.scala:54); 	at wom.values.WomMap$.coerceMap(WomMap.scala:34); 	at wom.values.WomMap$.apply(WomMap.scala:50); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$5.$anonfun$evaluateValue$12(LiteralEvaluators.scala:90); 	at cats.data.Validated.map(Validated.scala:559); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$5.evaluateValue(LiteralEvaluators.scala:87); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$5.evaluateValue(LiteralEvaluators.scala:73); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$ops$$anon$1.evaluateValue(ValueEvaluator.scala:10); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:36); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:22); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.express",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7385:2307,Validat,Validated,2307,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385,1,['Validat'],['Validated']
Security,"4); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: 500 Internal Server Error; {; ""error"" : ""internal_failure""; }; at cromwell.filesystems.gcs.auth.GoogleAuthMode$class.validate(GoogleAuthMode.scala:66); at cromwell.filesystems.gcs.auth.GoogleAuthMode$class.validateCredential(GoogleAuthMode.scala:62); at cromwell.filesystems.gcs.auth.RefreshTokenMode.validateCredential(GoogleAuthMode.scala:127); at cromwell.filesystems.gcs.auth.RefreshTokenMode.credential(GoogleAuthMode.scala:147); at cromwell.filesystems.gcs.GcsPathBuilder.<init>(GcsPathBuilder.scala:57); at cromwell.filesystems.gcs.GcsPathBuilderFactory.withOptions(GcsPathBuilderFactory.scala:37); at cromwell.backend.impl.jes.JesWorkflowPaths.<init>(JesWorkflowPaths.scala:25); at cromwell.backend.impl.jes.JesWorkflowPaths.copy(JesWorkflowPaths.scala:19); at cromwell.backend.impl.jes.JesWorkflowPaths.withDescriptor(JesWorkflowPaths.scala:54); at cromwell.backend.io.WorkflowPaths$class.toJobPaths(WorkflowPaths.scala:51); at cromwell.backend.impl.jes.JesWorkflowPaths.toJobPaths(JesWorkflowPaths.scala:19); at cromwell.backend.io.WorkflowPaths$class.toJobPaths(WorkflowPaths.scala:36); at cromwell.backend.impl.jes.JesWorkflowPaths.toJobPaths(JesWorkflowPaths.scala:19); at cromwell.backend.standard.StandardCachingActorHelper$class.jobPaths(StandardCachingActorHelper.scala:64); at cromwell.backend.impl.j",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2270:2132,validat,validateCredential,2132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2270,1,['validat'],['validateCredential']
Security,"40c8-acbd-d42f75040f1b""; }; ```. Before:; ```; ERROR - Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; java.lang.UnsupportedOperationException: Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; 	at wom.values.WomMap.<init>(WomMap.scala:79); 	at wom.values.WomMap$.apply(WomMap.scala:54); 	at wom.values.WomMap$.coerceMap(WomMap.scala:34); 	at wom.values.WomMap$.apply(WomMap.scala:50); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$5.$anonfun$evaluateValue$12(LiteralEvaluators.scala:90); 	at cats.data.Validated.map(Validated.scala:559); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$5.evaluateValue(LiteralEvaluators.scala:87); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$5.evaluateValue(LiteralEvaluators.scala:73); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$ops$$anon$1.evaluateValue(ValueEvaluator.scala:10); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:36); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:22); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEva",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7385:2321,Validat,Validated,2321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385,1,['Validat'],['Validated']
Security,47a-44ba-aff0-7ab48bc10677|callCaching:hashes:runtime attribute:docker|4AD3C387725244C1348F252B031B956D|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:continueOnReturnCode|CFCD208495D565EF66E7DFF9F98764DA|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:runtime attribute:continueOnReturnCode|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:output expression:File sorted_bam|816FBD6EEA5D806309AA0664E2F2AB86|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output expression:File sorted_bam|816FBD6EEA5D806309AA0664E2F2AB86|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String default_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:S,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:2961,hash,hashes,2961,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,"4842beb\"",\""os\"":\""linux\"",\""parent\"":\""633c756fc95cb19efd13b84a949066c65f5b6683d0922d1980b6a19deb855fa0\"",\""throwaway\"":true}""; },; {; ""v1Compatibility"": ""{\""id\"":\""633c756fc95cb19efd13b84a949066c65f5b6683d0922d1980b6a19deb855fa0\"",\""created\"":\""2017-08-07T23:50:27.303876981Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) ADD file:fb17197475bb59bfb365c41f28d4bc15134b8dcb8907819e7be54bce53328c03 in / \""]}}""; }; ],; ""schemaVersion"": 1,; ""signatures"": [; {; ""header"": {; ""jwk"": {; ""crv"": ""P-256"",; ""kid"": ""4DPT:XWGC:ESR7:JVJY:2RON:CMML:IXIT:QXQ5:LGLL:LPJF:PWDL:AJSO"",; ""kty"": ""EC"",; ""x"": ""BjSTZs2e-5wP1bu4deBughI6YALM3vbLbZL-CGBRcmM"",; ""y"": ""Qj5Fr4Z1BQRe4EXMe-75dOkvIDzP-0u5cks8my7hkCA""; },; ""alg"": ""ES256""; },; ""signature"": ""ewZ_2lh2-uWSpw5tcprbhoFvjLoxGqsI06YSlvq4w2eXB5EEpMsk5Jo6WYRBeYeJxv0vnoe7SbN_1qarBAU9uQ"",; ""protected"": ""eyJmb3JtYXRMZW5ndGgiOjIxOTUsImZvcm1hdFRhaWwiOiJmUSIsInRpbWUiOiIyMDE3LTExLTA2VDE2OjE2OjE4WiJ9""; }; ]; }; ```. ```bash; $ curl -i -s -H 'Accept: application/vnd.docker.distribution.manifest.v2+json' https://gcr.io/v2/google-containers/ubuntu-slim/manifests/0.14; HTTP/1.1 200 OK; Docker-Distribution-API-Version: registry/2.0; Content-Type: application/vnd.docker.distribution.manifest.v2+json; Content-Length: 529; Docker-Content-Digest: sha256:1d5c0118358fc7651388805e404fe491a80f489bf0e7c5f8ae4156250d6ec7d8; Date: Mon, 06 Nov 2017 16:16:59 GMT; Server: Docker Registry; X-XSS-Protection: 1; mode=block; X-Frame-Options: SAMEORIGIN; Alt-Svc: quic="":443""; ma=2592000; v=""41,39,38,37,35"". {; ""schemaVersion"": 2,; ""mediaType"": ""application/vnd.docker.distribution.manifest.v2+json"",; ""config"": {; ""mediaType"": ""application/vnd.docker.container.image.v1+json"",; ""size"": 1528,; ""digest"": ""sha256:fba1281b32ffd9048881f99d8de0218c71552c4c91f09844b4b189b16e51cdca""; },; ""layers"": [; {; ""mediaType"": ""application/vnd.docker.image.rootfs.diff.tar.gzip"",; ""size"": 18278657,; ""digest"": ""sha256:1c4816548d6a2a08f89c304bf09503e791a338c4be90629610152124c7285d3f""; }; ]; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2826:5099,XSS,XSS-Protection,5099,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2826,1,['XSS'],['XSS-Protection']
Security,"5-9791-9011a2fae80f/call-batch_for_variantcall -o /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/stdout -e /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/stderr -t 1-00:00 -p core --cpus-per-task=1 --mem=4026 --wrap ""/usr/bin/env bash /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/script""; [2018-05-02 15:16:57,63] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: job id: 134053; [2018-05-02 15:16:57,66] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-05-02 15:17:05,03] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-05-02 15:22:54,62] [[38;5;1merror[0m] Failed to hash null; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrEl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584:4830,hash,hash,4830,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584,1,['hash'],['hash']
Security,500 Internal error when trying to get task hash for call caching,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/600:43,hash,hash,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/600,1,['hash'],['hash']
Security,"5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450:1317,hash,hash,1317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450,1,['hash'],['hash']
Security,"5a79a-1369-4dc1-8f41-4180d7b3c1ab.0000.g.vcf.gz.tbi"". Whoever picks up this ticket can talk to me about specifics and why we think its an 85MB issue. ```; 2016-06-02 00:18:27,540 cromwell-system-akka.actor.default-dispatcher-341 INFO - WorkflowActor [UUID(fa18fa5f)]: persisting status of RotateGVCFIndex to Running.; 2016-06-01T20:19:00.527-0400: 104257.028: [GC (Allocation Failure) [PSYoungGen: 1821155K->270322K(1864192K)] 4006446K->2894561K(7456768K), 0.1718483 secs] [Times: user=1.22 sys=0.01, real=0.17 secs] ; 2016-06-02 00:19:39,491 ForkJoinPool-3-worker-7 ERROR - Exception not convertible into handled response; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method) ~[na:1.8.0_72]; at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[na:1.8.0_72]; at java.net.SocketInputStream.read(SocketInputStream.java:170) ~[na:1.8.0_72]; at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[na:1.8.0_72]; at sun.security.ssl.InputRecord.readFully(InputRecord.java:465) ~[na:1.8.0_72]; at sun.security.ssl.InputRecord.read(InputRecord.java:503) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930) ~[na:1.8.0_72]; at sun.security.ssl.AppInputStream.read(AppInputStream.java:105) ~[na:1.8.0_72]; at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_72]; at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_72]; at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_72]; at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704) ~[na:1.8.0_72]; at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647) ~[na:1.8.0_72]; at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536) ~[na:1.8.0_72]; at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/932:2288,secur,security,2288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/932,1,['secur'],['security']
Security,"6); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:316); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/xxx/o?projection=full&userProject=xxx&uploadType=multipart; {; ""code"" : 403,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project."",; ""reason"" : ""forbidden""; } ],; ""message"" : ""xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.""; }; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:150); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:555); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:475); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:592); 	at com.google.cloud.storage.spi.v1.H",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594:3483,access,access,3483,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594,2,['access'],['access']
Security,"67] [info] Running with database db.url = jdbc:hsqldb:mem:6713284f-67ff-4eb9-9fd6-3fde0a4cc0ce;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:10,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:36:10,87] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:36:11,02] [info] Running with database db.url = jdbc:hsqldb:mem:5893545c-e081-4c3d-827d-000af3765fc4;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:11,72] [info] Slf4jLogger started; Exception in thread ""main"" cromwell.CromwellEntryPoint$$anon$1: ERROR: Unable to submit workflow to Cromwell::; Workflow source does not exist: does-not-exist.wdl; 	at cromwell.CromwellEntryPoint$.$anonfun$validOrFailSubmission$1(CromwellEntryPoint.scala:219); 	at cats.data.Validated.valueOr(Validated.scala:48); 	at cromwell.CromwellEntryPoint$.validOrFailSubmission(CromwellEntryPoint.scala:219); 	at cromwell.CromwellEntryPoint$.validateRunArguments(CromwellEntryPoint.scala:215); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:56); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ```; Command-line tools are subject to usability standards identical to those of our other user interfaces. Unless the intended audience of this tool is Cromwell engineers, the stacktrace information is ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4060:1096,validat,validateRunArguments,1096,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4060,1,['validat'],['validateRunArguments']
Security,"73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143:1904,validat,validateWdlNamespace,1904,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143,1,['validat'],['validateWdlNamespace']
Security,74); at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470); at io.grpc.netty.shaded.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:403); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997); at io.grpc.netty.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74); at io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30); ... 1 common frames omitted; Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target; at java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:388); at java.base/sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:271); at java.base/sun.security.validator.Validator.validate(Validator.java:256); at java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:284); at java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:144); at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslClientContext$ExtendedTrustManagerVerifyCallback.verify(ReferenceCountedOpenSslClientContext.java:234); at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslContext$AbstractCertificateVerifier.verify(ReferenceCountedOpenSslContext.java:779); at io.grpc.netty.shaded.io.netty.internal.tcnative.CertificateVerifierTask.runTask(CertificateVerifierTask.java:36); at io.grpc.netty.shaded.io.netty.internal.tcnative.SSLTask.run(SSLTask.java:48); at io.grpc.netty.shaded.io.netty.internal.tcnative.SSLTask.run(SSLTask.java:42); at io.grpc.netty.shaded.io.netty.handler.ssl.Reference,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7551:7152,secur,security,7152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7551,1,['secur'],['security']
Security,74); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); at wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); at wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); at wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:83); at wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); at scala.util.Either$RightProjection.flatMap(Either.scala:702); at cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); at cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); at cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); at languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala:49); at scala.util.Either.flatMap(Either.scala:338); at womtool.input.WomGraphMaker$.getBundleAndFactory(WomGraphMaker.scala:40); at womtool.input.WomGraphMaker$.getBundle(WomGraphMaker.scala:22); at womtool.validate.Validate$.validate(Validate.scala:14); at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:44); at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:125); at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:130); at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:18); at scala.Function0.apply$mcV$sp(Function0.scala:34); at scala.Function0.apply$mcV$sp$(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App.$anonfun$main$1$adapted(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:389); at scala.App.main(App.scala:76); at scala.App.main$(App.scala:74); at womtool.WomtoolMain$.main(WomtoolMain.scala:18); at womtool.WomtoolMain.main(WomtoolMain.scala); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3977:5595,validat,validate,5595,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977,4,"['Validat', 'validat']","['Validate', 'validate']"
Security,"7ab48bc10677|callCaching:hashes:input:Int default_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Float runtime_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Float runtime_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Float default_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Float default_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:File R2Fastq|""9f1cf8859a902eb75202a5c048cd43aa-388""|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:File R2Fastq|""9f1cf8859a902eb75202a5c048cd43aa-388""|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:File R1Fastq|""62396abd6b589747ee16034888c9a0b5-381""|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:File R1Fastq|""62396abd6b589747ee16034888c9a0b5-381""|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input count|9BF31C7FF062936A96D3C8BD1F8F2FF3|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input count|9BF31C7FF062936A96D3C8BD1F8F2FF3|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:command template|7BCEDB02C5FC300FF83F07417B49229E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:command template|7BCEDB02C5FC300FF83F07417B49229E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:backend name|2267EF43AEF6BB551F414FEC2390F68A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:backend name|2267EF43AEF6BB551F414FEC2390F68A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:effectiveCallCachingMode|ReadAndWriteCache|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:effectiveCallCachingMode|ReadAndWriteCache|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:allowResultReuse|true|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:6403,hash,hashes,6403,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,"7f71b554), I am recommending we just delete the test instead of spending any more time on this. ```; > gcloud beta lifesciences operations describe projects/1005074806481/locations/us-central1/operations/8650136336352694244 --format=json; {; ""done"": true,; ""error"": {; ""code"": 9,; ""message"": ""Execution failed: generic::failed_precondition: while running \""-c /bin/bash /cromwell_root/gcs_localization.sh\"": unexpected exit status 1 was not ignored""; },; ""metadata"": {; ""@type"": ""type.googleapis.com/google.cloud.lifesciences.v2beta.Metadata"",; ""createTime"": ""2023-12-04T20:36:45.056562Z"",; ""endTime"": ""2023-12-04T21:10:43.697318162Z"" # <- WTF!!; }; [...]; ```. ```; Long duration; Warning: arning] Using a password on the command line interface can be insecure.; +--------------------------------------+-----------------+----------------------------+----------------------------+; | name | RUNTIME_MINUTES | start | end |; +--------------------------------------+-----------------+----------------------------+----------------------------+; | localize_file_larger_than_disk_space | 35 | 2023-12-05 01:01:27.836000 | 2023-12-05 01:37:10.789000 |; | lots_of_inputs | 32 | 2023-12-05 01:02:03.292000 | 2023-12-05 01:34:26.490000 |; | draft3_call_cache_capoeira | 27 | 2023-12-05 01:03:01.338000 | 2023-12-05 01:30:34.171000 |; ```. ```; Late finishers; Warning: arning] Using a password on the command line interface can be insecure.; +------------------------------------------+-----------------+----------------------------+----------------------------+; | name | runtime_minutes | start | END |; +------------------------------------------+-----------------+----------------------------+----------------------------+; | restart_while_failing | 16 | 2023-12-05 01:33:04.179000 | 2023-12-05 01:49:36.795000 |; | localize_file_larger_than_disk_space | 35 | 2023-12-05 01:01:27.836000 | 2023-12-05 01:37:10.789000 |; | lots_of_inputs | 32 | 2023-12-05 01:02:03.292000 | 2023-12-05 01:34:26.490000 |; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7330:2094,password,password,2094,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7330,1,['password'],['password']
Security,"8-13347: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; mpatch.c in Mercurial before 4.6.1 mishandles integer addition and subtraction, aka OVE-20180430-0002.; https://security-tracker.debian.org/tracker/CVE-2018-13347; -----------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; https://security-tracker.debian.org/tracker/CVE-2017-17458; -----------------------------------------; CVE-2017-12562: [High] ; Found in: libsndfile [1.0.27-3]; Fixed By: ; Heap-based Buffer Overflow in the psf_binheader_writef function in common.c in libsndfile through 1.0.28 allows remote attackers to cause a denial of service (application crash) or possibly have unspecified other impact.; https://security-tracker.debian.org/tracker/CVE-2017-12562; -----------------------------------------; CVE-2018-1000001: [High] ; Found in: glibc [2.24-11+deb9u4]; Fixed By: ; In glibc 2.26 and earlier there is confusion in the usage of getcwd() by realpath() which can be used to write before the destination buffer leading to a buffer underflow and potential code execution.; https://security-tracker.debian.org/tracker/CVE-2018-1000001; -----------------------------------------; CVE-2016-2779: [High] ; Found in: util-linux [2.29.2-1+deb9u1]; Fixed By: ; runuser in util-linux allows local users to escape to the parent session via a crafted TIOCSTI ioctl call, which pushes characters to the terminal's input buffer.; https://security-tracker.debian.org/tracker/CVE-2016-2779; -----------------------------------------; CVE-2017-14062: [High] ; Found in: libidn [1.33-1]; Fixed By: ; Integer overflow in the decode_digit function in p",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4979:2290,attack,attackers,2290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979,1,['attack'],['attackers']
Security,8764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:output expression:File sorted_bam|816FBD6EEA5D806309AA0664E2F2AB86|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output expression:File sorted_bam|816FBD6EEA5D806309AA0664E2F2AB86|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String default_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String default_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String default_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String default_attr:docker|A5281F25296D4311ED0C46422719018E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCach,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:3320,hash,hashes,3320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,"8bc10677|callCaching:hashes:input:Int runtime_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int runtime_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int runtime_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int numCPUs|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int numCPUs|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int default_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int default_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int default_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int default_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Float runtime_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Float runtime_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Float default_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Float default_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:File R2Fastq|""9f1cf8859a902eb75202a5c048cd43aa-388""|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:File R2Fastq|""9f1cf8859a902eb75202a5c048cd43aa-388""|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:File R1Fastq|""62396abd6b589747ee16034888c9a0b5-381""|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:File R1Fastq|""62396",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:5431,hash,hashes,5431,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,"8bc10677|callCaching:hashes:input:String default_attr:docker|A5281F25296D4311ED0C46422719018E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int runtime_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int runtime_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int runtime_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int runtime_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int numCPUs|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int numCPUs|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int default_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int default_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int default_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int default_attr:cpu_cores|C74D97B01EAE257E44AA9D5BADE97BAF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Float runtime_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Float runtime_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Float default_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Float default_attr:mem_gb|5BA1DE412E01037F8843D097DCFAF28A|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:File R2Fastq|""9f1cf8859a902eb75202a5c048cd43aa-388""|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:F",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:5179,hash,hashes,5179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,8bc10677|callCaching:hashes:runtime attribute:failOnStderr|68934A3E9455FA72420237EB05902327|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:docker|4AD3C387725244C1348F252B031B956D|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:runtime attribute:docker|4AD3C387725244C1348F252B031B956D|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:continueOnReturnCode|CFCD208495D565EF66E7DFF9F98764DA|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:runtime attribute:continueOnReturnCode|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:output expression:File sorted_bam|816FBD6EEA5D806309AA0664E2F2AB86|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output expression:File sorted_bam|816FBD6EEA5D806309AA0664E2F2AB86|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:docker|A528,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:2739,hash,hashes,2739,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,9];   at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19];   at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19];   at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19];   at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19];   at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19];   at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$.withRetry(GcsFileSystemProvider.scala:44) ~[cromwell.jar:0.19];   at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider.crc32cHash(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19];   at cromwell.engine.backend.io.package$PathEnhanced$.hash$extension(package.scala:32) ~[cromwell.jar:0.19];   at cromwell.engine.backend.WorkflowDescriptor$$anonfun$fileHasher$1.apply(WorkflowDescriptor.scala:65) ~[cromwell.jar:0.19];   at cromwell.engine.backend.WorkflowDescriptor$$anonfun$fileHasher$1.apply(WorkflowDescriptor.scala:63) ~[cromwell.jar:0.19];   at wdl4s.values.WdlValue$class.computeHash(WdlValue.scala:63) ~[cromwell.jar:0.19];   at wdl4s.values.WdlSingleFile.computeHash(WdlFile.scala:39) ~[cromwell.jar:0.19];   at cromwell.engine.backend.WorkflowDescriptor.hash(WorkflowDescriptor.scala:229) ~[cromwell.jar:0.19];   at cromwell.engine.backend.jes.JesBackend$$anonfun$postProcess$1$$anonfun$apply$14.apply(JesBackend.scala:627) ~[cromwell.jar:0.19];   at cromwell.engine.backend.jes.JesBackend$$anonfun$postProcess$1$$anonfun$apply$14.apply(JesBackend.scala:626) ~[cromwell.jar:0.19];   at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245) ~[cromwell.jar:0.19];   at scala.collection.MapLike$MappedValues$$anonf,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/810:2568,hash,hash,2568,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/810,1,['hash'],['hash']
Security,9];   at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19];   at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19];   at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19];   at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19];   at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19];   at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$.withRetry(GcsFileSystemProvider.scala:44) ~[cromwell.jar:0.19];   at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider.crc32cHash(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19];   at cromwell.engine.backend.io.package$PathEnhanced$.hash$extension(package.scala:32) ~[cromwell.jar:0.19];   at cromwell.engine.backend.WorkflowDescriptor$$anonfun$fileHasher$1.apply(WorkflowDescriptor.scala:65) ~[cromwell.jar:0.19];   at cromwell.engine.backend.WorkflowDescriptor$$anonfun$fileHasher$1.apply(WorkflowDescriptor.scala:63) ~[cromwell.jar:0.19];   at wdl4s.values.WdlValue$class.computeHash(WdlValue.scala:63) ~[cromwell.jar:0.19];   at wdl4s.values.WdlSingleFile.computeHash(WdlFile.scala:39) ~[cromwell.jar:0.19];   at wdl4s.values.WdlValue$$anonfun$computeHash$3.apply(WdlValue.scala:62) ~[cromwell.jar:0.19];   at wdl4s.values.WdlValue$$anonfun$computeHash$3.apply(WdlValue.scala:62) ~[cromwell.jar:0.19];   at scala.collection.immutable.Stream$$anonfun$map$1.apply(Stream.scala:418) ~[cromwell.jar:0.19];   at scala.collection.immutable.Stream$$anonfun$map$1.apply(Stream.scala:418) ~[cromwell.jar:0.19];   at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233) ~[cromwell.jar:0.19];   at scala.collection.immutable.Stream$Cons.tail(,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/826:3337,hash,hash,3337,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/826,1,['hash'],['hash']
Security,: General OpenSslEngine problem; at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslEngine.handshakeException(ReferenceCountedOpenSslEngine.java:1907); at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslEngine.wrap(ReferenceCountedOpenSslEngine.java:834); at java.base/javax.net.ssl.SSLEngine.wrap(SSLEngine.java:564); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.wrap(SslHandler.java:1041); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.wrapNonAppData(SslHandler.java:927); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1409); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.unwrapNonAppData(SslHandler.java:1327); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.access$1800(SslHandler.java:169); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner.resumeOnEventExecutor(SslHandler.java:1718); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner.access$2000(SslHandler.java:1609); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner$2.run(SslHandler.java:1770); at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174); at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470); at io.grpc.netty.shaded.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:403); at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997); at io.grpc.netty.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74); at io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30); ... 1 common frames omitted; Caused by: sun.security.validator.ValidatorException: PKIX path building fai,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7551:5914,access,access,5914,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7551,1,['access'],['access']
Security,:192); at cromwell.engine.backend.io.filesystem.gcs.StorageFactory$.cromwellAuthenticated$lzycompute(StorageFactory.scala:20); at cromwell.engine.backend.io.filesystem.gcs.StorageFactory$.cromwellAuthenticated(StorageFactory.scala:18); at cromwell.engine.backend.local.LocalBackend$$anonfun$14.apply(LocalBackend.scala:239); at cromwell.engine.backend.local.LocalBackend$$anonfun$14.apply(LocalBackend.scala:239); at scala.util.Try.orElse(Try.scala:84); at cromwell.engine.backend.local.LocalBackend.fileSystems(LocalBackend.scala:239); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:89); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:86); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1(MaterializeWorkflowDescriptorActor.scala:86); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:110); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:109); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:109); at cromwell.engine.workflow.MaterializeWorkflowDescrip,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/705:2723,Validat,Validation,2723,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/705,2,['Validat'],['Validation']
Security,:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); languages.wdl.draft3.WdlDraft3LanguageFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:128); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$ano,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:8648,validat,validateNamespace,8648,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['validat'],['validateNamespace']
Security,":NA:1]: job id: projects/gred-cumulus-sb-01-991a49c4/operations/15427360049616748078; 2021-09-27 13:49:07,692 cromwell-system-akka.dispatchers.backend-dispatcher-35 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Status change from - to Running; 2021-09-27 13:50:48,340 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Status change from Running to Failed; 2021-09-27 13:50:49,875 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowManagerActor: Workflow 075e0cf3-194b-4f53-a43d-d31f0b370f79 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Execution failed: generic::permission_denied: pulling image: docker pull: running [""docker"" ""pull"" ""gcr.io/broad-cumulus/cellranger@sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356""]: exit status 1 (standard error: ""Error response from daemon: pull access denied for gcr.io/broad-cumulus/cellranger, repository does not exist or may require 'docker login': denied: Permission denied for \""sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"" from request \""/v2/broad-cumulus/cellranger/manifests/sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"".\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:91); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:803); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:815); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:13125,access,access,13125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['access'],['access']
Security,":expanse_figures.CBL_assoc:-1:1-20000000025 [b303ae23expanse_figures.CBL_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 13:07:47,67] [info] BT-322 58e64982:expanse_figures.CBL_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = C3078AB9F63DD3A59655953B1975D6CF.; [2023-03-29 13:07:47,67] [info] 58e64982-cf3d-4e77-ad72-acfda8299d1b-EngineJobExecutionActor-expanse_figures.CBL_assoc:NA:1 [58e64982]: Call cache hit process had 0 total hit failures before completing successfully; ```. Can someone help me diagnose why call caching isn't near instantaneous, and what I can do to make it much faster? Happy to provide more information as necessary. Thanks!. Config:; ```; # See https://cromwell.readthedocs.io/en/stable/Configuring/; # this configuration only accepts double quotes! not singule quotes; include required(classpath(""application"")). system {; abort-jobs-on-terminate = true; io {; number-of-requests = 30; per = 1 second; }; file-hash-cache = true; }. # necessary for call result caching; # will need to stand up the MySQL server each time before running cromwell; # stand it up on the same node that's running cromwell; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true""; user = ""root""; password = ""pass""; connectionTimeout = 5000; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. docker {; hash-lookup {; enabled = true; method = ""remote""; }; }. backend {; # which backend do you want to use?; # Right now I don't know how to choose this via command line, only here; default = ""Local"" # For running jobs on an interactive node; #default = ""SLURM"" # For running jobs by submitting them from an interactive node to the cluster; providers { ; # For running jobs on an interactive node; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigB",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:3191,hash,hash-cache,3191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['hash'],['hash-cache']
Security,"; # https://github.com/broadinstitute/cromwell/issues. call-caching {; enabled = false; }. backend {; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = true; runtime-attributes = ""String? docker Int? max_runtime = 2""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". # Root directory where Cromwell writes job results. This directory must be; # visible and writeable by the Cromwell process as well as the jobs that Cromwell; # launches.; root: ""cromwell-executions"". filesystems {; local {; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]. caching {; duplication-strategy: [; ""soft-link""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; hashing-strategy: ""path"". # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.; # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.; check-sibling-md5: false; }; }; }; }; }; }; }. database {; db.url = ""jdbc:mysql://mysql-db/cromwell_db?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true""; db.user = ""cromwell""; db.password = ""cromwell""; db.driver = ""com.mysql.cj.jdbc.Driver""; profile = ""slick.jdbc.MySQLProfile$""; db.connectionTimeout = 15000; }; ```. and here is my cormwell dockerfile:. ```; FROM broadinstitute/cromwell:develop. RUN git clone https://github.com/vishnubob/wait-for-it.git; RUN mkdir cromwell-working-dir; WORKDIR cromwell-working-dir. COPY ./app-config /app-config. ENTRYPOINT [""/bin/sh"", ""-c""]; ```. when i submit a wdl did not use docker it ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7006:2000,hash,hashed,2000,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7006,1,['hash'],['hashed']
Security,"; 2021-09-27 13:48:20,511 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowManagerActor: Successfully started WorkflowActor-075e0cf3-194b-4f53-a43d-d31f0b370f79; 2021-09-27 13:48:20,511 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2021-09-27 13:48:20,547 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; Sep 27, 2021 1:48:20 PM com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 2021-09-27 13:48:21,326 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - MaterializeWorkflowDescriptorActor [UUID(075e0cf3)]: Parsing workflow as WDL draft-2; 2021-09-27 13:48:22,359 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - MaterializeWorkflowDescriptorActor [UUID(075e0cf3)]: Call-to-Backend assignments: wf_hello.hello -> PAPIv2; 2021-09-27 13:48:24,671 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowExecutionActor-075e0cf3-194b-4f53-a43d-d31f0b370f79 [UUID(075e0cf3)]: Starting wf_hello.hello; 2021-09-27 13:48:29,304 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Assigned new job execution tokens to the following groups: 075e0cf3: 1; 2021-09-27 13:48:31,233 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - BT-322 075e0cf3:wf_hello.hello:-1:1 is eligible for call caching with read = true and write = true; 2021-09-27 13:48:31,314 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - BT-322 0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:10002,authenticat,authentication,10002,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['authenticat'],['authentication']
Security,"; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13:16,20] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,23] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,25] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-02-11 10:13:16,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-02-11 10:13:16,33] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-02-11 10:13:17,45] [info] SingleWorkflowRunnerActor: Version 37; [2019-02-11 10:13:17,46] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-02-11 10:13:17,59] [info] Unspecified type (Unspecified version) workflow 52999e15-953f-44d6-aaae-1774c74d2910 submitted; [2019-02-11 10:13:17,65] [info] SingleWorkflowRunnerActor: Workflow submitted 52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,65] [info] 1 new workflows fetched; [2019-02-11 10:13:17,66] [info] WorkflowManagerActor Starting workflow 52999e15-953f-44d6-aaae-1774c74d2910; [2019-02-11 10:13:17,67] [info] WorkflowManagerActor Successfully started WorkflowActor-529",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:2349,hash,hash-lookup,2349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,1,['hash'],['hash-lookup']
Security,"; centrifugeOutput= centrifuge.outputDir,; domain=centrifuge.domain,; database=if defined(centrifuge.database) then centrifuge.database else ""refseq""; }; }; }; ```; The `centrifugeList` is a list of dictionaries. The resulting `centrifuge` `object` may or may not have a key database. . ## Expected behaviour:; `database` defaults to `""refseq""` if no `database` key is present in the dictionary. It will use the database key if it exists. ## Observed behaviour:; ```; java.lang.RuntimeException: Evaluating if defined(centrifuge.database) then centrifuge.database else ""refseq"" failed: Could not find key database in WdlObject; at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2(ExpressionKey.scala:36); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2$adapted(ExpressionKey.scala:31); at scala.Function1.$anonfun$andThen$1(Function1.scala:52); at cats.data.Validated.fold(Validated.scala:14); at cats.data.Validated.bimap(Validated.scala:109); at cats.data.Validated.map(Validated.scala:152); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:31); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$4(WorkflowExecutionActor.scala:452); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$2(WorkflowExecutionActor.scala:449); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$1(WorkflowExecutionActor.scala:448); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processRunnableTaskCallInputExpression(WorkflowExecutionActor.scala:447); at cromwell.engine.workflow.lifecycle.execution.Workf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3093:1108,Validat,Validated,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3093,1,['Validat'],['Validated']
Security,"; version 1.0. workflow wdl_v1_tests {; scatter (x in [0]) {; scatter (y in [0]) {; call input_default_not_used; }; }; }. task input_default_not_used {; input { String greeting = ""hello"" }; command { echo ~{greeting} }; runtime { docker: ""bash"" }; }; ```. ```; [2018-06-08 01:30:26,49] [error] WorkflowManagerActor Workflow 58ccc276-40f7-447c-bbff-87a47aa7163e failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; key not found: wdl_v1_tests.input_default_not_used.greeting; scala.collection.immutable.Map$Map1.apply(Map.scala:111); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$8(ScatterElementToGraphNode.scala:103); scala.collection.immutable.List.map(List.scala:283); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$7(ScatterElementToGraphNode.scala:102); cats.data.Validated.map(Validated.scala:194); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertInnerScatter(ScatterElementToGraphNode.scala:99); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:31); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOpti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:1061,Validat,Validated,1061,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Validat'],['Validated']
Security,"<!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Validating a workflow with circular imports causes a stack overflow in womtool. ## Expected behavior; I'm the fool who wrote a workflow with circular imports, but if womtool is here to check for errors, I'd like for it to suggest exactly how I'm being foolish. Something like miniwdl's output would work, where it follows the trail of imports and eventually says ""hey, this could be circular.""; ```; >miniwdl check ../fairyland/ld-pruning/ld-pruning-wf.wdl. (../fairyland/ld-pruning/ld-pruning-wf.wdl Ln 5 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6964:105,Validat,Validating,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6964,1,['Validat'],['Validating']
Security,"<!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->; Trying to set up a genomics workflow with AWS backend; References : https://aws.amazon.com/blogs/compute/using-cromwell-with-aws-batch/; https://cromwell.readthedocs.io/en/stable/tutorials/AwsBatch101/ . <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; AWS ; java -Dconfig.file=aws-cromwell-batch.conf -jar cromwell-75.jar run hello.wdl -i hello.inputs; ; ![AWS-Batch](https://user-images.githubusercontent.com/25282254/153039990-0d0b2c96-a33b-454f-9617-aee83137337a.PNG); [Cromwell-Error.docx](https://github.com/broadinstitute/cromwell/files/8026009/Cromwell-Error.docx); ; <!-- Paste/Attach your workflow if possible: -->; java -Dconfig.file=aws-cromwell-batch.conf -jar cromwell-75.jar run hello.wdl -i hello.inputs. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; include required(classpath(""application"")). aws {. application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""us-east-1""; }; engine {; filesystems {; s3.auth = ""default""; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; docker {; hash-lookup {; enabled = false; # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub and gcr; method = ""remote""; }; }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; concurrent-job-limit = 1000; root = ""s3://cromwell-aws-hello/cromwell-execution""; auth = ""default""; default-runtime-attributes {; queueArn = ""arn:aws:batch:us-east-1:XXXXXXXXX:job-queue/python-batch"" ,",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6671:1730,PASSWORD,PASSWORDS,1730,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671,1,['PASSWORD'],['PASSWORDS']
Security,"<!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; ![2018-10-26 23 24 43](https://user-images.githubusercontent.com/4966343/47572651-79585300-d976-11e8-8027-a9bade3f91d4.png). <!-- Which backend are you running? -->; aws. <!-- Paste/Attach your workflow if possible: -->; https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/integrationTestCases/germline/haplotype-caller-workflow/HaplotypeCallerWF.aws.wdl. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; AWS Access Key ID [****************R62Q]: ; AWS Secret Access Key [****************uDg5]:; Default region name [ap-northeast-2]:; Default output format [None]:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4321:545,PASSWORD,PASSWORDS,545,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4321,3,"['Access', 'PASSWORD']","['Access', 'PASSWORDS']"
Security,"<!-- Which backend are you running? -->; SGE. conf:; ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }; workflow-options {; workflow-log-temporary = false; }; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; filesystems {; local {; caching {; # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:; duplication-strategy: [; ""soft-link"", ""copy""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; # Default: file; hashing-strategy: ""path""; }; }; }; ...; backend.default = SGE; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql://0.0.0.0:40001/testuser_db?useSSL=false&rewriteBatchedStatements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3825:615,hash,hash,615,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825,4,['hash'],"['hash', 'hashed', 'hashing-strategy']"
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. **Which backend are you running?**. broadinstitute/cromwell:36. **Paste/Attach your workflow if possible**. For any workflow, when I query its metadata endpoint with `excludeKey=calls` parameter, it returns a response with all `""calls""` key nevertheless. This doesn't seem to happen to other keys, like `inputs` or `submittedFiles`. Excluding `calls` would make a huge difference for us, because for large workflows it takes a long time for Cromwell to aggregate all calls, the response becomes large, and sometimes it even timeouts. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4362:1197,PASSWORD,PASSWORDS,1197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4362,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3781:754,PASSWORD,PASSWORDS,754,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3781,6,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hi folks,. I try to launch cromwell in its server mode, however I get the following error:. ```; java -jar ./cromwell-34.jar server; Exception in thread ""main"" java.lang.VerifyError: Uninitialized object exists on backward branch 209; Exception Details:; Location:; scala/collection/immutable/HashMap$HashTrieMap.split()Lscala/collection/immutable/Seq; @249: goto; Reason:; Error exists in the bytecode; Bytecode:; 0x0000000: 2ab6 0060 04a0 001e b200 b8b2 00bd 04bd; 0x0000010: 0002 5903 2a53 c000 bfb6 00c3 b600 c7c0; 0x0000020: 00c9 b02a b600 36b8 0040 3c1b 04a4 015e; 0x0000030: 1b05 6c3d 2a1b 056c 2ab6 0036 b700 cb3e; 0x0000040: 2ab6 0036 021d 787e 3604 2ab6 0036 0210; 0x0000050: 201d 647c 7e36 05bb 0019 59b2 00bd 2ab6; 0x0000060: 0038 c000 bfb6 00cf b700 d21c b600 d63a; 0x0000070: 0619 06c6 001a 1906 b600 dac0 0086 3a07; 0x0000080: 1906 b600 ddc0 0086 3a08 a700 0dbb 00df; 0x0000090: 5919 06b7 00e2 bf19 073a 0919 083a 0abb; 0x00000a0: 0002 5915 0419 09bb 0019 59b2 00bd 1909; 0x00000b0: c000 bfb6 00cf b700 d203 b800 e83a 0e3a. ```. OS: redhat 6.9 ; Java: ; ```; java -version; java version ""1.8.0_20""; Java(TM) SE Runtime Environment (build 1.8.0_20-b26); Java HotSpot(TM) 64-Bit Se",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4082:754,PASSWORD,PASSWORDS,754,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4082,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I am running Cromwell on GCP, launching a workflow that shards into ~5,000 pieces. I am getting the following error: `cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out`. ```; 2019-04-29 00:02:13,419 cromwell-system-akka.dispatchers.backend-dispatcher-139 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(95b34a77)vcf2bigquery.convertVCF:2058:1]: Status chang; e from Running to Success; 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:171); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStre",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:754,PASSWORD,PASSWORDS,754,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; The [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4913:754,PASSWORD,PASSWORDS,754,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Backend: Local. Several basic array functions are not working on optional arrays. Test code passes wdltool validate with no errors.; Tested types: Array[String]? and Array[Int]?; ### Length(); WDL code:; ```; Array[String]? strings; Int num = length(strings); ```; Error:; ```; [2018-10-08 13:12:09,55] [error] WorkflowManagerActor Workflow 3dfb9c92-4e2e-4754-a35e-cfcbf9d6c006 failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Workflow has invalid declarations: Could not evaluate workflow declarations:; Test_optional.num:; length() expects one parameter of type Array but got one parameter of type Array[String]?; ```; ### Indexing; WDL code:; ```; Array[String]? strings. scatter (idx in range(4)) { # strings is provided in the JSON file as an array of 4 strings; call testtask{input: str=strings[idx]}; }; ```; Error:; ```; [2018-10-08 13:27:31,22] [error] WorkflowManagerActor Workflow c2ac7273-c209-4e74-b1f0-a208e89922d8 failed (during ExecutingWorkflowState): Can't index Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))) with index Success(WdlInteger(0)); wdl4s.wdl.WdlExpressionException: Can't index Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))) with index Success(WdlInteger(0)); ```; ### Zip(); WDL code:; ```; Array",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4218:750,validat,validate,750,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4218,1,['validat'],['validate']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Backend:; Local, no conf file. <!-- Paste/Attach your workflow if possible: -->. Workflow: Files are here:; https://github.com/FredHutch/reproducible-workflows/tree/master/CWL/SingleStepWorkflow. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Details (see also [this post](https://gatkforums.broadinstitute.org/wdl/discussion/23265/cwl-workflow-fails-running-locally#latest)):. I can run this workflow just fine using cwltool/cwl-runner as follows:. ```; cwl-runner bwa-memWorkflow.cwl localInputs.yml; ```. When I try and run it with cromwell I get an error that ""The job was aborted from outside Cromwell"" but I definitely did not abort it myself. Here is the command I used to run this workflow in Cromwell:. ```; java -jar cromwell-36.jar run bwa-memWorkflow.cwl -i localInputs.yml -p bwa-pe.cwl.zip; ```. (`bwa-pe.cwl.zip` just contains the dependency `bwa-pe.cwl`). And here's the full output of it:. https://gist.github.com/dtenenba/61bcf60f129b817cd894ee222789369a. My ultimate goal is to switch over to the AWS Batch back end (in case you are wondering why I don't just stick with cwltool) but first I wanted to get the workflow running locally in cromwell. Any ideas about this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4587:900,PASSWORD,PASSWORDS,900,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4587,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; Hi all,. When running a workflow with `write_objects(Array[Struct])` in a task, the workflow fails with **runtime** error `Failed to evaluate input 'out' (reason 1 of 1): Failed to write_objects(...) (reason 1 of 1): Cannot TSV serialize a Array[WomCompositeType {\n a -> String \n}] (valid types are Array[Primitive], Array[Array[Primitive]], or Array[Object])` **if the array is empty**. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->; ```wdl; version 1.0. struct Input {; String a; }. workflow Test {; call test; }. task test {; input {; Array[Input] inputs = []; }. File out = write_objects(inputs). command {; cat '~{out}'; }. runtime {; docker: 'debian:stable-slim'; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Our Cromwell build is `37-a52c415-SNAP` (config available upon request)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4595:1390,PASSWORD,PASSWORDS,1390,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4595,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hello,. I'm wondering if there is a way to specify the `zones` in Cromwell's configuration file that overwrites all the jobs executed through the Cromwell server running it. . When checking the documentation, I only found `config.default-runtime-attributes`, which only works when WDL tasks do not specify `zones`. I also see `config.runtime-attributes`, but it did not work. Could you please guide me on the issue? Thanks!. Sincerely,; Yiming",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6493:882,PASSWORD,PASSWORDS,882,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6493,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hi Cromwell team,; I am running a Cromwell server on Google Cloud have recently upgraded from Cromwell 50 to 53. I am running into an issue that I believe relates to how the new `monitoring_image_script` interacts with the docker entrypoint. I have docker images where each container needs to ping a license server for authentication, and this happens via and `ENTRYPOINT`-executed script called `auth.sh`. With Cromwell 53, this is no longer being run when the container is being executed. This is how the actual docker command . ```; # Cromwell 53; 2020/09/30 13:07:42 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint=/bin/bash gcr.io/bioskryb/sentieon-201911-run@sha256:b4af9423297bb6566763b2c47b1da1620a68a4d34c210f0786a34a0ae85f62db /cromwell_root/script ; ```. ```; #Cromwell 50; 2020/09/23 06:03:37 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint= gcr.io/bioskryb/sentieon-201911-run@sha256:b4af9423297bb6566763b2c47b1da1620a68a4d34c210f0786a34a0ae85f62db /bin/bash /cromwell_root/script ; ```. I h",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5901:882,PASSWORD,PASSWORDS,882,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5901,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hi,. I recently encountered an issue on running Cromwell jobs on AWS Batch. In brief, all of my testing WDL jobs failed with the following error message:. ```; 2021-09-23 00:08:18,813 INFO - Submitting taskId: cumulus.cluster-None-1, job definition : arn:aws:batch:us-west-2:752311211819:job-definition/cromwell_quay_io_cumulus_cumulus_1_4_377407181fbea1f33a22931df258b16d20d4c6ab3:1, script: s3://gred-cumulus-dev/scripts/c157137e2097795846ae1f4069ccd7a2; 2021-09-23 00:08:20,269 cromwell-system-akka.dispatchers.backend-dispatcher-118 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(cf00212c)cumulus.cluster:NA:1]: job id: 12836c6a-6d1a-4429-bb86-68b8f9883acf; 2021-09-23 00:08:20,287 cromwell-system-akka.dispatchers.backend-dispatcher-118 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(cf00212c)cumulus.cluster:NA:1]: Status change from - to Initializing; 2021-09-23 00:12:17,120 cromwell-system-akka.dispatchers.backend-dispatcher-136 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(cf00212c)cumulus.cluster:NA:1]: Status change from Initializing to R",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6504:882,PASSWORD,PASSWORDS,882,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Hello,. I wonder if the LSF job array functionality was implemented in the latest Cromwell releases?. I found a couple of previous mentions on this (for AWS) a while ago:; [1](https://github.com/broadinstitute/cromwell/issues/4496); [2](https://github.com/broadinstitute/cromwell/issues/4707). If such functionality exists, are there docs on this subject?. Thank you!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6379:882,PASSWORD,PASSWORDS,882,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6379,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I want to set AWS_BATCH_JOB_ATTEMPT in my pipeline. My pipeline is a cromwell wdl pipeline which utilizes AWS batch as the backend. I submit jobs like so. ```; curl -X POST ""http://172.31.77.179:8000/api/workflows/v1"" \; -H ""accept: application/json"" \; -F ""workflowSource=@rnaseq_pipeline.wdl"" \; -F ""workflowInputs=@rnaseq_pipeline.json"" \; -F ""workflowDependencies=@tasks.zip""; ```; From what I read on aws, they seem to set the environment variables for jobs through the websites GUI. I submit my jobs with curl, how would I add the AWS_BATCH_JOB_ATTEMPT value?. Any help would be appreciated, I am not familiar with either cromwell or batch.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5511:882,PASSWORD,PASSWORDS,882,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5085:882,PASSWORD,PASSWORDS,882,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->; I am trying to run Cromwell with Google backend using PAPIv2.conf. Is there a way NOT doing the project network label for network Virtual Private Network? So that I can give the full network and subnetwork address as parameter into the PAPIv2.conf file? . I saw the questions around supporting the shared VPC network in previous issues . May be using the path like ; --network=projects/host-gcp-project/global/networks/name-of-the-network ; --subnetwork=projects/host-gcp-project/regions/us-central1/subnetworks/name-of-the-subnetwork. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; Backend: Google; <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6453:1435,PASSWORD,PASSWORDS,1435,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6453,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. **_Are you seeing something that looks like a bug? Please attach as much information as possible._**; No. The job is killed because of memory limitations _e.g_ `/bin/bash: line 1: 172 Killed`. **_Which backend are you running?_**; I'm running `Cromwell` on a local machine, I should have enough memory to run the process but somehow to amount visible/accessible by `Cromwell` is limited. **_Paste/Attach your workflow if possible:_**; ```; version 1.0. workflow step2 {; input {; String PANGENIE_CONTAINER = ""overcraft90/eblerjana_pangenie:2.1.2""; ; File FORWARD_FASTQ # compressed R1; File REVERSE_FASTQ # compressed R2; String NAME = ""sample"" # how to loop over samples' name in numerical order (maybe grub names' prefix)!?. File PANGENOME_VCF # input vcf with variants to be genotyped; File REF_GENOME # reference for variant calling; String VCF_PREFIX = ""genotype"" # string to attach to a sample's genotype; String EXE_PATH = ""/app/pangenie/build/src/PanGenie"" # path to PanGenie executable in Docker. Int CORES = 24 # number of cores to allocate for PanGenie execution; Int DISK = 300 # storage memory for output files; Int MEM = 100 # RAM memory allocated; }. call reads_extraction_and_merging {; input:; in_container_pangenie=PANGENIE_CONTAINER,; in_forward_fastq=FORWARD_FASTQ,; in_reverse_fastq=REVERSE_FASTQ,; in_label=NAME, #later can be plural; in_cores=CORES,; in_disk=DISK,; in_mem=MEM; }. call genome_inference {; input:; in_container_pangenie=PANGENIE_CONTAINER, # not sure whether Docker needs to be re-run; in_pangenome_vcf=PANGENOME_VCF,; in_reference_genome=REF_GENOME,; in_executable=EXE_PATH,; in_fastq_file=reads",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6966:717,access,accessible,717,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966,1,['access'],['accessible']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. # Description. I believe this is a bug. I tried to use `stderr()` in the `output` section of a `workflow`, rather than the output section of a `task`. The resulting WDL validated fine using `womtool validate` (and it validated fine on Terra with the automatic validation they do). But the job would run about halfway and then automatically switch to ""Aborting"" status with no explanation or error message. The workflow would eventually fail after a huge delay (about 22 hours), and there would be no real error message. All tasks that ran were successful (but not all tasks ran). # Minimal WDL example. Here is a working example:. ```wdl; version 1.0. workflow my_workflow {; call my_task; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. And here is a non-working example that still validates fine using `womtool validate`:. ```wdl; version 1.0. workflow my_workflow {; input {; Boolean run_task; }. if (run_task) {; call my_task; }. output {; File out = select_first([my_task.out, stdout()]); }; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. The above gives; ```console; (cromwell) [sfleming@laptop:~/cromwell]$ womtool validate test.wdl ; Success!; ```. # The problem. The problem is that the non-working WDL example above should not validate successfully, as it is NOT a valid WDL. The `stdout()` built-in inside the `select_first()` in the `output` block of the `workflow` is not actually allowed. It will cause a very bizarre err",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6976:640,validat,validated,640,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6976,4,['validat'],"['validate', 'validated', 'validation']"
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7117:623,PASSWORD,PASSWORDS,623,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7117,2,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. When trying to configure metadata-archive in cromwell server by adding the configuration below:; ```; archive-metadata {; # A filesystem able to access the specified bucket:; filesystems {; gcs {; # A reference to the auth to use for storing and retrieving metadata:; auth = ""user-service-account""; }; }. # Which bucket to use for storing the archived metadata; bucket = ""{{ backend_bucket }}""; }; ```. when the user-service-account auth is declared up in the configuration :; ```; google {. application-name = ""cromwell"". auths = [; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```; We got the following error in Cromwell server initialization :; cromwell_1 | [ERROR] [06/21/2023 11:55:25.094] [cromwell-system-akka.actor.default-dispatcher-30] [akka://cromwell-system/user] Failed to parse the archive-metadata config:; cromwell_1 | Failed to construct archiver path builders from factories (reason 1 of 1): Missing parameters in workflow options: user_service_account_json; cromwell_1 | akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/ServiceRegistryActor/MetadataService: exception during creation; cromwell_1 | 	at akka.actor.ActorInitializationException$.apply(Actor.scala:202); cromwell_1 | 	at akka.actor.ActorCell.create(ActorCell.scala:698); crom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7171:623,PASSWORD,PASSWORDS,623,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7171,2,"['PASSWORD', 'access']","['PASSWORDS', 'access']"
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. womtool graph bug:; Exception in thread ""main"" java.util.NoSuchElementException: key not found: ScatterVariableNode(WomIdentifier(LocalName(lane),FullyQualifiedName(lane)),PlainAnonymousExpressionNode(WomIdentifier(LocalName(lane),FullyQualifiedName(lane)),WdlWomExpression(WdlExpression(<string:255:22 identifier ""TWVyZ2VJbnB1dFJlYWRz"">),[Scatter fqn=MAPRSEQSingleSampleMasterWF.$scatter_0, item=lane, collection=MergeInputReads]),WomMaybeEmptyArrayType(WomMaybeEmptyArrayType(WomSingleFileType)),Map(MergeInputReads -> ConnectedInputPort(MergeInputReads,WomMaybeEmptyArrayType(WomMaybeEmptyArrayType(WomSingleFileType)),GraphNodeOutputPort(MAPRSEQSingleSampleMasterWF.MergeInputReads),wom.graph.GraphNode$GraphNodeSetter$$Lambda$512/0x0000000800491040@2a9fd482))),WomMaybeEmptyArrayType(WomSingleFileType)); 	at scala.collection.immutable.Map$EmptyMap$.apply(Map.scala:101); 	at scala.collection.immutable.Map$EmptyMap$.apply(Map.scala:99); 	at wom.views.GraphPrint$.relevantAsUpstream$1(GraphPrint.scala:177); 	at wom.views.GraphPrint$.upstreamPortToRelevantNodes$1(GraphPrint.scala:187); 	at wom.views.GraphPrint$.$anonfun$upstreamLinks$1(GraphPrint.scala:190); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.Set$Set1.foreach(Set.scala:97); 	at scala.co",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6744:623,PASSWORD,PASSWORDS,623,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6744,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I `docker load` a image locally on my device, found that there is no digest of it. ; No internet connection, but with docker-loaded images, how can I run docker image locally?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6940:623,PASSWORD,PASSWORDS,623,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6940,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->; Backend: GCP Batch; <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. When running a workflow with `google_project` and/or `google_compute_service_account` workflow options defined, those two options don't take/have any effect on the workflow. The workflow executes in the project defined in the Batch backend config, using the service account defined in the Batch backend config. This breaks the ability to run workflows cross-project from a single cromwell instance with a single gcp backend (which was supported in PAPI). Workarounds like defining multiple backends (each for a separate project/compute sa) are not ideal. Looking through the Batch [backend code](https://github.com/broadinstitute/cromwell/blob/4cddc6163b0b1d73e02f3723a82634df852b754b/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/api/GcpBatchRequestFactoryImpl.scala#L168-L169), it seems the issue is the `parent` and `gcpSa` are populated from `batchAttributes` and `gcpBatchParameters` rather than `createParameters` where the workflow options would override the values defined in the backend config. The fix for this seems trivial, so I will submit a PR.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7459:643,PASSWORD,PASSWORDS,643,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7459,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Cromwell doesn't resolve relative file imports when main wdl is referenced using URL in azure. However GCP does resolve these relative paths. . Ideally you can just point to the wdl on github or dockstore and it would resolve the url relative paths correctly. <!-- Which backend are you running? -->; Azure batch. <!-- Paste/Attach your workflow if possible: -->; https://github.com/broadinstitute/viral-pipelines/blob/master/pipes/WDL/workflows/fetch_sra_to_bam.wdl. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6936:1000,PASSWORD,PASSWORDS,1000,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6936,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Hello, I'm trying to run a workflow locally and it gets stalled on BackgroundConfigAsyncJobExecutionActor [5b4a3086Regenie.RegenieStep1WholeGenomeModel:NA:1]: Status change from - to WaitingForReturnCode; <!-- Which backend are you running? -->; I'm using a local backend; <!-- Paste/Attach your workflow if possible: -->; I'm attaching the workflow which is written in WDL language; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm attaching a screenshot of the configuration file and the input files which I provide to cromwell in json format; <img width=""988"" alt=""Screenshot 2023-08-02 at 3 27 39 PM"" src=""https://github.com/broadinstitute/cromwell/assets/56558154/e676859c-4f47-4f3a-ae20-68b2ead010e9"">. Also I'm proving the workflow called regenie.txt; [regenie.txt](https://github.com/broadinstitute/cromwell/files/12243956/regenie.txt); <img width=""714"" alt=""Screenshot 2023-08-02 at 3 29 07 PM"" src=""https://github.com/broadinstitute/cromwell/assets/56558154/567932ac-ce61-41dc-b1c1-8321c2ccab54"">; I'm providing the full log file as well. ; [regenie.log](https://github.com/broadinstitute/cromwell/files/12243995/regenie.log); If I run the commands locally without using a WDL workflow this runs in under 10 seconds in my local computer. . I appreciate any insight. . I'm using cromwell-85.jar and my java version is ; openjdk version ""17.0.6"" 2023-01-17; OpenJDK Runtime Environment Temurin-17.0.6+10 (build 17.0.6+10); OpenJDK 64-Bit Server VM Temur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7191:916,PASSWORD,PASSWORDS,916,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7191,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; The GCP batch backend preemption handling seems to have issue. When preemption happens the job had very high possibility to be error. The typical error would be : time=“…” level=error msg=“error waiting for container:” . It will take the preempt events as the error from Cromwell logs. However, in the google batch console, it shows clearly ""preemption notice has received and will be processed"". . <!-- Which backend are you running? -->; GCP batch ; <!-- Paste/Attach your workflow if possible: -->; The workflow works perfectly in GCP life science backend; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7407:1092,PASSWORD,PASSWORDS,1092,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. Hi, . I wrote my first WDL (yay!) and troubleshot it locally using miniwdl. Now, I'm trying to get that WDL uploaded to Terra and the WOMtool validation step continues to pass me a fatal error that I can't seem to figure out. I've reduced the WDL to a single step that can reproduce this error and pasted below. I can't imagine I'm the first person to have this issue, but couldn't find evidence of it on the interwebs! In sum, I have a WDL that appears to be working fine (via miniwdl), but WOMtool (and Dockstore for that matter) finds a fatal error that prevents me from using it on Terra. Please help, thanks!!!. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; `ERROR: Unexpected symbol (line 6, col 5) when parsing 'setter'. Expected equal, got ""String"". String bam_to_reads_mem_size ^ $setter = :equal $e -> $1 `. <!-- Which backend are you running? -->; `womtool v61`; `miniwdl v1.5.2`. <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0 . #WORKFLOW DEFINITION; workflow StripReadsFromBam {; String bam_to_reads_disk_size; String bam_to_reads_mem_size. #converts BAM to FASTQ (R1 + R2); call BamToReads {; 	input:; 	disk_size = bam_to_reads_disk_size,; 	mem_size = bam_to_reads_mem_size; }. #Outputs single reads file; output {; File outputReads = BamToReads.outputReads; }; }. #Task Definitions; task BamToReads {; File InputBam; String SampleName; String disk_size; String mem_size. #Calls samtools view to do the conversion; command {; #Set -e and -o says if any command I run fails in this script, make sure to return a failure; set -e; set -o pipefai",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6767:508,validat,validation,508,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767,1,['validat'],['validation']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. Hi,. Since last week, our cromwell server instance on GCP started to encounter the following error in all the jobs:. ```; 2024-07-31 19:08:59 cromwell-system-akka.dispatchers.backend-dispatcher-35 WARN - PAPI request worker had 1 failures making 1 requests: ; Unable to complete PAPI request due to system or connection error (Unknown Error.); 2024-07-31 19:09:33 cromwell-system-akka.dispatchers.backend-dispatcher-56 WARN - PAPI request worker had 1 failures making 1 requests: ; Unable to complete PAPI request due to system or connection error (Unknown Error.); 2024-07-31 19:10:06 cromwell-system-akka.dispatchers.backend-dispatcher-56 WARN - PAPI request worker had 1 failures making 1 requests: ; Unable to complete PAPI request due to system or connection error (Unknown Error.); ```. However, with `Unknown Error` message, I don't know where to go for help. Do you have any suggestion?. Here are the configurations:. * Cromwell v85; * Genomics API; * PAPIv2 with `actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""`. Many thanks!. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7482:1707,PASSWORD,PASSWORDS,1707,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7482,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. I'm using Cromwell v87 on GCP Genomics API. When submitting a job, the error I got is the following:. ```; Caused by: java.lang.IllegalStateException: You are currently running with version 2.2.0 of google-api-client. You need at least version 1.31.1 of google-api-client to run version 1.32.1 of the Genomics API library.; at com.google.common.base.Preconditions.checkState(Preconditions.java:534); at com.google.api.client.util.Preconditions.checkState(Preconditions.java:113); at com.google.api.services.genomics.v2alpha1.Genomics.<clinit>(Genomics.java:44); ... 12 common frames omitted; ```. It seems that I need to downgrade the version of `google-api-client`. However, I don't know how to do it on my machine. Could anyone help? Thanks!. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7481:1368,PASSWORD,PASSWORDS,1368,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7481,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->; I am using `checkpointFile` in the `runtime` section of a WDL `task`. . I accidentally included a space in the checkpoint file name, and I see in the logs that this (probably) breaks checkpointing. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Log file shows; ```; CHECKPOINTING: Making local copy of /cromwell_root/noise_prompting_classical monocyte_H_shard0.csv; cp: can't create 'monocyte_H_shard0.csv-tmp/noise_prompting_classical': No such file or directory; cp: can't create 'monocyte_H_shard0.csv-tmp/monocyte_H_shard0.csv': No such file or directory; cp: can't create 'monocyte_H_shard0.csv-tmp/noise_prompting_classical': No such file or directory; CHECKPOINTING: Uploading new checkpoint content; ```. <!-- Which backend are you running? -->; Running on GCP via Terra. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. When I remove the space in the filename, I see this in the logs, which appears to be working fine:. ```; CHECKPOINTING: Making local copy of /cromwell_root/noise_prompting_classical_monocyte_H_shard0.csv; CHECKPOINTING: Uploading new checkpoint content; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7441:1315,PASSWORD,PASSWORDS,1315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7441,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->; I think the minimum to reproduce the bug is just. ```; Array[File] foo = []; Array[String]? bar = foo; ```. which fails with. ```; ""failures"": [; {; ""causedBy"": [; {; ""message"": ""Failed to evaluate 'bar' (reason 1 of 1): Evaluating foo failed: assertion failed: base member type WomMaybeEmptyArrayType(WomStringType) and womtype WomMaybeEmptyArrayType(WomSingleFileType) are not compatible"",; ""causedBy"": []; }; ],; ""message"": ""Workflow failed""; }; ],; ```. Interestingly enough, this passes if the array is non-empty, or if the target is not optional, or if the source is type `Array[String]`. I am running cromwell ""v85 (ish)"" according to the administrator. Backend is AWS batch.; <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7399:1307,PASSWORD,PASSWORDS,1307,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7399,1,['PASSWORD'],['PASSWORDS']
Security,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->; womtool cannot be used in cwl; <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->; java -jar womtool-84.jar validate hello_world.cwl; No getWomBundle method implemented in CWL v1; <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6973:568,validat,validate,568,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6973,2,"['PASSWORD', 'validat']","['PASSWORDS', 'validate']"
Security,"<script type=""text/javascript"" src=""https://www.gstatic.com/charts/loader.js""></script>; . I am proposing to update workflow timing web page as the google timeline chart codes have been revised. Here the current code uses 'http://www.google.com/jsapi' (https://github.com/broadinstitute/cromwell/blob/c8bf8fdc51e2df19d8e8a3d066acc64f588f1b1c/engine/src/main/resources/workflowTimings/workflowTimings.html#L4). It is not suggested by Google Chart documentation (https://developers.google.com/chart/interactive/docs/gallery/timeline), and this URL is blocked by our firewall. In order to use cromwell timing API for myself, I suggest to change relevant lines to:. <script type=""text/javascript"" src=""https://www.gstatic.com/charts/loader.js""></script>; <script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js""></script>; <script type=""text/javascript"">. var parentWorkflowNames = [];; var expandedParentWorkflows = [];; var chartView;; google.charts.load('current', {packages: ['timeline']});; google.charts.setOnLoadCallback(drawChart);. Can you update cromwell source codes to do that? Or please let me know if you need a pull request. Thanks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3887:564,firewall,firewall,564,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3887,1,['firewall'],['firewall']
Security,"@Horneth commented on [Mon Sep 11 2017](https://github.com/broadinstitute/wdl4s/issues/202). The CWL parser currently hardcode the same name for all workflows.; Find out if CWL has a way to specify a workflow name. If not use a hash ? A new endpoint parameter ?; @katevoss this could use PO input if it turns out CWL has no way to specify a workflow name. ---. @danbills commented on [Tue Oct 03 2017](https://github.com/broadinstitute/wdl4s/issues/202#issuecomment-333894259). So it turns out that CWL v1.0.2 has an `id` field for Workflow, and cwltool gives us one for free:. http://www.commonwl.org/v1.0/Workflow.html#Workflow. so we should update the model to have a required `id: String` and this problem goes away. ---. @danbills commented on [Tue Oct 03 2017](https://github.com/broadinstitute/wdl4s/issues/202#issuecomment-333894392). This also applies to `CommandLineTool`, cwltool pre-processing gives us one.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2728:228,hash,hash,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2728,1,['hash'],['hash']
Security,"@MatthewMah commented on [Thu Jun 15 2017](https://github.com/broadinstitute/wdltool/issues/32). The following example passes validation, and I think it should not. I think validation should be able to identify that a nonexistent output field is trying to be read. . ```; workflow ShouldNotValidate{; 	call A{}; 	Array[File] simple = [A.nonexistent]; 	call B{ input:; 		in = simple; 	}; }. task A{; 	command{; 		echo ""A"" > out; 	}; 	output{; 		File out = ""out""; 	}; }. task B{; 	Array[File] in. 	command{; 		cat ${sep=' ' in}; 	}; 	output{; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2881:126,validat,validation,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2881,2,['validat'],['validation']
Security,"@anton-khodak commented on [Wed Jan 25 2017](https://github.com/broadinstitute/wdltool/issues/22). I use `wdltool` to parse descriptions from the main repository, for instance, [this one](https://github.com/broadinstitute/wdl/blob/develop/scripts/broad_dsde_workflows/ValidateBamsWf_170107.wdl) and [this](https://github.com/broadinstitute/wdl/blob/develop/scripts/broad_dsde_workflows/ConvertPairedFastQToUnmappedBamWf_170107.wdl) . Both descriptions have valid syntax (`validate` and `parse` run smoothly). However, when I run `highlight` on either of them, I get the following error:; ``` ; $ java -jar ~/Downloads/wdltool-0.8.jar highlight ""/media/anton/ECFA959BFA95631E/Programming/wdl2cwl/ValidateBamsWf_170107.wdl"" console. Exception in thread ""main"" scala.MatchError: [Declaration type=Array[File] name=validation_reports expr=Some(ValidateBAM.validation_report)] (of class wdl4s.WorkflowOutput); at wdl4s.formatter.SyntaxFormatter.wdl4s$formatter$SyntaxFormatter$$formatScope(SyntaxFormatter.scala:188); at wdl4s.formatter.SyntaxFormatter$$anonfun$4.applyOrElse(SyntaxFormatter.scala:153); at wdl4s.formatter.SyntaxFormatter$$anonfun$4.applyOrElse(SyntaxFormatter.scala:153); at scala.PartialFunction$$anonfun$runWith$1.apply(PartialFunction.scala:141); at scala.PartialFunction$$anonfun$runWith$1.apply(PartialFunction.scala:140); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike$class.collect(TraversableLike.scala:271); at scala.collection.AbstractTraversable.collect(Traversable.scala:104); at wdl4s.formatter.SyntaxFormatter.wdl4s$formatter$SyntaxFormatter$$formatWorkflow(SyntaxFormatter.scala:153); at wdl4s.formatter.SyntaxFormatter$$anonfun$2.applyOrElse(SyntaxFormatter.scala:73); at wdl4s.formatter.SyntaxFormatter$$anonfun$2.a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2878:472,validat,validate,472,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2878,2,"['Validat', 'validat']","['ValidateBAM', 'validate']"
Security,"@antonkulaga commented on [Thu Feb 16 2017](https://github.com/broadinstitute/wdl4s/issues/86). If there will be ScalaJS support, then it will be possible to do things like validation of wdl4s directly in the browser. ---. @geoffjentry commented on [Thu Feb 16 2017](https://github.com/broadinstitute/wdl4s/issues/86#issuecomment-280335418). that's an interesting point. My take is that I have nothing against doing it, for me personally it'd likely involve flipping the switch and if it Just Works great and if not (IIRC scala.js isn't 100% source compatible?) I'm not going to go much further. . I mean this in a non-snarky way but this is really going to be in a ""patches welcome"" territory as I doubt it'll be officially prioritized and while I just added it to my ""it'd be a good thing to do"" mental todo list it's not the top item and I don't pop things off that list as frequently as I'd prefer.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2693:173,validat,validation,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2693,1,['validat'],['validation']
Security,"@cjllanwarne commented on [Mon May 22 2017](https://github.com/broadinstitute/wdl4s/issues/112). This problem presents itself when using `wdltool` but it looks like it's a match error coming from inside `WDL4S`. null.wdl:; ```; task empty{; command {}; output {; File out = ""${output}""; }; }; ```. On validate:; ```; $ java -jar target/scala-2.12/wdltool-0.11.jar validate ~/myWorkflows/null.wdl; null; ```. We can see more details when we try to graph it:; ```; $ java -jar target/scala-2.12/wdltool-0.11.jar graph ~/myWorkflows/null.wdl; Exception in thread ""main"" scala.MatchError: null; 	at wdl4s.expression.ValueEvaluator.evaluate(ValueEvaluator.scala:44); 	at wdl4s.WdlExpression$.evaluate(WdlExpression.scala:85); 	at wdl4s.WdlExpression.evaluate(WdlExpression.scala:161); 	at wdl4s.expression.ValueEvaluator.replaceInterpolationTag(ValueEvaluator.scala:20); 	at wdl4s.expression.ValueEvaluator.$anonfun$interpolate$2(ValueEvaluator.scala:33); 	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:157); 	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:157); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2703:301,validat,validate,301,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2703,2,['validat'],['validate']
Security,"@curoli commented on [Wed Oct 04 2017](https://github.com/broadinstitute/wdl4s/issues/248). Currently, subclasses of WOM GraphNodes are case classes which auto-generate equals and hashCode based on components. This doesn't work well for two reasons.; First, WOM Graph assumes object identity and will fail if a node is replaced by one that is equal based on components. The code is already in several places using eq instead of == to account for this.; Second, since graph nodes contain all upstream nodes, component-based hashCode or equals are unworkable for real-life pipelines, because hashCode/equals will be called for every upstream node through every possible upstream path, which grows exponentially with graph size for workflows that contain branches that rejoin (and most do). . ---. @danbills commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334536133). I think this comment is more appropriate here:. Equals is supposed to be bitwise/value-based equality, whereas `eq` indicates reference equality. I don't recall the exact motivation behind using `eq` in the test, maybe @cjllanwarne can comment when he returns on Tuesday. ---. @curoli commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334555964). If you don't like this fix, what do you suggest we do instead? How about we accept this fix now and you can rework the whole thing later, if you want?. This is a show-stopper once you want to create larger (aka real-life) workflows. . For example: if node a has two outputs, which are inputs of b, then b.hashCode calls a.hasCode twice. If b produces two outputs which are inputsof c, then c.hashCode calls b.hashCode twice, and a.hashCode is called four times. If c produces two outputs which are inputs of d, d.hashCode calls c.hashCode twice, b.hashCode four times and a.hashCode eight times, etc. . ---. @curoli commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2694:180,hash,hashCode,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2694,6,['hash'],['hashCode']
Security,"@delocalizer commented on [Tue Feb 07 2017](https://github.com/broadinstitute/wdltool/issues/23). Currently `wdltool validate` accepts only one argument — a single WDL workflow file. If that file contains import statements then validation fails unless the imports happen to live in the right place relative to the local directory where you're running the command. It'd be great if `wdltool validate` would do the same as cromwell, i.e. accept a zipfile of imports to resolve against, so that you can validate the files that you're actually going to submit to the server, e.g.; `wdltool validate myWorkflow.wdl myImports.zip`. ---. @geoffjentry commented on [Tue Feb 07 2017](https://github.com/broadinstitute/wdltool/issues/23#issuecomment-278205682). This is a great idea. Tagging @katevoss in case she doesn't yet watch this repo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2879:117,validat,validate,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2879,5,['validat'],"['validate', 'validation']"
Security,"@eddiebroad commented on [Wed Aug 10 2016](https://github.com/broadinstitute/wdltool/issues/12). I try to use wdltool validate on a WDL and it seems to _not_ catch; an error in the WDL. In the example ""bad"" WDL attached the call to VCF_to_MAF_task; has the line ""inputVCF=inputVCF"" but the ""inputVCF"" exists only; in the task but not at the workflow level ; but invoking wdltool 0.4 on it seems to _NOT_; cause an error. Shouldn't it be saying the WDL has an error; because inputVCF does _not_ exist at the workflow level?. I downloaded the wdltool from the latest release; https://github.com/broadinstitute/wdltool/releases/download/0.4/wdltool-0.4.jar. the two WDLs are attached. [wdl_files.zip](https://github.com/broadinstitute/wdltool/files/412067/wdl_files.zip). ```; wm8b1-75c:red_bug esalinas$ find *.wdl -exec java -jar wdltool-0.4.jar validate {} \;. wm8b1-75c:red_bug esalinas$ diff good.wdl bad.wdl ; 188c188; < inputVCF=inVCF,. ---; > inputVCF=inputVCF,; wm8b1-75c:red_bug esalinas$ ; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2876:118,validat,validate,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2876,2,['validat'],['validate']
Security,"@katevoss commented on [Thu Dec 08 2016](https://github.com/broadinstitute/dsde-docs/issues/1515). - [ ] Link to [Dev Docs](https://software.broadinstitute.org/wdl/devzone), latest version and release notes, blog, contact us (Slack and Forum), WDL Spec, [WDL website](https://software.broadinstitute.org/wdl/), [GATK website](https://software.broadinstitute.org/gatk/); - [ ] Link and describe the following files: [Authors](https://github.com/broadinstitute/cromwell/blob/develop/AUTHORS), [Changelog](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md), [Apache License](https://github.com/broadinstitute/cromwell/blob/develop/LICENSE-ASL-2.0), [Broad License](https://github.com/broadinstitute/cromwell/blob/develop/LICENSE.txt), [Migration](https://github.com/broadinstitute/cromwell/blob/develop/MIGRATION.md), [Making a Backend](https://github.com/broadinstitute/cromwell/blob/develop/MakingABackend.MD), [Notice](https://github.com/broadinstitute/cromwell/blob/develop/NOTICE), [Security](https://github.com/broadinstitute/cromwell/blob/develop/SecurityRecommendations.md).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1768:1004,Secur,Security,1004,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1768,2,['Secur'],"['Security', 'SecurityRecommendations']"
Security,@leetl1220 is seeing an issue on 0.19 with a large Docker hash overflowing the DOCKER_IMAGE_HASH. Expanding from 100 chars to 255 gets him around the problem.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1301:58,hash,hash,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1301,1,['hash'],['hash']
Security,"@meganshand commented on [Fri Apr 29 2016](https://github.com/broadinstitute/wdltool/issues/9). The following task results in an uninformative error message when using `validate`: . ```; task printReads {; File bam; File ref_fasta; File ref_fasta_index; File ref_dict. command {; java -jar /usr/gitc/GenomeAnalysisTK.jar \; -T PrintReads \; -I ${bam} \; -o smaller.bam \; -L chr1 \; -R ${ref_fasta}; }; runtime {; docker: ""broadinstitute/genomes-in-the-cloud:1.1010_with_gatk3.5""; disks: ""local-disk 400 SSD""; memory: ""10 GB""; }; output {; File smaller = smaller.bam; }; }; ```. results in this error message:. ```; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; ```. The problem is that there aren't quotes around ""smaller.bam"" in the outputs of the task. It would be great if the error message could tell you which line or object was causing the problem. The error message from cromwell is different, but also uninformative and it would be great if the error message could be clearer there as well.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2875:169,validat,validate,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2875,1,['validat'],['validate']
Security,"@mwalker174 originally reported:. > Hi all, I’ve got a critical problem where call caching times out on a particular WDL task (`“message”: “Hashing request timed out for gs://...“’). This makes some sense since the task is checking ~200 files on each of ~200 shards. This is on cromwell v36/papiv2. I thinking bundling the files would probably fix this, but is there any way to increase the timeout limit in the server settings? Would upgrading to v38 help? Thanks!. There's currently a non-configurable 5 minute timeout per GCS hash request. Assuming no batching (which I didn't come across) for ~40K individual requests that's about 100 requests/second to GCS. I'm pretty sure w/ GCS request throttling & internal cromwell backoffs at least one of those requests would fail to return in 5 minutes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4873:140,Hash,Hashing,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4873,2,"['Hash', 'hash']","['Hashing', 'hash']"
Security,"@orodeh it looks like the `WdlStandardLibraryFunctionsType#flatten` method was missing, which meant Cromwell wasn't able to validate workflows with `flatten`s in them. I also added the coercion so that `flatten(_: Array[Map[_]])` works as expected.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2959:124,validat,validate,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2959,1,['validat'],['validate']
Security,"@pshapiro4broad commented on [Tue Aug 01 2017](https://github.com/broadinstitute/wdltool/issues/36). We would like to automate the process of validating a JSON input file against our WDL. Using the `inputs` command is helpful but it would be even easier to use for validation if `wdltool` had a command that generates the JSON schema for the inputs. The format for a JSON schema is here: http://json-schema.org/. ---. @geoffjentry commented on [Tue Aug 01 2017](https://github.com/broadinstitute/wdltool/issues/36#issuecomment-319471781). Not a comment on the actual topic but just a heads up that the `wdltool` repo is one of the dustiest corners in terms of developer attention :). Also since `wdltool` is really just a command line wrapper around `wdl4s`, really any functionality request would involve a ticket there, might be more efficient to cut out the middle man and go there w/ these requests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2882:142,validat,validating,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2882,2,['validat'],"['validating', 'validation']"
Security,"@ruchim commented on [Fri Aug 11 2017](https://github.com/broadinstitute/wdl4s/issues/168). The wdl docs say that when using the true/false syntax in the command block, one can get away with declaring just one of the 2, however the WDL fails validation and fails to create a namespace and therefore never gets run. . AC:; Actually allow being able to declare True or False and statements OR Keep enforcing that both the behavior for true/false need to be defined, and adjust that expectation in the docs accordingly. https://github.com/broadinstitute/wdl/blob/develop/SPEC.md#true-and-false",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2713:242,validat,validation,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2713,1,['validat'],['validation']
Security,"@ruchim commented on [Thu Aug 24 2017](https://github.com/broadinstitute/wdltool/issues/46). When validating the workflow below with wdltool-0.14.jar, the response is simply ""null"". . ```; workflow w {; String s = ""test"". output {; String o =; }; }; ```; It would be great if a more comprehensive error is returned in the case of invalid workflow outputs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2883:98,validat,validating,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2883,1,['validat'],['validating']
Security,"@ruchim commented on [Wed May 24 2017](https://github.com/broadinstitute/wdltool/issues/29). Given a task:. task myTask {; File f; command {; touch ${f.bam.bai}; }; }. A workflow with such a task validates in wdltool-0.10 but when run on cromwell-26, it fails with an error: java.lang.UnsupportedOperationException: Could not evaluate expression:.... Given a slightly altered version of that previous task:. task myTask {; File f; command {; touch ${f%%.bam.bai}; }; }. This task also validates but fails before the Workflow is about to run with the error: scala.MatchError: null. ---. @geoffjentry commented on [Wed May 24 2017](https://github.com/broadinstitute/wdltool/issues/29#issuecomment-303819742). Is this an artifact of wdltool being out of synch? it happens way too often :(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2873:196,validat,validates,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2873,2,['validat'],['validates']
Security,"@tmdefreitas commented on [Tue Mar 22 2016](https://github.com/broadinstitute/wdltool/issues/7). I came across this while helping a colleague debug her WDL file. When this WDL file is validated, wdltool claims an ""Error: finished parsing without consuming all tokens"", even though the error is commented out:. ```; task comment_bug {; #String an_input. command {; echo ""Alternate command""; # The following line has a WDL syntax error, but in a comment!; #echo {an_input} ; }. output {. }; }. workflow test {; call comment_bug; }; ```. EDIT: error message, for completeness:. ```; $ java -jar wdltool.jar validate comment_bug.wdl; ERROR: Finished parsing without consuming all tokens. output {; ^; ```. I can get rid of the error by changing the comment line to `#echo ${an_input}`. ; I think errors in comment lines should probably be ignored by the validator. As an aside, is there a more helpful error message for this? The message sounds like an unused input variable or something, not that the bracket syntax was off, so it was hard to diagnose (The above is a toy example, the real task had a much more complicated command). ---. @scottfrazer commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200416975). This is happening because the the `command {...}` section is parsed differently than the rest of WDL. The parser thinks that the closing brace in `#echo {an_input}` is actually trying to close the `command` section. If you use the alternative delimiters (`command <<< ... >>>`) this is another way to get around it. We parse the command as ""opaque strings intermixed with `${...}` blocks"". That means that the `#`-style comments inside a command section are not interpreted as WDL comments, but instead as part of the command. More thought would have to go into figuring out what the right thing to do here is. ---. @tmdefreitas commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200442613). Admittedly",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2870:184,validat,validated,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2870,3,['validat'],"['validate', 'validated', 'validator']"
Security,"@tmdefreitas commented on [Tue Mar 29 2016](https://github.com/broadinstitute/wdltool/issues/8). In the following WDL, **GSEA_v_1_0_fwer_p_val_threshold** was not declared as an input, but the validator didn't raise an error. Cromwell choked when trying to run the task. Is there a reason wdltool shouldn't throw an error here?. ```; task tool_gsea_mrnaseq_subtypes {; String outputprefix; String pheno_from_aggregate_molecular_subtype_clusters; String pheno_name; File tcga_pheno_FileName; File tcga_exp_FileName; File gs_db; String GSEA_v_1_0_reshuffling_type; String GSEA_v_1_0_nperm; String GSEA_v_1_0_weighted_score_type; String GSEA_v_1_0_nom_p_val_threshold; String GSEA_v_1_0_topgs; String GSEA_v_1_0_adjust_FDR_q_val; String GSEA_v_1_0_gs_size_threshold_min; String GSEA_v_1_0_gs_size_threshold_max; String GSEA_v_1_0_reverse_sign; String GSEA_v_1_0_perm_type. command {; /R/RunR.sh -f main /src/Pathway_GSEA.R --libdir/src --disease_type${outputprefix} --pheno.from.Aggregate_Molecular_Subtype_Clusters${pheno_from_aggregate_molecular_subtype_clusters} --pheno.name${pheno_name} --tcga.pheno.FileName${tcga_pheno_FileName} --tcga.exp.FileName${tcga_exp_FileName} --gs.db${gs_db} --GSEA.v.1.0.reshuffling.type${GSEA_v_1_0_reshuffling_type} --GSEA.v.1.0.nperm${GSEA_v_1_0_nperm} --GSEA.v.1.0.weighted.score.type${GSEA_v_1_0_weighted_score_type} --GSEA.v.1.0.nom.p.val.threshold${GSEA_v_1_0_nom_p_val_threshold} --GSEA.v.1.0.fwer.p.val.threshold${GSEA_v_1_0_fwer_p_val_threshold} --GSEA.v.1.0.fdr.q.val.threshold${GSEA_v_1_0_fdr_q_val_threshold} --GSEA.v.1.0.topgs${GSEA_v_1_0_topgs} --GSEA.v.1.0.adjust.FDR.q.val${GSEA_v_1_0_adjust_FDR_q_val} --GSEA.v.1.0.gs.size.threshold.min${GSEA_v_1_0_gs_size_threshold_min} --GSEA.v.1.0.gs.size.threshold.max${GSEA_v_1_0_gs_size_threshold_max} --GSEA.v.1.0.reverse.sign${GSEA_v_1_0_reverse_sign} --GSEA.v.1.0.perm.type${GSEA_v_1_0_perm_type}. zip -r ${outputprefix}.pathway_gsea_mrnaseq_subtypes.zip . ; }. output {; File zip_results=""${outputprefix}.pat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2874:193,validat,validator,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2874,1,['validat'],['validator']
Security,"@vdauwera commented on [Mon Apr 24 2017](https://github.com/broadinstitute/dsde-docs/issues/1996). Need to put in a Cromwell ticket for this. Basic ask: have Cromwell automatically look for (and co-localize) accessory files when given files with a specific extensions. E.g. if I give it foo.bam file it should look for foo.bai. . Note that sometimes it's just a matter of swapping the extension, but sometimes it's adding another extension, and there can be multiple accessory files, e.g. reference.fasta is always accompanied by both reference.fasta.fai and reference.dict. . This would ideally be configurable by the Cromwell admin, who would set up a list of primary file extensions and their accessory file naming patterns. Bonus points if the user can provide their own config on the command line to override the server's config. And also I want a pet unicorn that farts glitter. ----. WDL folks;; This is a followup from a recent discussion about getting compatible bcbio generated WDL (http://gatkforums.broadinstitute.org/wdl/discussion/9257/object-attribute-access-and-secondary-index-files). Thanks to all the great help you've provided we now have compatible WDL output that passes validation:. https://github.com/bcbio/test_bcbio_cwl/blob/master/run_info-cwl-wdl. This is brilliant, and I'd like to move into testing runs with Cromwell. Before starting this, there is one major area I know we're missing in the conversion, handling of secondary files and directories of files. CWL has the notion of secondaryFiles (http://www.commonwl.org/v1.0/Workflow.html#File) which you can use to block these and ensure they get staged/run next to each other. I use this in bcbio and wanted to figure out the best way to map it into WDL. There are two cases we use these for:. - Index files associated with compressed inputs, like BAM bai indices and bgzip VCF tbi indices. These are a single index file attached to the original file that should get staged in the same directory when running.; - Direc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2269:208,access,accessory,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269,3,['access'],['accessory']
Security,"A few observations:; - If the EJEA is aborted, we could stop hashing the remaining files; - If the EJHA is done, it could stop hashing the remaining files; - Since the EJHA already blocks work into chunks of 100 (and BackendFileHashers tend to be synchronous), it could simply not send the next batch if it knows it doesn't need to; - If the set of initial hashes are a cache miss (and cache writing is disabled), we don't need to send the files for hashing in the first place",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1503:61,hash,hashing,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1503,4,['hash'],"['hashes', 'hashing']"
Security,"A simple grep through the source code reveals several hits with Log4j:. ```; CromwellRefdiskManifestCreator/pom.xml: <groupId>org.apache.logging.log4j</groupId>; CromwellRefdiskManifestCreator/pom.xml: <artifactId>log4j-core</artifactId>; CromwellRefdiskManifestCreator/pom.xml: <groupId>org.apache.logging.log4j</groupId>; CromwellRefdiskManifestCreator/pom.xml: <artifactId>log4j-api</artifactId>; CromwellRefdiskManifestCreator/src/main/java/org/broadinstitute/manifestcreator/CromwellRefdiskManifestCreatorApp.java:import org.apache.logging.log4j.Level;; CromwellRefdiskManifestCreator/src/main/java/org/broadinstitute/manifestcreator/CromwellRefdiskManifestCreatorApp.java:import org.apache.logging.log4j.LogManager;; CromwellRefdiskManifestCreator/src/main/java/org/broadinstitute/manifestcreator/CromwellRefdiskManifestCreatorApp.java:import org.apache.logging.log4j.Logger;; CromwellRefdiskManifestCreator/src/main/java/org/broadinstitute/manifestcreator/CromwellRefdiskManifestCreatorApp.java:import org.apache.logging.log4j.core.config.Configurator;; project/Dependencies.scala: // Replace all log4j usage with slf4j; project/Dependencies.scala: // https://www.slf4j.org/legacy.html#log4j-over-slf4j; project/Dependencies.scala: ""org.slf4j"" % ""log4j-over-slf4j"" % slf4jV; ```. I wasn't able to expose a vulnerability by using malicious code but my test is probably not extensive.; It looks like this lib is used in a packaging tool of Cromwell so probably not executed during production.; On the other hand, slj4j seems to be used everywere. Is that abstraction layer vulnerable ?. Could you please let us know if you believe Cromwell is affected by Log4shell ?. Thanks,",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6588:1304,expose,expose,1304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588,1,['expose'],['expose']
Security,"A very common way of producing a side .md5 file is to use something like. `md5 -q dbsnp_138.vcf > dbsnp_138.vcf.md5`. Which produces a trailing newline character, which cromwell reads and interprets as part of the hash, thus causing cache misses against files that (a) were hashed by cromwell or (b) don't have a newline. Not only isn't this the desired behavior... it's very confusing because it appears that sometimes call caching works and other times it does not. Cromwell should strip out all trailing whitespace (e.g. \n, \r\n) from the data read in the .md5 file",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2621:214,hash,hash,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2621,2,['hash'],"['hash', 'hashed']"
Security,A/C:; - centaur test that validates that inputs schemas are used for validation when the input specifies a format; - centaur test that validates that inputs schemas are ignored for validation when the input doesn't specify a format,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3569:26,validat,validates,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3569,4,['validat'],"['validates', 'validation']"
Security,"A/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam -> /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam (cp failed: gsutil -q -m cp gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam, command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205, in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1221, in Get\n required)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py\"", line 155, in TryFunc\n return func(*args, **kwargs), None\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 41, in _ReadNoP",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:13433,validat,validate,13433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['validat'],['validate']
Security,"A/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam -> /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam (cp failed: gsutil -q -m cp gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam, command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205, in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1221, in Get\n required)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py\"", line 155, in TryFunc\n return func(*args, **kwargs), None\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 41, in _ReadNoP",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:10246,validat,validate,10246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['validat'],['validate']
Security,AB86|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String runtime_attr:docker|A5281F25296D4311ED0C46422719018E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String default_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String default_attr:queueArn|2382ECF7BF8F09D7CD67C24B52AD78BF|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String default_attr:docker|A5281F25296D4311ED0C46422719018E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String default_attr:docker|A5281F25296D4311ED0C46422719018E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:Int runtime_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:Int runtime_attr:max_retries|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCach,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:3574,hash,hashes,3574,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,AN-184 Remove unused Batch restrict-metadata-access config,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7577:45,access,access,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7577,1,['access'],['access']
Security,API Workflow ID validation,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3517:16,validat,validation,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3517,1,['validat'],['validation']
Security,"ASTDB=~{blastdb} ; blastn \; -query ~{fasta} -db nt -num_threads 24 -evalue 1 -outfmt '6' -out ~{out_file}; >>>; output { File out = out_file }; runtime { docker: ""ncbi/blast:2.10.1"" }; }; ```. **confiuration snippet - localization only:**; ```; filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; # Call caching strategies; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""md5""; check-sibling-md5: false; }; }; }; ```. **logs:**; ```; [2020-08-08 19:20:00,49] [error] Failed to hash ""../../data/blast/blastdb"": Is a directory; [2020-08-08 19:20:00,49] [warn] Localization via hard link has failed: /workflows/cromwell-executions/good_donor_good_recipient/f7947643-2729-483f-b987-44ef932f88bd/call-blaster/main/6e4fa8a1-0d72-486e-a9ae-254319c4915d/call-blaster/shard-20/inputs/2058596876/blastdb -> /data/blast/blastdb: Operation not permitted; [2020-08-08 19:20:00,49] [error] 6e4fa8a1:main.blaster:46:1: Hash error (Is a directory), disabling call caching for this job.; ```. **contents of the BLASTDB directory:**; ```; /data/blast/blastdb$ ls; nt.00.nhd nt.01.nhd nt.02.nhd nt.03.nhd nt.04.nhd nt.05.nhd nt.06.nhd nt.07.nhd nt.08.nhd nt.09.nhd nt.10.nhd nt.11.nhd nt.12.nhd nt.13.nhd nt.14.nhd nt.15.nhd nt.16.nhd nt.17.nhd nt.18.nhd nt.19.nhd nt.20.nhd nt.21.nhd nt.22.nhd nt.23.nhd nt.24.nhd nt.nal nt.00.nhi nt.01.nhi nt.02.nhi nt.03.nhi nt.04.nhi nt.05.nhi nt.06.nhi nt.07.nhi nt.08.nhi nt.09.nhi nt.10.nhi nt.11.nhi nt.12.nhi nt.13.nhi nt.14.nhi nt.15.nhi nt.16.nhi nt.17.nhi nt.18.nhi nt.19.nhi nt.20.nhi nt.21.nhi nt.22.nhi nt.23.nhi nt.24.nhi nt.ndb nt.00.nhr nt.01.nhr nt.02.nhr nt.03.nhr nt.04.nhr nt.05.nhr nt.06.nhr nt.07.nhr nt.08.nhr nt.09.nhr nt.10.nhr nt.11.nhr nt.12.nhr nt.13.nhr nt.14.nhr nt.15.nhr nt.16.nhr nt.17.nhr nt.18.nhr nt.19.nhr nt.20.nhr nt.21.nhr nt.22.nhr nt.23.nhr nt.24.nhr nt.nos nt.00.nin nt.01.nin nt.02.nin nt.03.nin nt.04.nin nt.05.nin nt.06.nin nt.07.nin nt.08.nin nt.09.nin nt.10.nin nt.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5737:2068,Hash,Hash,2068,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737,1,['Hash'],['Hash']
Security,AWS Batch: Docker hash lookup failed with code,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4345:18,hash,hash,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4345,1,['hash'],['hash']
Security,AWS ECR Remote Hashing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7444:15,Hash,Hashing,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7444,1,['Hash'],['Hashing']
Security,AWS S3: Can't access data outside my region (Status Code: 301),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4731:14,access,access,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731,1,['access'],['access']
Security,Access to optional variables in conditionals broken for WDL 1.0 (works w draft-2),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4981:0,Access,Access,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981,1,['Access'],['Access']
Security,Access to runtime attributes within tasks,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4741:0,Access,Access,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4741,1,['Access'],['Access']
Security,ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadata,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:6114,validat,validation,6114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,1,['validat'],['validation']
Security,"ActorFactory""; config {; run-in-background = true; runtime-attributes = ""String? docker Int? max_runtime = 2""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". # Root directory where Cromwell writes job results. This directory must be; # visible and writeable by the Cromwell process as well as the jobs that Cromwell; # launches.; root: ""cromwell-executions"". filesystems {; local {; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]. caching {; duplication-strategy: [; ""soft-link""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; hashing-strategy: ""path"". # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.; # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.; check-sibling-md5: false; }; }; }; }; }; }; }. database {; db.url = ""jdbc:mysql://mysql-db/cromwell_db?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true""; db.user = ""cromwell""; db.password = ""cromwell""; db.driver = ""com.mysql.cj.jdbc.Driver""; profile = ""slick.jdbc.MySQLProfile$""; db.connectionTimeout = 15000; }; ```. and here is my cormwell dockerfile:. ```; FROM broadinstitute/cromwell:develop. RUN git clone https://github.com/vishnubob/wait-for-it.git; RUN mkdir cromwell-working-dir; WORKDIR cromwell-working-dir. COPY ./app-config /app-config. ENTRYPOINT [""/bin/sh"", ""-c""]; ```. when i submit a wdl did not use docker it was ok. but when i submit a wdl need to use docker, a error apear.; ```; /cromwell-working-dir/cromwell-executions/RNAseq/26e3c339-39d3-442f-b93e-8269dc7f9fa6/call-fastp_pe/shard-7/execution/script.submit: line 2: do",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7006:2177,hash,hash,2177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7006,1,['hash'],['hash']
Security,Add WDL draft 3 to womtool validation tests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3479:27,validat,validation,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3479,1,['validat'],['validation']
Security,Add a default-application authentication mode that uses Application D…,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/329:26,authenticat,authentication,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/329,2,['authenticat'],['authentication']
Security,Add a new API endpoint to CromIAM which will allow a user to update the collection name for one or more workflows in a one to many fashion - i.e. one collection name will be applied to 1+ workflows. If Sam says that the user does not have access to this collection name it will be an error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2839:239,access,access,239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2839,1,['access'],['access']
Security,"Add abort, workflow store delete to coordinated access actor [WA-334]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906:48,access,access,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906,1,['access'],['access']
Security,Add all required validation to ValidationActor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/488:17,validat,validation,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/488,2,"['Validat', 'validat']","['ValidationActor', 'validation']"
Security,Add an option to turn off Docker hash lookups,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2600:33,hash,hash,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2600,1,['hash'],['hash']
Security,Add an option to use filename as part of the call caching hash,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3054:58,hash,hash,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3054,1,['hash'],['hash']
Security,Add authenticated LDAP to proxy config,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/381:4,authenticat,authenticated,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/381,1,['authenticat'],['authenticated']
Security,Add basic backend validation for JES PBE config,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/812:18,validat,validation,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/812,1,['validat'],['validation']
Security,Add basic backend validation for Local PBE config,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/811:18,validat,validation,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/811,1,['validat'],['validation']
Security,Add call caching option to treat file paths as the hash,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1271:51,hash,hash,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1271,2,['hash'],['hash']
Security,Add integration tests for AWS authentication,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3747:30,authenticat,authentication,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3747,1,['authenticat'],['authentication']
Security,Add new hash path strategy with last modified time,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4405:8,hash,hash,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4405,1,['hash'],['hash']
Security,Add restrict-metadata-access,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2435:22,access,access,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2435,1,['access'],['access']
Security,Add retries to credential validation,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2314:26,validat,validation,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2314,1,['validat'],['validation']
Security,Add tests for more complex member accesses,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3564:34,access,accesses,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3564,1,['access'],['accesses']
Security,"Add the fuse support into the new Google Batch backend. It works like the Life Scientist API beta 2. I use the privileged mode, as we don't usually care about the security of the host OS. I can revise to limited permission if needed. . This function is useful to our team. Thanks for reviewing the changes. . Regards,; Zhili",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7378:163,secur,security,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7378,1,['secur'],['security']
Security,"Add workflow ""validate"" as an API endpoint",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2652:14,validat,validate,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2652,1,['validat'],['validate']
Security,"Added Docker specification content types to the manifest unmarshaller.; Updated spec for testing GCR w/o authentication, as cromwell's Google Credentials utility now automatically detects the application default credentials on a system.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/715:105,authenticat,authentication,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/715,1,['authenticat'],['authentication']
Security,Added a CRC hash get function for helping Job Avoision,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/272:12,hash,hash,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/272,2,['hash'],['hash']
Security,"Added a new Shared File System (SFS) Backend package as a base for any/most backends wanting to execute cromwell calls on an SFS.; Added SGE and LSF configs (commented out) to the main `application.conf`, plus other small tweaks.; Non-main `application.conf` files only set the keys that they need to add or override from the main.; More of the config is defaulted across the board.; A number of classes moved/refactored from other backends for use by the SFS backend.; New trait `RuntimeAttributesValidation[ValidatedType]` for proccessing runtime attributes.; First pass at a `WorkflowFileSystemProvider` that combines a config and workflow options to create a filesystem. Only implemented for local and GCS files.; Added a Config backend that implements the SFS backend by reading wdl-lite command strings and `job-id-regex`.; Currently extending SFS backend to create the SGE backend until one can specify runtime attributes via the Config backend.; Added a ""shadow"" local backend, extending the SFS backend, that will be evaluated for a while before hopefully usurping the original.; Fixed integration tests that were attempting to mock the gcs file system.; Gave `MetadataDatabaseAccessSpec` a little bit more time the non-split up ""create and query a workflow"".; Fixed the `SprayDockerRegistryApiClientSpec` for v1 registries by switching from the `ubuntu` repo to the `registry` repo (we query the registry for a repo hosting images of the docker registry-- inception).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1202:509,Validat,ValidatedType,509,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1202,1,['Validat'],['ValidatedType']
Security,Added basic parsing validation of the swagger yaml.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/211:20,validat,validation,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/211,1,['validat'],['validation']
Security,Added data access identity to workflow options [BT-442],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6563:11,access,access,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6563,1,['access'],['access']
Security,Added scalaz lib support to ValidateActor in order to match Workflow descriptor.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/562:28,Validat,ValidateActor,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/562,1,['Validat'],['ValidateActor']
Security,Added scalaz lib to ValidateActor. Closes #543,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/562:20,Validat,ValidateActor,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/562,1,['Validat'],['ValidateActor']
Security,Added validation back into the preStart of a BackendWorkflowActor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/686:6,validat,validation,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/686,1,['validat'],['validation']
Security,Adding a test exposed some previously unknown bugs 😬 which should be fixed now.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5295:14,expose,exposed,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5295,1,['expose'],['exposed']
Security,Additional options for hashing-strategy,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1843:23,hash,hashing-strategy,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1843,1,['hash'],['hashing-strategy']
Security,Adds a test to PR #6072. Can be used to validate the existing fix or any alternative implementation based on the resolution to [this discussion](https://github.com/broadinstitute/cromwell/pull/6072#issuecomment-745492666).,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6725:40,validat,validate,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6725,1,['validat'],['validate']
Security,"Adds more runtime parameters, adjusted case class to bring all runtime parameters, add user agent to requests and validated with google coming in, finalization fix for workflow parameters.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6987:114,validat,validated,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6987,1,['validat'],['validated']
Security,Adds the AES256 encryption flag to the function that copies RC and STDOUT/ERR logs to S3,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4377:16,encrypt,encryption,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4377,1,['encrypt'],['encryption']
Security,"After workflow succesffuly ran this failed with the below error when trying to copy the final outputs. ```; 2016-06-01 16:10:15,093 cromwell-system-akka.actor.default-dispatcher-15 ERROR - WorkflowActor [UUID(88b21d2d)]: failed to create call actor for PairedEndSingleSampleWorkflow.$final_call$copy_workflow_log.; java.util.NoSuchElementException: None.get; at scala.None$.get(Option.scala:347) ~[cromwell.jar:0.19]; at scala.None$.get(Option.scala:345) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.CallMetadataBuilder$$anonfun$buildCallFailureTransformer$1$$anonfun$apply$26.apply(CallMetadataBuilder.scala:156) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.CallMetadataBuilder$$anonfun$buildCallFailureTransformer$1$$anonfun$apply$26.apply(CallMetadataBuilder.scala:155) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply(TraversableLike.scala:728) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:727) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.CallMetadataBuilder$$anonfun$buildCallFailureTransformer$1.apply(CallMetadataBuilder.scala:155) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.CallMetadataBuilder$$anonfun$buildCallFailureTransformer$1.apply(CallMetadataBuilder.scala:153) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.CallMetadataBuilder$$anonfun$15.apply(CallMetadataBuilder.scala:233) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.CallMetadataBuilder$$anonfun$15.apply(CallMetadataBuilder.scala:232) ~[cromwell.jar:0.19]; at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124) ~[cromwell.jar:0.19]; at scala.collection.immutable.List.foldLeft(List.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/927:952,Hash,HashMap,952,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/927,2,['Hash'],['HashMap']
Security,"Akka 2.5.4 provides the new [AffinityPool](https://github.com/akka/akka/pull/23104) which is expected to provide performance benefit in cases where you have long lived actors maintaining lots of state. Because it works a lot like a `PinnedDispatcher` it wouldn't be a panacea for us even if it was useful in some of our cases but I can imagine using it for a handful of carefully selected actors (and perhaps only specified in a handful of use cases for Cromwell) could have benefits. . This ticket is mostly a benchmarking exercise to explore what using this pool might do to performance in Cromwell. Try to hash out an envelope of where/if this pool would be useful. For instance, would adding it to a few key actors provide measurable impact? Does it depend on how many cores are in use altogether, i.e. does the pinning effect mean that you really need excess cpus to see benefit? etc etc etc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2571:609,hash,hash,609,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2571,1,['hash'],['hash']
Security,"All the tests that pass on the Local backend currently pass on PAPI with the hackery documented below. This does not attempt to run tests that fail on Local because there's no realistic reason to believe a conformance test that fails Local would succeed on PAPI and the PAPI conformance test run would take hours. On the topic of slowness, the PAPI conformance run will probably have to be converted to a cron job once more conformance tests start passing. The hacks:; - Cromwell now allows for a default Docker image to be specified in config. This is required for those conformance tests that don't specify a `DockerRequirement`.; - Cromwell allows for a ""GCS default input prefix"" of any input files when using the JES backend. This allows for the conformance input JSONs to be used as-is. It also works to hack the input JSONs to specify the GCS paths where input files are staged and not bother with the ""GCS default input prefix"".; - I had to turn off call caching since the hashing actor was looking at the non-GCS paths of files. It's possible this could be worked around if anyone felt it was worth the effort, but I didn't.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3086:981,hash,hashing,981,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3086,1,['hash'],['hashing']
Security,"Allow Cromwell system administrators to restrict WDL HTTP imports to specific, trusted hosts/domains. This prevents the import mechanism from being used to inappropriately access resources that the Cromwell instance has access to on its internal LAN, but which are not exposed to the end users. Terra configuration here: https://github.com/broadinstitute/firecloud-develop/pull/3138",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6938:172,access,access,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6938,3,"['access', 'expose']","['access', 'exposed']"
Security,Allow ENV injection into swagger yaml [BW-861],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6552:10,inject,injection,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6552,1,['inject'],['injection']
Security,Allow member accesses to parse,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3064:13,access,accesses,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3064,1,['access'],['accesses']
Security,Allow nested structs and member accesses,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3358:32,access,accesses,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3358,1,['access'],['accesses']
Security,Allow task-only validation in draft 2 WDL,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3772:16,validat,validation,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3772,1,['validat'],['validation']
Security,Allow the underlying actor system to be accessed by Backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/323:40,access,accessed,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/323,1,['access'],['accessed']
Security,Allows for and tests `womtool validate draft2_task.wdl`. Fixes #3762,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3772:30,validat,validate,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3772,1,['validat'],['validate']
Security,"Allows reading of WDL 1.0 and 1.1 `Ast`s through a shared set of `CheckedAtoB` functions, with the flexibility to inject different transform behavior into each usage of the instantiations of the transforms. Note: of the 9013 added lines, 7484 are the new 1.1 WDL parser and presumably about 810 are files which got moved out of draft-3/transforms and into base/transforms",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3852:114,inject,inject,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3852,1,['inject'],['inject']
Security,"Already part of the plans for reworked, centralized validation",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/570:52,validat,validation,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/570,1,['validat'],['validation']
Security,"An audit of Cromwell traffic shows >96% of traffic hits the status endpoint for polling. A sample of this traffic shows an average of 40/s. As a perf benchmark we'd like to ensure that CromIAM can handle 200 reqs/s to this endpoint, ideally assuming SAM & Cromwell are perfectly performant.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4614:3,audit,audit,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4614,1,['audit'],['audit']
Security,"An example from a failed CRON test:. ```; 2018-07-04 07:18:56,909 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(b2e34f33)Arrays.AutoCall:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861:874,access,access,874,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861,1,['access'],['access']
Security,"An incorrect key was being used to specify hashing strategy, but Cromwell didn't complain and the user wrongly thought their value was accepted.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1598:43,hash,hashing,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1598,1,['hash'],['hashing']
Security,And another possible bug: why are we trying to upload an auth file when running in application default auth mode for both genomics and filesystems?. ```; [ERROR] [01/27/2017 14:39:36.100] [cromwell-system-akka.dispatchers.engine-dispatcher-5] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow 732474fd-88b0-4a5e-ad19-5ee5cd71d141 failed (during InitializingWorkflowState): Failed to upload authentication file; java.io.IOException: Failed to upload authentication file; 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$1.applyOrElse(JesInitializationActor.scala:81); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$1.applyOrElse(JesInitializationActor.scala:80); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinP,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1924:438,authenticat,authentication,438,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1924,2,['authenticat'],['authentication']
Security,"Apologies, but I can not currently access jira. First time user question:. # The problem. When running `cromwell` locally, I get an excessive number of messages (thousands) from `liquibase`. Here is an example:. ```; Jan 31, 2022 5:36:07 PM liquibase.changelog; INFO: Custom SQL executed; Jan 31, 2022 5:36:07 PM liquibase.changelog; INFO: ChangeSet metadata_changesets/remove_non_summarizable_metadata_from_queue.xml::delete_non_summarizable_metadata_from_queue::mcovarr ran successfully in 1ms; Jan 31, 2022 5:36:07 PM liquibase.changelog; INFO: Index IX_WORKFLOW_METADATA_SUMMARY_ENTRY_MAS dropped from table WORKFLOW_METADATA_SUMMARY_ENTRY; ```. Is it possible to control these from cromwell? Is this a liquibase issue? ; The standard `-DLOG_LEVEL=WARN` does not seem to effect log messages. - Version: cromwell-74.jar; - Backend: local ; - Java version: 17.0.2+8 (azul). ## Workflow:; ```wdl; version 1.0. task say_hello {; input {; String name; }. command {; set -euxo pipefail; echo ""Hello ~{name}""; echo ""Hello ~{name}"" > greeting.txt; }. output {; File greeting = ""greeting.txt""; }. runtime {; docker: ""debian:bullseye-slim""; }; }. workflow hello {; input {; String name; }. call say_hello {; input: ; name = name; }. output {; File greeting = say_hello.greeting; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6664:35,access,access,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6664,1,['access'],['access']
Security,Appears to be a good case for retry love:. ```; java.io.IOException: Failed to upload authentication file; at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializati\; onActor$$writeAuthenticationFile$1.apply(JesInitializationActor.scala:61); at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializati\; onActor$$writeAuthenticationFile$1.apply(JesInitializationActor.scala:58); at scala.Option.foreach(Option.scala:257); at cromwell.backend.impl.jes.JesInitializationActor.cromwell$backend$impl$jes$JesInitializationActor$$\; writeAuthenticationFile(JesInitializationActor.scala:58); at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.apply(JesInitializationActor.\; scala:52); at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.apply(JesInitializationActor.\; scala:51); at scala.util.Try$.apply(Try.scala:192); at cromwell.backend.impl.jes.JesInitializationActor.beforeAll(JesInitializationActor.scala:51); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$initSequence$1$$anonfun$apply$1.apply(\; BackendWorkflowInitializationActor.scala:156); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$initSequence$1$$anonfun$apply$1.apply(\; BackendWorkflowInitializationActor.scala:155); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91\; ); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(Bloc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2009:86,authenticat,authentication,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2009,1,['authenticat'],['authentication']
Security,"As `WomValue` can now be represented in `WomExpression` using `ValueAsExpression`, this validation was reinstated.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2954:88,validat,validation,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2954,1,['validat'],['validation']
Security,"As a **workflow runner**, I want **certain parameters to be ignored in the hashing process**, so that I can **call cache on more workflows when the result is exactly the same**.; - Effort: **?**; - Risk: **Medium** ; - We should err on the side of hashing a workflow differently if we are not absolutely confident that the parameter does not impact the result.; - Which parameters are ignored is NOT user-editable. This is to prevent users from accidentally ignoring parameters that do impact the result.; - Business value: **Medium**. Some parameters, such as `preemptible_attempts` and `CPU`, don't affect the outcome of the workflow but workflows with different CPU values will not call cache. @LeeTL1220 and @geoffjentry to provide additional thoughts and context if helpful.; Related issue #1210",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2604:75,hash,hashing,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2604,2,['hash'],['hashing']
Security,"As a WDL author, I would like to be able to write a sub workflow and call it from my main workflow as a subworkflow. I would like the main workflow to be backwards compatible, so I would like to use a keyword modifier to flag the _other_ workflows as being non-primary. . E.g. perhaps the keyword `sub` so I would write `sub workflow { //blah }`. Since these workflows would be callable when they are included via an import I'd prefer to not have a keyword like `private` since that implies visibility/access",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1475:502,access,access,502,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1475,1,['access'],['access']
Security,"As a part of supporting sub-workflows, workflow outputs need to behave similarly to task outputs. Task outputs are defined as typed variable declarations (e.g. File myout = ""${foo}.bam""). Currently workflow outputs just ""expose"" the outputs of tasks, and operate more like a whitelist or filter. You can not fabricate a workflow output based on a task output (like the about myout example). However, this should still be backwards compatible with the current definitions. For example you can write:. `output {; task.value; }`. However, we should be able to allow this as syntactic sugar by inferring the type of this from the task definition. For example if this was a File, the above would be the same as `File task.value = task.value`. `output {; File task.value = task.value; }`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1473:221,expose,expose,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1473,1,['expose'],['expose']
Security,"As a pipeline author, I don't enjoy having to spin up a VM, with docker in order to do string substitutions on my parameters. In the GOTC pipeline, we do this in order to strip off the extension of the input file in order to get a base-name, which happens ~40 times in a 20-plex workflow. . This causes a real problem because by requiring so many VMs to spun up, we spend more money (although that cost is quite small) but also eat into our quotas and QPS limits, which actually does hurt our scalability. The proposal is to add a new expression language function which allows for regex substitutions:. sub(string, pattern, replacement). For example,. to strip off an extension from a file you could use `sub(filename, "".bam$"","""")`; to swap an extension, you could use `sub(filename, "".bam$"", "".metrics"")`. By being constrained to a regex (unlike an arbitrary code block) we don't have concerns about security or evaluating these in the cromwell engine. This does not eliminate the need for a generally more expressive expression language or user defined functions, but does solve a large class of common usages that impact ease of use and performance",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/597:901,secur,security,901,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/597,1,['secur'],['security']
Security,"As a user who runs large scale pipelines in Google, I may run into quota limitations on external IP addresses as they are consumed by my JES compute VMs. In many cases (using GCR and using GCS only) I would like to have JES create a node without an external IP address. JES has added this feature (see https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines) through the ""noAddress"" flag. noAddress must be set in both ephermeralPipeline and pipelineArgs. NOTE: Your project must be whitelisted in ""Google Access for Private IPs Early Access Program"". If your project is not whitelisted and you use noAddress, the operation will hang. This should be specified in the WDL as a runtime attribute, similar to the way cpus, memory or preemptible VMs are requested",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1325:518,Access,Access,518,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1325,2,['Access'],['Access']
Security,"As a user with a controlled file system (like GOTC or the cromwell execution directory) where the I know that a file path is immutable and uniquely identifying, I would like to run the cromwell server in a mode where the file path can be used in call caching rather than computing the actual hash. I will take on the risk that if I break that contract (by modifying files), workflows will not execute properly. I want to do this because it will be a big performance gain when I have many files and I know that their paths are unique. @cjllanwarne gets credit for raising this as a cool feature, @jsotobroad and @dshiga agreed",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1271:292,hash,hash,292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1271,2,['hash'],['hash']
Security,"As discussed in https://github.com/broadinstitute/cromwell/issues/6235, developers of workflows for GCP who store their images in Google Container Repositories can be exposed to large Google GCS egress charges when users attempt to run workflows in different continental regions, resulting in many trans-continental container pulls. There currently does not seem to be a satisfactory way to guard against this:. - We can't make our image repositories private because we want to make the workflows available to the public via Terra.; - We can't make the repositories requester-pays because the pipelines API does not support pulling images from requester-pays repositories.; - We can mirror our repositories to different regions, but we are still dependent on our users to configure their workflows to point to the right region and take good-faith extra steps to help us avoid these charges. Some possible ideas were suggested by @freeseek in https://github.com/broadinstitute/cromwell/issues/6235:. - Convince Google to support requester-pays buckets for container pulls in PAPI.; - Modify some combination of Cromwell/PAPI to cache images rather than pulling them for each task that is run.; - Develop infrastructure within Cromwell to know what region the workflow is running in and automatically select the right GCR mirror to pull from.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442:167,expose,exposed,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442,1,['expose'],['exposed']
Security,As extra `womtool validate` test to make sure WDL 1.0 is catching this. Unfortunately there's no draft-2 equivalent of this test yet because of the bug spotted in #4550,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4554:18,validat,validate,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4554,1,['validat'],['validate']
Security,"As far as I remember, in the Past it was possible to reference global workflow variables inside a task. But now I get wdl validation errors like this:; ```; ERROR: Variable genome does not reference any declaration in the task (line 36, col 27):. curl -z ${folder}""/""${genome} --max-time 10 --retry 3 --retry-delay 1 ${genomeURL}; ^. Task defined here (line 26, col 6):. task download_genome {; ```; Here is the wdl; ```wdl; workflow indexes {. File genomesFolder; String version #release version; String species #species and also name of the index/. String releaseURL #path to releseas. String transcriptome #relative file name (.fa.gz); String genome #relative file name (.fa.gz); String annotation #relative annotation file name (.gtf). call download_genome {; input:; genomeURL = releaseURL + ""/"" + genome,; transcriptomeURL = releaseURL + ""/"" + transcriptome,; annotationURL = releaseURL + ""/"" + annotation,; folder = genomesFolder + ""/"" + species + ""/"" + version; }. }. task download_genome {. String genomeURL; String transcriptomeURL; String annotationURL; String folder. command {; mkdir -p ${folder}; curl -z ${folder}""/""${genome} --max-time 10 --retry 3 --retry-delay 1 ${genomeURL}; curl -z ${folder}""/""${transcriptome} --max-time 10 --retry 3 --retry-delay 1 ${transcriptomeURL}; curl -z ${folder}""/""${annotation} --max-time 10 --retry 3 --retry-delay 1 ${annotationURL}; }. }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2504:122,validat,validation,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2504,1,['validat'],['validation']
Security,"As most of biological containers (like http://biocontainers.pro) are hosted at quay.io it will be useful to get docker hash from there for call caching. Right now with the latest release I get the following message:; """"""; Cromwell attempted to retrieve the current hash for this docker image but failed.\nThis is not necessarily a cause for concern as Cromwell is currently only able to retrieve hashes for Dockerhub and GCR images.\nThe job will be dispatched to the appropriate backend that will attempt to run it.; """"""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2252:119,hash,hash,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2252,3,['hash'],"['hash', 'hashes']"
Security,As part of auditing our Codecov secrets leak I am trying to trim down the number of items we maintain in Vault. This changeset allows us to delete `secret/dsde/cromwell/common/cromwell-refresh-token`.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6331:11,audit,auditing,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6331,1,['audit'],['auditing']
Security,"As pointed out by @davidbernick, there are some vulnerabilities in Cromwell's docker image. Beyond that, it's a good idea to periodically update the underlying image. This is not deemed to be a critical issue (yet) from a security perspective, but we should make sure to clear this up when we get a chance. $ docker run -it --rm -e CLAIR_ADDR=http://clair.bits-infosec.broadinstitute.org:6060 -e CLAIR_OUTPUT=High -e CLAIR_THRESHOLD=10 -e DOCKER_USER=davidbernick -e DOCKER_PASSWORD='xxxxx' broadinstitute/klar broadinstitute/cromwell:dev; clair timeout 1m0s; docker timeout: 1m0s; no whitelist file; Analysing 10 layers; Got results from Clair API v1; Found 139 vulnerabilities; Unknown: 3; Negligible: 47; Low: 38; Medium: 44; High: 7. CVE-2017-12424: [High] ; Found in: shadow [1:4.4-4.1]; Fixed By: ; In shadow before 4.5, the newusers tool could be made to manipulate internal data structures in ways unintended by the authors. Malformed input may lead to crashes (with a buffer overflow or other memory corruption) or other unspecified behaviors. This crosses a privilege boundary in, for example, certain web-hosting environments in which a Control Panel allows an unprivileged user account to create subaccounts.; https://security-tracker.debian.org/tracker/CVE-2017-12424; -----------------------------------------; CVE-2018-13347: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; mpatch.c in Mercurial before 4.6.1 mishandles integer addition and subtraction, aka OVE-20180430-0002.; https://security-tracker.debian.org/tracker/CVE-2018-13347; -----------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; htt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4979:222,secur,security,222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979,1,['secur'],['security']
Security,"As side quests, this included:. * No longer requiring inputs in order to build a `WomBundle` (what `wom graph` really wants, and what imports should really produce).; * A second step to go from `WomBundle` to `WomExecutable`/`ValidatedWomNamespace`; * Fixing draft 3 input processing in the `LanguageFactory`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3323:226,Validat,ValidatedWomNamespace,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3323,1,['Validat'],['ValidatedWomNamespace']
Security,As you see on the screenshot all pairs are highlighted as errors in Intellij while wdltool validates everything without an issue.; ![pair_highlightning_error](https://cloud.githubusercontent.com/assets/842436/25742086/783c9136-3196-11e7-8649-9fd5e1403b70.png),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2246:91,validat,validates,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2246,1,['validat'],['validates']
Security,"At least ""allows result reuse"" and ""results cloned"" (the latter being transformed from an fk to a boolean). Returning hashes might be helpful too for diagnosing why executions don't avoid.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/559:118,hash,hashes,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/559,1,['hash'],['hashes']
Security,"At some point we will have to update our liquibase library, either for vulnerability patches or other bug fixes. . However there is a report that the current liquibase version (3.6.3) plus the way our changelog differentiates databases (ex: using strings matching for things like `mysql`) causes an issue with MariaDB. This ticket is not about updating liquibase. Before that can occur we need a CI regression test to ensure that MariaDB is supported. A/C:; - At least centaur-local running against mariadb 10.3+. Links:; - https://github.com/broadinstitute/cromwell/issues/4605; - https://www.sourceclear.com/vulnerability-database/security/cross-site-scripting-xss-/java/sid-6098; - https://liquibase.jira.com/browse/CORE-3203; - https://liquibase.jira.com/browse/CORE-3263 (may not be related) ; - https://docs.travis-ci.com/user/database-setup/#mariadb; - https://docs.travis-ci.com/user/build-matrix/#explicitly-including-jobs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4618:633,secur,security,633,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4618,2,"['secur', 'xss']","['security', 'xss']"
Security,At the moment the PAPI v2 backend uses hardcoded public docker images to localize and delocalize files / directories.; This is not desirable for several reasons:. 1) Dependency on external images; 2) Lack of flexibility; 3) Potentially unoptimized or oversized images. Infrastructure should be put in place so that we can have control over those images while ensuring they can be accessed by all Cromwell users.; Those images (along with the command they run maybe ?) should be configurable.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3680:380,access,accessed,380,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3680,1,['access'],['accessed']
Security,"At the moment there's a concept of a submission whitelist for users in CromIAM. Allow for this to be optional for situations where all users are allowed to submit workflows, this will reduce traffic between CromIAM & Sam. Although this CromIAM will be exposed to the internet, the Cromwell service will still be protected from drive-by whitelist-not-specified submissions by the Google authentication. This authentication will be enforced by the proxy.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4475:252,expose,exposed,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4475,3,"['authenticat', 'expose']","['authentication', 'exposed']"
Security,"Authentication configuration has been coded but not properly tested for the ability to assume roles, etc. Integration tests should exist for this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3747:0,Authenticat,Authentication,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3747,1,['Authenticat'],['Authentication']
Security,Available system variables accessible from Cromwell configuration,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6005:27,access,accessible,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6005,1,['access'],['accessible']
Security,Avoid hashing Scopes. Closes #1457,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1468:6,hash,hashing,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1468,1,['hash'],['hashing']
Security,"BCS: Investigate hashing/caching support for OSS ""docker""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3518:17,hash,hashing,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518,1,['hash'],['hashing']
Security,BT-431 Update DRS Localizer for multiple access token strategies,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6553:41,access,access,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6553,1,['access'],['access']
Security,BT-732 Checksum validation for blobs read by engine,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6838:7,Checksum,Checksum,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6838,2,"['Checksum', 'validat']","['Checksum', 'validation']"
Security,BW-1224 Security upgrade for jackson-databind,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6747:8,Secur,Security,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6747,1,['Secur'],['Security']
Security,BW-1228 security upgrades,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6793:8,secur,security,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6793,1,['secur'],['security']
Security,Backend request: Hashicorp Nomad,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6001:17,Hash,Hashicorp,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6001,1,['Hash'],['Hashicorp']
Security,Backend side Validation. Closes #651.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/708:13,Validat,Validation,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/708,1,['Validat'],['Validation']
Security,Backend validation expression evaluation exceptions,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/725:8,validat,validation,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/725,1,['validat'],['validation']
Security,"Backend: AWS Batch. Workflow: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/frankenstein.wdl. Input file: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/map-variantcall-hg38.json. Possibly related to #4412 but not sure as I don't see the same error message. When submitting a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-anothe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563:469,hash,hash,469,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563,3,['hash'],"['hash', 'hashes']"
Security,"Backend: AWS Batch; Cromwell version: 45.1; ----; I am building a WDL pipeline using the CloudFormation set up provided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162:519,secur,security,519,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162,3,"['authenticat', 'secur']","['authentication', 'security']"
Security,Backends should report whether using tag or hash,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2096:44,hash,hash,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2096,1,['hash'],['hash']
Security,Bad WDL both validates and causes cromwell to hang,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1774:13,validat,validates,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1774,1,['validat'],['validates']
Security,Bad errors reported validating null.wdl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2703:20,validat,validating,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2703,1,['validat'],['validating']
Security,"Bad validation output: ""MatchError: null""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4081:4,validat,validation,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4081,1,['validat'],['validation']
Security,"Based on a report from @curoli . I can't immediately reason out what the correct behavior is (post-vacation fuzziness...) so this is just a a log of stuff that looked suspicious to me. ---. A workflow like; ```; version 1.0. workflow test {; ; Map[String, String] m = {""a"": ""a"", ""b"": ""b""}; String s = ""string"". output {; File write_attempt = write_json({""m"": m, ""s"": s}); }; }; ```; validates in Womtool but fails at runtime with error; ```; WorkflowManagerActor Workflow 2a3db889-e126-467b-be60-6abb815ea46e failed (during ExecutingWorkflowState):; java.lang.UnsupportedOperationException:; Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types:; Map(; WomString(m) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(a) -> WomString(a), WomString(b) -> WomString(b))),; WomString(s) -> WomString(string); ); ```. ---. The problem is not so simple as heterogeneous types in the map values; the workflow; ```; version 1.0. workflow test {; ; String s = ""string""; Float f = 0.1; File file = ""asdf"". output {; File write_attempt = write_json({""s"": s, ""f"": f, ""file"": file}); }. }; ```; works just fine:; ```; {""s"":""string"",""f"":""0.1"",""file"":""asdf""}; ```. ---. Interestingly, if we take out `String s = ""string""` we do get an error in Womtool, but it's a confusing one - why would we say a map value has to be an `Object` when we clearly used `String`, `Float`, and `File` right above?; ```; version 1.0. workflow test {; ; Map[String, String] m = {""a"": ""a"", ""b"": ""b""}. output {; File write_attempt = write_json({""m"": m}); }; }; ```; yields; ```; womtool validate any_map.wdl ; Failed to process workflow definition 'test' (reason 1 of 1):; Failed to process declaration 'File write_attempt = write_json({ ""m"": m })' (reason 1 of 1):; Failed to process expression 'write_json({ ""m"": m })' (reason 1 of 1):; Invalid parameter 'MapLiteral(Map(StringLiteral(m) -> IdentifierLookup(m)))'. Expected 'Object' but got 'Map[String, Map[String, String]]'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4512:383,validat,validates,383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4512,2,['validat'],"['validate', 'validates']"
Security,Based on new security/permissions requirements there is a need to add extra functionality to HtCondor backend. Basically the functionality should provide a way to link cached file / array of files outputs to the current workflow execution. All paths should point to current workflow execution dir.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1425:13,secur,security,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1425,1,['secur'],['security']
Security,"Based on the documentation https://cromwell.readthedocs.io/en/stable/Imports/, we should have the ability to import from any public HTTPS link. I am getting error on import: ; ```; womtool validate wf_test.wdl; Failed to import 'https://github.com/broadinstitute/cromwell/blob/master/engine/src/main/resources/3step.wdl' (reason 1 of 1): Unrecognized token on line 8, column 1:. <!DOCTYPE html>; ^; ```. Using cromwell 78, and running local with version development (also tried with 1.0). Has this feature been disabled? . Running a very simple WDL workflow: ; ```; version development. import ""https://github.com/broadinstitute/cromwell/blob/master/engine/src/main/resources/3step.wdl"" as http_import2; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6788:189,validat,validate,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6788,1,['validat'],['validate']
Security,Based on this report on the forums: https://gatkforums.broadinstitute.org/wdl/discussion/12878/exception-in-thread-main-scala-matcherror-null-validating-my-wdl. In this case the mistake was using `if (is_exome !=) {` instead of `if (!is_exome)` - but that should be nicely turned into a reportable error... rather than throwing up some obtuse scala-internals error message.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4081:142,validat,validating-my-wdl,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4081,1,['validat'],['validating-my-wdl']
Security,Basic validate server endpoint. DSDEEPB-651,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/133:6,validat,validate,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/133,1,['validat'],['validate']
Security,"Batch I am able to run the tests using the suggested edits to aws.conf. I then try to specify the output directories using the following `options.json` file. ```; {; ""final_workflow_outputs_dir"": ""s3://bucket/cromwell/outputs"",; ""final_call_logs_dir"": ""s3:/bucket/cromwell/call_logs"",; ""final_workflow_log_dir"": ""s3://bucket/cromwell/wf_logs""; }; ```. When running cromwell : `java -Dconfig.file=awsbatch/aws.conf -jar cromwell-36.jar run awsbatch/hello.wdl -i awsbatch/hello.inputs -o options.json`. it results with the error:. ```; [2019-01-12 00:31:03,94] [info] $a [866d19d0]: Copying workflow logs from /rcecloud/kmavrommatis/workspace/Workflows/cromwell/cromwell-workflow-logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log to s3://celgene-rnd-riku-researchanalytics/cromwell/wf_logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log; [2019-01-12 00:31:04,03] [error] Key cannot be empty; java.lang.IllegalArgumentException: Key cannot be empty; 	at software.amazon.awssdk.core.util.ValidationUtils.assertStringNotEmpty(ValidationUtils.java:111); 	at software.amazon.awssdk.core.runtime.transform.PathMarshallers$GreedyPathMarshaller.marshall(PathMarshallers.java:109); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:87); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:31); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:88); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.headObject(DefaultS3Client.java:1628); 	at org.lerch.s3fs.util.S3Utils.getS3ObjectSummary(S3Utils.java:47); 	at org.lerch.s3fs.S3FileSystemProvider.exists(S3FileSystemProvider.java:624); 	at org.lerch.s3fs.S3FileSystemProvide",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4541:1037,Validat,ValidationUtils,1037,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4541,1,['Validat'],['ValidationUtils']
Security,Batch call caching hash entries and simpletons,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2227:19,hash,hash,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2227,2,['hash'],['hash']
Security,"Because writing to the call caching store and the job store is not atomic, the following chain of events is possible and not necessarily desirable:. - A job start; - A cache hit is found; - The outputs are copied; - The hashes / simpletons are written to the DB; - ** Cromwell Stops **: This is after the hashes are written successfully but before the EJEA had a chance to write the outputs to the job store and mark the job as complete.; - Cromwell starts; - The workflow is restarted; - The job is not found in the job store; - At this point the EJEA has a state to check if there are hashes existing for this job already. If there is, it disables call caching (so that the EJEA doesn't try to call cache to himself, and that we don't write to the hash store again - which would fail because of the unique index in the call cache table).; - However since we've disabled call caching we then proceed to try and recover the job, which fails because it was never run (since we found a cache hit the first time), and then falls back to running the job for reals. This is not great because this job already has all the outputs it needs, files have been copied already, but we run the job on top of it, which seems to increase the likelihood of having empty files at least locally when trying to read outputs and cause `cannot create an Int from """"` types of failures. Maybe a better way would be to re-use the outputs that have been written to the cache to make the job succeed and bypass all the rest. Relevant code in the EJEA: https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L153",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3074:220,hash,hashes,220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3074,4,['hash'],"['hash', 'hashes']"
Security,"Before cromwell uses a previously cached call it needs to know if the docker image content has changed. Some images such as the [GATK](https://hub.docker.com/r/broadinstitute/gatk/tags/) are so large that they must not be pulled at scale from DockerHub. Instead these images be hosted elsewhere per cloud service provider, including [ECR](https://aws.amazon.com/ecr/). A/C: When call references an image in ECR cromwell should contact ECR to get and record the image hash.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3822:467,hash,hash,467,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3822,1,['hash'],['hash']
Security,Better error message for string member accesses,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3867:39,access,accesses,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3867,1,['access'],['accesses']
Security,Brand new version of Cromwell authentication process to JES.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/225:30,authenticat,authentication,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/225,1,['authenticat'],['authentication']
Security,Bring in validation messages from WDL4S. Closes #2211,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2214:9,validat,validation,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2214,1,['validat'],['validation']
Security,Bringing validation code to ValidateActor. Closes #488,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/522:9,validat,validation,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/522,2,"['Validat', 'validat']","['ValidateActor', 'validation']"
Security,"Bumps [jackson-databind](https://github.com/FasterXML/jackson) from 2.11.1 to 2.12.6.1.; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/FasterXML/jackson/commits"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.fasterxml.jackson.core:jackson-databind&package-manager=maven&previous-version=2.11.1&new-version=2.12.6.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6743:534,secur,security-vulnerabilities,534,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6743,2,['secur'],"['security-updates', 'security-vulnerabilities']"
Security,"Bumps [jackson-databind](https://github.com/FasterXML/jackson) from 2.13.2.2 to 2.13.4.1.; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/FasterXML/jackson/commits"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.fasterxml.jackson.core:jackson-databind&package-manager=maven&previous-version=2.13.2.2&new-version=2.13.4.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6935:538,secur,security-vulnerabilities,538,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6935,2,['secur'],"['security-updates', 'security-vulnerabilities']"
Security,"Bumps [jackson-databind](https://github.com/FasterXML/jackson) from 2.13.4.1 to 2.13.4.2.; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/FasterXML/jackson/commits"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.fasterxml.jackson.core:jackson-databind&package-manager=maven&previous-version=2.13.4.1&new-version=2.13.4.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). You can trigger a rebase of this PR by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7110:538,secur,security-vulnerabilities,538,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7110,2,['secur'],"['security-updates', 'security-vulnerabilities']"
Security,"Bumps [junit](https://github.com/junit-team/junit4) from 4.13 to 4.13.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/junit-team/junit4/releases"">junit's releases</a>.</em></p>; <blockquote>; <h2>JUnit 4.13.1</h2>; <p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.13.1.md"">release notes</a> for details.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/junit-team/junit4/blob/main/doc/ReleaseNotes4.13.1.md"">junit's changelog</a>.</em></p>; <blockquote>; <h2>Summary of changes in version 4.13.1</h2>; <h1>Rules</h1>; <h3>Security fix: <code>TemporaryFolder</code> now limits access to temporary folders on Java 1.7 or later</h3>; <p>A local information disclosure vulnerability in <code>TemporaryFolder</code> has been fixed. See the published <a href=""https://github.com/junit-team/junit4/security/advisories/GHSA-269g-pwp5-87pp"">security advisory</a> for details.</p>; <h1>Test Runners</h1>; <h3>[Pull request <a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1669"">#1669</a>:](<a href=""https://github-redirect.dependabot.com/junit-team/junit/pull/1669"">junit-team/junit#1669</a>) Make <code>FrameworkField</code> constructor public</h3>; <p>Prior to this change, custom runners could make <code>FrameworkMethod</code> instances, but not <code>FrameworkField</code> instances. This small change allows for both now, because <code>FrameworkField</code>'s constructor has been promoted from package-private to public.</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/junit-team/junit4/commit/1b683f4ec07bcfa40149f086d32240f805487e66""><code>1b683f4</code></a> [maven-release-plugin] prepare release r4.13.1</li>; <li><a href=""https://github.com/junit-team/junit4/commit/ce6ce3aadc070db2902698fe0d3dc6729cd631f2""><code>ce6ce3a</code></a> Draft 4.13.1 ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5941:690,Secur,Security,690,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5941,4,"['Secur', 'access', 'secur']","['Security', 'access', 'security']"
Security,"Bumps log4j-api from 2.13.3 to 2.15.0. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.apache.logging.log4j:log4j-api&package-manager=maven&previous-version=2.13.3&new-version=2.15.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6586:302,secur,security-vulnerabilities,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6586,2,['secur'],"['security-updates', 'security-vulnerabilities']"
Security,"Bumps log4j-api from 2.13.3 to 2.16.0. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.apache.logging.log4j:log4j-api&package-manager=maven&previous-version=2.13.3&new-version=2.16.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6591:302,secur,security-vulnerabilities,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6591,2,['secur'],"['security-updates', 'security-vulnerabilities']"
Security,"Bumps log4j-api from 2.16.0 to 2.17.0. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.apache.logging.log4j:log4j-api&package-manager=maven&previous-version=2.16.0&new-version=2.17.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6594:302,secur,security-vulnerabilities,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6594,2,['secur'],"['security-updates', 'security-vulnerabilities']"
Security,"Bumps log4j-api from 2.17.0 to 2.17.1. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.apache.logging.log4j:log4j-api&package-manager=maven&previous-version=2.17.0&new-version=2.17.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6639:302,secur,security-vulnerabilities,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6639,2,['secur'],"['security-updates', 'security-vulnerabilities']"
Security,"Bumps log4j-core from 2.13.3 to 2.15.0. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.apache.logging.log4j:log4j-core&package-manager=maven&previous-version=2.13.3&new-version=2.15.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6587:304,secur,security-vulnerabilities,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6587,2,['secur'],"['security-updates', 'security-vulnerabilities']"
Security,"Bumps log4j-core from 2.13.3 to 2.16.0. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.apache.logging.log4j:log4j-core&package-manager=maven&previous-version=2.13.3&new-version=2.16.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6592:304,secur,security-vulnerabilities,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6592,2,['secur'],"['security-updates', 'security-vulnerabilities']"
Security,"Bumps log4j-core from 2.16.0 to 2.17.0. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.apache.logging.log4j:log4j-core&package-manager=maven&previous-version=2.16.0&new-version=2.17.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6595:304,secur,security-vulnerabilities,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6595,2,['secur'],"['security-updates', 'security-vulnerabilities']"
Security,"Bumps log4j-core from 2.17.0 to 2.17.1. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.apache.logging.log4j:log4j-core&package-manager=maven&previous-version=2.17.0&new-version=2.17.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6640:304,secur,security-vulnerabilities,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6640,2,['secur'],"['security-updates', 'security-vulnerabilities']"
Security,"CFCD208495D565EF66E7DFF9F98764DA"",; ""failOnStderr"": ""68934A3E9455FA72420237EB05902327""; },; ""output expression"": {; ""File output_greeting"": ""DFC652723D8EBD4BB25CAC21431BB6C0""; },; ""input count"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""backend name"": ""2A2AB400D355AC301859E4ABB5432138"",; ""command template"": ""AFAC58B849BD67585A857F538B8E92F6""; },; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hit"": false,; ""result"": ""Cache Miss""; },; ```. ```; # simple sge apptainer conf (modified from the slurm one); #; workflow-options; {; workflow-log-dir: ""cromwell-workflow-logs""; workflow-log-temporary: false; workflow-failure-mode: ""ContinueWhilePossible""; default; {; workflow-type: WDL; workflow-type-version: ""draft-2""; }; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; metadata {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql:<dburl>?rewriteBatchedStatements=true""; driver = ""com.mysql.cj.jdbc.Driver""; user = ""<user>""; password = ""<pass>"" ; connectionTimeout = 5000; }; }; }. call-caching; {; enabled = true; invalidate-bad-cache-result = true; }. docker {; hash-lookup {; enabled = true; }; }. backend {; default = sge; providers {. ; sge {; 	actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; #concurrent-job-limit = 5. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. runtime-attributes = """"""; String time = ""11:00:00""; Int cpu = 4; Float? memory_gb; String sge_queue = ""hammer.q""; String? sge_project; String? docker; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:2029,password,password,2029,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,1,['password'],['password']
Security,CROM-6872 Temporarily disable ssh access test,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6728:34,access,access,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6728,1,['access'],['access']
Security,CWL input file format validation. Closes #3569,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3567:22,validat,validation,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3567,1,['validat'],['validation']
Security,CWL static type validation,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3046:16,validat,validation,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3046,1,['validat'],['validation']
Security,CWL: Validate input input files against OWL/RDF,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3569:5,Validat,Validate,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3569,1,['Validat'],['Validate']
Security,Call cache capoeira for 1.0 (plus ensure task hashes match draft-2),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3653:46,hash,hashes,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3653,1,['hash'],['hashes']
Security,Call caching database access wired through to engine. Closes #1224,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1268:22,access,access,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1268,1,['access'],['access']
Security,Call caching hashes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1307:13,hash,hashes,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1307,1,['hash'],['hashes']
Security,Call caching hashes (complete!),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1303:13,hash,hashes,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1303,1,['hash'],['hashes']
Security,Call caching hashing rebased on Call Cache reading,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1291:13,hash,hashing,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1291,1,['hash'],['hashing']
Security,Call caching hashing strategies,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1487:13,hash,hashing,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1487,1,['hash'],['hashing']
Security,Call caching isn't working in V25 with docker hashes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2229:46,hash,hashes,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229,1,['hash'],['hashes']
Security,"Call caching works sometimes for me but not all the time. I find it especially strange when working on a scatter job and some of the scatter jobs get a cache hit but others get a cache miss. . I have queried the METADATA_ENTRY table for the two workflows and all the call cache entries look identical. . Here is my process:. 1. I queried METADATA_ENTRY with this WHERE condition: `(WORKFLOW_EXECUTION_UUID ='29791b64-b47a-44ba-aff0-7ab48bc10677' or WORKFLOW_EXECUTION_UUID ='5de042e3-7a03-4c77-8972-f0e4cd010e4b') and CALL_FQN = 'sampleLevelWorkflow_WGS.align' and JOB_SCATTER_INDEX =0`; 2. I sort by METADATA_KEY; 3. Then I go down the list and compare the hashes for the two workflows for each METADATA_KEY. Here is a case where workflow 29791b64 is a restart of 5de042e3. (Workflow 5de042e3 is itself a restart but I don't think that is important here.) I have shown below all the records from METADATA_ENTRY that start with ""callCaching"" and they all look identical, yet it clearly says it is a ""Cache Miss"". **Is there anywhere I can see a log message stating exactly which hashes resulted in the cache miss?** I have tried to enable LOG_LEVEL=DEBUG but couldn't see it there. Thanks in advance for your help!. |WORKFLOW_EXECUTION_UUID|METADATA_KEY|METADATA_VALUE|; |-----------------------|------------|--------------|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:result|Cache Miss|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:result|Cache Miss|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hit|false|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hit|false|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:failOnStderr|68934A3E9455FA72420237EB05902327|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:runtime attribute:failOnStderr|68934A3E9455FA72420237EB05902327|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:docker|4AD3C387725244C1348F252B031B956D|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCachin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:658,hash,hashes,658,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,"Call input validation expects LHS of MemberAccess in the input mappings to be Calls, but now they can be scatter items too if the item is a pair.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2692:11,validat,validation,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2692,1,['validat'],['validation']
Security,Catch sub-workflow task input passing from inputs.json at validation time.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3640:58,validat,validation,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3640,1,['validat'],['validation']
Security,Centaur reference image test should validate symlinks [VS-796],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6996:36,validat,validate,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6996,1,['validat'],['validate']
Security,Centaur tests poorly assess the format of output files in the metadata. To avoid regressions it would be preferable to have better coverage of this.; The main issue comes from the fact that files path are dynamic and hard to validate with the static test definitions centaur has.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3160:225,validat,validate,225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3160,1,['validat'],['validate']
Security,Centaur to assert output hashes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4904:25,hash,hashes,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4904,1,['hash'],['hashes']
Security,"Certain error messages that Cromwell receives are longer than the default limit, which is a big pain when debugging. Going from 64 to 1024 characters (1kb) doesn't seem unreasonable and solves this issue. For context, the error message below is 364 characters. . [Relevant Akka Doc](https://doc.akka.io/docs/akka-http/10.0/configuration.html). . Before:; ```; 2024-04-12 14:58:18 cromwell-system-akka.actor.default-dispatcher-26 ERROR - Error in stage [akka.http.impl.engine.client.OutgoingConnectionBlueprint$PrepareResponse@71a2a20e]: Response reason phrase exceeds the configured limit of 64 characters; akka.http.scaladsl.model.IllegalResponseException: Response reason phrase exceeds the configured limit of 64 characters; 	at akka.http.impl.engine.client.OutgoingConnectionBlueprint$PrepareResponse$$anon$3.onPush(OutgoingConnectionBlueprint.scala:191); 	at akka.stream.impl.fusing.GraphInterpreter.processPush(GraphInterpreter.scala:523); 	at akka.stream.impl.fusing.GraphInterpreter.execute(GraphInterpreter.scala:409); 	at akka.stream.impl.fusing.GraphInterpreterShell.runBatch(ActorGraphInterpreter.scala:606); 	at akka.stream.impl.fusing.ActorGraphInterpreter$SimpleBoundaryEvent.execute(ActorGraphInterpreter.scala:47); 	at akka.stream.impl.fusing.ActorGraphInterpreter$SimpleBoundaryEvent.execute$(ActorGraphInterpreter.scala:43); 	at akka.stream.impl.fusing.ActorGraphInterpreter$BatchingActorInputBoundary$OnNext.execute(ActorGraphInterpreter.scala:85); 	at akka.stream.impl.fusing.GraphInterpreterShell.processEvent(ActorGraphInterpreter.scala:581); 	at ; ...; ```. After: ; ```; <!DOCTYPE HTML PUBLIC ""-//IETF//DTD HTML 2.0//EN"">; <html><head>; <title>401 Unauthorized</title>; </head><body>; <h1>Unauthorized</h1>; <p>This server could not verify that you; are authorized to access the document; requested. Either you supplied the wrong; credentials (e.g., bad password), or your; browser doesn't understand how to supply; the credentials required.</p>; </body></html>; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7406:1779,authoriz,authorized,1779,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7406,3,"['access', 'authoriz', 'password']","['access', 'authorized', 'password']"
Security,"Certain heterogeneous maps validate in Womtool, fail in Cromwell",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4512:27,validat,validate,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4512,1,['validat'],['validate']
Security,"Changes are related only to the Shadow world. The expectations of this PR is to extend the current state of things in Workflow Execution (i.e currently we only run a single call workflow) to allow arbitrarily sized workflows (i.e. an N-call workflow). The intention is _not_ to support scatters in this PR, but allow it to be extensible for scatters (or Inception-esque nested scatters, which I hope to take up as my next ticket). ~~The original WA used Data Access and symbol store to pass around information between tasks. I am not quite sure how that would work with the shadow world, also considering we don't (yet) have engine functions at that level. So I have used a little different algorithm to orchestrate the calls in a workflow (preparing a small call graph and sorting that graph to obtain the logical ordering among tasks, and then orchestrate that via information in the FSM state data).~~. ~~I might wait for @Horneth for getting the engine functions in and have thoughts from you guys on plugging in outputs of a task to it's dependent task.~~. ~~I talked with Thibault about this and I honestly don't mind if this PR does't get merged at all if we see a problem with this, just need a fresh pair of eyes to look through it.~~. ~~Currently, I'm adding tests for all this new code.~~",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/743:459,Access,Access,459,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/743,1,['Access'],['Access']
Security,Changes to proxy compose settings to support LDAP Authorization.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/359:50,Authoriz,Authorization,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/359,1,['Authoriz'],['Authorization']
Security,Chaos monkey Cromwell. Put up a Firewall and deny access to Cromwell. Tear down. See what happened. Block ports. Tables. ; Putting up a block. . The real Chaos Monkey. Much hectic. Simian army. . Henry knows how to spin up a Cromwell test environment. Much magic.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2113:32,Firewall,Firewall,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2113,2,"['Firewall', 'access']","['Firewall', 'access']"
Security,Check that Cromwell has proper access rights to read Carbonited metadata from Carboniter's GCS bucket [BA-6323],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5459:31,access,access,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5459,1,['access'],['access']
Security,Checksum S3 signed URL downloads during localization [BT-257],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6485:0,Checksum,Checksum,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6485,1,['Checksum'],['Checksum']
Security,Cleanup documentation. Remove references to lifesciences and update docker authentication.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7196:75,authenticat,authentication,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7196,1,['authenticat'],['authentication']
Security,Client no longer always tries to inject refresh token.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2516:33,inject,inject,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2516,1,['inject'],['inject']
Security,"Closes #3195 . - Mostly cut and paste from `MaterializeWorkflowDescriptorActor` to the two new language projects.; - `MaterializeWorkflowDescriptorActor` now gets its language support from one of the plugged in languages and `engine` is not allowed to access language classes directly (except that it still can via `databaseMigration` but pretend you don't know that); - Doesn't do anything to split WDL into two versions. That can come later...!; - If no version is specified, we just pick the first version from the list. I suspect we'll actually want some kind of a ""decider"" function based on the file contents but walking before running...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3211:252,access,access,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3211,1,['access'],['access']
Security,"Closes #3297. If you unzip a zip file containing a directory, don't assume the user wants to reference things from within that directory. Especially don't ignore the fact that there might be *other* directories in the zip file as well, which now cannot be accessed at all",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3434:256,access,accessed,256,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3434,1,['access'],['accessed']
Security,"Closes #3811 . Use a hint to the resolver to ignore the local scope of a `WdlTaskCall` and start the member access search in the next higher scope. ---. Deleted earlier comment about allowing search only in earlier lines within the same scope because it wouldn't help with; ```; call y as shouldntBeProblematic {; input:; cram = ""asdf"",; slam = cram.scram; }; ```; which is equally as problematic as; ```; call y as shouldntBeProblematic {; input:; cram = cram.scram; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4095:108,access,access,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4095,1,['access'],['access']
Security,Closes #4433 . The changes to `MaterializeWorkflowDescriptorActor.scala` and `CromwellApiService.scala` are basically just refactoring out reusable code into a shared place. It is a known issue (#4119) that Womtool cannot validate CWL; this limitation applies to the endpoint as well.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4467:222,validat,validate,222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4467,1,['validat'],['validate']
Security,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run does-not-exist.wdl; ```; Output:; ```; [2018-08-30 17:36:02,67] [info] Running with database db.url = jdbc:hsqldb:mem:6713284f-67ff-4eb9-9fd6-3fde0a4cc0ce;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:10,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:36:10,87] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:36:11,02] [info] Running with database db.url = jdbc:hsqldb:mem:5893545c-e081-4c3d-827d-000af3765fc4;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:11,72] [info] Slf4jLogger started; Exception in thread ""main"" cromwell.CromwellEntryPoint$$anon$1: ERROR: Unable to submit workflow to Cromwell::; Workflow source does not exist: does-not-exist.wdl; 	at cromwell.CromwellEntryPoint$.$anonfun$validOrFailSubmission$1(CromwellEntryPoint.scala:219); 	at cats.data.Validated.valueOr(Validated.scala:48); 	at cromwell.CromwellEntryPoint$.validOrFailSubmission(CromwellEntryPoint.scala:219); 	at cromwell.CromwellEntryPoint$.validateRunArguments(CromwellEntryPoint.scala:215); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:56); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ```; Command-line tools are subject to usability standards identical to those of our oth",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4060:938,Validat,Validated,938,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4060,2,['Validat'],['Validated']
Security,Configure looking up of docker hash,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/336:31,hash,hash,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/336,1,['hash'],['hash']
Security,"Consider the following WDL using the [if then else](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#if-then-else) construct:; ```; version 1.0. workflow main {; call main {; input:; x = 1; }; }. task main {; input {; Int x; }. command <<<; echo ~{if x == 1 then 1 else 0}; >>>; }; ```; when I parse it:; ```; $ java -jar womtool-52.jar validate main.wdl ; ERROR: Unexpected symbol (line 20, col 15) when parsing 'e'. Expected then, got """". echo ~{if x == 1 then 1 else 0}; ^. $e = :if $e :then $e :else $e -> TernaryIf( cond=$1, iftrue=$3, iffalse=$5 ); ```. The following equivalent WDL instead:; ```; version 1.0. workflow main {; call main {; input:; x = 1; }; }. task main {; input {; Int x; }. Int y	= if x == 1 then 1 else 0; command <<<; echo ~{y}; >>>; }; ```; when I parse it:; ```; $ java -jar womtool-52.jar validate main.wdl ; Success!; ```. Similarly this equivalent WDL:; ```; version 1.0. workflow main {; call main {; input:; x = 1; }; }. task main {; input {; Int x; }. command <<<; echo ~{if !(x != 1) then 1 else 0}; >>>; }; ```; when I parse it:; ```; $ java -jar womtool-52.jar validate main.wdl ; Success!; ```; It seems like the parser does not accept the `==` operator in the condition of the `TernaryIf` for some reasons, but only in the case it is included in a `command <<< >>>` section.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5602:350,validat,validate,350,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602,3,['validat'],['validate']
Security,Consistently resolve Docker labels to hashes within a workflow,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2094:38,hash,hashes,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2094,1,['hash'],['hashes']
Security,"Copying chat log below:. ---. dshiga [4:46 PM]; hi @kshakir, we're seeing a lot of workflows that have been stuck in submitted status for the past ~45 minutes. is something wrong in caas?; for example this workflow is stuck: 8de76a93-6b66-4c29-a2fe-31e6cd1f969e. kshakir [4:48 PM]; ```; Workflow 8de76a93-6b66-4c29-a2fe-31e6cd1f969e in state Running and restarted = false cannot be started and should not have been fetched.; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.engine.workflow.workflowstore.SqlWorkflowStore.$anonfun$fetchStartableWorkflows$1(SqlWorkflowStore.scala:57); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 P",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3673:435,validat,validation,435,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673,8,"['Validat', 'validat']","['Validation', 'ValidationTry', 'validation']"
Security,Crc32c validation for archive uploads [BW-626],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6303:7,validat,validation,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6303,1,['validat'],['validation']
Security,Create a github account with push access that can be used for the release,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2402:34,access,access,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2402,1,['access'],['access']
Security,"Create a web service in a similar vein to CromIAM which will act as a passthrough to an underlying Cromwell server. This server will expose the [TES](https://github.com/ga4gh/task-execution-schemas) API and interact with Cromwell presumably by creating a single task workflow out of the what was passed in. The ultimate goal of this project is not to have some whizbang standalone server but to use this as an opportunity to identify what the translation path from submitting and interacting with a single task via TES looks like in the Cromwell world, instead of diving in and grafting TES directly into Cromwell's front end. . DoD: A completed server (language, etc is dealer's choice) which we can use to guide another stage of development to directly support TES as part of Cromwell's front end API *or* a document detailing why this can't happen.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2202:133,expose,expose,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2202,1,['expose'],['expose']
Security,"Create an actor, for use by the EJHA, which can dive into the DB (#1224) and retrieve the list of all CCRIDs which match a given (HashKey, HashResult) combination.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1232:130,Hash,HashKey,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1232,2,['Hash'],"['HashKey', 'HashResult']"
Security,"Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1290:8,hash,hashes,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1290,7,['hash'],"['hash', 'hash-docker-names', 'hash-file-contents', 'hash-file-paths', 'hashes', 'hashing']"
Security,Creating WorkflowDescriptor after validation in WorkflowManagerActor. Closes #544.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/582:34,validat,validation,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/582,1,['validat'],['validation']
Security,CromIAM check user authorization before forwarding request to Cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4348:19,authoriz,authorization,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4348,1,['authoriz'],['authorization']
Security,CromIAM will now allow access to the stats endpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2137:23,access,access,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2137,1,['access'],['access']
Security,"Cromwell appears to be spending a lot of time calculating MD5 hashes, even though the file strategy is path.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1597:62,hash,hashes,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597,1,['hash'],['hashes']
Security,Cromwell can fallback to configured default runtime attributes. The current implementation leverages the fact that `WdlExpression` extends `WdlValue`. This is not true anymore with WomExpressions. Find a way to fix this preferably without having WomExpression extend WdlValue.; This breaks runtime attribute validation in the `BackendWorkflowInitializationActor` and default runtime attributes.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2606:308,validat,validation,308,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2606,1,['validat'],['validation']
Security,"Cromwell cannot handle any output that is struct base. I enclose a workflow where it is very clear to see that even though both latest development versions (as well as latest releases) of Cromwell and Womtool validated and executed the workflow for some strange reason at runtime Cromwell consider that it is a Map and not a struct and crashes in the very end of execution; ```; QuantifiedRun quantified_run = {""run"": srr, ""folder"": quant_folder, ""quant"": quant, ""lib"": quant_lib}; ```; ![screenshot_2019-02-15 screenshot](https://user-images.githubusercontent.com/842436/52889800-4dade280-318a-11e9-87b9-8b364e3408dd.png); [crashes_at_runtime.zip](https://github.com/broadinstitute/cromwell/files/2871310/crashes_at_runtime.zip)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4663:209,validat,validated,209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4663,1,['validat'],['validated']
Security,Cromwell engine DRS checksum [BT-389],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6683:20,checksum,checksum,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6683,1,['checksum'],['checksum']
Security,"Cromwell is localizing inputs to unconventionally named directories. Specifically directories starting with a '-'. - is conventionally used in program arguments to mean 'not a path, this is an option'. This is a pain. Previously cromwell was localising with a full path, but now seems to be using some kind of hash prefixed with an '-'. Maybe you meant to use an unsigned int?. ```; ls cromwell-executions/PreProcessingForVariantDiscovery_GATK4/51a9f551-defa-4e20-8573-d55d25622248/call-SamToFastqAndBwaMem/inputs/; -1232659437/ -21323395/ -941963188/; cd cromwell-executions/PreProcessingForVariantDiscovery_GATK4/51a9f551-defa-4e20-8573-d55d25622248/call-SamToFastqAndBwaMem/inputs/; cd -941963188; cd: Unknown option “-941963188”; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3824:310,hash,hash,310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3824,1,['hash'],['hash']
Security,Cromwell metadata servers in FC prod have been brought down by repeated requests for the same workflow metadata before a response has been returned for the first request. It should be fairly easy to implement a requesters map that would allow Cromwell to only assemble metadata once and respond to all requests with that same metadata. The implementation here could be very similar to (though simpler than) the [file hash caching](https://github.com/broadinstitute/cromwell/pull/4143/files) system.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4226:417,hash,hash,417,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4226,1,['hash'],['hash']
Security,"Cromwell piece of hashing, closes #490",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/510:18,hash,hashing,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/510,1,['hash'],['hashing']
Security,"Cromwell seems to erroneously treat private/internal variables with File type as input files and attempts to localize them:. ```; version 1.0. task mytask {; 	input {; 		String mystr = ""Hello World!""; 	}. 	File myfile = ""myfile"". 	command <<<; 		echo ~{mystr} > ~{myfile}; 	>>>. 	output {; 		String file_contents = read_string(myfile); 	}; }. workflow wf {; 	call mytask; }; ```. ```; Could not localize myfile -> /home/jared/projects/gambit/data/misc/211031-apollo_illumina_pe-miniwdl/test-cromwell/cromwell-executions/wf/51d2f863-0e91-45b6-9e7b-f2365c259144/call-mytask/inputs/1979661608/myfile:; 	myfile doesn't exist; 	File not found /home/jared/projects/gambit/data/misc/211031-apollo_illumina_pe-miniwdl/test-cromwell/cromwell-executions/wf/51d2f863-0e91-45b6-9e7b-f2365c259144/call-mytask/inputs/1979661608/myfile -> /home/jared/projects/gambit/data/misc/211031-apollo_illumina_pe-miniwdl/test-cromwell/myfile; 	File not found myfile; 	File not found /home/jared/projects/gambit/data/misc/211031-apollo_illumina_pe-miniwdl/test-cromwell/myfile; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:94); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:90); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:669); 	... 35 common frames omitted; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6562:1063,validat,validation,1063,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6562,8,"['Validat', 'validat']","['Validation', 'ValidationTry', 'validation']"
Security,Cromwell should validate output of a task before running it,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2226:16,validat,validate,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2226,1,['validat'],['validate']
Security,"Cromwell will need to provide an endpoint called status which will return a 200 with other information on subsystems. For examples of how this was implemented in Rawls, see their [StatusService](https://github.com/broadinstitute/rawls/blob/eccaae155ef7a7bba7cbd7403405beb155a723a9/core/src/main/scala/org/broadinstitute/dsde/rawls/status/StatusService.scala) and [HealthMonitor](https://github.com/broadinstitute/rawls/blob/eccaae155ef7a7bba7cbd7403405beb155a723a9/core/src/main/scala/org/broadinstitute/dsde/rawls/monitor/HealthMonitor.scala). Subsystems to monitor include the CloudSQL database, Dockerhub, GCS and PAPI. If you can think of something else which seems useful feel free to add it in. This endpoint should also be exposed via CromIAM w/o needing authZ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2432:730,expose,exposed,730,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2432,1,['expose'],['exposed']
Security,Cromwell with aws s3: Access Denied,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4677:22,Access,Access,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4677,1,['Access'],['Access']
Security,"Cromwell won't validate you, despite its promises.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1427:15,validat,validate,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1427,1,['validat'],['validate']
Security,"Cromwell: Development (`37-3b2affa`); Backend: HPC (`ConfigBackendLifecycleActorFactory`). I wanted access to a recently merged pull request (#4437), so I built a development version of Cromwell. However, when I run it with the same configuration file as I used for Cromwell 36, I get this error:; ```; [ERROR] [01/24/2019 11:10:24.126] [cromwell-system-akka.actor.default-dispatcher-4] [akka://cromwell-system/user/SingleWorkflowRunnerActor] No configuration setting found for key 'services' ; akka.actor.ActorInitializationException: akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577:100,access,access,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577,1,['access'],['access']
Security,CromwellApiHandler now uses a ValidateActor to validate a WF received via the API endpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548:30,Validat,ValidateActor,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548,2,"['Validat', 'validat']","['ValidateActor', 'validate']"
Security,"CromwellApiService issues a `ValidateWorkflowId` to the metadata service, breaking our rule of not reading from metadata internally. This is problematic if e.g. one is using a write-only metadata implementation.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3104:29,Validat,ValidateWorkflowId,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3104,1,['Validat'],['ValidateWorkflowId']
Security,"Crossposting here as I was not able to get an answer on WDL forum. Post is here:; https://gatkforums.broadinstitute.org/wdl/discussion/13540/unable-to-do-docker-lookup#latest. Trying to get hello world working on AWS Batch and cromwell. I am able to spin up the servers however it fails in pulling the docker image with the following error message in the cromwell logs:. `; 2018-10-30 15:43:14,845 cromwell-system-akka.dispatchers.engine-dispatcher-9 WARN - BackendPreparationActor_for_7d0c30ad:wf_hello.hello:-1:1 [UUID(7d0c30ad)]: Docker lookup failed java.lang.Exception: Failed to get docker hash for ubuntu:latest Docker hash lookup failed with code 503. The server is currently unavailable (because it is overloaded or down for maintenance). at cromwell.engine.workflow.WorkflowDockerLookupActor.cromwell$engine$workflow$WorkflowDockerLookupActor$$handleLookupFailure(WorkflowDockerLookupActor.scala:188); `. Here is my wdl:. ```; task hello {; String addressee; command {; echo ""Hello ${addressee}! Welcome to Cromwell on AWS""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. This is originally from tutorial by @wleepang found here:; https://www.youtube.com/watch?v=jcC3pz_K4gI. Any idea on how to diagnose? I was not able to find a similar issue. Thanks so much for the help.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4345:596,hash,hash,596,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4345,2,['hash'],['hash']
Security,"Currently a new job definition is created per job submitted to AWS Batch. There should be built into AwsBatchJob.scala a mechanism to reuse definitions. One thought here is to perform a hash of the parameters used to create a job definition, then setting the job definition name to that hash. It is then relatively easy to perform a describeJobDefinitions call against aws batch to look for that name, and create it if it does not exist.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3750:186,hash,hash,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3750,2,['hash'],['hash']
Security,"Currently it barfs on aliased call commands yielding the following error:; ```; Exception in thread ""main"" wdl4s.parser.WdlParser$SyntaxError: ERROR: Expression references output on call that doesn't exist (line 1572, col 5):. ValidateCram.*; ^; ; 	at wdl4s.Workflow.$anonfun$expandedWildcardOutputs$5(Workflow.scala:166); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:241); 	at scala.collection.immutable.List.foreach(List.scala:378); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:241); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:238); 	at scala.collection.immutable.List.flatMap(List.scala:341); ```. FYI: `ValidateCram` was called as such in the file:; ```; call ValidateSamFile as ValidateCram {; input:; input_bam = ConvertToCram.output_cram,; input_bam_index = ConvertToCram.output_cram_index,; report_filename = sample_name + "".cram.validation_report"",; .....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2125:227,Validat,ValidateCram,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2125,4,['Validat'],"['ValidateCram', 'ValidateSamFile']"
Security,Currently running as `cromwell-cron-papiv2-alpha1` job 730 for validation,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5750:63,validat,validation,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5750,1,['validat'],['validation']
Security,Currently the endpoints not off of `/api` (e.g. `/engine/Segment/version`) still require an authorization header. This should not be the case. These endpoints should be fully public.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2766:92,authoriz,authorization,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2766,1,['authoriz'],['authorization']
Security,"Currently we have the ability to turn caching on/off at a cromwell level, or a workflow level, but sometimes there are specific tasks in a workflow that should either participate or not in call caching. Similar to the workflow options, a task-level runtime set of options should exist to expose this behavior:; write_to_cache: don't add the results of this call to the cache; read_from_cache: don't attempt to use the cache for this call. Requested by Lee as they migrate off Queue",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1695:288,expose,expose,288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695,1,['expose'],['expose']
Security,"Currently when I ask for metadata or for logs, I get something like:; ```; {; ""calls"": {; ""annotate_de_novo.transdecoder"": [; {; ""stderr"": ""/pipelines/cromwell_1/cromwell-executions/annotate_de_novo/e64a866e-d5c1-4779-8f9d-75457f22f43e/call-transdecoder/execution/stderr"",; ""stdout"": ""/pipelines/cromwell_1/cromwell-executions/annotate_de_novo/e64a866e-d5c1-4779-8f9d-75457f22f43e/call-transdecoder/execution/stdout"",; ""attempt"": 1,; ""shardIndex"": -1; }; ]; },; ""id"": ""e64a866e-d5c1-4779-8f9d-75457f22f43e""; }; ```; All the details about what really happened (stderr and stdout of the tools that failed) are not accessible via REST API and I have to ssh to the server to read those files. What can be useful is to have a way to get stdout/stderr content for each call with Cromwell REST API, that can save a lot of time on ssh-ing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2928:612,access,accessible,612,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2928,1,['access'],['accessible']
Security,"Currently when an external user attempts to access our cloud accounts, Travis does not hand out the encryption keys. This is desired behavior, as we do not want to run up bills on unknown code from outside users. However, currently our test scripts are not checking the variable [`TRAVIS_SECURE_ENV_VARS`](https://docs.travis-ci.com/user/environment-variables/#Default-Environment-Variables), and are instead trying to access encrypted variables that are not and never will be present. This leads to false negatives where the tests are marked as failed, when instead they should best case be marked as ignored, worst case be marked as implicitly passed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1717:44,access,access,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1717,4,"['access', 'encrypt']","['access', 'encrypted', 'encryption']"
Security,"Currently, the file systems available to the engine for functions like read_\* are statically defined. GCS, Local, etc. This issue is to make that driven by the config file. The reason this is important is because if you are running a cromwell server and can not disable the ""Local Shared Filesystem"" from the engine... someone could write a WDL that does a read_ on any file that the cromwell server has access to (e.g. read_lines(""./cromwell.conf"")... which is bad",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/821:405,access,access,405,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/821,1,['access'],['access']
Security,"D OTHER SENSITIVE MATERIAL: -->; I am running Cromwell on GCP, launching a workflow that shards into ~5,000 pieces. I am getting the following error: `cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out`. ```; 2019-04-29 00:02:13,419 cromwell-system-akka.dispatchers.backend-dispatcher-139 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(95b34a77)vcf2bigquery.convertVCF:2058:1]: Status chang; e from Running to Success; 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:171); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:735); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1587); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:1770,secur,security,1770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['secur'],['security']
Security,DO NOT MERGE: example code... seems to work for access tokens initially... don't ha…,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/197:48,access,access,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/197,1,['access'],['access']
Security,DSDEEPB-1178 Add validate to the Cromwell routes.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/166:17,validat,validate,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/166,1,['validat'],['validate']
Security,Data access index aware for scatter-gather,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/156:5,access,access,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/156,1,['access'],['access']
Security,Data access via DRS URIs frequently fails in Ammonite script,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069:5,access,access,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069,1,['access'],['access']
Security,"Dear Cromwell dev team,. This is an enhancement suggestion. . When using the google backend for resources allocation, one can specify `gpuCount` and `gpuType` to request for specific resources. I am currently trying to design a task that optionally needs to access a GPU (function of input/parameters). I tried different approach to dynamically schedule GPUs, but `gpuCount` seems constrain to a non-null positive integer. https://github.com/broadinstitute/cromwell/blob/bfef756ca35b46570dff3fda57f77dd4b2b0d25c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L190 . https://github.com/broadinstitute/cromwell/blob/bfef756ca35b46570dff3fda57f77dd4b2b0d25c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/GpuValidation.scala#L28-L40. To allow for dynamic access to GPUs, I propose to extend `gpuCount` type to allow for a null value, and to check for a non-null value for resource allocation. https://github.com/broadinstitute/cromwell/blob/bfef756ca35b46570dff3fda57f77dd4b2b0d25c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L193. Please let me know if such a feature is not desired for any reason. . \* I tried accessing the Jira tracker but `doesn't have access to Jira on broadworkbench.atlassian.net.`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6679:258,access,access,258,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679,4,['access'],"['access', 'accessing']"
Security,"Dear developers,. During testing I ran into the problem that the `HashPathStrategy` does not include the last modified date of the file. It assumes: ""if the path is there, it is the same file"". This is not necessarily the case. Files can be modified or replaced.Therefore the current `HashPathStrategy` is a big liability when trying to get reproducible results. By adding a ""last modified date"" to the `HashPathStrategy` this will ensure that nothing has happened to the file from the user or system side. This of course is not as safe as the `HashFileStrategy` since it does not protect against filesystem or hardware errors, but it provides a lot more safety compared to the current `HashPathStrategy`. ; This is also how Snakemake checks if files are the same and it works quite well. Alternatively there could be an option in the Configfile that allows you to set this behaviour. Please let me know what you think of this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4405:66,Hash,HashPathStrategy,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4405,5,['Hash'],"['HashFileStrategy', 'HashPathStrategy']"
Security,"Define a `WomExpression` `trait` or `abstract class`; It should have an abstract `evaluate` method taking WOM I/O functions as well as some kind of ""variable context"" containing values for the variable referenced in the expression (likely a `Map[String, WomValue]` of some sort).The values should be accurate. E.g: correct shard index if inside the same scatter, array of all shards if outside etc... It should also expose a Set of variables referenced in the expression. Other methods might reveal themselves useful / needed as this is built. WDL and CWL will implement this abstract class and provide their own implementation of the abstract methods.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2522:416,expose,expose,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2522,1,['expose'],['expose']
Security,"Defining inputs in a call overwrites/affects inputs to other calls when these inputs have the same name. This happens in cromwell 34, as well as the develop version (9bee537). It happens for WDL version 1.0 and Biscayne, but not for draft-2. example:; ```; version 1.0; workflow test {; String out = ""hello""; call echo1 { #Should run `echo hello1`, but runs `echo21` if run second; input:; out = out + ""1""; }; call echo2 { #should run `echo hello2`, but runs `echo 12` if run second; input:; out = out + ""2""; }; }; task echo1 {; input {; String out; }; command {; echo ~{out}; }; }; task echo2 {; input {; String out; }; command {; echo ~{out}; }; }; ```; I added the echo task twice to check if it might be caused by running the same task multiple times, but this also happens when it's two different tasks with equally named inputs. Defining one or both inputs as variables before passing them to the call seems works as expected:; ```; workflow test {; String out = ""hello""; call echo1 { # runs `echo hello1`; input:; out = out + ""1""; }; String out2 = out + ""2""; call echo2 { # runs `echo hello2`; input:; out = out2; }; }; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3999:1886,PASSWORD,PASSWORDS,1886,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999,1,['PASSWORD'],['PASSWORDS']
Security,Deny access to stats endpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2137:5,access,access,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2137,1,['access'],['access']
Security,Dependency injection version of a draft2/draft3 split,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3234:11,inject,injection,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3234,1,['inject'],['injection']
Security,"Differences might be:; - documentation; - authentication; - no `stats` endpoint. (depending on how Swagger actually works, some of these might be unnecessary/for free)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2148:42,authenticat,authentication,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2148,1,['authenticat'],['authentication']
Security,DigestUtils.java:794); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:52); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:73); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3383:2648,hash,hash,2648,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383,1,['hash'],['hash']
Security,"Directory structure:. ```; WDLTesting; -src; --wdl; ---Workflow.wdl; ---WriteTask.wdl; ---Child; ----ChildWF.wdl; ----ChildTask.wdl; ```. Under draft 2 (which is to say, no version specified), Workflow.wdl has the following import statements:; ```; import ""WDLTesting/src/wdl/WriteTask.wdl"" as Write; import ""WDLTesting/src/wdl/Child/ChildWF.wdl"" as Child; ```; ChildWF.wdl has the following import statement:; `import ""WDLTesting/src/wdl/Child/ChildTask.wdl"" as Child`. This works correctly. it works letting things be access from the file system, and it works if we zip up WDLTesting and pass it to Cromwell as --imports. Under version development, this doesn't work. Workflow.wdl's imports still work, but ChildWF.wdl's do not. I must trim it down to ; `import ""Child/ChildTask.wdl"" as Child`; Before it will work. This completely breaks our pipeline, especially since workflows calling files up and down the tree, and even in other projects that start at the same level as ours. Why was this horrible breaking change made, and how do we specify an import that works with respect to the --imports .zip?. Thank you",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6441:520,access,access,520,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6441,1,['access'],['access']
Security,Disable hash lookup when we have the docker digest,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5545:8,hash,hash,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545,1,['hash'],['hash']
Security,Do not retry Centaur tests which try to validate `callCaching` keys in metadata - retries will likely be waste of time in this case [BA-6130],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5322:40,validat,validate,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5322,1,['validat'],['validate']
Security,"Doc section on ""security by sysadmin""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1653:16,secur,security,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653,1,['secur'],['security']
Security,Docker Hash,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1969:7,Hash,Hash,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1969,1,['Hash'],['Hash']
Security,Docker Hash Lookup v3,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2048:7,Hash,Hash,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048,1,['Hash'],['Hash']
Security,"Docker Hub is incredibly slow when accessed directly from Hangzhou or any other location within China. There is no known Alibaba Cloud provided CDN or cache for docker images on BCS. To significantly speed up docker pulls users are able to upload docker images to their own private docker registry hosted within one of their OSS buckets. This uses a plugin contributed to the docker codebase that stores and retrieves docker images via an OSS client. Currently the BCS backend allows users to specify the private OSS registry within the `docker` runtime attribute. For portability, the `docker` runtime attribute should only specify the image, and a separate `dockerRegistry` runtime attribute should optionally specify a private OSS registry. Ideally there should be a way for a user to cache docker images on their own private OSS registry while still using contributed by others WDLs. One particular issue for call caching may be that the docker image hashes are probably registry specific. Cromwell's call caching code requires WDL to specify a hash that may only be available on docker hub, and may not be available on an OSS mirror, even if the image contains the exact same content. Also it should be decided if the BCS backend should behave like the JES/PAPI backend and only allow jobs that specify a `docker` runtime attribute, or if the behavior should continue to be like the `Local`/`SFS` backends and allow running jobs on the bare VM without a docker container. Links regarding BCS/OSS and docker:; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F28022.html)) https://help.aliyun.com/document_detail/28022.html; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F42402.html)) https://help.aliyun.com/document_detail/42402.html; - https://docs.docker.com/registry/storage-drivers/; - https://github.com/docker/distribution",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3518:35,access,accessed,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518,2,"['access', 'hash']","['accessed', 'hashes']"
Security,Docker authentication for AWS Backend [BT-293],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6359:7,authenticat,authentication,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6359,3,['authenticat'],['authentication']
Security,Docker hash mismatch between 26 and 27,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2359:7,hash,hash,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2359,1,['hash'],['hash']
Security,Docker hash-lookup when there is no digest of a docker image,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6940:7,hash,hash-lookup,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6940,1,['hash'],['hash-lookup']
Security,Docker hashing gets its own subproject.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2099:7,hash,hashing,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2099,1,['hash'],['hashing']
Security,Docker image hash field too small,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1301:13,hash,hash,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1301,1,['hash'],['hash']
Security,DockerHub authentication for AWS Backend. Previous PR: https://github.com/broadinstitute/cromwell/pull/6359,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6489:10,authenticat,authentication,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6489,2,['authenticat'],['authentication']
Security,Docs and Error messaging for Pair access,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2300:34,access,access,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2300,1,['access'],['access']
Security,"Docs:; - Fixed a bunch of broken links; - I think the Scala Steward updates gave us a new version of doc generation that is more strict; - There are more broken links than just these, did not attempt to be comprehensive; - IntelliJ's markdown validation is helpful:. ![Screen Shot 2020-08-19 at 12 15 43 PM](https://user-images.githubusercontent.com/1087943/90661978-d2613d00-e215-11ea-8c1d-5ae4c842213e.png). Error messages:; - Attempted to make them more concise and consistent; - Sometimes we didn't make it obvious that a limit is configurable; - Did not attempt to be comprehensive",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5779:243,validat,validation,243,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5779,1,['validat'],['validation']
Security,Document noAddress feature with instructions to set Google Private Access BA-5740,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5045:67,Access,Access,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5045,1,['Access'],['Access']
Security,"Doesn't close #751 but stabs in its general direction. There is no persistent DB here, I replaced the DB-based approach in `KeyValueServiceActor` with a simple `Map` since the DB-based approach always failed to write due to referential integrity constraints that aren't going to be fixed (no `EXECUTION` or `WORKFLOW_EXECUTION` data for FK constraints). Ruchi and/or I will have a follow-on PR to put a DB table behind this. Also fixed ""abort"" to message the KV service instead of metadata service, removed now-unused `EXECUTION_INFO`-related methods, other assorted cleanup.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1197:236,integrity,integrity,236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1197,1,['integrity'],['integrity']
Security,Don't compute call caching hashes if call caching is turned off,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/355:27,hash,hashes,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/355,1,['hash'],['hashes']
Security,Don't compute task hash when Call Caching is turned of via Workflow Options Closes #837,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/846:19,hash,hash,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/846,1,['hash'],['hash']
Security,Don't render secure resources unless build script can use them.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4038:13,secur,secure,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4038,1,['secur'],['secure']
Security,"Draft PR TODOs:; - [x] Add some unit tests; - [X] ImportResolver; - [X] GithubAuthVendingSupport; - [x] Find a better way to do the await result; - [x] Remove from standard config and allow the system to work in the absence of a github auth vending service OR allow it to be configured OFF in config and make that the default 🤔 ; - [x] Move the auth vending message and helper classes out of `impl`. To test this - ; - Go to github and create a limited scope personal access token; - Scope to a single repo; - Add readonly access for Content, and no other permissions; - Update the config of a running instance with the raw token under `GithubAuthVending.config.access-token`. NB don't include the `Bearer` before the token in the config file.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7365:468,access,access,468,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7365,3,['access'],"['access', 'access-token']"
Security,"During FireCloud testing, one job out of many similar jobs (all others succeeded) failed in the following way:. ```; $ gcloud alpha genomics operations describe EPKmsKy8KxiFwqr275_Y6hwg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU. message: ""15: Gsutil failed: failed to upload logs for \""gs://fc-7ffecbef-0544-4db7-92a0-a3a0d4bafdfe/447d9c70-0173-47f3-8a00-e59a69290dde/echo_strings/3f63e284-cfd2-4a3d-a846-d468258e1aaf/call-echo_files/\""\; : cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log\; \ gs://fc-7ffecbef-0544-4db7-92a0-a3a0d4bafdfe/447d9c70-0173-47f3-8a00-e59a69290dde/echo_strings/3f63e284-cfd2-4a3d-a846-d468258e1aaf/call-echo_files/,\; \ command failed: AccessDeniedException: 403 Caller does not have storage.objects.create\; \ access to bucket fc-7ffecbef-0544-4db7-92a0-a3a0d4bafdfe.\nAccessDeniedException:\; \ 403 Caller does not have storage.objects.create access to bucket fc-7ffecbef-0544-4db7-92a0-a3a0d4bafdfe.\n\; AccessDeniedException: 403 Caller does not have storage.objects.create access\; \ to bucket fc-7ffecbef-0544-4db7-92a0-a3a0d4bafdfe.\nCommandException: 3 files/objects\; \ could not be transferred.\n: ""; ```; I verified the user's permissions by auth-ing as that user and manually copying a file to the bucket successfully. We should probably add this one to the retry list.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2233:689,Access,AccessDeniedException,689,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233,5,"['Access', 'access']","['AccessDeniedException', 'access']"
Security,"During code review for #1836 @cjllanwarne noted that `processSource` in what is currently named `WorkflowStoreActor` and most likely `WorfklowStoreSubmitActor` by the time this is acted upon looked suspicious as we had (we think) intended json validation to not happen until later and a workflow ID would always be handed back to the user. Further, the failed Future doesn't appear to be getting handed back to the API at all (I think), which would lead to a timeout response. Further since the sources are being processed monadically it is possible for a user to have multiple borked files but only the first will be reported (if we were reporting). Check into what's up here - either don't perform this check on submission or ensure that appropriate error messages are handed back",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1882:244,validat,validation,244,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1882,1,['validat'],['validation']
Security,"EAs; 2016-12-12 18:35:46,427 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(b5d75f00)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:35:56,417 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(0891d5a0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:05,640 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(51571dc0)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:36:15,513 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(c43b628c)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:36:24,633 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(50785284)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:36:34,764 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(8dd2fe57)]: Abort received. Aborting 2 EJEAs; 2016-12-12 18:36:45,132 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(7f1250f8)]: Abort received. Aborting 8 EJEAs; 2016-12-12 18:37:06,029 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(3d36fdc3)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:37:14,145 cromwell-system-akka.dispatchers.engine-dispatcher-177 INFO - WorkflowExecutionActor [UUID(60ec6228)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:23,720 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(a442dc1c)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:37:31,421 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(17bed42e)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:37:40,098 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(e9851ba1)]: Abort received. Aborting 3 EJEAs; `; Cromwell hash: 192ea6025613df967d60e9e975693144035379d7",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:5679,hash,hash,5679,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,1,['hash'],['hash']
Security,"Early access MVP version of the extractor script, to make sure the extractor, digester and comparer can all be singing from the same hymn sheet as soon as possible. Also to course correct as soon as possible if I've gone in completely the wrong direction, I guess. Featuring:; * Extraction and upload of:; * Workflow metadata; * Operations metadata (PAPI v2alpha1). Not yet implemented; coming soon in part 2 (or 3 (or ...)):; * Subworkflows; * Other operations metadata flavors; * Cromwell code dump upload; * Config dump upload. Especially interested in early feedback on:; * Coding styles (if we want to standardize on one for these scripts); * User interface (eg if you try to use it, is it intuitive?); * If I got the directory structure completely wrong. Changes I suspect might happen in subsequent PRs but I'm reluctant to make in this one-script MVP:; * This function could be re-used, can we extract it to a shared location?; * Could we use a data structure to store this type of information between scripts",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5485:6,access,access,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5485,1,['access'],['access']
Security,Ease optional access with scatter-like block,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1852:14,access,access,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1852,1,['access'],['access']
Security,"Edit (by @cjllanwarne) in light of #4806:. Following #4806 we will be able to read Google project metadata to specify a VPC network and subnet. Therefore what will remain for ***this*** ticket is making the same functionality available on a per-workflow basis... eg an ability to supply the same network/subnetwork information via workflow-options?. ---. ### Original issue text:. https://cloud.google.com/vpc/docs/vpc -- for a primer on GCP Subnets. Users should be able to tell Cromwell to launch nodes into a subnet. For environments like Firecloud, we should have some mechanism (like maybe SAM) to make sure the user actually has the right to use a particular subnet. . The main reason to do this is https://cloud.google.com/vpc/docs/using-flow-logs -- we want to be able to monitor traffic in and out of the network for more significant audited environments. So the driver is ultimately ""compliance"". But it's probably a good idea anyhow. After this is done, please work with FC team to make sure they can take advantage of this. I'm not sure who to tag to make sure this cross-team work is done.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4070:843,audit,audited,843,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4070,1,['audit'],['audited']
Security,"Eg spot the difference between the exposed `err` declaration and the anonymous `b_prime.in_file` expression in:; ```wdl; workflow stdout_stderr_passing {; call a; call b {input: in_file=a.out}; File err = a.err; call b as b_prime {input: in_file=err}; output {; b_prime.out; }; }; ```. ![test](https://user-images.githubusercontent.com/13006282/33454657-7eba9d28-d5e7-11e7-9774-a17cbf3c63e8.png). NB: the dashes on the output are accidental, and I believe to-be-removed in an upcoming @mcovarr PR",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2958:35,expose,exposed,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2958,1,['expose'],['exposed']
Security,"Emit ""time since workflow ended"" time metric when accessing metadata [BW-710]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6365:50,access,accessing,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6365,1,['access'],['accessing']
Security,Enable validation memory test once wdl4s PR is merged,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/726:7,validat,validation,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/726,1,['validat'],['validation']
Security,Encrypt / clear workflow options in DB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1638:0,Encrypt,Encrypt,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1638,1,['Encrypt'],['Encrypt']
Security,Encrypting and clearing workflow options for 0.22.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1648:0,Encrypt,Encrypting,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1648,1,['Encrypt'],['Encrypting']
Security,Encrypting and clearing workflow options.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1647:0,Encrypt,Encrypting,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1647,1,['Encrypt'],['Encrypting']
Security,Encrypting and clearing workflow options. Closes #1638,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1650:0,Encrypt,Encrypting,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1650,1,['Encrypt'],['Encrypting']
Security,Encrypting workflow options,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/204:0,Encrypt,Encrypting,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/204,1,['Encrypt'],['Encrypting']
Security,"Endpoint with these arguments:; - `call1_fqn`; - `call1_index`; - `call2_fqn`; - `call2_index`. Which would return a set of call caching hashes which were generated for call2 which do not match call1. Has to be exactly that, to mirror the call caching algorithm",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1990:137,hash,hashes,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1990,1,['hash'],['hashes']
Security,"Example 1 (valid WDL code):; ```; version 1.0. workflow main {; output {; Array[String] x = flatten([[""1""], [""2""]]); }; }; ```; It will validate:; ```; $ java -jar womtool-55.jar validate main.wdl ; Success!; ```; And it will run successfully:; ```; $ java -jar cromwell-55.jar run main.wdl; ...; ""main.x"": [""1"", ""2""]; ...; ```. Example 2 (invalid WDL code):; ```; version 1.0. workflow main {; output {; Array[String] x = flatten([[""1""], [[""2""]]]); }; }; ```; It will validate:; ```; $ java -jar womtool-55.jar validate main.wdl ; Success!; ```; But it will error out:; ```; $ java -jar cromwell-55.jar run main.wdl; ...; Failed to evaluate 'main.x' (reason 1 of 1): Evaluating flatten([[""1""], [[""2""]]]) failed: No coercion defined from wom value(s) '[""2""]' of type 'Array[String]' to 'String'.; ...; ```. Example 3 (invalid WDL code):; ```; version 1.0. workflow main {; output {; Array[String] x = flatten([[[""1""]], [[""2""]]]); }; }; ```; It will not validate:; ```; $ java -jar womtool-55.jar validate main.wdl ; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process declaration 'Array[String] x = flatten([[[""1""]], [[""2""]]])' (reason 1 of 1): Cannot coerce expression of type 'Array[Array[String]+]+' to 'Array[String]'; ```. It is not a big deal, as the offending construct in Example 2 is not widely used, but it seems like a missed opportunity for flagging as an issue during the validation process. And why is it that womtool can flag Example 3 but not Example 2?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6185:136,validat,validate,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6185,7,['validat'],"['validate', 'validation']"
Security,ExecutionActor.scala:549); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:539); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:980); at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1363); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1391); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1375); at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:563); at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientReques,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1782:4094,secur,security,4094,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782,1,['secur'],['security']
Security,Expose Meta object or labels in Cromwell Metadata Output,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2421:0,Expose,Expose,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2421,1,['Expose'],['Expose']
Security,Expose an option that can be of some limited use in increasing cache hit throughput.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2027:0,Expose,Expose,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2027,1,['Expose'],['Expose']
Security,Expose new /query fields,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4831:0,Expose,Expose,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4831,1,['Expose'],['Expose']
Security,Expressions in array accesses won't parse,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2599:21,access,accesses,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2599,1,['access'],['accesses']
Security,Fail validation if call inputs will be ignored,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2967:5,validat,validation,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2967,1,['validat'],['validation']
Security,"Failure metadata will look something like:. ```; ""failures"": [{; ""failure"": ""Task 874ad75a-e19e-4b53-8225-746df1436c51:ValidateAggregatedSamFile was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3).\nError code 10. Message: Some(14: VM ggp-15150083938845849899 stopped unexpectedly.)"",; ""timestamp"": ""2016-12-18T05:54:07.296Z""; }],; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1893:119,Validat,ValidateAggregatedSamFile,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1893,1,['Validat'],['ValidateAggregatedSamFile']
Security,FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:6008,Validat,ValidatedRuntimeAttributesBuilder,6008,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,1,['Validat'],['ValidatedRuntimeAttributesBuilder']
Security,"Feature request: allow call caching to work across different engines. Currently, only calls run on the same engine can be reused. But if task inputs and docker hash are the same, results should be reusable across engines. This might require copying files across filesystems; but if there is a Local filesystem, then any file can be copied first to that and then to the target filesystem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4616:160,hash,hash,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4616,1,['hash'],['hash']
Security,"Feedback from today's workshop: `womtool validate` uselessly prints a few newlines on successful exit. Various workshop participants thought:; 1. It should exit with no output to harmonize with Unix CLI tool conventions; 2. It should confirm success in a way that is obvious to users who are e.g. biologists first, programmers second. Either would be an improvement over the current state. cc @rmeffan @ndbolliger ; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4040:41,validat,validate,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4040,2,"['PASSWORD', 'validat']","['PASSWORDS', 'validate']"
Security,File hash caching,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4143:5,hash,hash,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4143,1,['hash'],['hash']
Security,File hash short-circuiting,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1316:5,hash,hash,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1316,1,['hash'],['hash']
Security,File hashing exception with CWL run on SLURM,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584:5,hash,hashing,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584,1,['hash'],['hashing']
Security,"First implementation of a pluggable LocalBackend. This is more a light basic implementation and a starting point to iterate over.; What is implemented : ; - Support for non-docker jobs; - Support for docker jobs; - Support for ""ContinueOnReturnCode"" ""FailOnStderr"" and ""docker"" runtime attributes; - Engine functions; - Abort. Things to think about:; - How to share code between backends ? runtime attributes validation, engine functions, shared filesystem code.. ; - Testing. Note: some code is duplicated from the engine as it's still used by the current non-PBE implementation. Eventually this will replace all local backend code in the engine. Currently adding more tests for ; - [x] abort. ~~\- [ ] engine functions~~; - [x] input localization; - [x] expression evaluation; - [x] coercion ; - [x] scatter",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/712:409,validat,validation,409,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/712,1,['validat'],['validation']
Security,"Fix Docker hash throttling, remove deprecation warning [BT-336]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6437:11,hash,hash,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6437,1,['hash'],['hash']
Security,Fix Travis Encryptions,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1953:11,Encrypt,Encryptions,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1953,1,['Encrypt'],['Encryptions']
Security,Fix input validation and add to womtool,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3473:10,validat,validation,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3473,1,['validat'],['validation']
Security,Fix member access on scatter variables,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3763:11,access,access,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3763,1,['access'],['access']
Security,Fix name collision issue in call block member access,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4095:46,access,access,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4095,1,['access'],['access']
Security,"Fix/update for [WX-1210](https://broadworkbench.atlassian.net/browse/WX-1210). Turns out that the `head_commit` attribute is not available on `pull_request` actions, which is why the JIRA ID check kept failing. I'm opting to use `github.event.pull_request.title` which is accessible on `pull_request`. [WX-1210]: https://broadworkbench.atlassian.net/browse/WX-1210?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7184:272,access,accessible,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7184,1,['access'],['accessible']
Security,Fixed bug with actor system being created and not stopped if exception occurs during validation of command line arguments BA-4061,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5236:85,validat,validation,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236,1,['validat'],['validation']
Security,"Fixed duplicate dependencies for core module.; Added tighter yaml validation, including looking for duplicate yaml keys.; Added swagger validation to swagger spec.; Fixed broken cromwell.yaml.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/792:66,validat,validation,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/792,2,['validat'],['validation']
Security,Fmc 725 validation exp eval. Closes #725.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/746:8,validat,validation,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/746,1,['validat'],['validation']
Security,Fmc 726 memory validation. Closes #726.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/741:15,validat,validation,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/741,1,['validat'],['validation']
Security,"Following the docs at https://github.com/broadinstitute/cromwell#runtime-attributes, I'd like to be able to pass runtime attributes as the inputs to a task, for example:; ```; task iRun {; String runtimeMemory; Int runtimeCpu; command {; echo ""so far away""; }; output {; String out = read_string(stdout()); }; runtime {; memory: runtimeMemory; #cpu: runtimeCpu; }; }; ```; When using a configurable backend, I can confirm this works for the String type attribute `memory` but not the Int type `cpu`: running the above with the cpu runtime attribute uncommented I get this in the logs:; ```; [ERROR] [11/24/2016 10:49:13.299] [cromwell-system-akka.dispatchers.engine-dispatcher-22] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow beb03899-5f22-4f2c-8a85-d619a2d8a969 failed (during InitializingWorkflowState): java.lang.IllegalArgumentException: Task iRun has an invalid runtime attribute cpu = runtimeCpu; ```. My custom backend application.conf section: ; ```; PBS { ; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config { ; runtime-attributes = """"""; Int cpu = 1 ; Int memory_mb = 1000; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; ...; }; }; ```. I thought it might be because of the special nature of the `cpu` runtime attribute, but I tested with a different custom runtime attribute `Int pbs_cpu` and got the same result, so my guess is that it's the Int type that is the problem. I am working around this by defining `String pbs_cpu` which is then interpreted as an expression in the runtime block, as documented, but it feels wrong because the value should really be validated as an Int.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1702:1697,validat,validated,1697,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1702,1,['validat'],['validated']
Security,"Following the pattern of the GCP metadata actor, these actors will send Cromwell metadata to both the metadata database as well as to AWS SNS pubsub service. By sending to SNS users can setup custom accessory behavior without needing to modify Cromwell by creating subscribers to the SNS topic. Example use cases are alerting users when jobs succeed (or fail), updating slack channels, tagging workflow output S3 objects with workflow metadata. By creating AWS Lambda function subscribers, very flexible event driven actions can be taken.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5748:199,access,accessory,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5748,1,['access'],['accessory']
Security,"For FC provenance tracking we need to keep an eye on the file hashes of workflow inputs. My understanding is that call-caching stores the CRC32c of each _call_ input, but it's difficult to trace those hashes back to workflow inputs. Could you store the hashes of workflow inputs at job submit time and throw them in the workflow metadata?. Ping @abaumann for prio but I don't think it's super urgent.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1629:62,hash,hashes,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629,3,['hash'],['hashes']
Security,"For `Object`s, `Map`s and `Pair`s, and perhaps `Array`s. Eg We need to be able to distinguish between `p.left`, accessing the left hand side of a pair, and `p.left` accessing the `left` output of a task called `p`. Make sure member access works for:; - [X] Maps; - [X] Arrays; - [x] Pairs; - ~~Objects~~ Not Supported (but it's no longer the member access bit holding us back)!. Then reinstate tests:; - [x] `scatter_chain`; - ~~`if_then_else_expressions`~~ Punted to #2860 ; - [X] `array_and_map_indexing` (enhanced!)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2861:112,access,accessing,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2861,4,['access'],"['access', 'accessing']"
Security,"For endpoints which provide a single workflow ID to get data, e.g. metadata, request if this workflow ID is accessible to the principal (see #2131). If so pass through to Cromwell, otherwise reject",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2136:108,access,accessible,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2136,1,['access'],['accessible']
Security,"For more specific details see the DB meeting notes. Remove the biz logic-y portions of the DataAccess trait and move them into something alongside/in the appropriate locations (metadata store, kv store, etc) and have that code take a SlickDatabase. Move globalDataAccess to be a globally (in core package? Something like that) accessible singleton SlickDatabase instance, which the individual ex-DataAccess stuff can use. The DataAccess trait also had a withRetry function which does seem inappropriate for SlickDatabase, at the moment it's just a wrapper around the main withRetry w/ some defaults set - perhaps that lives w/ the global SlickDatabase? Perhaps allow individual ex-DataAccess stuff set their own? Up to you.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1177:327,access,accessible,327,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1177,1,['access'],['accessible']
Security,"For now at least, the implementation isn’t that big of a deal as long as it’s pushbutton GCP stuff. The goal is to store the JSON events coming out of PubSub as-is in a fashion that we can access them in the future as necessary. We don’t need efficient querying of these events but we do need the ability to easily get all the events associated with a workflow. Cloud Datastore seems like it’d work (for instance, kind being workflow ID and entity being the metadatum) but I don’t really know. We could also just do something like store these in a google bucket. Let’s not get too complicated here, the idea is to find something simple and verify that it’s feasible over time. Once this is squared away, enhance the application from #3244 to also dump the events **as-is** into this event store w/o modification.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3246:189,access,access,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3246,1,['access'],['access']
Security,"For now treat this like refresh tokens, e.g. encrypted, deleted afterwards, etc. user provides json key via workflow options.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2669:45,encrypt,encrypted,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2669,1,['encrypt'],['encrypted']
Security,"For the integration tests Cromwell runs, it would be good for the test definition to assert the hash of the expected output and actual output. This particular case is to ensure that there are no changes to the engine that could cause the contents of the outputs to change over time, and alert the team when such a case happens.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4904:96,hash,hash,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4904,1,['hash'],['hash']
Security,"For the new /describe endpoint being added for the purposes of Womtool-as-a-Service, implement the ability to validate a given workflow source file. AC: Given a WDL workflow source file (not http url), return whether its a valid WDL workflow, and provide errors if its invalid. ```; {; ""valid"": true,; ""errors"": [; ""The 'errors' field will be filled if 'valid' is false"",; ""We might also provide warnings to a 'valid' workflow here"",; ""Otherwise, 'errors' will be empty or unspecified""; ]; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4433:110,validat,validate,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4433,1,['validat'],['validate']
Security,"Formation set up provided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs""; workflow-log-temporary = false; }. call-caching {; enabled = true; invalidate-bad-cache-resu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162:1101,secur,security,1101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162,1,['secur'],['security']
Security,"Found during workshop - error formatter caret points to the wrong thing. Looks like a tabs vs. spaces thing.; ```; workflow HelloWorld {. 	call WriteGreetings; }. task WriteGreeting {. 	command {; 		echo ""Hello World""; 	}; 	output {; 		File outfile = stdout(); 	}; }; ```; ```; (gatk) root@5721c54d094c:/gatk/workshop_bundle/workshop_bundle# java -jar jars/womtool-34.jar validate hello_world/hello_world_0.wdl ; ERROR: Call references a task (WriteGreetings) that doesn't exist (line 3, col 7). 	call WriteGreetings; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4041:372,validat,validate,372,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4041,1,['validat'],['validate']
Security,"Found during workshop - incomprehensible error output.; ```; workflow HelloWorld {. 	call WriteGreeting; }. task WriteGreeting {. 	command {; 		echo ""Hello World""; 	}; 	output {; 		File outfile = asdf(); 	}; }; ```; ```; (gatk) root@5721c54d094c:/gatk/workshop_bundle/workshop_bundle# java -jar jars/womtool-34.jar validate hello_world/hello_world_0.wdl ; wdl.draft2.model.expression.WdlStandardLibraryFunctionsType.asdf(scala.collection.Seq); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4042:315,validat,validate,315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4042,1,['validat'],['validate']
Security,"From @jsotobroad. Even though getting MD5 from GCS is fast for call caching, when you do 20k of these it does take a bunch of time. Would it be possible to adopt a strategy like the ""File"" hashing strategy if the user wanted?. from Kris: especially for interior (e.g. from another task) inputs where we can be confident it hasn't been fiddled with. from @cjllanwarne : question: since call caching actually copies files, it's the MD5 we really need because we copy outputs around",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1623:189,hash,hashing,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1623,1,['hash'],['hashing']
Security,"From Henry --. My comment is a little different than the commenter above. So I will try to provide some context in order to differentiate. If one looks at Cromwell as being part of a larger application infrastructure/service such as a Genomics Pipeline where sequenced data is constantly being processed through the system or providing a service where users on the Internet can launch workflows ""as a Service"" - whenever they way. In these systems, uptime and availability are critical - at all times. Below are several scenarios where I could see running multiple Cromwells would be very beneficial. High availability:; Having multiple Cromwells running where jobs are ""load balanced"" between them would allow us to continue operate when there are systems issues or failures with one of the Cromwell instances. Zero-downtime deployments:; Supporting multiple Cromwells running different versions of the code, could provide the ability to upgrade Cromwell with little or no user impact. Essentially the new version is deployed to a new server, it is started up and at an appropriate time traffic (via a load balancer or proxy maybe) is directed away from the ""old version"" Cromwell to the new. Similar to item 2: being able to introduce infrastructure changes (host OS, security patches, host resizing,..) more seamlessly:; If I can support multiple instances of Cromwell, I can build a new instance of the host with all the updates and changes I require - deploy cromwell to the new node and cut over. A corollary to this request is also the ability to ""move"" workflows from one Cromwell instance to another. Maybe this is just workflows not in flight or active - but waiting to run. This capability could make it easier to retire older cromwell instances (once multiple cromwell instances support is in place). Some workflows may take days to run, being able to ""relocate"" these workflows to the ""new"" cromwell - allows us to decommission ""old"" cromwells faster.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/691:1270,secur,security,1270,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/691,1,['secur'],['security']
Security,From a quick investigation it looks like the EJEA would use the default supervision strategy here. The PAPI BJEA would be restarted and then possibly nothing would happen since the broken disks runtime attribute prevented the job from actually starting. Should be easy enough to reproduce. ```; ERROR akka.actor.OneForOneStrategy - Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:350,validat,validation,350,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,10,"['Validat', 'validat']","['ValidatedRuntimeAttributesBuilder', 'validation']"
Security,"From a quick reading of the parent `WorkflowManagerActor` code it appears the default supervision strategy with ""restart on generic Exception"" is being used. Simply restarting a crashed `WorkflowActor` FSM appears to put it back into its initial `WorkflowUnstartedState` where it wouldn't do anything to progress a workflow until it receives a `StartWorkflowCommand` which is not being re-sent. So it looks like this would create a zombie workflow, though it does appear to be abortable.; ```; ERROR akka.actor.OneForOneStrategy - Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; java.lang.RuntimeException: Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials(GoogleAuthMode.scala:175); 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials$(GoogleAuthMode.scala:173); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.validateCredentials(GoogleAuthMode.scala:237); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:250); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:237); 	at cromwell.filesystems.drs.DrsPathBuilderFactory.withOptions(DrsPathBuilderFactory.scala:86); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.val",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4916:577,access,access,577,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916,3,"['access', 'validat']","['access', 'validateCredentials']"
Security,"From both swagger & purely from command line verify that the following workflow (pardon the pun) works. If it does not, file tickets here or with Sam as appropriate. Create a doc detailing the outcome. - Perform OAuth authentication (via clicky buttons in swagger, gcloud on CLI); - Register user in Sam; - Submit workflow to Cromwell; - Get final results from Cromwell for that workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2598:218,authenticat,authentication,218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2598,1,['authenticat'],['authentication']
Security,"From cromwell:; ```; ....snip....; java.lang.RuntimeException: Task 773d051e-2e93-4248-bca4-e40292e0e59d:generate_true_positives failed: error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list -> /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list (cp failed: gsutil -q -m cp gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list, command failed: AccessDeniedException: 403 Caller does not have storage.objects.list access to bucket firecloud-tcga-open-access.\nCommandException: 1 file/object could not be transferred.\n)""; at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:489); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:61); ....snip....; ```; ; BUT I would think this next operation would fail and it does not:; ```; lichtens@lichtens-big:~/test_dl_oxoq/create_bs$ gsutil ls gs://firecloud-tcga-open-access/tutorial/reference/; gs://firecloud-tcga-open-access/tutorial/reference/CNV.hg19.bypos.111213.txt; gs://firecloud-tcga-open-access/tutorial/reference/Homo_sapiens_assembly19.dict; gs://firecloud-tcga-open-access/tutorial/reference/Homo_sapiens_assembly19.fasta; gs://firecloud-tcga-open-access/tutorial/reference/Homo_sapiens_assembly19.fasta.fai; ```. and: ; ```; lichtens@lichtens-big:~/test",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1960:1163,Access,AccessDeniedException,1163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1960,1,['Access'],['AccessDeniedException']
Security,"From user reports, this is upsetting cost estimation (and is scary anyway wrt re-writing history!. Needs validation and a reproducible case, but presumably something like:. - Run a long workflow, one shard fails; - Re-run the same workflow, most shard call cache; - The shards in the original workflow get updated to have the call-cache timings, rather than the original long timings.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4141:105,validat,validation,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4141,1,['validat'],['validation']
Security,From: #5464 ; Closes: #5460 . Recreated to be in the Cromwell repo with access to testing. Thanks @aednichols!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5583:72,access,access,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5583,1,['access'],['access']
Security,"Function.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at akka.actor.Actor$class.aroundReceive(Actor.scala:496); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:67); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); ... 5 more; Caused by: com.google.api.client.http.HttpResponseException: 500 Internal Server Error; {; ""error"" : ""internal_failure""; }; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1070); at com.google.auth.oauth2.UserCredentials.refreshAccessToken(UserCredentials.java:207); at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:149); at cromwell.filesystems.gcs.auth.GoogleAuthMode$$anonfun$validateCredential$1.apply$mcV$sp(GoogleAuthMode.scala:62); at cromwell.filesystems.gcs.auth.GoogleAuthMode$$anonfun$validateCredential$1.apply(GoogleAuthMode.scala:62); at cromwell.filesystems.gcs.auth.GoogleAuthMode$$anonfun$validateCredential$1.apply(GoogleAuthMode.scala:62); at scala.util.Try$.apply(Try.scala:192); at cromwell.filesystems.gcs.auth.GoogleAuthMode$class.validate(GoogleAuthMode.scala:65); ... 46 more. [INFO] [05/12/2017 21:47:03.777] [cromwell-system-akka.dispatchers.engine-dispatcher-7] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor WorkflowActor-6c383c35-d791-4971-aecd-0723726c8a7b is in a terminal state: WorkflowFailedState; ^C[INFO] [05/15/2017 14:06:33.456] [shutdownHook1] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor: Received shutdown signal.; [INFO] [05/15/2017 14:06:33.457] [cromwell-system-akka.dispatchers.engine-dispatcher-91] [akka://cromwell-system/user/cromwell-service/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2270:6346,validat,validateCredential,6346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2270,1,['validat'],['validateCredential']
Security,Functionality added to localize WdlFile / WdlArray[WdlFile] cached results in HtCondor so in that way cached results are link to the new task. i.e.:; 1. w0/task1 execution produces w0/result1.; 2. w0/result1 is stored in the cache.; 3. w1/task1 (w1/task1 is the same as w0/task1) is executed again.; 4. Cache result is hit.; 5. File cached results are used to generate new symlinks to point to them in current task.; 6. Result is generated using new paths based on symlinks and/or other cached results. This change is based on new security/permissions requirements.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1424:531,secur,security,531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1424,1,['secur'],['security']
Security,GCPBATCH: accessing private gcr.io docker for callcaching raises error: unauthorized,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:10,access,accessing,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['access'],['accessing']
Security,Generates call caching hashes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1290:23,hash,hashes,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1290,1,['hash'],['hashes']
Security,"GitHub ""helpfully"" collapses `JsonEditorSpec` due to the scope of the changes, but actually that does need to be reviewed. 🙂 . Does:. * Fix `exclude` to only examine workflows and calls; * Support `:` syntax in excludes; * Add `ErrorOr` validation to method signatures; * ""Adjust"" `JsonEditorSpec` to not actively test for incorrect behavior. Does not:. * Fix `include` to only examine workflows and calls; * Support `:` syntax for include; * Add as many real-world or rigorous tests as I would like, mostly because the aforementioned things are still broken",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5309:237,validat,validation,237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5309,1,['validat'],['validation']
Security,"Give Centaur-managed Cromwell more time to restart and a custom exit code.; Publish artifacts again on each build tag.; Login to docker before trying to push images.; Functions using secure variables ensure that xtrace is not enabled, thus no longer need a subshell, thus do not need to be exported.; Artifactory and Docker Hub credentials added to vault.; Docker login can use environment variables or vault once dsde-toolbox is public.; Split setup_secure_environment into setup_common_environment and setup_secure_resources.; Sbt environment variables prefixed as CROMWELL_SBT_*.; Print out a warning instead of exiting when vault resources cannot be rendered when testing locally.; Minor updates for more consistent shell variable usage.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3661:183,secur,secure,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3661,1,['secur'],['secure']
Security,Give a JES-pointing server a non-GCS url in the inputs file and the API gives no response. Looking in the server logs you get . ```; java.lang.IllegalArgumentException: Not a valid Google Cloud Storage URI: /Users/chrisl/Desktop/workflowTimings.html; at cromwell.engine.io.gcs.GcsPath$.parse(GcsPath.scala:32); at cromwell.engine.io.gcs.GcsPath$.apply(GcsPath.scala:22); at cromwell.engine.io.gcs.GoogleCloudStorage.hash(GoogleCloudStorage.scala:76); at cromwell.engine.WorkflowDescriptor$$anonfun$fileHasher$1.apply(WorkflowDescriptor.scala:62); at cromwell.engine.WorkflowDescriptor$$anonfun$fileHasher$1.apply(WorkflowDescriptor.scala:62); at wdl4s.values.WdlValue$class.computeHash(WdlValue.scala:63); at wdl4s.values.WdlSingleFile.computeHash(WdlFile.scala:39); at cromwell.engine.WorkflowDescriptor.hash(WorkflowDescriptor.scala:176); at cromwell.engine.workflow.WorkflowActor$$anonfun$52.apply(WorkflowActor.scala:1050); at cromwell.engine.workflow.WorkflowActor$$anonfun$52.apply(WorkflowActor.scala:1049); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.immutable.Map$Map2.foreach(Map.scala:137); at scala.collection.TraversableLike$class.map(TraversableLike.scala:245); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.WorkflowActor.buildSymbolStoreEntries(WorkflowActor.scala:1049); at cromwell.engine.workflow.WorkflowActor.createWorkflow(WorkflowActor.scala:241); at cromwell.engine.workflow.WorkflowActor$Start.runInitialization(WorkflowActor.scala:120); at cromwell.engine.workflow.WorkflowActor.cromwell$engine$workflow$WorkflowActor$$initializeExecutionStore(WorkflowActor.scala:326); at cromwell.engine.workflow.WorkflowActor$$anonfun$1.applyOrElse(WorkflowActor.scala:364); at cromwell.engine.workflow.WorkflowActor$$anonfun$1.applyOrElse(WorkflowActor.scala:360); at scala.runtime.AbstractPar,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/520:416,hash,hash,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/520,2,['hash'],['hash']
Security,"Given the following call to a sub-workflow in a larger workflow:; ```; call CNVOncotator.CNVOncotatorWorkflow as CNVOncotatorWorkflow {; input:; called_file = CallCopyRatioSegmentsTumor.called_copy_ratio_segments,; additional_args = additional_args,; oncotator_docker = oncotator_docker,; mem_gb_for_oncotator = mem_gb_for_oncotator,; preemptible_attempts = preemptible_attempts; }; }; ```. Even though `additional_args` and `mem_gb_for_oncotator` are not defined, this workflow still validates.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3230:485,validat,validates,485,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3230,1,['validat'],['validates']
Security,Google Artifact Registry image hashing support [BT-335],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6436:31,hash,hashing,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6436,1,['hash'],['hashing']
Security,"Google JES backend: access to ""requester pays"" bucket",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2916:20,access,access,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2916,1,['access'],['access']
Security,"Got a centaur failure with `Failed to upload auth file` caused by the following exception, not considered retryable:. ```; 017-04-18 21:11:49,413 cromwell-system-akka.dispatchers.engine-dispatcher-64 ERROR - WorkflowManagerActor Workflow 6e23463e-3fc6-4b18-aeb0-fc7c920cd758 failed (during InitializingWorkflowState): Failed to upload authentication file; java.io.IOException: Failed to upload authentication file; 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.applyOrElse(JesInitializationActor.scala:63); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.applyOrElse(JesInitializationActor.scala:62); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.core.CromwellFatalException: com.google.cloud.storage.StorageExc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183:335,authenticat,authentication,335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183,2,['authenticat'],['authentication']
Security,Handle null access token from Google Credentials,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2489:12,access,access,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2489,1,['access'],['access']
Security,Hash WdlValues,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/286:0,Hash,Hash,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/286,1,['Hash'],['Hash']
Security,Hash outputs with call caching,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1964:0,Hash,Hash,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1964,1,['Hash'],['Hash']
Security,Hashing strategies v2 - Closes #1483,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1508:0,Hash,Hashing,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1508,1,['Hash'],['Hashing']
Security,Hashpocalypse postmortem,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3714:0,Hash,Hashpocalypse,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3714,1,['Hash'],['Hashpocalypse']
Security,Having a tail function in the array is very useful as it is quite common to read tsv files where first column is name of condition and others are sample accession numbers. I also opened a forum question on this http://gatkforums.broadinstitute.org/wdl/discussion/9893/getting-tail-of-the-array/p1?new=1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2414:153,access,accession,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2414,1,['access'],['accession']
Security,"Hello Cromwell Team, . I've updated a prior pull request: https://github.com/broadinstitute/cromwell/pull/6225 in order to support shared-vpc (https://cloud.google.com/vpc/docs/shared-vpc) in GCP. The changes include _shared-project-id_ and _shared-region_ parameters in order to form a Subnetwork label. Since the parameters depend on other some validations checks were updated.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6465:347,validat,validations,347,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6465,1,['validat'],['validations']
Security,"Hello! I used this snippet of code successfully on the Cromwell with Google backend:. ```; task fastqc {; input {; File f1; File? f2; Int? cpu=1; Int? machine_mem_gb; Int? preemptible_attempts; String mode. Float f2size = if (mode == ""PE"") then size(f2, ""GB"") else 0.0; Int disk_space_gb = ceil(size(f1, ""GB"") + f2size) + 20; }. command {; fastqc -t ~{cpu} --outdir $PWD ~{f1} ~{f2}; }. output {; Array[File] html = glob(""*html""); Array[File] zip = glob(""*zip""); }. runtime {; docker: ""biocontainers/fastqc:v0.11.5_cv2""; memory: select_first([machine_mem_gb, 4]) + "" GB""; cpu: cpu; disks: ""local-disk "" + disk_space_gb + "" HDD""; preemptible: select_first([preemptible_attempts, 3]); }; }; ```; On local backend I got this error:; _Failed to evaluate input '__disk_space_gb' (reason 1 of 1): Sorry! Operation + is not supported on empty optional values. You might resolve this using select_first([optional, default]) to guarantee that you have a filled value._. I tried following the advice (even though I don't see what is missing); Float f2size = select_first([if (mode == ""PE"") then size(f2, ""GB"") else 0.0]); And then I got an even more confusing error:; _Failed to process task definition 'fastqc' (reason 1 of 1): Failed to process expression 'select_first([f2size, select_first([if (mode == ""PE"") then size(f2, ""GB"") else 0.0])])' (reason 1 of 1): Invalid parameter 'ArrayLiteral(Vector(TernaryIf(Equals(IdentifierLookup(mode),StringLiteral(PE)),Size(IdentifierLookup(f2),Some(StringLiteral(GB))),PrimitiveLiteralExpressionElement(WomFloat(0.0)))))'. Expected an array of optional values (eg 'Array[X?]') but got 'Array[Float]+'_. Can you help me understand? It all passes Womtools validation.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5694:1688,validat,validation,1688,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5694,1,['validat'],['validation']
Security,"Hello, I am running Cromwell 36 configured with the GCS/JES backend to run jobs on GCP. When running massive batches of workflows, I frequently encounter the IP-address quota from Google. To avoid this, I've reconfigured my default VPC to allow private google access (see #1325). I've added the following to my Cromwell configuration (other unrelated configuration entries removed):; ```; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; default-runtime-attributes {; noAddress: true; }; }; }; }; }; ```. This appears to have the desired effect, as my instances are now launching without an external IP, however, the jobs end up failing because docker cannot fetch the image `stedolan/jq` (as it resides on docker hub). Is there a way to configure Cromwell to use a different image for that pipeline action?. I could reconfigure the VPC to allow access to docker(hub), but that would require connecting a NAT instance which would increase the cost of using Cromwell. ---. Edit: Cromwell 36. Sorry!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676:260,access,access,260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676,2,['access'],['access']
Security,"Hello,. I am having a problem that has been already discussed but I haven't been able to solve it using the suggestions. Basically, In the wdl workflow, I have two tasks (at the moment). The first works fine but the second is not starting because the output of the first task cannot be 'linked' or 'copied'. This cause the workflow to fail. The interesting part is that in the input folder of the second task there are two subfolders: 1 is empty named as `13016223` and the other is not accessible `-1976550098`. The workflow to run needs installed:; `cutadapt` and the script named `moveBarcodeToID.pl` that can be downloaded from here:. https://drive.google.com/open?id=1AizxTwjOEhL5XA7rsx-wbY97p0duB1nw. input fastq files can be retrieved here (they are small ~10000 reads each):. https://drive.google.com/file/d/1-c14Tja4zY3lyr6icFWT06stznR_-Zqr/view?usp=sharing; https://drive.google.com/file/d/1oJd_U9MjTllL0_kpNivw8I_LtSyvqpXH/view?usp=sharing. How can I solve this issue and make the workflow running smoothly?. ### Which backend are you running? ; I am running locally the workflow for now (because I am in the first phase of the development). ### Workflow is this:; ```; #workflow validated before running with: wdltool validate example.wdl and womtool validate scMeth_v2.wdl.sh -i scMeth_input_3.json. workflow scMeth {; # information for trimming the cell barcode; File command; Int bases; File input_fastq1; File input_fastq2; String sampleName. # information for trimming the adapters and low quality reads; File file_format; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; call trimCellBarcode {; input:; sampleName=sampleName,; bases=bases,; input_fastq1=input_fastq1,; input_fastq2=input_fastq2,; command=command; }; call trimAdapters {; input:; file_format=file_format,; input_r1 = trimCellBarcode.fastq_debarcoded_R1,; input_r2 = trimCellBarcode.fastq_debarcod",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:487,access,accessible,487,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['access'],['accessible']
Security,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5434:29,validat,validate,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434,29,"['Validat', 'validat']","['ValidateBAM', 'ValidateBamsWf', 'validate', 'validate-bam', 'validation']"
Security,"Hello,. I wonder if it is possible to specify the GCP Batch task scheduling policy via Cromwell configuration. In specific, can I set `taskCountPerNode` to be `1` to enforce one job per VM (as in https://cloud.google.com/batch/docs/reference/rest/v1/projects.locations.jobs#taskgroup)?. Sincerely,; Yiming. <!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7521:930,PASSWORD,PASSWORDS,930,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7521,1,['PASSWORD'],['PASSWORDS']
Security,"Hello,. I'm running a Cromwell service as Google Cloud VM instance. The Cromwell's version is 68, with the following conf:. ```; include required(classpath(""application"")). webservice {; interface = xx.xxx.xxx.xx; port = xxxx; }. google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""gred-cumulus-sb-01-991a49c4""; }. }; }. workflow-options {; workflow-log-temporary = false; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.cj.jdbc.Driver""; 	url = ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true""; 	user = ""root""; 	password = ""cromwell""; 	connectionTimeout = 5000; }; }. backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; # Google project; project = ""gred-cumulus-sb-01-991a49c4"". # Base bucket for workflow executions; root = ""gs://gred-cumulus-output/cromwell_execution"". virtual-private-cloud {; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private-subnetwork""; auth = ""application-default""; }. # Make the name of the backend used for call caching purposes insensitive to the PAPI version.; name-for-call-caching-purposes: PAPI. # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in PAPI.; slow-job-warning-time: ""24 hours"". # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Querie",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:777,password,password,777,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['password'],['password']
Security,"Hello,. I'm using Cromwell with Google Pipelines as backend and sometimes (maybe when more than 30 analysis running at same time) I'm getting workflow error (~2 of the 30). When inspecting the metadata for the workflow I can see a error message that contains ""ServiceException: 401 Requester pays bucket access requires authentication."". **Edit:** Using Cromwell 35. Has anyone had a similar problem? Here are the WDL task that are affected (from Broad's five dollar genome workflow):. ```wdl; task BaseRecalibrator {; File input_bam; File input_bam_index; String recalibration_report_filename; Array[String] sequence_group_interval; File dbSNP_vcf; File dbSNP_vcf_index; Array[File] known_indels_sites_VCFs; Array[File] known_indels_sites_indices; File ref_dict; File ref_fasta; File ref_fasta_index; Int disk_size; Int preemptible_tries. command {; /usr/gitc/gatk4/gatk-launch --javaOptions ""-XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -XX:+PrintFlagsFinal \; -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails \; -Xloggc:gc_log.log -Xms4000m"" \; BaseRecalibrator \; -R ${ref_fasta} \; -I ${input_bam} \; --useOriginalQualities \; -O ${recalibration_report_filename} \; -knownSites ${dbSNP_vcf} \; -knownSites ${sep="" -knownSites "" known_indels_sites_VCFs} \; -L ${sep="" -L "" sequence_group_interval}; }; runtime {; docker: ""us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.3.2-1510681135""; memory: ""6 GB""; disks: ""local-disk "" + disk_size + "" HDD""; preemptible: preemptible_tries; }; output {; File recalibration_report = ""${recalibration_report_filename}""; }; }; ```. And here is my cromwell server config:. ```scala; include required(classpath(""application"")). webservice {; port = 8000; }. system {; workflow-restart = true; }. engine {; filesystems {. gcs {; auth = ""service-account""; }. http {}. local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; }; }; }. backend {; default = ""Local""; providers {. Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBack",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336:304,access,access,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336,2,"['access', 'authenticat']","['access', 'authentication']"
Security,"Hello,. It seems to me that cromwell (at least `cromwell-37.jar`) can run `version 1.0` WDL scripts. Would you confirm this? It would also be helpful if you had this info readily on the ReadTheDocs page (https://github.com/broadinstitute/cromwell/blob/develop/docs/LanguageSupport.md). Thank you,; Azza . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4678:1059,PASSWORD,PASSWORDS,1059,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4678,1,['PASSWORD'],['PASSWORDS']
Security,"Hello,. when I try to run my workflow using a config file using. `$ java -Dconfig.file=../config/LSF.conf cromwell.jar cromwell run ../pipelines/bismark_pid.wdl -i ../pipelines/bismark_wgbs_pid.json`. I get the following message; ```; Error: Could not find or load main class cromwell.jar; Caused by: java.lang.ClassNotFoundException: cromwell.jar; ```. Without specifying a config file, the pipeline runs without any problems. ; I installed Cromwell (version 79) using conda. I also tried the following:. `$ java -Dconfig.file=../config/LSF.conf cromwell-79.jar run ../pipelines/bismark_pid.wdl -i ../pipelines/bismark_wgbs_pid.json `. ```; Error: Could not find or load main class cromwell-79.jar; Caused by: java.lang.ClassNotFoundException: cromwell-79.jar; ```. I also checked where the cromwell.jar file is saved in my conda environment and tried the following:. `; java -Dconfig.file=./LSF.conf /path/to/env/share/cromwell/cromwell.jar cromwell run ../pipelines/bismark_pid.wdl -i ../pipelines/bismark_wgbs_pid.json `. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. This is the config file LSF.config:; ```; include required(classpath(""application"")). backend {. # Override the default backend.; default = LSF. # The list of providers. Copy paste the contents of a backend provider in this section; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; 	submit = ""bsub -J ${job_name} -cwd ${cwd} -o ${out} -e ${err} /usr/bin/env bash ${script}""; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; # Second backend provider would be copy pasted here!. }; }; ```. I have not much experienced with cromwell and would be very grateful for help. Thank you,; Johannes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6796:1087,PASSWORD,PASSWORDS,1087,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6796,1,['PASSWORD'],['PASSWORDS']
Security,"Hello,; I'm using Cromwell to run a pipeline on an LSF cluster.; Depending on the input, jobs can take longer than the average case. I was therefore wondering if it is possible to increase the LSF time limit after a job fails (due to LSF time limit which has to be specified in my case).; Follow-up question: Is it possible to access the previous number of failed attempts for a job?. I saw the option to increase memory after a job fails, but not for other parameters and only for Google Cloud backends. Many thanks in advance!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6825:327,access,access,327,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6825,1,['access'],['access']
Security,"Here is a very simple WDL (test.wdl):; ```; version 1.0. workflow test {; input {; Array[File] list; }. call test {; input:; list = list; }; }. task test {; input {; Array[File] list; String docker = ""ubuntu:20.04""; }. File lines = write_lines(list). command <<<; cat ~{lines}; cat ~{write_lines(list)}; echo -e ""~{sep=""\\n"" list}""; >>>. runtime {; docker: docker; }; }; ```. Here is a basic input for the WDL (test.json):; ```; {; ""test.list"": [""/tmp/1"", ""/tmp/2"", ""/tmp/3""]; }; ```. When I run the workflow on my laptop with the following command:; ```; java -jar cromwell-51.jar run test.wdl -i test.json; ```; I get the following stdout output:; ```; /tmp/1; /tmp/2; /tmp/3; /cromwell-executions/test/00112233-4455-6677-8899-aabbccddeeff/call-test/inputs/1515144/1; /cromwell-executions/test/00112233-4455-6677-8899-aabbccddeeff/call-test/inputs/1515144/2; /cromwell-executions/test/00112233-4455-6677-8899-aabbccddeeff/call-test/inputs/1515144/3; /cromwell-executions/test/00112233-4455-6677-8899-aabbccddeeff/call-test/inputs/1515144/1; /cromwell-executions/test/00112233-4455-6677-8899-aabbccddeeff/call-test/inputs/1515144/2; /cromwell-executions/test/00112233-4455-6677-8899-aabbccddeeff/call-test/inputs/1515144/3; ```; Which shows that the absolute paths of the files passed to the test workflow have been exposed. I cannot imagine this being the expected behavior. How do I get to have `write_lines(...)` behave more like `~{sep="" "" ...}` even when it is not run inside a command <<< ... >>> instance?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5540:1317,expose,exposed,1317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5540,1,['expose'],['exposed']
Security,"Hi , ; When submitting jobs requiring GPU, we specified in the runtime session: ; gpuCount: 2; gpuType: ""nvidia-tesla-k80""; the jobs failed with following errors:; 2019/05/03 14:40:50 E: command failed: nvidia-docker | 2019/05/03 14:40:50 Error: Could not load UVM kernel module. Is nvidia-modprobe installed?. The same WDL file (with same docker and runtime attributes) used to work before. Please help!. Thanks!. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4935:1169,PASSWORD,PASSWORDS,1169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4935,1,['PASSWORD'],['PASSWORDS']
Security,"Hi All,; This PR is intended to review the implementation to perform ""runtime attributes"" validation in the backend side. Please, feel free to add any comment/idea that can add value to this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/708:90,validat,validation,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/708,1,['validat'],['validation']
Security,"Hi Atlas Team,. . I have installed the Atlas(apache-atlas-sources-2.1.0) in; our server by following in the link; ""https://atlas.apache.org/2.0.0/InstallationSteps.html. After all the setup; have been done, we ran the atlas-start.py and the atlas is running on port; 21000.When we are accessing the atlas, we are facing 503 Service Unavailable; Error. We checked the logs from application.log file. We got below issue . . 2020-12-13 06:29:02,309 WARN - [main:] ~ JMX is not enabled to receive; remote connections. Please see cassandra-env.sh for more info.; (CassandraDaemon:81). 2020-12-13 06:29:02,310 ERROR - [main:] ~ cassandra.jmx.local.port missing; from cassandra-env.sh, unable to start local JMX service.null; (CassandraDaemon:87). . We see there is no ""cassandra-env.sh"" file in atlas and we; tried other ways and didn't find any solution for the above error. . Could you please help us in rectify these problem so that it; will be very helpful to us.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6133:285,access,accessing,285,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6133,1,['access'],['accessing']
Security,"Hi Want to run my pipeline in gcp with nvidia-tesla-a100. Get errors for insert machine. Trake into the code since cromwell set n2-custom machine meanwhile a100 require a2-highgpu-1g. I guess it is not a big change but can extend capbilities. runtime {bootDiskSizeGb: 100; disks: ""/mnt 3000 HDD""; gpuType: ""nvidia-tesla-a100""; gpuCount: 1; nvidiaDriverVersion: ""418.87.00""; zones: [""us-central1-c""]; } (edited) . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6558:1521,PASSWORD,PASSWORDS,1521,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6558,1,['PASSWORD'],['PASSWORDS']
Security,"Hi all;; I'm testing out a CWL run (https://github.com/bcbio/bcbio_validation_workflows/tree/master/somatic-giab-mix) with a SLURM backend using file based caching:; ```; [2018-05-02 13:10:20,09] [info] Running with database db.url = jdbc:hsqldb:file:/projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/persist/metadata;shutdown=false;hsqldb.tx=mvcc; ```; and running into a hash exception at a consistent spot in the pipeline:; ```; [2018-05-02 15:16:51,49] [info] WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f [[38;5;2mbc4644da[0m]: Starting batch_for_variantcall; [2018-05-02 15:16:52,47] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Unrecognized runtime attribute keys: memoryMax, tmpDirMin, cpuMax, cpuMin, tmpDirMax, outDirMin, memoryMin, outDirMax; [2018-05-02 15:16:55,18] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: [38;5;5m'bcbio_nextgen.py' 'runfn' 'batch_for_variantcall' 'cwl' 'sentinel_runtime=cores,1,ram,3839.9999999999995' 'sentinel_parallel=multi-batch' 'sentinel_outputs=batch_rec:resources;description;reference__fasta__base;config__algorithm__variantcaller;reference__snpeff__GRCh37_75;config__algorithm__coverage_interval;genome_resources__variation__train_hapmap;genome_resources__variation__encode_blacklist;metadata__batch;genome_resources__variation__lcr;metadata__phenotype;vrn_file;reference__twobit;config__algorithm__validate;config__algorithm__validate_regions;genome_build;genome_resources__aliases__human;config__algorithm__tools_off;genome_resources__variation__dbsnp;genome_resources__variation__polyx;genome_resources__variation__cosmic;reference__genome_context;analysis;config__algorithm__tools_on;config__algorithm__effects;config__algorithm__variant_regions;genome_resources__aliases__ensembl;config__algorithm__exclude_regions;reference__rtg;genome_resources__variation__train_indels;genome",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584:408,hash,hash,408,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584,1,['hash'],['hash']
Security,"Hi all;; In testing release 35 with CWL inputs I've also been looking at supporting remote URL references. This is working correctly for GS URLs but not for http URLs. I've put together a test case that demonstrates the problem:. https://github.com/bcbio/test_bcbio_cwl/tree/master/gcp. The `somatic-workflow-http` CWL workflow uses http URLs and doesn't work, while the comparable `somatic-workflow` CWL workflow uses GS URLs referencing the same data and does work. The workflow fails with:; ```; java.io.FileNotFoundException: Cannot hash file https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genom; es/hg19/seq/hg19.fa; ```; when running tasks. The files get downloaded to the input directories but get numerical values instead of the original file names so never seem to sync over and get translated correctly to the workflow; ```; ls -lh cromwell_work/cromwell-executions/main-somatic.cwl/eaa632df-52a8-4aae-826f-647a42fa7145/call-prep_samples_to_rec/inputs/1515144/; total 136K; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 225050424226294657; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 2612405277530248055; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 503001634356675169; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 5802330287039666628; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 5809676514510180826; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 6090832304768530540; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 6105514522473810611; -rw------- 3 chapmanb chapmanb 37K Sep 26 14:07 6807576659333162957; -rw------- 3 chapmanb chapmanb 150 Sep 26 14:07 6853384576121493061; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 7483350933664987331; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 7538690575330349970; -rw------- 3 chapmanb chapmanb 37K Sep 26 14:07 7691692211431528147; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 7783203266940950463; -rw------- 3 chapmanb chapmanb 150 Sep 26 14:07 8389565043859020157; -rw------- 2 chapmanb chapmanb 43 Sep 26",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184:537,hash,hash,537,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184,1,['hash'],['hash']
Security,"Hi folks,; I am running cromwell 36 with AWS batch. Doing the hello world example from the following:. https://aws.amazon.com/blogs/compute/using-cromwell-with-aws-batch/. I am able to submit from the swagger UI and am getting the following erro:. `2018-10-30 00:39:25,929 INFO - jobQueueArn: arn:aws:batch:us-east-2:365166883642:job-queue/GenomicsHighPriorityQue-0c2108973103ca2; 2018-10-30 00:39:25,929 INFO - taskId: wf_hello.hello-None-1; 2018-10-30 00:39:25,929 INFO - hostpath root: wf_hello/hello/bcc91ab0-fd91-41a8-b3e6-cbf091cb511d/None/1; 2018-10-30 00:39:25,965 cromwell-system-akka.dispatchers.backend-dispatcher-229 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(bcc91ab0)wf_hello.hello:NA:1]: Error attempting to Execute; software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: batch.default.amazonaws.com: Name or service not known`. Any idea the source of this error?; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4334:1678,PASSWORD,PASSWORDS,1678,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4334,1,['PASSWORD'],['PASSWORDS']
Security,"Hi there,. Biocontainers are incredibly popular tools in bioinformatics. Each time a [Bioconda recipe](https://bioconda.github.io/recipes.html) is made, a Biocontainer is automatically generated. This creates a collection of Dockerized tools immediately available to everybody. To reduce footprint, Biocontainers run on BusyBox, a Linux version tailored to embedded systems. Some tools therefore do not expose the full set of options of matching GNU tools on Ubuntu, CentOS, etc. Given the popularity of Biocontainers, it would be great if Cromwell could support them fully. There are a couple of small bugs related to `find` and `xargs` that should be easy to fix when one explores the file `stderr.background` for a task run from a Biocontainer:. ```; find: unrecognized: -empty; BusyBox v1.22.1 (2014-05-23 01:24:27 UTC) multi-call binary. Usage: find [-HL] [PATH]... [OPTIONS] [ACTIONS]. Search for files and perform actions on them.; First failed action stops processing of current file.; Defaults: PATH is current directory, action is '-print'. 	-L,-follow	Follow symlinks; 	-H		...on command line only; 	-xdev		Don't descend directories on other filesystems; 	-maxdepth N	Descend at most N levels. -maxdepth 0 applies; 			actions to command line arguments only; 	-mindepth N	Don't act on first N levels; 	-depth		Act on directory *after* traversing it. Actions:; 	( ACTIONS )	Group actions for -o / -a; 	! ACT		Invert ACT's success/failure; 	ACT1 [-a] ACT2	If ACT1 fails, stop, else do ACT2; 	ACT1 -o ACT2	If ACT1 succeeds, stop, else do ACT2; 			Note: -a has higher priority than -o; 	-name PATTERN	Match file name (w/o directory name) to PATTERN; 	-iname PATTERN	Case insensitive -name; 	-path PATTERN	Match path to PATTERN; 	-ipath PATTERN	Case insensitive -path; 	-regex PATTERN	Match path to regex PATTERN; 	-type X		File type is X (one of: f,d,l,b,c,...); 	-perm MASK	At least one mask bit (+MASK), all bits (-MASK),; 			or exactly MASK bits are set in file's mode; 	-mtime DAYS	mtime is ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4607:403,expose,expose,403,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4607,1,['expose'],['expose']
Security,"Hi there,. I'm new to both WDL and Cromwell and am trying to get an analysis pipeline up and running. I'm using call-caching to speed up my development, so that I don't have to repeat multi-hour steps. However, I'm currently seeing ~8 minute delays for processing cache hits. With multiple steps in serial, this means that nothing in my pipeline starts running till 14 minutes after I start the run. Can you help me fix that?. Thank you for the help!. Happy to provide any more info than the below if that's helpful. I'm running with cromwell 84. Here's the command I'm running `java -Dconfig.file=workflow/cromwell.conf -jar utilities/cromwell-84.jar run workflow/expanse_workflow.wdl`. Here's my configuration (ignore the SLURM part, I'm not using it yet). Potentially important bits:; * I'm running with the local backend.; * I'm using symlink caching so that should be fast, with path+timestamp hash codes so the whole file doesn't need to be read; * I'm using the file based Hsql database. I don't see why that should matter, but maybe it does.; ```; # See https://cromwell.readthedocs.io/en/stable/Configuring/; # only use double quotes!; include required(classpath(""application"")). system {; abort-jobs-on-terminate = true; io {; number-of-requests = 30; per = 1 second; }; }. ## file based persistent database; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 120000; numThreads = 1; }; }. call-caching {; enabled = true; }. backend {; default = ""Local""; providers { ; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10; run-in-background = true; submit = ""/usr/bin/env ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:899,hash,hash,899,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['hash'],['hash']
Security,"Hi, . I have a question/require help. When implementing call caching and using a scatter, if a task/shard fails does cromwell restart then entire task?. Example. I am running ~200 alignments creating ~200 bam files, rather large ones, say if 190 complete and the last one fails, when I restart I am seeing that the entire 200 bam alignments are run again. Is it possible to have cromwell's call caching resume after the 190 completed alignments as opposed to rerunning them?. Has anyone come across this? I am thinking it might be possible using the following parameters. 1. - ContinueWhilePossible = true; 2. - System.file-hash-cache = true; 3. - System.graceful-server-shutdown = true. We are currently trying this but I wanted to see if anyone has come across this as well?. Thanks for your help!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5966:624,hash,hash-cache,624,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5966,1,['hash'],['hash-cache']
Security,"Hi, I added a recipe for cromwell to the bioconda channel:; https://github.com/bioconda/bioconda-recipes/tree/master/recipes/cromwell. Updating it is currently a manual affair, mostly consisting of updating the package source, version number, and hash. It would be nice if Travis could update the recipe automatically upon successful builds of new tagged releases. We do this in our viral-ngs recipe, and render a [jinja2 template](https://github.com/broadinstitute/viral-ngs/tree/master/packaging/conda-recipe) based on conda requirement files, though we [publish](https://github.com/broadinstitute/viral-ngs/blob/master/travis/deploy.sh) to a separate channel on anaconda.org, and not bioconda. Is automatic deployment of a cromwell conda package something the team would support?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2258:247,hash,hash,247,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2258,1,['hash'],['hash']
Security,"Hi, I am using cromwell-59.jar, and run in local backend mode. ; Used command:; `java -Dconfig.file=cromwell.examples.conf -jar ~/softwares/cromwell-59.jar run example.wdl -i input_detail.json`. however, when I try to re-run a successed workflow to validate the cache calling function, I got the following information that confused me. very much. Would you be kind to give me some ideas on what's going on? . `a588e03e-a4fc-4809-b5f5-bb540cac9ca3-EngineJobExecutionActor-rnaseq_pipeline.fastp:NA:1 [UUID(a588e03e)]: Could not copy a suitable cache hit for a588e03e:rnaseq_pipeline.fastp:-1:1. No copy attempts were made.`. following is the source code related I fetched, but still cannot understand it. `if (data.cacheHitFailureCount > 0) {; val totalHits = data.cacheHitFailureCount; val copyFails = data.failedCopyAttempts; val blacklisted = totalHits - copyFails; workflowLogger.info(; s""Could not copy a suitable cache hit for $jobTag. "" +; s""EJEA attempted to copy $totalHits cache hits before failing. "" +; s""Of these $copyFails failed to copy and $blacklisted were already blacklisted from previous attempts). "" +; s""Falling back to running job.""; ); val template = s""BT-322 {} cache hit copying failure: {} failed copy attempts of maximum {} with {}.""; log.info(template, jobTag, data.failedCopyAttempts, callCachingParameters.maxFailedCopyAttempts, data.aggregatedHashString); } ; else {; log.info(s""BT-322 {} cache hit copying nomatch: could not find a suitable cache hit."", jobTag); workflowLogger.info(""Could not copy a suitable cache hit for {}. No copy attempts were made."", jobTag); }`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6484:249,validat,validate,249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6484,1,['validat'],['validate']
Security,"Hi, in order to have access to all akka features (e.g. ask-Pattern) it would be good to use the latest version of the framework for Cromwell (as per today 2.4.8)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1188:21,access,access,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1188,1,['access'],['access']
Security,"Hi, is it possible to invalidate cache with timeout? I am asking because we do not keep the results of calls infinitely, only for 6 weeks. I assume that cache will be kept in DB and call will try to copy a directory that is corrupted (we delete files but not directory structure). Otherwise we would have to access DB and remove calls manually. Rafal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5174:308,access,access,308,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5174,1,['access'],['access']
Security,"Hi,. I have built a WDL workflow which works well with SLURM but now I am trying to get it to be able to be run on a standalone server. . I have Slurm as my provider and have created one for Local. ` Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. run-in-background = true; exit-code-timeout-seconds = 300; workflow-reset = true; read_from_cache = true; write_to_cache = true; system.file-hash-cache=true; concurrent-job-limit = 2. runtime-attributes = """"""; String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg""; """""". submit = ""singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}"". filesystems {; local {; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; } ## end local; } ## end file systems; } ## end config; } ## End Local`. Oddly, when running the workflow I get a submit docker error. ie. as per below. I have no idea why it's looking for docker as I'm not knowingly using it. I'm not using docker in my run time parameters. I have been able to get standalone working on another workflow by passing a singularity container to each task command output but I was wondering if there was a more elegant solution I could use such as just changing to a pre-made provider. I have searched Google and through here but not found anything. I did find one issue here but they seemed to want to use docker where as I don't. . Thanks for the help!. `task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_scri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862:447,hash,hash-cache,447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862,1,['hash'],['hash-cache']
Security,"Hi,. I just moved to a new cluster (no sudo) and try to run a WDL script with Cromwell-31.; Everything worked fine on my previous cluster (same Cromwell / WDL / Java versions and same script). After creating the wdl script, I validated it and generated a JSON file (with wdl-0.14), no problem. After running `java -jar cromwell-31.jar run my_script.wdl -i my_script.JSON` the workflow stops, outputting `Permission denied` for the program I call in my .wdl script (which is called from the 'cromwell_executions' folder during the process). I changed the permission to 777 for this program located in my `/usr/bin`, but still the same issue. The program, once located in the 'cromwell_executions' folder, loses the execution permission for the owner.; Same issue no matter if I submit a PBS job or run it interactively. Is there anything to mention in a cromwell configuration file or something to tune in the cluster?. Thanks !",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3500:226,validat,validated,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3500,1,['validat'],['validated']
Security,"Hi,. I was wondering if there's any interest to support a new backend for submitting jobs to a Hashicorp Nomad cluster?; I suppose this will be fairly similar to the AWS batch system. Thanks; Matthias. ping @tomiles",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6001:95,Hash,Hashicorp,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6001,1,['Hash'],['Hashicorp']
Security,"Hi,. I'm finding that Cromwell is using too many resources in a shared environment. It's spawning other threads and using up to 400% CPU, which I can't see why when the logs just show it's watching for jobs (maybe it's trying to hash files for call-caching?). I'd love a way to limit it, even at the expense of speed of the program, it would also be great if there was tooling (maybe an endpoint) to gauge the resource usage, and see what Cromwell's actually doing. For context, I'm running Cromwell on a (shared) login node, submitting jobs to Slurm (custom SFS config). The workflow is scattering a subworkflow 25 times, each with 4 steps. All of those have actually executed prior and it's taking a long time (sometimes > 30 minutes) to look up a call result. I am using call-caching with a mysql database.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4945:229,hash,hash,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4945,1,['hash'],['hash']
Security,"Hi,. What is the way to specify resource requirements (cores, ram etc) to AWSBatch? . I am executing a CWL workflow with AWS batch backend. ; Each task is submitted by cromwell and I can verify on AWS console that all jobs ended successfully.; However, for jobs that I have set coresMin and coresMax requirements I get the warning:. ```; [warn] AwsBatchAsyncBackendJobExecutionActor [6bd79e09fastqc_1:NA:1]: Unrecognized runtime attribute keys: cpuMax; ```. and at the end of the workflow the error:; ```; [error] Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:532,validat,validation,532,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,7,"['Validat', 'validat']","['ValidatedRuntimeAttributesBuilder', 'validation']"
Security,"Hi,; I am trying to run a workflow on AWS Batch using the genomics-ami.; The ami was built following the instructions in the relevant pages and i have confirmed that it contains a /cromwell-root mount point and has rw access to the bucket we use.; The AWS batch backpoint was tested with the hello.wdl workflow and it went through. When running the workflow on the local filesystem it completes without errors but when running it using the AWS Batch backend the first step fails with the following error:. ```; 2019-01-11 20:27:06,80] [error] WorkflowManagerActor Workflow 8fa7a9e4-f30d-4c19-b8cb-68be6442f317 failed (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; Caused by: java.io.IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4542:218,access,access,218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542,1,['access'],['access']
Security,"Hi,; I am using cromwell 51. I am currently in the process of moving my WDL scripts from draft-2 to 1.0. When I try to access metadata the `inputs` key is different between versions of WDL:; draft-2:; ```; ""inputs"": {; ""hello.another_input"": ""Test"",; ""hello.input_json"": ""xxx"",; ""hello.name"": ""John""; }; ```; 1.0; ```; ""inputs"": {; ""another_input"": ""Test"",; ""input_json"": ""xxx"",; ""name"": ""John""; }; ```. The workflow name is removed. I searched docs and source code but I could not find any info about this. Is there any info about it and if this is correct behaviour?. Edit.; I do not have simple example but when there is a subworkflow the inputs for subworkflow have correct workflow suffix which is inconsistent. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5563:119,access,access,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5563,1,['access'],['access']
Security,"Hi-. The HuBMAP consortium has been implementing some workflows in CWL and running these via `cwltool` -- we're quite interested in storing the provenance information for a workflow run in Research Object format. This would include the inputs and outputs for a certain run, in addition to (a normalized version of) the workflow itself. This is already implemented in `cwltool` and accessible through its `--provenance` flag; is anything like this planned for Cromwell?. Some of the HuBMAP tissue mapping centers are interested in or have been using pipelines written in WDL (e.g. [ENCODE's ATAC-seq pipeline](https://github.com/ENCODE-DCC/atac-seq-pipeline), and we would like to support these without giving up the ability to store workflow run provenance in a standard format. Is anything like this planned for Cromwell? I didn't see anything in the issue/forum/PR searches I've been doing. Thank you!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5052:381,access,accessible,381,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5052,1,['access'],['accessible']
Security,"Hi. I'm trying to enable call caching using a local file database and I can't seem to get it to work. Everything that I try does not seem to make a difference, and each run always starts from the first task. I'm running cromwell in run mode from the command line, and I am testing on both cromwell 43 and cromwell 47. I also have write-to-cache and read-from-cache set to true in my options.json (although I understand that is the default behaviour). I am unable to use a mySQL or postgres database at this current time. Is there something that I'm missing? Is there any additional information that is needed to help diagnose this?. My cromwell.conf is as follows:. backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 600. runtime-attributes = """"""; Int cpus; Float memory_mb; String lsf_queue; String lsf_project; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpus} \; -R rusage[mem=${memory_mb}] \; /usr/bin/env bash ${script}; """""". job-id-regex = ""Job <(\\d+)>.*"". kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=100000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 86400000; numThreads = 1; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5370:1381,hash,hashing-strategy,1381,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5370,1,['hash'],['hashing-strategy']
Security,How do I access the JIRA board? Getting an access denied?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6505:9,access,access,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6505,2,['access'],['access']
Security,How do I set docker hash with cache calls?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:20,hash,hash,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['hash'],['hash']
Security,"I am developing a pipeline and want to share google storage buckets for genome data (bowtie2 index tar ball and other big files) with users (Google authenticated) but want users to pay for the network traffic to download genome data. My pipeline works fine if storage bucket for genome data is set as ""owner pays"". But if I set it ""requester pays"" then I get the following error `BadRequestException: 400 Bucket is requester pays bucket but no user project provided`. I googled it and found that [`gsutil` must use JSON API (not CLI)](https://cloud.google.com/storage/docs/requester-pays) for ""requester pays"" buckets. Is there any plan to support ""requester pays"" buckets for JES backend? . ```; [2017-11-18 16:01:09,72] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: python $(which encode_bowtie2.py) \; /cromwell_root/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/d57a5f97-8542-4fcc-89c4-b7c487957dea/call-trim_adapter/shard-0/glob-019a547c7b0dda79121d0398158a07d0/ENCFF439VSY.trim.merged.R1.fastq.gz \; \; --multimapping 4 \; \; --nth 4; [2017-11-18 16:01:09,72] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:1:1]: python $(which encode_bowtie2.py) \; /cromwell_root/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/d57a5f97-8542-4fcc-89c4-b7c487957dea/call-trim_adapter/shard-1/glob-019a547c7b0dda79121d0398158a07d0/ENCFF463QCX.trim.merged.R1.fastq.gz \; \; --multimapping 4 \; \; --nth 4; [2017-11-18 16:01:18,97] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: job id: operations/EJSf0Yz9Kxjs8__E9aKWivQBILWN-vrbGyoPcHJvZHVjdGlvblF1ZXVl; [2017-11-18 16:01:18,97] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:1:1]: job id: operations/EJWg0Yz9KxiXpeC4gsSenC4gtY36-tsbKg9wcm9kdWN0aW9uUXVldWU; [2017-11-18 16:01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2916:148,authenticat,authenticated,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2916,1,['authenticat'],['authenticated']
Security,"I am encountering call caching issues with images from google artifact registry (gar). . When I use image directly from dockerhub or gcr I have no call caching issues and see this in the logs. > 2024-07-19 14:35:24 cromwell-system-akka.dispatchers.engine-dispatcher-26890 INFO - BT-322 61ba2acc:garTest.simpleLs:-1:1 cache hit copying nomatch: could not find a suitable cache hit.; 2024-07-19 14:35:24 cromwell-system-akka.dispatchers.engine-dispatcher-26890 INFO - 61ba2acc-4274-423b-818a-8cf1da67cd44-EngineJobExecutionActor-garTest.simpleLs:NA:1 [UUID(61ba2acc)]: Could not copy a suitable cache hit for 61ba2acc:garTest.simpleLs:-1:1. No copy attempts were made. However, when I copy the same image to my access controlled google artifact registry I get this authentication error. > 2024-07-19 14:31:44 cromwell-system-akka.dispatchers.engine-dispatcher-3006 WARN - BackendPreparationActor_for_f20da4b8:garTest.simpleLs:-1:1 [UUID(f20da4b8)]: Docker lookup failed; java.lang.Exception: Failed to get docker hash for us-central1-docker.pkg.dev/xxx/yyy/aaa Request failed with status 403 and body {""errors"":[{""code"":""DENIED"",""message"":""Unauthenticated request. Unauthenticated requests do not have permission \""artifactregistry.repositories.downloadArtifacts\"" on resource \""projects/xxx/locations/us-central1/repositories/yyy\"" (or it may not exist)""}]}. The workflow completes successfully regardless of this error but call caching doesn't work when a gar image is used.; The service account I am using with the cromwell server has ""Artifact Registry Reader"" IAM role.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7473:709,access,access,709,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7473,3,"['access', 'authenticat', 'hash']","['access', 'authentication', 'hash']"
Security,"I am executing a CWL workflow with AWS batch backend. ; Each task is submitted by cromwell and I can verify on AWS console that all jobs ended successfully.; However, for jobs that I have set coresMin and coresMax requirements I get the warning:. ```; [warn] AwsBatchAsyncBackendJobExecutionActor [6bd79e09fastqc_1:NA:1]: Unrecognized runtime attribute keys: cpuMax; ```. and at the end of the workflow the error:; ```; [error] Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwel",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:1073,Validat,ValidatedRuntimeAttributesBuilder,1073,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,1,['Validat'],['ValidatedRuntimeAttributesBuilder']
Security,"I am following up on a [report](https://gatkforums.broadinstitute.org/wdl/discussion/23250/wdl-1-0-wont-let-me-call-an-optional-task-with-an-optional-input) (by someone else) filed over a year ago.; Now I have a slightly different need, that is, the task 2 will take the optional input `input_2_opt` for its required input.; So, the following is what I want to achieve. ```; version 1.0. workflow my_workflow {; input {; File input_1; File? input_2_opt; }. call task1 {; input:; input_1 = input_1; }. if (defined(input_2_opt)) {; call task2 {; input:; input_2 = input_2_opt; }; }. output {; File output_1 = task1.output_1; File? output_2 = task2.output_2; }; }. task task1 {; input{; File input_1; }; command {; echo ""Hello, world!"" > hello.txt; }; output {; File output_1 = ""hello.txt""; }; }. task task2 {; input{; File input_2; }; command {; cat ${input_2} > goodbye.txt; }; output {; File output_2 = ""goodbye.txt""; }; }. ```. Running `womtool validate` on this gives. ```; Failed to process workflow definition 'my_workflow' (reason 1 of 1): Failed to process 'call task2' (reason 1 of 1): Failed to supply input input_2 = input_2_opt (reason 1 of 1): Cannot coerce expression of type 'File?' to 'File'; ```. But like in the original post, if I take out the version specification and the `input` braces in the workflow and tasks, womtool thinks the WDL is OK. Can you please explain what is the cause? And is there a solution on my end?. Thanks. ----------------------; ### The Jira interface is way too overwhelming ; ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5354:946,validat,validate,946,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5354,1,['validat'],['validate']
Security,"I am looking to switch over from the Google Life Sciences API to Batch backend, as I recently saw that the service will be deprecated. When I look at the stable documentation, I see documentation for PAPIv2 and I can only access GCPBatch documentation on the development docs. https://cromwell.readthedocs.io/en/develop/backends/GCPBatch/. I have tried to run a few test workflows using GCPBATCH backend with cromwell-85 release. Before I get too far, I wanted to inquire to see if there is full support for GCPBATCH or if this is something that will be supported in the future and we should keep using PAPIv2 for the time being?. In the cromwell.examples.conf listed providers I also do not see GCPBATCH https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf#L324C3-L341C30. While there is an example for this backend:; https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/GCPBATCH.conf. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7215:222,access,access,222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7215,1,['access'],['access']
Security,"I am testing the cromwell workflow on NAVER Cloud Platform (NCP). I tried setting using custom configuration of your documentation in github (https://github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends), but I failed. I would like to set configuration of backend including docker container and object storage (comparable with AWS S3) to the same level as AWS or GCP configuration in cromwell documentation. How can I do this?. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6548:1331,PASSWORD,PASSWORDS,1331,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6548,1,['PASSWORD'],['PASSWORDS']
Security,"I am trying to disable task-level call caching by using the Volatile optimization: (https://cromwell.readthedocs.io/en/stable/optimizations/VolatileTasks/#the-volatile-optimization). Since the documentation is written for Version 1.0, and I am using draft-2, I have the parameter as a string, as opposed to a boolean, as follows:. ```; meta {; volatile: ""true""; }; ```. However, while the WDL validates successfully, after running I can see that the task is still copying from the cache as opposed to no call-caching. . Does volatile keyword not work when using draft-2, or am I not using this correctly? . I am running Cromwell v52. Please let me know if there is any information I can include that would be useful.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6808:393,validat,validates,393,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6808,1,['validat'],['validates']
Security,"I am trying to disable task-level call-caching using the volatile optimization (https://cromwell.readthedocs.io/en/stable/optimizations/VolatileTasks/#backend-support), but since the documentation is written for version 1.0 and I am using draft-2, I have the ""true"" as a string as opposed to a boolean as follows:. ```; meta {; volatile: ""true""; } ; ```; More info on boolean vs string in this issue (https://github.com/broadinstitute/cromwell/issues/5476). However, I find that despite having the volatile option in my task, and the WDL successfully validating via womtools, call-caching is still enabled for the task. Is the volatile optimization only allowable on version 1.0 or am I not including it correctly in my task?. I am on Cromwell v52. Please let me know if any other information would be useful.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6811:551,validat,validating,551,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6811,1,['validat'],['validating']
Security,"I am trying to run Cromwell with docker images that were loaded with `docker load`. This means that the digests are unavailable (i.e. `<none>`). Unforutnately, this means that when looking up the image locally (i.e. when the config `docker.hash-lookup.method=""local""` is used), the image is not found. The offending lines of code are:; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L26; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L78-L92. Can we instead use the image ID instead of the digest when using local images?. <details>. <summary>log output</summary>. ```; [INFO] [09/16/2019 11:07:14.821] [cromwell-system-akka.dispatchers.engine-dispatcher-40] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Not triggering log of token queue status. Effective log interval = None; [INFO] [09/16/2019 11:07:14.830] [cromwell-system-akka.dispatchers.engine-dispatcher-76] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Assigned new job execution tokens to the following groups: 2b766fe6: 1; [2019-09-16 11:07:16,20] [error] Docker pull failed; java.lang.RuntimeException: Error running: docker pull <image>; Exit code: 1; Error response from daemon: pull access denied for <image> repository does not exist or may require 'docker login': denied: requested access to the resource is denied. 	at cromwell.docker.local.DockerCliClient.$anonfun$forRun$1(DockerCliClient.scala:58); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.docker.local.DockerCliClient.forRun(DockerCliClient.scala:50); 	at cromwell.docker.local.DockerCliClient.pull(DockerCliClient.scala:37); 	at cromwell.docker.local.DockerCliClient.pull$(DockerCliClient.scala:36); 	at cromwell.docker.local.DockerCliClient$.pull(DockerCliC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5178:240,hash,hash-lookup,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178,1,['hash'],['hash-lookup']
Security,"I am trying to test my wdl and docker image on my local computer before publishing. ```; [2022-02-09 11:16:10,66] [warn] BackendPreparationActor_for_35c8738b:deseq_one_vs_all.one_vs_all:-1:1 [35c8738b]: Docker lookup failed; java.lang.Exception: Unauthorized to get docker hash aedavids/test-1vs-all-2:latest; at cromwell.engine.workflow.WorkflowDockerLookupActor.cromwell$engine$workflow$WorkflowDockerLookupActor$$handleLookupFailure(WorkflowDockerLookupActor.scala:222); ```. using docker image I see that my image exists. I am able to start the container using docker run. I also tried using the sha instead of the tag name. same error. I run Cromwell as follows; ```; $ java -jar ${WDL_TOOLS}/cromwell-74.jar run --inputs ../1vsAllTask.wdl.inputs.json ../1vsAllTask.wdl; ```. ```; $ cat ../1vsAllTask.wdl.inputs.json ; {; stuff deleted; ""deseq_one_vs_all.one_vs_all.dockerImg"": ""aedavids/test-1vs-all-2""; }; ```. ```; $ docker images |grep aedavids; aedavids/test-1vs-all-2 latest 0d33407a54e3 19 hours ago 6.28GB; ```; Any idea what my issue might be?. Andy. fyi https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team is a bad URL. I not have access to Jira on broadworkbench.atlassian.net.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6674:273,hash,hash,273,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6674,2,"['access', 'hash']","['access', 'hash']"
Security,"I cant get the sra filesystem to work. Here is the error:. ```; [2020-08-21 11:08:59,62] [info] WorkflowManagerActor Workflow dbd5cdc0-c79a-42cd-b929-56ddb1115467 failed (during InitializingWorkflowState): common.exception.AggregatedMessageException: Failed to instantiate backend filesystem:; Cannot find a filesystem with name sra in the configuration. Available filesystems: ftp, s3, gcs, oss, drs, http; 	at common.validation.Validation$ValidationChecked$.$anonfun$unsafe$2(Validation.scala:98); 	at cats.syntax.EitherOps$.valueOr$extension(either.scala:66); 	at common.validation.Validation$ValidationChecked$.unsafe$extension(Validation.scala:98); 	at cromwell.backend.BackendConfigurationDescriptor.configuredPathBuilderFactories$lzycompute(backend.scala:109); 	at cromwell.backend.BackendConfigurationDescriptor.configuredPathBuilderFactories(backend.scala:108); 	at cromwell.backend.BackendConfigurationDescriptor.pathBuilders(backend.scala:120); 	at cromwell.backend.standard.StandardInitializationActor.pathBuilders$lzycompute(StandardInitializationActor.scala:62); 	at cromwell.backend.standard.StandardInitializationActor.pathBuilders(StandardInitializationActor.scala:62); 	at cromwell.backend.google.pipelines.common.PipelinesApiInitializationActor.$anonfun$workflowPaths$2(PipelinesApiInitializationActor.scala:137); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(Abst",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793:419,validat,validation,419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793,8,"['Validat', 'validat']","['Validation', 'ValidationChecked', 'validation']"
Security,I did not add documentation for the user authentication portion because I didn't understand how it works! If somebody would like to explain it to me I can expand these docs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/532:41,authenticat,authentication,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/532,1,['authenticat'],['authentication']
Security,"I did not have any experience with this little Maven-based utility so I performed the following steps for post-upgrade verification.; ```; sdk install maven; mvn compile; mvn test; mvn package // I think this is a superset of `compile` and `test` but they all take just a few seconds so 🤷‍♂️ ; ```; Closing automatic PR https://github.com/broadinstitute/cromwell/pull/6743 in favor of this one because we can trivially upgrade to the latest version, not just a security-hotfixed older version.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6747:461,secur,security-hotfixed,461,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6747,1,['secur'],['security-hotfixed']
Security,"I do not see a good explanation in docs on how soft/hard links work in cromwell.; For instance, you say:; ```; # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; hashing-strategy: ""path""; ```; here it is not clear how to make it check BOTH file content and the path. Or if path implies that both path and file is schecked. It is also not clear what will be the difference (other than this option) between hard and soft link localization.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4077:171,hash,hash,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4077,4,['hash'],"['hash', 'hashed', 'hashing-strategy']"
Security,"I encountered this error when running a WDL:; ```message: Runtime validation failed; causedBy: ; message: Task hello has an invalid runtime attribute docker = !! NOT FOUND !!; ```. I understand that it requires a docker attribute. The issue is that the error gets found at runtime. This should be caught when validating the WDL. . The risk is that users could ""run half their tasks and only find out mid-workflow that one needs an extra parameter"" (ChrisL's words).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2932:66,validat,validation,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2932,2,['validat'],"['validating', 'validation']"
Security,"I got this error while running JES on single workflow mode. ; However, I really do not know what happened. It seems like some jobs ran fine. Cromwell hangs and won't shutdown properly after a Ctl-C... ```; [2016-10-31 19:16:34,92] [error] 5a34e38c:crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File gt_seg_file hash. Caused by 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:51); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:45); at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167); at akka.actor.FSM$class.processEvent(FSM.scala:666); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobHashingActor.scala:18); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.processEvent(EngineJobHashingActor.scala:18); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.aroundReceive(EngineJobHashingActor.scala:18); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1637:323,Hash,Hash,323,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637,2,"['Hash', 'hash']","['Hash', 'hash']"
Security,"I had `hasing-strategy` instead of `hashing-strategy` in my config and lost loads of hours trying to debug the unbearably slow caching I was experiencing, and it turns out that the entire time Cromwell was simply ignoring the config I had written because of the typo. Cromwell should emit warnings when it sees config it doesn't recognize instead of silently ignoring them.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7109:36,hash,hashing-strategy,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7109,1,['hash'],['hashing-strategy']
Security,"I have ; ```; java version ""1.8.0_172""; Java(TM) SE Runtime Environment (build 1.8.0_172-b11); ```; and I was getting ; ```; Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/hsqldb/jdbcDriver has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:763); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:73); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:368); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:362); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:361); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at com.zaxxer.hikari.HikariConfig.attemptFromContextLoader(HikariConfig.java:970); 	at com.zaxxer.hikari.HikariConfig.setDriverClassName(HikariConfig.java:480); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.$anonfun$forConfig$3(HikariCPJdbcDataSource.scala:33); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.$anonfun$forConfig$3$adapted(HikariCPJdbcDataSource.scala:33); 	at scala.Option.foreach(Option.scala:437); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:33); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21); 	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47); 	at slick.jdbc.JdbcBackend$DatabaseFactoryDef.forConfig(JdbcBackend.scala:341); 	at slick.jdbc.JdbcBackend$DatabaseFactoryDef.forConfig$(JdbcBackend.scala:337); 	at slick.jdbc.JdbcBackend$$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6830:510,secur,security,510,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6830,6,"['Access', 'Secur', 'access', 'secur']","['AccessController', 'SecureClassLoader', 'access', 'security']"
Security,"I have a following pipeline which fails validation, and I cannot see what can be wrong with it:. ```; version 1.0. ## ; # Git URL import; import ""https://raw.githubusercontent.com/DSLituiev/five-dollar-genome-analysis-pipeline/master/tasks/to_uBam.wdl"" as touBam. # WORKFLOW DEFINITION; workflow WGUnMap {; input {; File mapped_bam; String bam_base; }. call touBam.unMap {; input:; File mapped_bam=mapped_bam,; String unmapped_base=bam_base; }. # Outputs that will be retained when execution is complete; output {; File unmapped_bam = touBam.out; }; }; ```. The error is:; ```; java -jar `which womtool.jar` validate unmap.wdl; ERROR: Unexpected symbol (line 46, col 7) when parsing '_gen19'. Expected rbrace, got ""File"". File mapped_bam=mapped_bam,; ^. $call_body = :input :colon $_gen19 -> CallBody( inputs=$2 ); ```. Any ideas? Why brace is expected to be closed right after the colon?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5256:40,validat,validation,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5256,2,['validat'],"['validate', 'validation']"
Security,"I have a simple workflow with the following input cache structure:. ```json; {; ""String sampleName"": ""1B5BE2D031348FBA1A6E8624811B57E3"",; ""File reference"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reference_bwt"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reads"": [; ""fa22ef528d4abd40315c885e784ff6c2"",; ""df337314b38af64554899eb5ebe81c74""; ]; }; ```. When I rerun the workflow I purely get a `cacheMiss`, but the metadata comparison between two of the inputs gives the following error:. ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]\nFailed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]""; }; },; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]""; }; }; ]; }; }; }; ```. I presume this means that `processField` [[CallCacheDiffActor.scala#L164-L168](https://github.com/broadinstitute/cromwell/blob/8415afa3ee7ffe83",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5348:719,hash,hashes,719,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5348,2,['hash'],['hashes']
Security,"I have a wdl task that doesn't handle an optional parameter as it should.; The wdl validates (wdltool-0.8.jar), but when I submit to my server, an exception is thrown over and over again. ```; [ERROR] [01/10/2017 15:07:12.214] [cromwell-system-akka.actor.default-dispatcher-5] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-98777f84-7e; 14-4408-b31e-9b57db5d813b/WorkflowExecutionActor-98777f84-7e14-4408-b31e-9b57db5d813b/98777f84-7e14-4408-b31e-9b57db5d813b-EngineJobExecutionActor-snps.testHC:NA:1/98777f84-7e1; 4-4408-b31e-9b57db5d813b-BackendJobExecutionActor-98777f84:snps.testHC:-1:1/DispatchedConfigAsyncJobExecutionActor] DispatchedConfigAsyncJobExecutionActor [UUID(98777f84)snps.t; estHC:NA:1]: Error attempting to Execute; java.lang.UnsupportedOperationException: Could not find declaration for WdlOptionalValue(WdlStringType,None); at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:48); at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at wdl4s.Task$$anonfun$instantiateCommand$1.apply(Task.scala:108); at wdl4s.Task$$anonfun$instantiateCommand$1.apply(Task.scala:108); at scala.util.Try$.apply(Try.scala:192); at wdl4s.Task.instantiateCommand(Task.scala:108); ...; ```. Restarting the service seems to be the only way of stoppin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1830:83,validat,validates,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1830,1,['validat'],['validates']
Security,"I have a workflow where one of the inputs was created by a brand-new version of a tool, but it had a cache-hit for a run from 2 months ago. I can only assume that this is because the new input file had the exact same hash as the old input file, but because the google bucket the old input file was in is gone, I have no way to confirm this. It seems like it would be fairly trivial, and extremely helpful, for this information to be contained in the call_caching_placeholder.txt, or some equivalent file when outputs are copied. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2681:217,hash,hash,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2681,1,['hash'],['hash']
Security,"I have a working cromwell/AWS batch configuration.; I have a simple workflow called three_task_sequence.wdl which I am able to run on AWS backend, and see the outputs in s3. However, submitting this job to my cromwell server:; `curl -X POST ""http://172.20.1.67:8001/api/workflows/v1"" -H ""accept: application/json"" -F ""workflowSource=@three_task_sequence.wdl"" -F ""workflowOptions=@workflow_options.json""; `; Where workflow_options.json content is:; ```; {; ""final_workflow_outputs_dir"": ""s3://nrglab-cromwell-genomics/cromwell-execution/out_bin_test""; }. ```. I'm getting the following error at the end of the workflow cromwell log:. ````; 2019-02-28 08:30:32,167 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686:725,Access,Access,725,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686,2,['Access'],['Access']
Security,"I have tried the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) to run Cromwell on Google Cloud. I did not get very far. I have followed the long set of instructions. I have logged in with my `<google-user-id>`, I have set my own `<google-project-id>`. I have created my own bucket. I have generate my service account key with the command:; ```; gcloud iam service-accounts keys create sa.json --iam-account ""$EMAIL""; ```. Then I run the hello.wdl with the command:; ```; GOOGLE_APPLICATION_CREDENTIALS=sa.json; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```. But I get the following error:; ```; [2020-07-27 18:34:00,37] [error] PipelinesApiAsyncBackendJobExecutionActor [3d2d7a27wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.create(HttpStorageRpc.java:308); 	at com.google.cloud.storage.StorageImpl$3.call(StorageImpl.java:213); 	at com.google.cloud.storage.StorageImpl$3.call(StorageImpl.java:210); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); 	at com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at com.google.cloud.storage.StorageImpl.internalCreate(StorageImpl.java:209); 	at com.google.cloud.storage.StorageImpl.create(StorageImpl.java:171); 	at cromwell.filesystems.gcs.GcsPath.request$1(GcsPathBuilder.scala:196); 	at cromwell.filesystems.gcs.GcsPath.$anonfun$writeContent$2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594:979,access,access,979,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594,1,['access'],['access']
Security,"I have zero idea whether this is the correct place to post an issue. The gatk forums [say to post here](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team). There are messages here saying post on Jira?. I am attempting to set cromwell up to run with singularity. This is in an HPC environment with a brand new install of cromwell, where I don't have the ability to access or overwrite any global files, i.e. the application.conf file with all the defaults in it. It's a documentation issue rather than a problem with cromwell, which runs fine on the default configuration. . Documentation [here](https://cromwell.readthedocs.io/en/develop/tutorials/Containers/#singularity) suggests that I need to add code similar to that found [here](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/singularity.conf) to a config block of the backend.providers section in a configuration file similar to the file [cromwell.examples.conf](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf). Which, if you click that link, you'll see is broken. It might possibly be linked to [this issue](https://broadworkbench.atlassian.net/browse/BA-4810) on Jira?. As a result I have no idea what the conf file is supposed to look like, nor to be honest where it goes or how it's meant to be referenced. There's an issue [here](https://gatkforums.broadinstitute.org/wdl/discussion/12789/cromwell-configuration-on-slurm) which tells me I have to have ""include required(classpath(""application""))"" in the first line of the conf file, but apart from that I can't find anything on what the file should look like. . The documentation [here](https://cromwell.readthedocs.io/en/stable/tutorials/ConfigurationFiles/) and [here](https://cromwell.readthedocs.io/en/stable/Configuring/#overview) both suggest that the configuration files are for a server version of cromwell, whereas I have to run it from the command line, i.e. . ```; cromwell run <-o confi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5560:384,access,access,384,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5560,1,['access'],['access']
Security,"I ran into the following problem when upgrading from cromwell 31.1 to 36. ; It looks like in cromwell 31.1, it is possible to pass a single File to a task that expects an Array[File]. In cromwell 36 however, this gives the following error: `Failed to evaluate input 'files' (reason 1 of 1): No coercion defined from wom value(s) '""cromwell-36.jar""' of type 'File' to 'Array[File]'.`; However, both womtool 31.1 and womtool 36 give no errors when validating the workflow. I'm not sure which of the two is the correct behaviour according to the WDL spec, but I think womtool and cromwell should agree on whether or not the wdl file is valid or not. Cromwell Womtool 31. $ java -jar womtool-31.jar validate wf.wdl ; ; $ java -jar cromwell-31.1.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:07:53,81] [info] Running with database db.url = jdbc:hsqldb:mem:6b74f862-dc18-4cf1-8e2e-0b9002bba0bf;shutdown=false;hsqldb.tx=mvcc; .; .; [2019-01-15 15:08:11,54] [info] WorkflowExecutionActor-977d0c47-9cf5-4893-8dcf-465c27da13d7 [977d0c47]: Workflow wf complete. Final Outputs:; {; ""wf.F"": [[""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-0/inputs/home/redmar/devel/wdl/test/issue/cromwell-31.1.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-1/inputs/home/redmar/devel/wdl/test/issue/cromwell-36.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4550:446,validat,validating,446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550,2,['validat'],"['validate', 'validating']"
Security,"I ran it using json ways and after configuring mysql and Call Caching it is still create a new project. here is my commands and config file. ```; java -jar -Dconfig.file=/work/share/ac7m4df1o5/bin/cromwell/3_config/udocker_slum.conf ../cromwell-84.jar run /work/share/ac7m4df1o5/bin/cromwell/1_pipeline/Exome_Germline_Single_Sample/ExomeGermlineSingleSample_v3.1.5.wdl -i D5327.NA12878.json -o ../options.json; ```. conf is. ```; include required(classpath(""application"")). docker {; hash-lookup {; enable = false; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }. backend {; default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"" ; config {; 	concurrent-job-limit = 5; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -t ${runtime_minutes} \; 	 -p wzhcexclu06 \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # Pull the image using the head node, in case our workers don't have network access; # udocker pull ${docker}. sbatch \; -J ${job_name} \; -D ${cwd} \; -t ${runtime_minutes} \; 	 -p wzhcexclu06 \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }. ```. help pleas",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6920:484,hash,hash-lookup,484,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6920,2,"['access', 'hash']","['access', 'hash-lookup']"
Security,"I recently attempted to run a workflow with files in a separate non public google project as inputs. I submitted it using both Swagger and curl but neither gave very informative error messages. Here's the curl command and output:. ```; curl -v https://cromwell.gotc-dev.broadinstitute.org/api/workflows/v1 -F wdlSource=@PairedSingleSampleWf.wdl -F workflowInputs=@ExampleOfIncorrectProject.json; * Trying 104.197.140.34...; * Connected to cromwell.gotc-dev.broadinstitute.org (104.197.140.34) port 443 (#0); * TLS 1.2 connection using TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384; * Server certificate: cromwell.gotc-dev.broadinstitute.org; * Server certificate: InCommon RSA Server CA; * Server certificate: USERTrust RSA Certification Authority; * Server certificate: AddTrust External CA Root; > POST /api/workflows/v1 HTTP/1.1; > Host: cromwell.gotc-dev.broadinstitute.org; > User-Agent: curl/7.43.0; > Accept: */*; > Content-Length: 38237; > Expect: 100-continue; > Content-Type: multipart/form-data; boundary=------------------------c2956aa00c34148a; > ; < HTTP/1.1 100 Continue; < HTTP/1.1 500 Internal Server Error; < Date: Fri, 18 Mar 2016 17:59:04 GMT; < Server: spray-can/1.3.2; < X-Frame-Options: SAMEORIGIN; < Access-Control-Allow-Origin: *; < Access-Control-Allow-Headers: authorization,content-type,accept,origin; < Access-Control-Allow-Methods: GET,POST,PUT,PATCH,DELETE,OPTIONS,HEAD; < Access-Control-Max-Age: 1728000; < Content-Type: text/plain; charset=UTF-8; < Content-Length: 69; < Connection: close; < ; * Closing connection 0; The server was not able to produce a timely response to your request.%; ```. The issue was that I was using the broad-gotc-dev project to run the workflow, but two of my input files were in broad-gp-gotc-pilot, but I had trouble figuring that out from this error message. Happy to provide the wdl and json files as well.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/589:583,certificate,certificate,583,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/589,9,"['Access', 'authoriz', 'certificate']","['Access-Control-Allow-Headers', 'Access-Control-Allow-Methods', 'Access-Control-Allow-Origin', 'Access-Control-Max-Age', 'authorization', 'certificate']"
Security,"I tried out underscore 1.13.1 to address a [dependabot alert](https://github.com/broadinstitute/cromwell/security/dependabot) for a [CVE](https://github.com/broadinstitute/cromwell/security/dependabot/cwl/src/test/resources/cwl/underscore.js/underscore/open), but unfortunately this crashes with the default `null` value of `self` in CWL. At least for the `template` function, lodash is a drop-in replacement for underscore.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6343:105,secur,security,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6343,2,['secur'],['security']
Security,"I validated `cnv_param_sweep.wdl` and received this error:; ```; ERROR: Missing value or call: Couldn't find value or call with name 'CollectAllelicCountsNormal' in workflow (line 136):. Int model_segments_disk = ceil(size(DenoiseReadCountsTumor.denoised_copy_ratios, ""GB"")) + ceil(size(CollectAllelicCountsTumor.allelic_counts, ""GB"")) + ceil(size(CollectAllelicCountsNormal.allelic_counts, ""GB"")) + disk_pad; ```; `cnv_param_sweep.wdl` imports 3 WDLs, and the error was actually in `cnv_somatic_pair_workflow.wdl`. It would be great to know what WDL the error is in, especially if it's not the primary WDL.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3055:2,validat,validated,2,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3055,1,['validat'],['validated']
Security,"I was writing a wdl with this line in the task definition. `Float vaf = .01`. and was getting this error when validating . `Error: Invalid WDL: ERROR: Unexpected symbol (line 28, col 16) when parsing 'e'. Expected identifier, got 01. Float vaf = .01 ^ $e = :identifier <=> :dot :identifier -> MemberAccess( lhs=$0, rhs=$2 )`. if I change it to; ; `Float vaf = 0.01`. it's fine but I think looking at the spec the first iteration should work too - https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. `$float = (([0-9]+)?\.([0-9]+)|[0-9]+\.|[0-9]+)([eE][-+]?[0-9]+)?`. Easily worked around just a little annoying. This was run on a FireCloud method so I assume its one of the more recent versions? (cromwell 34) but 🤷‍♂️",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4089:110,validat,validating,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4089,1,['validat'],['validating']
Security,"I'm getting an error when trying to run the following WDL, which is using an `Object` type for the output of one of the tasks: https://github.com/HumanCellAtlas/pipeline-tools/blob/master/adapter_pipelines/smart_seq2/adapter.wdl#L46. This WDL previously worked in Cromwell 29. The WDL fails immediately on validation with this error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Some([Declaration type=Object name=prep.inputs expr=Some(prep.inputs)]) (of class scala.Some)""; }; ],; ""message"": ""Workflow input processing failed""; }; ]; ```. We're relying on objects in our HCA pipelines so it would be great if this could get fixed soon!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3060:306,validat,validation,306,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060,1,['validat'],['validation']
Security,"I'm having trouble getting call caching to work with Singularity and SGE, and I'm wondering if anyone has a working example config or some pointers. My config is below, minus passwords and specific paths/urls, which I've replaced with a label encased in <>. I've tried switching to slower hashing strategies finagling with the command construction to no avail. If there's not an obvious solution, is there an easy way to debug this? There are no network issues preventing connections to dockerhub - pulling images and converting to .sif works fine. It's only call caching that's broken. Even when I see, in the metadata, identical hashes for the docker image and all inputs and outputs, I see a ""Cache Miss"" as the result, every time. . The call caching stanza in my metadata looks like this, for example. Am I missing something? ; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hashes"": {; ""output count"": ""C4CA4238A0B923820DCC509A6F75849B"",; ""runtime attribute"": {; ""docker"": ""4B2AB7B9EA875BF5290210F27BB9654D"",; ""continueOnReturnCode"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""failOnStderr"": ""68934A3E9455FA72420237EB05902327""; },; ""output expression"": {; ""File output_greeting"": ""DFC652723D8EBD4BB25CAC21431BB6C0""; },; ""input count"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""backend name"": ""2A2AB400D355AC301859E4ABB5432138"",; ""command template"": ""AFAC58B849BD67585A857F538B8E92F6""; },; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hit"": false,; ""result"": ""Cache Miss""; },; ```. ```; # simple sge apptainer conf (modified from the slurm one); #; workflow-options; {; workflow-log-dir: ""cromwell-workflow-logs""; workflow-log-temporary: false; workflow-failure-mode: ""ContinueWhilePossible""; default; {; workflow-type: WDL; workflow-type-version: ""draft-2""; }; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; metadata {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql:<dburl>?rewriteBatchedStatements=true""; driver = ""com.mysql.cj.jdb",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:175,password,passwords,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,3,"['hash', 'password']","['hashes', 'hashing', 'passwords']"
Security,"I'm running cromwell 46 with AWS, and having problems with call caching ... 2019-09-30 15:37:20,124 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Failed to hash ""s3://bdtx-scratch/Andrei/bdtx_dataset_1.00_anno_column_description.txt"": [Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null); 2019-09-30 15:37:20,125 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - 66419bab:count_lines.countLines:-1:1: Hash error ([Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null)), disabling call caching for this job. call caching settings:; call-caching {; enabled = true; invalidate-bad-cache-results = false; }. Thanks. ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5204:172,hash,hash,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5204,3,"['Hash', 'PASSWORD', 'hash']","['Hash', 'PASSWORDS', 'hash']"
Security,"I'm trying to pass default-runtime-attributes as following:; ```; default-runtime-attributes {; failOnStderr: false; continueOnReturnCode: 0; resource_queue_id: ""q-20230616171245-5j76z""; credential: {; accessKey: ""<secret>"",; secretKey: ""<secret>""; }; }; ```. however, when Cromwell trying to convert the map into womValue, it just turn into BadDefaultAttribute().; ![image](https://github.com/broadinstitute/cromwell/assets/32162780/0030d819-3f94-43a4-bca8-63bca01ff2b5); it seems like the; ```; val value = config.getValue(key).unwrapped(); ```; here it will return a java hashMap, but when it comes to coercion function of WomMapType, It will only works for Scala immutable Map.; ```; case class WomMapType(keyType: WomType, valueType: WomType) extends WomType {; val stableName: String = s""Map[${keyType.stableName}, ${valueType.stableName}]"". override protected def coercion = {; case m: Map[_, _] if m.nonEmpty => WomMap.coerceMap(m, this); case m: util.HashMap[_,_] if !m.isEmpty => WomMap coerceMap(m.asScala.toMap,this)// I add this ; case m: Map[_, _] if m.isEmpty => WomMap(WomMapType(keyType, valueType), Map()); case js: JsObject if js.fields.nonEmpty => WomMap.coerceMap(js.fields, this); case womMap: WomMap => WomMap.coerceMap(womMap.value, this); case o: WomObjectLike => WomMap.coerceMap(o.values, this); }; ```; I add a transformation here and it works. Not sure it's a bug or this is intended",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7174:202,access,accessKey,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7174,3,"['Hash', 'access', 'hash']","['HashMap', 'accessKey', 'hashMap']"
Security,"I'm trying to run a WDL file which specifies a Docker container in the runtime attributes of a task. I'm trying to do so on an SGE HPC with a shared file system. Running this with docker is not an option because I don't have root rights/access. Instead, I'm trying to run it using Singularity. This works fine when using a custom runtime attribute to define the container and using the `submit` configuration. However, if the `docker` attribute and `submit-docker` configuration are used I run into a problem:; Cromwell will use `docker_cwd` (instead of `cwd`) in the call's script. `docker_cwd` however does not exist in the container and can therefore not be mounted (due to a lack of sudo rights), like you would normally do when using Docker. The result is that the job will fail because it can't find a folder that is referenced in the script. Is there some way for me to override the `docker_cwd` value in my backend configuration? I would prefer not to use the `submit` configuration, as not all tasks currently list a container, so I need the `submit` configuration for those tasks. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4084:237,access,access,237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4084,2,"['PASSWORD', 'access']","['PASSWORDS', 'access']"
Security,"I/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam -> /mnt/loc; al-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam (cp failed: gsutil -q -m cp gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga; /STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-01A-21D-A25D-08.1.bam,; command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/s; hare/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205, in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1221, in Get\n required)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py\"", line 155, in TryFunc\n return func(*args, **kwargs), None\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 41, in _ReadNoP",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:6913,validat,validate,6913,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['validat'],['validate']
Security,"If a WDL task generates a file with a space in its name, and that file is an output, Cromwell fumbles the outputs and throws an error (at least on GCP-Terra!Cromwell). Additionally, this doesn't seem to be logged clearly. This workflow takes in a bunch of BioSample accessions, downloads their associated run FASTQs, and processes them. https://dockstore.org/workflows/github.com/aofarrel/myco/myco_sra:4.1.2?tab=files. During one run, I accidentally passed in a file of BioSample accessions which had two spaces before each accession, eg; ```; SAMEA104027315; SAMEA104027345; SAMEA104027406; SAMEA104164787; SAMEA104172469; SAMEA104172474; SAMEA104172508; SAMEA104221066; SAMEA104362398; SAMEA104394395; SAMEA104394505; SAMEA104414628; SAMEA104446901; ```. The workflow is scattered per BioSample, so one instance of the scattered task takes in ` SAMEA104027315` as the input `biosample_accession` (type String). The task writes a file like this:. ```; echo ""~{biosample_accession}"" >> ~{biosample_accession}_pull_results.txt; ```; eg ` SAMEA104027315_pull_results.txt`. The workflow output section contains:. ```; String results = read_string(""~{biosample_accession}_pull_results.txt""); ```. eg ` SAMEA104027315_pull_results.txt`, same as what's in the command section. . In the task level logs, I see . ```; 2023/04/18 21:54:34 Starting delocalization.; 2023/04/18 21:54:35 Delocalization script execution started...; 2023/04/18 21:54:35 Delocalizing output /cromwell_root/memory_retry_rc -> gs://fc-caa84e5a-8ef7-434e-af9c-feaf6366a042/submissions/93bf6971-bfa1-4cb8-bb22-c8a753f58c49/myco/10fa31a8-acbe-4ab7-a96a-6550ec08df12/call-pull/shard-0/memory_retry_rc; 2023/04/18 21:54:37 Delocalizing output /cromwell_root/rc -> gs://fc-caa84e5a-8ef7-434e-af9c-feaf6366a042/submissions/93bf6971-bfa1-4cb8-bb22-c8a753f58c49/myco/10fa31a8-acbe-4ab7-a96a-6550ec08df12/call-pull/shard-0/rc; 2023/04/18 21:54:39 Delocalizing output /cromwell_root/stdout -> gs://fc-caa84e5a-8ef7-434e-af9c-feaf6366a042/submis",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7121:266,access,accessions,266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7121,3,['access'],"['accession', 'accessions']"
Security,"If a java.util.concurrent.ExecutionException, check if it has an inner cause that is actually fatal.; Pre-update to SBT 1.x.; The cromwell multi-project with SBT 1.x consumes a lot of memory. To avoid OOME:; - Bump IntelliJ from 768 to something like 1786.; - Bump .sbtopts to something like 3072. Build updates while prepping fo sbt version bump, including using sbt-doge temporarily for simplified SBT 0.13 cross version build syntax.; DRYed out a bit of the module dependency graph.; Publish hotfix libraries, not just executables.; Replaced deprecated commons-lang with commons-text replacements.; Fixed test class errantly nested within its companion object.; Fixed test that was only liquibasing metadata, and not the original database.; Removed unused TestActorSystem.; Fixed missing test in ""common"".; Removed old ""common"" sbt files.; Cleaner version of assembly.; SwaggerUI injected into the cromwell-version.conf, so it only needs to be editted in one place.; Replaced usage of 'sbt ""project foo"" sometask' with 'sbt foo/sometask'.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2821:883,inject,injected,883,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2821,1,['inject'],['injected']
Security,"If call cache writing is disabled and a cache miss is known, we could update the various File Hasher actors and tell them not to bother computing no-longer-required file hashes. - [X] Depends on #1307",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1316:94,Hash,Hasher,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1316,2,"['Hash', 'hash']","['Hasher', 'hashes']"
Security,"If you use the example in the docs at https://github.com/broadinstitute/cromwell#database. you'll get a warning:. `Move the configuration directly under the 'database' element, and remove the key 'database.config'.`. The docs should be updated. Below is the config given in the readme. database {; config = main.mysql. main {; mysql {; db.url = ""jdbc:mysql://localhost:3306/cromwell""; db.user = ""root""; db.password = """"; db.driver = ""com.mysql.jdbc.Driver""; db.connectionTimeout = 5000 # NOTE: The default 1000ms is often too short for production mysql use; driver = ""slick.driver.MySQLDriver$""; }; }. test {; ...; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1730:406,password,password,406,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1730,1,['password'],['password']
Security,"Immutable Docker hash request keys to guard against credential mutations breaking lookups, more graceful handling of actual failure cases.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5230:17,hash,hash,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5230,1,['hash'],['hash']
Security,"Implement any necessary logic to calculate hashes. Follow the pattern of the FileHasherActor: The EJHA creates an actor which can make the hash. This actor sends each hash result back to the EngineJobHashingActor individually and lets it do the magic of combining everything together.; - [x] command string (one of the initial hashes in `EJHA`); - [x] all outputs expressions (one of the initial hashes in `EJHA`); - [x] all non-file inputs (included in the set of initial hashes in `EJHA`); - [x] dockerhub/GCR hash (`EngineDockerHashingActor`) or docker name (tuple3 of (namespace, repository, tag, one of the initial hashes in `EJHA`) depending on configuration; - [x] runtime attributes (`BackendRuntimeAttributeHashingActor`); - [x] file inputs (`FileHasherActor`, and onwards to GCS, SFS specific `XYZFileHasherActor`s)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1230:43,hash,hashes,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1230,8,['hash'],"['hash', 'hashes']"
Security,Implement call cache hash reading,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1232:21,hash,hash,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1232,1,['hash'],['hash']
Security,Implement call caching hashing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1230:23,hash,hashing,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1230,1,['hash'],['hashing']
Security,Implement hashing for JES PBE,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/809:10,hash,hashing,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/809,1,['hash'],['hashing']
Security,Implement the /describe endpoint as described in design docs. . The goal of this work is to make it easier for services like FireCloud & Saturn to adopt & expose new WDL language features more easily/quickly.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4441:155,expose,expose,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4441,1,['expose'],['expose']
Security,"Implement the `/describe` endpoint as described in the doc and discussed with @ruchim. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4333:841,PASSWORD,PASSWORDS,841,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4333,1,['PASSWORD'],['PASSWORDS']
Security,Implement the validate functionality for /describe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4433:14,validat,validate,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4433,1,['validat'],['validate']
Security,Implementation details exposed in normal command-line output,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4062:23,expose,exposed,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062,1,['expose'],['exposed']
Security,Improve AWS Integration:. - Docker Hub Authentication; - awsBatchRetryAttempts; - ulimits; - Call Caching with ECR private ([geertvandeweyer](https://github.com/geertvandeweyer) and [markjschreiber](https://github.com/markjschreiber/cromwell/)); - revised localization functions to improve stability ([geertvandeweyer](https://github.com/geertvandeweyer)); - Extra failure handling for Batch ([geertvandeweyer](https://github.com/geertvandeweyer)); - AWS/Batch error handling improvements ([geertvandeweyer](https://github.com/geertvandeweyer)); - Correct retry logic for spot kills ([geertvandeweyer](https://github.com/geertvandeweyer)); - handling of very rare early/late job killing ([geertvandeweyer](https://github.com/geertvandeweyer)); - Sychronize multipart uploads between callcache and jobscripts ([geertvandeweyer](https://github.com/geertvandeweyer)),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6835:39,Authenticat,Authentication,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6835,1,['Authenticat'],['Authentication']
Security,Improve output of womtool validate,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4040:26,validat,validate,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4040,1,['validat'],['validate']
Security,"Improves our ""munge this NIO path string into what Cromwell expects"" logic to handle paths that were initially passed into the Azure NIO library as full `http://` paths. I also added a config path (unpublished, intended for devs only) for passing a token through to TES requests. This enables us to locally submit work to a TES server in a (modified to expose TES) instance of the Azure app.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7138:353,expose,expose,353,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7138,1,['expose'],['expose']
Security,"In Google Compute Engine, one can create custom networks and even delete the default network. ; Docs: https://cloud.google.com/vpc/docs/using-vpc; List of networks: https://console.cloud.google.com/networking/networks/list. This is commonly done for projects that have high security requirements to enforce firewalls etc. The ability to specify a network where operations are created is supported in v2alpha1, but there is no place to specify it in Cromwell (which always uses the ""default"" network). AC: Add an option to Cromwell's global config where a user can specify the VPC network name, for the PAPI v2 backend. This would override the current ""default"" network used by Cromwell. Testing Criteria:; - Confirm that Cromwell honors using a non-default network when specified via the config.; - If the network name specified doesn't exist, the error returned to the user contains information about 1) a link to documentation on how to create a network and 2) how to confirm a network exists through the cloud console.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4017:274,secur,security,274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4017,2,"['firewall', 'secur']","['firewalls', 'security']"
Security,"In at least one example (I have the workflow & broad accessible inputs if someone needs them) the following leads to a cache miss:. Workflow A, Task A: Output is multipart uploaded to S3 and has an etag to match; Workflow A, Task B: Consumes this file as an input. Workflow B, Task A: Cache hit. Copies that file, but receives an md5 etag; Workflow B, Task B: Cache miss - the two etags are now mismatched. It seems highly likely that a solution to this would also solve #4805",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4828:53,access,accessible,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828,1,['access'],['accessible']
Security,"In certain cases, cromwell/wdltool is unable to resolve call statements in subworkflows (cromwell v29, wdltool v0.14). This may be related to #2753. A simple case to demonstrate this:. main.wdl:; ```; import ""sub.wdl"" as sub; workflow main {; call sub.wf; }; ```. sub.wdl:; ```; workflow wf {; call a; String b = a.out; }. task a {; command {; echo ""hello""; }; output {; String out = read_string(stdout()); }; }; ```; Running the validate command against sub.wdl returns no error messages. Running validate against main.wdl returns:; ```; ERROR: Missing value or call: Couldn't find value or call with name 'a' in workflow (line 4):. String b = a.out; ^; ```; Interestingly, if I add to sub.wdl:; ```; task b {; String msg; command {; echo ${msg}; }; output {; String out = read_string(stdout()); }; }; ```; and replace String b=a.out with:; ```; call b { input: msg=a.out }; ```; it validates ok. Am I doing something wrong with the WDL? Or is there a workaround for this?. My actual use case is a bit more complex - I have a series of mutually exclusive optional cases, so I want to put the File? output from each into an Array, and then use the select_first. But when I try to create the array from the outputs:; ```; if (a_condition) {; call task_a; }; if (b_condition) {; call task_b; }; ...; Array[File?] files = [task_a.out, task_b.out ...]; File file = select_first(files); ```; I see the above manifestation/error message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2756:430,validat,validate,430,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2756,3,['validat'],"['validate', 'validates']"
Security,"In https://github.com/broadinstitute/cromwell/pull/4502 I ignored three tests related to the refresh token functionality because they were blocking all other merges and I had no idea how to fix them. My rationale for doing so is my impression the feature is unused/end-of-life. @ruchim to decide whether we; 1. Fix the tests; or; 2. Delete the feature . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4504:1108,PASSWORD,PASSWORDS,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4504,1,['PASSWORD'],['PASSWORDS']
Security,"In other words, we should validate `docker` as an optional string so that we can do things like this:. ```; task foo {; String? dockerName; command { ... }; runtime {; docker: dockerName; }; output { ... }; }; ```. The expected behaviour here is:; - If `dockerName` is set, use docker and use the specified image name; - If `dockerName` is not set, do not use docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832:26,validat,validate,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832,1,['validat'],['validate']
Security,"In the MaterializeWDA, the `validateDeclarations` method now uses `NoFunctions` which prevents evaluation of workflow level declaration with `read_string` etc...; The way of achieving this may be different in WOM-world but this functionality must remain.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2625:28,validat,validateDeclarations,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2625,1,['validat'],['validateDeclarations']
Security,"In the cromwell README...; ```wdltool 0.4```. ```; import ""sub_wdl.wdl"" as sub. workflow main_workflow {. call sub.hello_and_goodbye { input: hello_and_goodbye_input = ""sub world"" }. # call myTask { input: hello_and_goodbye.hello_output }. output {; # I believe this will cause a validation failure. This is what is currently in the README...; String main_output = hello_and_goodbye.hello_output. # I believe this will NOT cause a validation failure. ; hello_and_goodbye.hello_output; }; }; ```. My real-world example:. ```; workflow dl_ob_training {. File bam_file; File bam_file_index; File gatk_jar; File ref_fasta; File oncotated_m1; String entity_id; File createOxoGIntervalList. call CollectSequencingArtifactMetrics {; input:; entity_id=entity_id,; bam_file=bam_file,; output_location_prepend=entity_id,; gatk_jar=gatk_jar,; ref_fasta=ref_fasta; }. call CreateObIntervalList {; input:; oncotated_m1=oncotated_m1,; entity_id=entity_id,; createOxoGIntervalList=createOxoGIntervalList; }. call ExtractReadInfo {; input:; bam_file=bam_file,; bam_file_index=bam_file_index,; gatk_jar=gatk_jar,; ref_fasta=ref_fasta,; pre_adapter_file=CollectSequencingArtifactMetrics.pre_adapter_detail_metrics,; interval_list=CreateObIntervalList.interval_list,; }. output {; # VALIDATES; ExtractReadInfo.read_infos. # DOES NOT VALIDATE; Array[File] read_infos = ExtractReadInfo.read_infos; }; }; .....snip.... task ExtractReadInfo {; File gatk_jar; File pre_adapter_file; File interval_list; File bam_file; File bam_file_index; File ref_fasta. command {; java -jar ${gatk_jar} ExtractReadInfo \; -P ${pre_adapter_file} \; -L ${interval_list} \; -I ${bam_file} \; -R ${ref_fasta} \; -OP out/; }. output {; Array[File] read_infos = glob(""out/*""); }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1837:280,validat,validation,280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837,4,"['VALIDAT', 'validat']","['VALIDATE', 'VALIDATES', 'validation']"
Security,"In the example configuration, the submit-docker tries to run. docker run ... ${docker} ${script}. but ${script} will not be accessible from the docker image unless, by coincidence, the location where we are running from in the local filesystem is the same as the dockerRoot. What we want to run instead is . docker run .... ${docker} ${docker_script}. Since this is the default configuration, it has the potential to cause a lot of unnecessary confusion (eg for me)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5015:124,access,accessible,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5015,1,['access'],['accessible']
Security,"In the near future PAPI will support the ability to pass a boolean argument called `restrictMetadataAccess`. We want to expose this both via workflow option as well as the ability to set the default value to `true` (although normally the default value should be `false`). The mechanism to specify this to PAPI is still TBD, so hit me up for more details when you get to that point.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2424:120,expose,expose,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2424,1,['expose'],['expose']
Security,"Including a new setting under _""virtual-private-cloud""_ to support the **host project** in a _shared vpc environment_ and detailed here: https://cloud.google.com/vpc/docs/shared-vpc. Currently validating in our own environment.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6225:193,validat,validating,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6225,1,['validat'],['validating']
Security,Initial support for DRS (DOS) filesystem: Size and File Hash,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4456:56,Hash,Hash,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4456,1,['Hash'],['Hash']
Security,"Initially, this PR was aimed to fix issue #5085. But it turned out that there are other related issues.; It looks like [test case](https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-492445214) made by @jeremiahsavage in issue #4969 has nothing to do with a zip file subdirectories. At least his test works fine if you apply the changes made in this PR.; Another issue described in [JIRA](https://broadworkbench.atlassian.net/browse/BA-5873). Although workflows fail and throw an exception even with changes in this PR, the exception is different. The exact reason is unclear for me, but this workflow fails even when you submit a task in a server mode, so maybe something is wrong with the workflow. It turns out the problem with running CWL files on Cromwell was caused by this [PR](https://github.com/broadinstitute/cromwell/pull/3988).; Something was changed in this PR, which required changes to the [`validateSubmitArguments`](https://github.com/broadinstitute/cromwell/pull/3988/files#diff-9ed42892250e1b424f671593631297a5R174) method of the `CromwellEntryPoint` object. But since the [`validateRunArguments`](https://github.com/broadinstitute/cromwell/pull/3988/files#diff-9ed42892250e1b424f671593631297a5R201) method is almost identical to the `validateSubmitArguments` method, similar changes should have been made there too. But this was not noticed, and therefore this problem arose. Therefore, to fix the issues it is enough to use in the `validateRunArguments` method the same logic as in the `validateSubmitArguments` method.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5104:923,validat,validateSubmitArguments,923,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104,5,['validat'],"['validateRunArguments', 'validateSubmitArguments']"
Security,Inject metadata about workflow run into workflow step?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7137:0,Inject,Inject,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137,1,['Inject'],['Inject']
Security,"Inspired by https://github.com/broadinstitute/cromwell/compare/rsa_ss_crom_6443, this should stop workflows from being deleted from the workflow store before their final metadata has been written to the database. * Workflows now request confirmation on the push of their final status to metadata.; * Workflows are not removed from the WorkflowStore until that confirmation happens. Bonus:. * Workflow actor states that follow on from completion (like log copying, output copying, metadata integrity checking, ...) are now recorded as ""Finalizing"" in metadata.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6196:489,integrity,integrity,489,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6196,1,['integrity'],['integrity']
Security,"Instrumentation was scheduled on a timer out of band of the actor's thread, and in some cases accessing non thread safe state inside the actor (in a read only fashion but still it can cause incoherent values: https://stackoverflow.com/questions/37690525/multiple-threads-checking-map-size-and-conccurency); Instead use messages to self to schedule instrumentation on the actor's thread",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4402:94,access,accessing,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4402,1,['access'],['accessing']
Security,Invalid WDL validates,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2394:12,validat,validates,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2394,1,['validat'],['validates']
Security,Investigate File Hashing Strategy for JES Backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1623:17,Hash,Hashing,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1623,1,['Hash'],['Hashing']
Security,Investigate call caching file hash caching,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4096:30,hash,hash,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4096,1,['hash'],['hash']
Security,Investigate potentially premature JSON validation for workflow submission,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1882:39,validat,validation,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1882,1,['validat'],['validation']
Security,Invocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.net.SocketTimeoutException: Read timed out; 	at java.net.SocketInputStream.socketRead0(Native Method); 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); 	at java.net.SocketInputStream.read(SocketInputStream.java:171); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	at sun.security.ssl.InputRecord.read(InputRecord.java:503); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); 	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1569); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1474); 	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); 	at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2229:3807,secur,security,3807,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229,1,['secur'],['security']
Security,It is sometimes inconvenient to use both WDLTool and Cromwell. Is it possible just to make a REST call in Cromwell specifically for validating WLDs and generating inputs? For people who also write UI-s that interact with Cromwell it will make life easier.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2918:132,validat,validating,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2918,1,['validat'],['validating']
Security,"It is very common to provide folders as inputs to different bioinformatic tools. For instance, STAR index is usually computed once per reference genome and then provided to each STAR-based RNA-Seq task as an input. However, when this is done a common caching failure is reported (because it is a folder):; ```; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Is a directory""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2735:312,hash,hashFailures,312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2735,1,['hash'],['hashFailures']
Security,"It seems that the protocol for setting runtime attributes is to do so within the task, thus allowing expressions based on their values. Say,. ```wdl; task foo {; Int cpus. runtime {; cpus = cpus; }. command {; ./my_binary --threads ${cpus * 2}; }; }; ```. However, a lot of the time, it's not appropriate (in a ""separation of concerns"" sense) to thread the value through the task invocation. For example, you may be setting a Unix group under which all data should be accessed, defining credentials, etc. We're doing this by using the `default_runtime_attributes`, passed in as workflow options. However, these are not visible to the task. This is what we'd like to be able to do, for example:. Workflow Options:; ```json; {; ""default_runtime_attributes"": {; ""AUTH_USER"": ""foo"",; ""AUTH_TOKEN"": ""bar""; }; }; ```. Workflow:; ```wdl; task {; command {; export AUTH_USER=""${AUTH_USER}"" # Taken from default_runtime_attributes; export AUTH_TOKEN=""${AUTH_TOKEN}"" # Taken from default_runtime_attributes; ./my_authenticated_command; }; }; ```. At the moment, this will fail, as `AUTH_USER` and `AUTH_TOKEN` are not defined within the task. Even if you explicitly define it in the task (`String AUTH_USER`, etc.), Cromwell won't automatically seed this from the default options. I can see why it would be useful to define the inputs in the task, for clarity's sake. I'm just thinking out aloud -- so this is very much half-baked -- but perhaps an option would therefore be to have an additional keyword that made it explicit that the task value was to be taken from options:. ```wdl; task {; Int something; runtime String AUTH_USER # ""runtime"" implies this is from the runtime attributes (default or otherwise); runtime String AUTH_TOKEN # Raise an error if undefined or modified within the task. command {; export AUTH_USER=""${AUTH_USER}""; export AUTH_TOKEN=""${AUTH_TOKEN}""; ./my_authenticated_command -n ${something}; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4741:468,access,accessed,468,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4741,1,['access'],['accessed']
Security,"It seems to me that callCaching is not working when a task takes a Directory as input. Take the following example WDL:; ```; version development. workflow main {; call task1 { input: s = ""file"" }; call task2 { input: d = task1.d }; output { String s = task2.s }; }. task task1 {; input {; String s; }. command <<<; set -euo pipefail; mkdir dir; touch ""dir/~{s}""; >>>. output {; Directory d = ""dir""; }. runtime {; docker: ""debian:stable-slim""; }; }. task task2 {; input {; Directory d; }. command <<<; set -euo pipefail; ls ""~{d}""; >>>. output {; String s = read_string(stdout()); }. runtime {; docker: ""debian:stable-slim""; }; }; ```. On a first `141477ef-e8e6-4fb9-ae58-5c2e8a646088` run, callCaching for `task2` is negative, as it should, with this error:; ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [; {; ""message"": ""gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir"",; ""causedBy"": []; }; ],; ""message"": ""[Attempted 1 time(s)] - FileNotFoundException: gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Now though, the directory has been created as a result of the WDL succeeding:; ```; $ gsutil ls gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir; gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir/file; ```. On a second `2690f8a5-4cd4-45e2-a93a-55125a1107f8` run, callCaching for `task2` is negative again though, with this error:; ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [; {; ""message"": ""gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir"",; ""causedBy"": []; }; ],; ""message"": ""[Attempted 1 time(s)] - FileNotFoundException: gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir""; }; ]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6509:783,hash,hashFailures,783,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6509,1,['hash'],['hashFailures']
Security,It will be great to be able to configure Access-Control-Allow-Origin * for cromwell to be able to call it via AJAX,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2824:41,Access,Access-Control-Allow-Origin,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2824,1,['Access'],['Access-Control-Allow-Origin']
Security,"It works with the standard backend. <!-- Which backend are you running? -->; SLURM. <!-- Paste/Attach your workflow if possible: -->; cwlVersion: v1.0; class: Workflow. requirements:; SubworkflowFeatureRequirement: {}. inputs:; fastqc_output_dir:; type: string; fastqc_input_files:; type: string[]; fastqc_thread_count:; type: int; multiqc_output_dir:; type: string; outputs: []. steps:; fastqc_mkdir:; run: mkdir-cmd.cwl; in:; directory: fastqc_output_dir; out: [created_directory]; fastqc_execute:; run: fastqc-step.cwl; in:; input_files: fastqc_input_files; output_dir: fastqc_mkdir/created_directory; thread_count: fastqc_thread_count; out: [output_directory]; multiqc_mkdir:; run: mkdir-cmd.cwl; in:; directory: multiqc_output_dir; out: [created_directory]; multiqc_execute:; run: multiqc-cmd.cwl; in:; output_dir: multiqc_mkdir/created_directory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560:983,PASSWORD,PASSWORDS,983,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560,1,['PASSWORD'],['PASSWORDS']
Security,It would be great to be able to include last modified date in the hashing-strategy when using the current `path` strategy. This would prevent having incorrect output from a task if the file was modified but the path was not.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1843:66,hash,hashing-strategy,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1843,1,['hash'],['hashing-strategy']
Security,"It would be helpful if the /query endpoint included total page count when requesting paginated results. **Use case:**; Currently, the response of `/query` endpoint returns workflows from oldest to newest. The requirements for the Job Manager are such that they require information about newest workflows first. One way that the Job Manager can access newest workflows first is they could paginate through the results in reverse order, which is possible if they have access to the total page count. . The other solution for this use case is if there was an option to provide a sort param and sort direction for the query endpoint. A separate [issue] (https://github.com/broadinstitute/cromwell/issues/3082) has been filed for that.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3083:344,access,access,344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3083,2,['access'],['access']
Security,"JECT"". ## Base bucket for workflow executions; root = ""$BUCKET""; name-for-call-caching-purposes: PAPI; #60000/min in google; ##genomics-api-queries-per-100-seconds = 90000; virtual-private-cloud {; network-name = ""$NET""; subnetwork-name = ""$SUBNET""; }; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; 	 request-workers = 4; batch-timeout = 7 days; 	 # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in PAPI.; 	 slow-job-warning-time: 24 hours; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; compute-service-account = ""default""; # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false; ## Location; location = ""europe-west1"". ; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""$PROJECT""; caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""reference""; }; }; }. default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2 GB""; bootDiskSizeGb: 10; # Allowed to be a String, or a list of Strings; disks: ""local-disk 10 HDD""; noAddress: false; preemptible: 1; zones: [""europe-west1-b""]; }; }; }; }; }. database {; ...; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:11469,access,accessible,11469,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['access'],['accessible']
Security,JES authentication documentation,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/532:4,authenticat,authentication,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/532,1,['authenticat'],['authentication']
Security,"JES now allows for labels to be applied to pipeline runs. Labels are important because they allow us to tag pipeline tasks with metadata which is exposed in other Google APIs, specifically in billing exports that are visible in BigQuery. This will allow us to, for example, calculate the exact cost of a pipeline run which is immensely important for FireCloud and @abaumann . The changes to the pipelines API are described here:. https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines/run#RunPipelineArgs.FIELDS.labels. Cromwell should propagate the workflow level labels to the JES calls for downstream use. According to Google, we should be able to see these labels right away in the operations metadata",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1624:146,expose,exposed,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1624,1,['expose'],['exposed']
Security,"JIRA Issue for ref: [BA-6364](https://broadworkbench.atlassian.net/browse/BA-6364). Hi, I have a simple workflow that import subworkflow. After the subworkflow is run I would like to access an output from particular task from it. The output is not declared as an output of the subworkflow, and subworkflow is not maintained by us so I do not have a (easy) possibility to modify it. Here is the workflow:. ```; import ""subworkflow.wdl"" as sub; workflow hello {; call sub.hello; call show {; input: data = hello.say_hello.out; }; }; task show {; String data; command {; cat ${data}; }; }; ```. And subworkflow.; ```; workflow hello {; String name = ""John""; call say_hello {input: name = name}; }; task say_hello {; String name; command {; which python; python --version; echo ""Hello ${name}!""; }; output {; String out = stdout(); }; }; ```; Of course I have an error: Call ‘hello’ doesn’t have an output ‘say_hello’ (line 6, col 29). But according to specs: `If the output {...} section is omitted, then the workflow includes all outputs from all calls in its final output.`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5479:183,access,access,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5479,1,['access'],['access']
Security,Jes Authentication v2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/225:4,Authenticat,Authentication,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/225,1,['Authenticat'],['Authentication']
Security,"Jira reference: https://broadworkbench.atlassian.net/jira/software/c/projects/BA/issues/BA-6638; Cromwell 51. When using local method in look-up Docker hash together with Docker digest (instead of tag) causes an error because ""@"" symbol is substituted with colon. I have to use tags to be able to get the digest. In ""submit-docker"" section the image is correctly inserted i.e. with ""@"". ```; 2020-10-08 16:08:57,342 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Assigned new job execution tokens to the following groups: 45d03417: 1; 2020-10-08 16:08:57,443 INFO - Attempting to pull python:sha256:d03b690584424b88488e555e26820e458cc624e9d004e3fa0fe3ff99aa81b2b4; 2020-10-08 16:08:57,503 ERROR - Docker pull failed; java.lang.RuntimeException: Error running: docker pull python:sha256:d03b690584424b88488e555e26820e458cc624e9d004e3fa0fe3ff99aa81b2b4; Exit code: 1; invalid reference format. 	at cromwell.docker.local.DockerCliClient.$anonfun$forRun$1(DockerCliClient.scala:58); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.docker.local.DockerCliClient.forRun(DockerCliClient.scala:50); 	at cromwell.docker.local.DockerCliClient.pull(DockerCliClient.scala:37); 	at cromwell.docker.local.DockerCliClient.pull$(DockerCliClient.scala:36); 	at cromwell.docker.local.DockerCliClient$.pull(DockerCliClient.scala:94); 	at cromwell.docker.local.DockerCliFlow$.pull(DockerCliFlow.scala:101); 	at cromwell.docker.local.DockerCliFlow.$anonfun$run$1(DockerCliFlow.scala:35); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:139); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:355); 	at cats.effect.internals.IORunLoop$RestartCallback.run(IORunLoop.scala:366); 	at cats.effect.internals.Trampoline.cats$effect$internals$Trampoline$$immediateLoop(Trampoline.scala:70); 	at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5925:152,hash,hash,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5925,1,['hash'],['hash']
Security,JsonMarshaller$1.apply(SprayJsonSupport.scala:42); #011at spray.httpx.SprayJsonSupport$$anonfun$sprayJsonMarshaller$1.apply(SprayJsonSupport.scala:44); #011at spray.json.PrettyPrinter$.apply(PrettyPrinter.scala:69); #011at spray.json.JsonPrinter$class.apply(JsonPrinter.scala:27); #011at spray.json.PrettyPrinter$.apply(PrettyPrinter.scala:69); #011at spray.json.JsonPrinter$class.apply(JsonPrinter.scala:37); #011at spray.json.PrettyPrinter$.print(PrettyPrinter.scala:69); #011at spray.json.PrettyPrinter$class.print(PrettyPrinter.scala:29); #011at spray.json.PrettyPrinter$.print(PrettyPrinter.scala:69); #011at spray.json.PrettyPrinter$class.print(PrettyPrinter.scala:34); #011at spray.json.PrettyPrinter$.printObject(PrettyPrinter.scala:69); #011at spray.json.PrettyPrinter$class.printObject(PrettyPrinter.scala:42); #011at spray.json.PrettyPrinter$.printSeq(PrettyPrinter.scala:69); #011at spray.json.JsonPrinter$class.printSeq(JsonPrinter.scala:90); #011at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); #011at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); #011at spray.json.JsonPrinter$$anonfun$printSeq$1.apply(JsonPrinter.scala:90); #011at spray.json.JsonPrinter$$anonfun$printSeq$1.apply(JsonPrinter.scala:92); #011at spray.json.PrettyPrinter$$anonfun$printObject$2.apply(PrettyPrinter.scala:42); #011at spray.json.PrettyPrinter$$anonfun$printObject$2.apply(PrettyPrinter.scala:46); #011at spray.json.PrettyPrinter$.print(PrettyPrinter.scala:69); #011at spray.json.PrettyPrinter$class.print(PrettyPrinter.scala:34); #011at spray.json.PrettyPrinter$.printObject(PrettyPrinter.scala:69); #011at spray.json.PrettyPrinter$class.printObject(PrettyPrinter.scala:42); #011at spray.json.PrettyPrinter$.printSeq(PrettyPrinter.scala:69); #011at spray.json.JsonPrinter$class.printSeq(JsonPrinter.scala:90); #011at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:245); #011at scala.collection.TraversableLike$WithFilter.foreach(Traversa,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2438:3201,Hash,HashMap,3201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2438,2,['Hash'],"['HashMap', 'HashTrieMap']"
Security,Just after merging the runtime attributes validation I realized that there is a better way to get a JesBackend instance in the tests than mocking half of Cromwell.. so here it is,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/388:42,validat,validation,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/388,1,['validat'],['validation']
Security,Lazy creation of WorkflowDescriptor in WorkflowManagerActor only after Validation succeeds,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/544:71,Validat,Validation,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/544,1,['Validat'],['Validation']
Security,"Local Cromwell & CromIAM, pointed at Sam dev,. `/describe` endpoint:. Disabled user `anichols@broadinstitute.org`; ```; 403 Forbidden; The supplied authentication is not authorized to access this resource; ```; Enabled user `oednichols@gmail.com`; ```; 200 OK; {; 	""valid"": true,; 	""errors"": [],; 	""validWorkflow"": true,; 	""name"": ""HelloWorld"",; 	""inputs"": [],; 	""outputs"": [],; 	""images"": [],; 	""submittedDescriptorType"": {; 		""descriptorType"": ""WDL"",; 		""descriptorTypeVersion"": ""1.0""; 	},; 	""importedDescriptorTypes"": [],; 	""meta"": {},; 	""parameterMeta"": {},; 	""isRunnableWorkflow"": true; }; ```. `/backends` endpoint:. Disabled user `anichols@broadinstitute.org`; ```; 403 Forbidden; The supplied authentication is not authorized to access this resource; ```; Enabled user `oednichols@gmail.com`; ```; 200 OK; {; 	""defaultBackend"": ""Local"",; 	""supportedBackends"": [; 		""Local"",; 		""LocalBourneShell"",; 		""LocalCacheableRuntimeAttribute"",; 		""LocalDockerSecure"",; 		""LocalNoDocker""; 	]; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6826:148,authenticat,authentication,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6826,6,"['access', 'authenticat', 'authoriz']","['access', 'authentication', 'authorized']"
Security,Local backend validation#433,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/476:14,validat,validation,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/476,1,['validat'],['validation']
Security,Log access URLs with sensitive parts masked BT-235,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6333:4,access,access,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6333,1,['access'],['access']
Security,Lookup docker hash locally,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2095:14,hash,hash,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2095,1,['hash'],['hash']
Security,"MBER output. This should correspond to the output_dir used in AMBER.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-amber\""\n },\n \""id\"": \""#purple-2.44.cwl/amber\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Location of circos binary.\\nOptional path to circos binary.\\nWhen supplied, circos graphs will be written to <output_dir>/plot\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-circos\""\n },\n \""id\"": \""#purple-2.44.cwl/circos\""\n },\n {\n \""type\"": \""Directory\"",\n \""doc\"": \""Path to COBALT output. This should correspond to the output_dir used in COBALT.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-cobalt\""\n },\n \""id\"": \""#purple-2.44.cwl/cobalt\""\n },\n {\n \""type\"": [\n \""null\"",\n \""boolean\""\n ],\n \""doc\"": \""Optionally include if you wish to persist results to a database. Database initialization script can be found here.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-db_enabled\""\n },\n \""id\"": \""#purple-2.44.cwl/db_enabled\""\n },\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""Database password. Mandatory if db_enabled.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-db_pass\""\n },\n \""id\"": \""#purple-2.44.cwl/db_pass\""\n },\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""Database url in form: mysql://host:port/database\\nMandatory if db_enabled.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-db_url\""\n },\n \""id\"": \""#purple-2.44.cwl/db_url\""\n },\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""Database user name. Mandatory if db_enabled.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-db_user\""\n },\n \""id\"": \""#purple-2.44.cwl/db_user\""\n },\n {\n \""type\"": [\n \""null\"",\n \""boolean\""\n ],\n \""doc\"": \""Persist data to DB.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-driver_catalog\""\n },\n \""default\"": false,\n \""id\"": \""#purple-2.44.cwl/driver_catalog\""\n },\n {\n \""type\"": \""File\"",\n \""doc\"": \""Path to GC profile.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-gc_profile\""\n },\n \""id\"": \""#purple-2.44.cw",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:93458,password,password,93458,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['password'],['password']
Security,METADATA_VALUE|; |-----------------------|------------|--------------|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:result|Cache Miss|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:result|Cache Miss|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hit|false|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hit|false|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:failOnStderr|68934A3E9455FA72420237EB05902327|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:runtime attribute:failOnStderr|68934A3E9455FA72420237EB05902327|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:docker|4AD3C387725244C1348F252B031B956D|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:runtime attribute:docker|4AD3C387725244C1348F252B031B956D|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:continueOnReturnCode|CFCD208495D565EF66E7DFF9F98764DA|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:runtime attribute:continueOnReturnCode|CFCD208495D565EF66E7DFF9F98764DA|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:output expression:File sorted_bam|816FBD6EEA5D806309AA0664E2F2AB86|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output expression:File sorted_bam|816FBD6EEA5D806309AA0664E2F2AB86|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:output count|C4CA4238A0B923820DCC509A6F75849B|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleRunID|A3F4D40B1A8326642B9C761E2FE16F3E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D311D5D189FC9|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:String sampleGenomicID|5844E24465D27D94BD0D,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:2251,hash,hashes,2251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['hash'],['hashes']
Security,"Made GoogleAuthMode scopes optional via a param/conf, still defaulting to the pipelines api scopes.; Removed unused scopes and data-dirs from google auth modes that do not uses them.; Moved access token TTL refresher from GcrAbstractFlow to GoogleAuthMode.; Fixed queryPostRoute logging method as ""GET"" instead of ""POST"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3369:190,access,access,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3369,1,['access'],['access']
Security,Made call cache hash endpoint more JSONic,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2471:16,hash,hash,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2471,1,['hash'],['hash']
Security,"Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathParsingException.<init>(PathParsingException.scala:5); 	... 35 common frames omitted; 2018-06-13 14:29:48,009 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - a67833cb:demux_only.illumina_demux:-1:1: Hash error, disabling call caching for this job.; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathFactory$.$anonfun$buildPath$4(PathFactory.scala:47); 	at scala.Option.getOrElse(Option.scala:121); 	at cromwell.core.path.PathFactory$.buildPath(PathFactory.scala:42); 	at cromwell.core.path.PathFactory.buildPath(PathFactory.scala:29); 	at cromwell.core.path.PathFactory.buildPath$(PathFactory.scala:29); 	at cromwell.backend.impl.aws.AwsBatchWorkflowPaths.buildPath(AwsBatchWorkflowPaths.scala:51); 	at cromwell.backend.io.WorkflowPaths.$anonfun$getPath$1(WorkflowPaths.scala:43); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:8843,Hash,Hash,8843,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['Hash'],['Hash']
Security,"Main purpose of this PR is to validate the idea of ""Runtime Attributes validation"" at Backend **initialization** an **execution** level. [x] LocalBackend; - Initialization => No runtime attribute keys validation. Just a warning for those attributes that are not supported.; - Execution => Validate values of supported runtime attributes. [x] JesBackend; - Initialization => Validate Docker key is present and warn for those attributes that are not supported.; - Execution => Validate values of supported runtime attributes. [x] HtCondorBackend; - Initialization => No runtime attribute keys validation. Just a warning for those attributes that are not supported.; - Execution => Validate values of supported runtime attributes. [x] SgeBackend; - Initialization => No runtime attribute keys validation. Just a warning for those attributes that are not supported.; - Execution => Validate values of supported runtime attributes. The idea is to have this public while I implement this functionality so we all are in the same page.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/746:30,validat,validate,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/746,10,"['Validat', 'validat']","['Validate', 'validate', 'validation']"
Security,Make /query endpoint accessible for testing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4715:21,access,accessible,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4715,1,['access'],['accessible']
Security,Make Cromwell 27 Docker hashing consistent with 26,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2360:24,hash,hashing,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2360,1,['hash'],['hashing']
Security,Make parsing of runtime attributes far more robust via ValidationNELs…,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/163:55,Validat,ValidationNELs,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/163,1,['Validat'],['ValidationNELs']
Security,Make sure Terra users have access to view Batch GUI in GCP Console,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7478:27,access,access,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7478,1,['access'],['access']
Security,"Make sure to also follow up on linked RFEs from Mike Noble. ----. I have in my WDL output a glob of an array of files like this : . ```; 	Array[File] ais=glob(""*.png""); ```. Then in my output (in the bucket) I see a list file and a ""directory"" (both having a hash) listing and containing files picked up from the glob. ```; gsutil cat gs://fc-9c84e685-79f8-4d84-9e52-640943257a9b/3beb06ee-6f01-4a9d-8bb7-e8eacd214561/lego_plotter_workflow/f525deea-91b9-4365-b31d-2a2a3ae3346d/call-lego_plotter_task/glob-fc36854b6867c1581ab159b09dd7e2f4.list; THCA-BJ-A0Z2-TP-NB.maf.mutation_CCG_histogram.png; THCA-BJ-A0Z2-TP-NB.maf.mutation_coverage.png; THCA-BJ-A0Z2-TP-NB.maf.mutation_histogram.png; THCA-BJ-A0Z2-TP-NB.maf.mutation_orientation_alt_counts.png; THCA-BJ-A0Z2-TP-NB.maf.mutation_orientation_weighted_mutations.png; THCA-BJ-A0Z2-TP-NB.maf.mutation_profile.png; THCA-BJ-A0Z2-TP-NB.maf.mutation_profile_AF.png; THCA-BJ-A0Z2-TP-NB.maf.mutation_profile_counts.png; THCA-BJ-A0Z2-TP-NB.maf.mutation_profile_normalized.png; THCA-BJ-A0Z2-TP-NB.maf.mutation_profile_samples_1-1.png; ```. (here are the PNGs). ```; wm8b1-75c:lp_glob esalinas$ gsutil ls gs://fc-9c84e685-79f8-4d84-9e52-640943257a9b/3beb06ee-6f01-4a9d-8bb7-e8eacd214561/lego_plotter_workflow/f525deea-91b9-4365-b31d-2a2a3ae3346d/call-lego_plotter_task/glob-fc36854b6867c1581ab159b09dd7e2f4; gs://fc-9c84e685-79f8-4d84-9e52-640943257a9b/3beb06ee-6f01-4a9d-8bb7-e8eacd214561/lego_plotter_workflow/f525deea-91b9-4365-b31d-2a2a3ae3346d/call-lego_plotter_task/glob-fc36854b6867c1581ab159b09dd7e2f4/THCA-BJ-A0Z2-TP-NB.maf.mutation_CCG_histogram.png; gs://fc-9c84e685-79f8-4d84-9e52-640943257a9b/3beb06ee-6f01-4a9d-8bb7-e8eacd214561/lego_plotter_workflow/f525deea-91b9-4365-b31d-2a2a3ae3346d/call-lego_plotter_task/glob-fc36854b6867c1581ab159b09dd7e2f4/THCA-BJ-A0Z2-TP-NB.maf.mutation_coverage.png; gs://fc-9c84e685-79f8-4d84-9e52-640943257a9b/3beb06ee-6f01-4a9d-8bb7-e8eacd214561/lego_plotter_workflow/f525deea-91b9-4365-b31d-2a2a3ae3346d/call-lego_plot",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2266:259,hash,hash,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2266,1,['hash'],['hash']
Security,Make timing diagram more accessible,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4473:25,access,accessible,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4473,1,['access'],['accessible']
Security,Making the Validate API endpoint use the new ValidateActor. Closes #489,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548:11,Validat,Validate,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548,2,['Validat'],"['Validate', 'ValidateActor']"
Security,Manually tested and working for public PDC and Kids First staging data. Not currently working for controlled access staging data though I suspect the issue is not in this code. I do not currently know of controlled access PDC data.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6485:109,access,access,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6485,2,['access'],['access']
Security,"Member access, binary, unary and if/then/else expression evaluation",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3355:7,access,access,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3355,1,['access'],['access']
Security,Metadata Integrity: await write confirmation on final workflow status metadatum [CROM-6443],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6196:9,Integrity,Integrity,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6196,1,['Integrity'],['Integrity']
Security,"Migrate existing failures in the metadata database from the former un-flat failure metadata style:; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. to the new flat style that new workflow metadata will be created with:; ```; ""failures"": [{; ""message"": ""connect timed out""; }, {; ""message"": ""Failed to upload authentication file""; }, {; ""message"": ""Error getting access token for service account: ""; }]; ```. Also note that aggregated exceptions were previously being input using the same random integer for every message part (i.e. every part of the aggregate was overwriting the previous one), so migration could... fix that.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2039:213,access,access,213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039,4,"['access', 'authenticat']","['access', 'authentication']"
Security,Mix one line of super obvious fix with 45 lines of conformance hash repinning noise.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3566:63,hash,hash,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3566,1,['hash'],['hash']
Security,"More details about recreating the error here: https://gatkforums.broadinstitute.org/firecloud/discussion/10740/error-the-local-copy-message-must-have-path-set. Essentially, if a task looks like ; ```; task t {; 	File x = """"; 	; 	command {; ...; 	}; 	runtime {; ...; 	}; }; ```; The job fails with: ; ```; BackendJobDescriptorKey_CommandCallNode_w.t:-1:1/CCHashingJobActor-b12fef61-w.t:NA:1] Failed to hash ; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: Either exists on a filesystem not supported by this instance of Cromwell, or a failure occurred while building an actionable path from it. Supported filesystems are: Google Cloud Storage. Failures: Google Cloud Storage: does not have a gcs scheme (IllegalArgumentException) Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathFactory$.$anonfun$buildPath$4(PathFactory.scala:64); 	at scala.Option.getOrElse(Option.scala:121); 	at cromwell.core.path.PathFactory$.buildPath(PathFactory.scala:58); 	at cromwell.core.path.PathFactory.buildPath(PathFactory.scala:30); ```. AC: ; 1. In case that a user has set the value of a required File as an empty string --this error message should instead accommodate for this special case and point out that an empty string isn't valid input for a File object. ; 1b. If easy, it would also be nice to remove the link to the HPC config docs and the rest of the info about supported filesystems. ; [Optional] 2. If this doesn't hurt performance somehow, it would be nice if the error message also included the name of the input that failed to hash, not just the name of the call/value of the file.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4158:401,hash,hash,401,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4158,2,['hash'],['hash']
Security,"Most of this is just wiring so start by looking at the changes in CallCacheReadActor. I've updated it and made it accessed via a global, routed actor in CromwellRoot. Tests will inject their own dummy version. Mainly this makes testing easier as it's easier to inject the actor rather than relying on it being created to aim at an empty database. Potential downside: actor hierarchy is a little bit more top heavy now :(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1326:114,access,accessed,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1326,3,"['access', 'inject']","['accessed', 'inject']"
Security,Motivating case: [link](https://github.com/broadinstitute/cromwell/blob/cjl_initial_work_dir_requirement_3/centaur/src/main/resources/standardTestCases/InitialWorkDirRequirement/input_string.cwl). The entry field of a Dirent can now be used to access inputs and evaluate ECMAScript expressions. - [x] Rebase and target develop after merging https://github.com/broadinstitute/cromwell/pull/3087,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3089:244,access,access,244,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3089,1,['access'],['access']
Security,Motivation: This is a feature requested by the Mint team. They want the total number of results matching a query to be included in the query response. Mint wants to expose the total results count through the Job-Manager UI.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3126:165,expose,expose,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3126,1,['expose'],['expose']
Security,Move + generalize DB access for restart / resume,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/581:21,access,access,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/581,2,['access'],['access']
Security,Move Backend Specific Validation into constructor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/651:22,Validat,Validation,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/651,1,['Validat'],['Validation']
Security,Move BackendCall.hash to JobDescriptor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/515:17,hash,hash,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/515,1,['hash'],['hash']
Security,Move hash to wdl4s + backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/490:5,hash,hash,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/490,1,['hash'],['hash']
Security,Moved hash into backend. Closes #515,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/549:6,hash,hash,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/549,1,['hash'],['hash']
Security,"Much like our operation polling the creation of new runs in jes winds up being a problem when lots of jobs are created at once as we wind up with a ton of threads gumming up the works but still can only submit at the rate of the genomics api qps. It'd be way more fun to use those threads for things like DB access, mining bitcoins, skynet, etc. Transform the JesPollingActor & friends such that that structure manages both create and get operations (the batch API allows for heterogenous requests) are going through the same structure metered by the QPS supplied by the config file.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1798:308,access,access,308,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1798,1,['access'],['access']
Security,"My WDL pipeline failed to run with Cromwell 55 configured with the `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` Google API with a long list of errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; I was under the expectation that this had been handled in issue #5344 and that Cromwell would retry to access the files until available (the files do indeed exist at the time of this writing).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154:1438,access,access,1438,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154,1,['access'],['access']
Security,"NULL, LABELS VARCHAR(255) NULL, DEPLOYMENT_ID VARCHAR(10) NULL); 2019-01-31 18:29:35,271 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,279 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-01-31 18:29:35,282 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.lif",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:2688,Validat,ValidatingVisitor,2688,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['Validat'],['ValidatingVisitor']
Security,"Name=METADATA_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3752074629'); 2019-07-21 23:34:35,956 INFO - Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.service-dispatcher-13 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2019-07-21 23:34:38,594 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2019-07-21 23:34:38,667 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:2174,hash,hash-lookup,2174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,1,['hash'],['hash-lookup']
Security,Need to validate that file inputs from JSON are valid paths,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2367:8,validat,validate,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2367,1,['validat'],['validate']
Security,"Next phase of #2835. A CWL centaur test should be added that validates expected call/workflow outputs. The famous 3-step workflow uses `ps` to generate outputs, meaning that the actual content is highly variable depending on what other processes are running at the time. Instead, a stable output should be generated, something like [`""OH NO!\nOH NO!\n1""`](https://github.com/broadinstitute/cromwell/blob/f2d1c3bdd5535d9f6a997eadaf136742d86adbe5/centaur/src/main/resources/standardTestCases/continue_on_return_code.test#L11). If the output of the task is a File (path or CWL-File), the output may be affected by #2899.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2924:61,validat,validates,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2924,1,['validat'],['validates']
Security,"Not a huge deal, but otherwise the temp directory may only be accessible by the container user (likely root) without a `sudo`. This can be an issue when trying to scan or clean up the workflow directory.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2053:62,access,accessible,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2053,1,['access'],['accessible']
Security,"Not sure if ""scatter"" is the right summary word here but:. Chris Whelan ran a job that sharded 60 different ways, each with 555 samples. . Cromwell was bottlenecked trying to hash all these, and ultimately many of the has requests timed out. It's potentially a spurious correlation, but CPU was pegged at 100% during this time.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3706:175,hash,hash,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3706,1,['hash'],['hash']
Security,"Note to @kcibul - this is just a placeholder for now, not a would-be punching bag like ""log smarter, not harder"" :). As I'm rewriting CromwellTestKitSpec and thus going through all of the variations of the ""run a wdl and check outputs/logs/etc"" tests in the codebase, I'm realizing that the extent of overlap with Centaur is even larger than I'd previously thought. We need to bring sanity to this ...; - On one hand, all of these ""run a full workflow"" type tests belong in Centaur. If not, define what Centaur is for, if anything; - On the other hand, having rapid feedback via sbt test is useful; - And people find it annoying to write proper ""unit"" tests on behavior; - I'd argue this is a code smell, our business logic is too wrapped up with the actor-y stuff. But that's part of what we need to decide. At the very least, we need to hash out WTF _is_ a ""unit test"" (i.e. running by 'sbt test'), what belongs in Centaur, etc.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1269:839,hash,hash,839,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1269,1,['hash'],['hash']
Security,"Note: BT-442 will require an additional change [here](https://github.com/broadinstitute/cromwell/blob/426f722a6c9087213ec6e5d840325c5de088a922/filesystems/drs/src/main/scala/cromwell/filesystems/drs/DrsPathBuilderFactory.scala#L28) (currently being reviewed in #6561) to use the new identity. Since the two identities are just different ways of specifying the same identity, I decided to make both required if one is provided. However, I can remove that piece of validation if y'all feel we shouldn't do that.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6563:463,validat,validation,463,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6563,1,['validat'],['validation']
Security,"Nothing really clever about this PR, just cashing in on past investments in separation of concerns. 1. Remove SBT projects; - Clean compile time 64s -> 53s on M1; - Dependencies removed, no longer subject to security updates or conflicts (see https://github.com/broadinstitute/cromwell/pull/6948); 2. Remove Centaur integration tests; - Slightly improved Travis build time; - Less stuff to port when we leave Travis; 3. Sever connections between CWL and the rest of Cromwell; - Because of Cromwell's extremely compartmentalized design, only two files really reference CWL directly:; - Entry point for server mode; - Entry point for command-line Womtool; - Only small logic updates needed; 4. Can now safely delete top-level `cwl` directory because nothing depends on it. ---. Reviewer's guide:; - Commits up through [Remove obsolete tests](https://github.com/broadinstitute/cromwell/pull/6955/commits/7a26149d9e70818edf852a16b114809ca9c0dc29) are self-contained and pass CI on their own; - [No longer minimal](https://github.com/broadinstitute/cromwell/pull/6955/commits/557d7b72a97651bcdca8ee27590ebfa29473ad05) removes most of the code; - [Remove *.cwl files](https://github.com/broadinstitute/cromwell/pull/6955/commits/eb4eaef0574ec06a256d38bb222d01ebc44a7e9f) speaks for itself",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6955:208,secur,security,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6955,1,['secur'],['security']
Security,"Now that we're stopping the appenders for workflow logs, access to the underlying Logback context must be synchronized since an underlying non-threadsafe java.util.HashMap is being modified.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3956:57,access,access,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3956,4,"['Hash', 'access']","['HashMap', 'access']"
Security,"OM)\n# https://askubuntu.com/a/823798\ntail /dev/zero"",; ""shardIndex"": -1,; ""jes"": {; ""endpointUrl"": ""https://lifesciences.googleapis.com/"",; ""machineType"": ""custom-1-2048"",; ""googleProject"": ""encode-dcc-1016"",; ""monitoringScript"": ""gs://caper-data/scripts/resource_monitor/resource_monitor.sh"",; ""executionBucket"": ""gs://encode-pipeline-test-runs/caper_out_10"",; ""zone"": ""us-central1-b"",; ""instanceName"": ""google-pipelines-worker-ead27fbad8aa73b157bfc126cd63331f""; },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""[0,137]"",; ""docker"": ""ubuntu:latest"",; ""maxRetries"": ""1"",; ""cpu"": ""1"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zones"": ""us-central1-b"",; ""memoryMin"": ""2 GB"",; ""memory"": ""2 GB""; },; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""hashes"": {; ""output count"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""runtime attribute"": {; ""failOnStderr"": ""68934A3E9455FA72420237EB05902327"",; ""docker"": ""A84529F7A095541F1249576699F24AA1"",; ""continueOnReturnCode"": ""614DAABB2D7AAB5D41921614A49E4F92""; },; ""input count"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""backend name"": ""50F66ECBC45488EE5826941BFBC50411"",; ""command template"": ""F41FEBA57D556A16A5F6C4EEF68ED1E0""; },; ""effectiveCallCachingMode"": ""ReadAndWriteCache""; },; ""inputs"": {},; ""backendLabels"": {; ""wdl-task-name"": ""fail-oom"",; ""cromwell-workflow-id"": ""cromwell-87492280-9828-4afa-b53e-bec675103c42""; },; ""labels"": {; ""wdl-task-name"": ""fail_oom"",; ""cromwell-workflow-id"": ""cromwell-87492280-9828-4afa-b53e-bec675103c42""; },; ""failures"": [; {; ""causedBy"": [],; ""message"": ""The compute backend terminated the job. If this termination is unexpected, examine likely causes such as preemption, running out of disk or memory on the compute instance, or exceeding the backend's maximum job duration.""; }; ],; ""jobId"": ""projects/99884963860/locations/us-central1/operations/1374639517116411519"",; ""monitoringLo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815:4222,hash,hashes,4222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815,1,['hash'],['hashes']
Security,"Ok, so validation fails because I had a typo in my input variable name, fine. But TELL ME WHICH VARIABLE IT WAS!!!. The rest of this description is an example:. ```; workflow foo {; 	call bar; 	# Oops, bar didn't have an output called b:; 	call baz as bad_output_name { input: b = bar.b }; 	# Oops, baz doesn't have an input called a:; 	call baz as bad_input_name { input: a = bar.a }; }. task bar {; 	command {; 		# noop; 	}; 	output {; 		String a = ""a""; 	}; 	runtime {; 		docker: ""ubuntu:latest""; 	}; }. task baz {; 	String b; 	command {; 		# noop; 	}; 	runtime {; 		docker: ""ubuntu:latest""; 	}; }; ```. The messages I actually get:; ```; Unable to load namespace from workflow: ERROR: Expression references input on call that doesn't exist (line 4, col 47):. 	call baz as bad_output_name { input: b = bar.b }; ^; Unable to load namespace from workflow: ERROR: Call references an input on task 'baz' that doesn't exist (line 6, col 38). 	call baz as bad_input_name { input: a = bar.a }; ^; ```. The message I want:; ```; Unable to load namespace from workflow: ERROR: Cannot use 'bar.b' as an input. That variable was never created. (line 4, col 47):. 	call baz as bad_output_name { input: b = bar.b }; ^; Unable to load namespace from workflow: ERROR: Call supplies an input 'a' that isn't declared in the 'baz' task (line 6, col 38). 	call baz as bad_input_name { input: a = bar.a }; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2211:7,validat,validation,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2211,1,['validat'],['validation']
Security,"On Cromwell version, ""34-5156b78-SNAP"", when making a request to the query endpoint that includes the `""includeSubworkflows"": ""false""` parameter, the database will throw an error if the query matches ~1000 results. . Example POST request:; ```; curl -X POST ""https://cromwell.caas-dev.broadinstitute.org/api/workflows/v1/query"" -H ""accept: application/json"" -H ""authorization: Bearer XXXXX"" -H ""Content-Type: application/json"" -d ""[ { \""status\"": \""Failed\"", \""includeSubworkflows\"": \""false\"" }]""; ```; Error message:; ```; {; ""status"": ""fail"",; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@27386688 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@2e52a800[Running, pool size = 200, active threads = 200, queued tasks = 999, completed tasks = 16375]""; }; ```; This error is very similar to the other issue: https://github.com/broadinstitute/cromwell/issues/3115; but `includeSubworkflows` shouldn't limit the number of results the `/query` endpoint can handle.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3873:362,authoriz,authorization,362,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3873,1,['authoriz'],['authorization']
Security,"Once wdl4s memory validation removal PR is merge, enable ignored unit test cases and create missing one if there is one for this functionality.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/726:18,validat,validation,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/726,1,['validat'],['validation']
Security,"One of Morgan's input files was missing an md5 in its object metadata. Cromwell was dutifully falling back to our backup option, which is to read every byte of the file into memory and calculate the hash itself. This resulted in extraordinary network and CPU usage that destabilized the instance and caused a continual crash/reboot cycle. We think this is also what Lori ran into with the featured workspaces. Now, we detect & avoid this condition, print a warning, and carry on without call caching:; ```; 41183c60:ImputationBeagle.SubsetVcfToRegion:3:1:; Hash error ([Attempted 1 time(s)] - Exception:; File of type BlobPath requires hash in object metadata, not present for; https://lz8b0d07a4d28c13150a1a12.blob.core.windows.net/sc-94fd136b-4231-4e80-ab0c-76d8a2811066/hg38/inputs/palantir_merged_input_samples.liftedover.vcf.gz),; disabling call caching for this job.; ```. Obviously, we'd like to enhance this in the future so that call caching is still possible for these jobs, but we have to walk before we can run. ---. Visualization eye candy section!. Swiftly downloading a file on the datacenter multi-gigabit LAN:. ![Screenshot 2024-05-02 at 19 24 04](https://github.com/broadinstitute/cromwell/assets/1087943/46484bbd-30e0-4f88-8f6c-05b50649c557). Telltale CPU curve as we chew through one file after another:. ![Screenshot 2024-05-03 at 11 32 13](https://github.com/broadinstitute/cromwell/assets/1087943/7916ce63-8d4c-46f7-a86a-b3313edf0d77). Flame graph showing the smoking gun, `generateMd5FileHashForPath`:. ![Screenshot 2024-05-02 at 14 02 25](https://github.com/broadinstitute/cromwell/assets/1087943/0d06f3ad-8155-4b43-bef7-6d9ccce35132)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7419:199,hash,hash,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7419,3,"['Hash', 'hash']","['Hash', 'hash']"
Security,"Only the refresh-token mode requires new credentials to be created/validated for every workflow, all other modes can be validated only from the configuration. This ensures we don't re-create unnecessary auth modes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1429:67,validat,validated,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1429,2,['validat'],['validated']
Security,Only try to inject a refresh_token when asked.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2511:12,inject,inject,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2511,1,['inject'],['inject']
Security,Open HTTP channels for Martha Access URL BT-193,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6351:30,Access,Access,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6351,1,['Access'],['Access']
Security,"Original report:; https://gatkforums.broadinstitute.org/firecloud/discussion/10434/weird-error-message-encountered-www-googleapis-com. ![screen shot 2018-09-27 at 2 10 26 am](https://user-images.githubusercontent.com/14941133/46126308-bbb53580-c1fa-11e8-9001-2a662b9374dd.png). Here is an example of a workflow that fails with not being able to access www.googleapis.com. Its not implicitly obvious to a user what the consequences of this are and whether it makes sense to re-run their workflow. . In the case above -- it seems like the job that failed with this error had an rc of 0 and the expected outputs were present -- so likely the output evaluation failed, or some other file operation never completed as this is a StorageException. AC: Supplement the existing top-level message with something like ```Please consult this https://status.cloud.google.com/ to ensure that there are no reported outages with the Google Cloud Platform. Otherwise, this is likely a transient error and the workflow should be re-run.```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4159:345,access,access,345,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4159,1,['access'],['access']
Security,PAPI V1 had a different way of handling private dockerhub credentials and there needs to be a new methodology to pass credentials for operations in PAPI v2. . Implement a system that allows a user to be able to pass in dockerhub credentials securely to their jobs container so a user can pull private docker images.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3844:241,secur,securely,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3844,1,['secur'],['securely']
Security,"PAPI error code 2. Execution failed: pulling image: docker login: generic::unknown: retry budget exhausted (10 attempts): running docker login: exit status 1 (standard error: ""WARNING! Using --password via the CLI is insecure. Use --password-stdin.\nError response from daemon: Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers). Seen [here](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-papiv2/170)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4438:193,password,password,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4438,2,['password'],"['password', 'password-stdin']"
Security,PAPIv2 upgrade test requires secure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4196:29,secur,secure,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4196,1,['secur'],['secure']
Security,PR 3 of 3:; 1. Remove cromwell-core dependency from cloud-support; 2. Run jes centaur on travis; 3. Generate coverage for integration tests. ---. Publishing test validates executables and cross-versioned libraries/docs.; Updated run_tests_parallel.sh for centaur monorepo.; Quieted centaur runs of sbt assembly and coverage gen/upload.; Don't run problematic no_new_calls on TES.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2955:162,validat,validates,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2955,1,['validat'],['validates']
Security,"Pair accessing was never added properly to the docs.; When a pair is accessed incorrectly, it'd be nice to have an error along the lines of `use pair.left and pair.right to access Pair members`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2300:5,access,accessing,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2300,3,['access'],"['access', 'accessed', 'accessing']"
Security,"Per the linked forum post, add an option to turn off Docker hash lookups with the understanding that this would not be compatible with call caching. https://gatkforums.broadinstitute.org/wdl/discussion/10279/problem-with-docker-images-pulled-self-hosted-registries",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2600:60,hash,hash,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2600,1,['hash'],['hash']
Security,"Perf testing has shown that removing this query improves CC time and reduces DB load (see last row in CC google doc); Unclear if it's worth keeping it as a configurable thing ?; This keeps storing the individual hashes, it just stops using them for ""fast"" cache miss detection.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4121:212,hash,hashes,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4121,1,['hash'],['hashes']
Security,Persist the encrypted refresh token. Closes #999,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1000:12,encrypt,encrypted,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1000,1,['encrypt'],['encrypted']
Security,Phase two of Hashing for Call Caching.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1964:13,Hash,Hashing,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1964,1,['Hash'],['Hashing']
Security,"Pipeline developers like me would benefit from being able to parse the wdl language model so I can write tests that aren't covered by the current wdltool validate. . I hear this could be easily accomplished by wdl4s dumping to json, which users could then parse with custom tools. . Desired tests:; 1. Specified inputs exist and are accessible (depends on backend); 2. Docker images exist and can be read/pulled; 3. dependencies.zip contains all of the necessary files to run the workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3502:154,validat,validate,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3502,2,"['access', 'validat']","['accessible', 'validate']"
Security,"Please callcache based on Docker image hashes, not the image string + tag",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1617:39,hash,hashes,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1617,1,['hash'],['hashes']
Security,"Please check this issue.; When I configure the aws, I followed up this page. (https://docs.opendata.aws/genomics-workflows/; ); I did not use the all-in-one template. With 3 step configure, I setup the cromwell server (Custom AMI -> VPC..and etc -> cromwell server instance). <!-- Which backend are you running? -->; aws. <!-- Paste/Attach your workflow if possible: -->; ```; task echoHello{; command {; echo ""Hello AWS!""; }; runtime {; docker: ""ubuntu:latest""; }. }. workflow printHelloAndGoodbye {; call echoHello; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; [cromwell-server.log](https://github.com/broadinstitute/cromwell/files/2897001/cromwell-server.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4677:587,PASSWORD,PASSWORDS,587,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4677,1,['PASSWORD'],['PASSWORDS']
