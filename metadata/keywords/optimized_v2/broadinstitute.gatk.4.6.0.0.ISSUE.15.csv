quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Safety,"The recent branch #8489 has demonstrated that there are some problematic edge cases in the pileup allele merging code that could cause pathological numbers of haplotypes to be handed to the genotyper. In updating the bug in that branch it was observed that it is very common that there are score ties at the 5th haplotype level for the pileupcaller as illustrated by the noise in the updated tests. This algorithm is not a good heuristic and we should replace it with something better, some ideas from that branch that might fix a few of its shortcomings:. 1) Increase/decouple the kmer size used with the reads from the assembly graph kmer size to prevent the filtering step from being redundant with assembly; 2) Normalize the scores to the haplotype lengths to deal with haplotype size bias.; 3) Change the scores to instead reflect the absolute count of unsupported kmers from the graph to also deal with hapotype size bias. ; 4) Iteratively expand the kmer size used for filtering to pare down the number of haplotypes in a more principled fashion.; 5) Utilize the read kmer occurrence counts to construct the scores in order to reduce the risk of spurious reads being sufficient support for a given haplotype. . We have observed that there can be significant changes to the actual genotyping engine output from the pileup engine from even relatively minor changes to the pileupcalling merging code. We should strive to find a more principled solution for merging haplotypes than the one we have currently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8494:687,redund,redundant,687,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8494,2,"['redund', 'risk']","['redundant', 'risk']"
Safety,"The retry parameters are in BucketUtils.java:setGenerousTimeouts. For the case where we're opening thousands and thousands of connections, it may be helpful to increase those timeouts a little.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-303784746:175,timeout,timeouts,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-303784746,1,['timeout'],['timeouts']
Safety,"The script is dangerous in its current state because it uses rm -Rf ${dir} arguments which can result in unwanted deletion if something goes wrong with the input arguments. We should either fix those arguments or remove the necessity for the script to be run with root permissions altogether to avoid any future problems that might arise and make the script safer for users. Additionally, the whole script could be refactored to be cleaner.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3798:295,avoid,avoid,295,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3798,2,"['avoid', 'safe']","['avoid', 'safer']"
Safety,"The segments VCFs contain the event segmentation results for each sample. Exact breakpoints are not given by this tool. Plots can be made using the denoised copy ratio TSV outputs. Currently our germline workflow does not incorporate the B-allele frequency evidence required for LOH detection, although that's a future goal.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6320#issuecomment-572176949:283,detect,detection,283,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6320#issuecomment-572176949,1,['detect'],['detection']
Safety,"The sequences of 150,119 genomes in the UK Biobank.](https://pubmed.ncbi.nlm.nih.gov/35859178/; Halldorsson BV, et al. Nature. 2022 Jul;607(7920):732-740. doi: 10.1038/s41586-022-04965-x. Epub 2022 Jul 20.PMID: 35859178. On page 69+ of this pdf, they describe the problem and how they cleverly worked around it. ; ; https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-022-04965-x/MediaObjects/41586_2022_4965_MOESM1_ESM.pdf. _It should be noted that running GATK out of the box will cause every job to read the entire; gVCF index file (.tbi) for each of the 150,119 samples. The average size of the index files is ; 4.15MB, so each job would have to read 4.15*150,126 = 623GB of data on top of the actual; gVCF slice data. For 60,000 jobs, this would amount to 623GB*60,000 = 37PB or 25.2GB/sec; of additional read overhead if the jobs are run on 20,000 cores in 17 days. This read; overhead will definitely prevent 20,000 cores from being used simultaneously. However,; this problem was avoided by pre-processing the .tbi files and modifying the software; reading the gVCF files from the central storage in a similar fashion as we did for GraphTyper; and the CRAM index files (.crai)._. This explains why chr1 requires more memory than chr22 despite running on the same number of samples. The larger chr1 tbi index is the source of the memory problem. The Decode solution is too limit the reading of the tbi index to the part that indexes the scattered region. There is a long pause at the beginning of the running GenotypeGVCFs which I never understood. GATK must be the reading of all the sample's gvcfs tbi into memory during that pause. So the reblocking of the gvcfs above reduced the memory foot print by decreasing the tbi size. Decode reduced it by chopping up the index so for each scattered region, GATK could only read a small subset of the index needed for that region. The combination of reblocking and chopping up the tbi would help with the memory requirements even more. Ho",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1374348579:1142,avoid,avoided,1142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1374348579,2,['avoid'],['avoided']
Safety,"The test failures in the branch build are clearly related to the recent travis key migration. The PR build (which is the one we care about) passes, so this should be safe to merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7393#issuecomment-953279491:166,safe,safe,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7393#issuecomment-953279491,2,['safe'],['safe']
Safety,"The tutorial data would be an option, except that it covers reference territory not currently in the repo. It looks like that would add nearly 1 gig just for the reference data, which I think we'd really want to avoid. Ideally we'd have just one PR for each of the (two) tools. If you want to keep `ConstrainedMateFixingManager` and `NWaySAMFileWriter` as separate PRs, thats fine, but I don't think we'd take them until we know there is, or will very soon be, code that depends on them. Certainly using `@Experimental` for the tools could make sense, once we're certain that we have a way forward for test coverage. One other note, it's very helpful to include the original GATK3 file as the first commit, as you did in this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-366271857:212,avoid,avoid,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-366271857,1,['avoid'],['avoid']
Safety,"The varianteval package (VariantEval, evaluators/stratifiers and stratification manager) was ported directly from GATK3 to minimize diffs for review, and needs a code-style cleanup pass:. - rename variables with names in ALL_CAPS; - remove redundant type instantiation params in favor of <> operator; - add finals; - revisit the use of generic type params and required casts, etc in StratificationManager and stratifier classes",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5440:240,redund,redundant,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5440,1,['redund'],['redundant']
Safety,"The warnings the user is seeing are due to spanning deletion alleles which are currently not annotated with Funcotator. The bug here is what is causing the stack trace. It's in the protein sequence prediction code and I suspect that it has to do with the position of the variant relative to the exon/transcript boundaries. I have not been able to look at it yet, but thanks to the user posting the variants that are causing issues, it should be straight-forward to track down.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-644794980:198,predict,prediction,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-644794980,1,['predict'],['prediction']
Safety,"There are a number of skipped tests on a successful run of `AlleleListUtilsUnitTest` These are deliberately skipped because the tests share a single data provider, but each test can only use a subset of the data. These should be refactored to avoid skipping tests. These tests also make use of random number generators. It looks like these may not be properly isolated and may introduce coupling between what should be independent tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/607:243,avoid,avoid,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/607,1,['avoid'],['avoid']
Safety,There are many useful utilities in there. Some of them should be pushed down to HTSJDK; some of them should be moved to Hellbender; and some of them are redundant with existing GATK utilities (e.g. MathUtil) and should be combined or skipped as necessary.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/215:153,redund,redundant,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/215,1,['redund'],['redundant']
Safety,"There are many ways of corrupting data in TileDB/GenomicsDB - I am listing the protections available.; * Under the current setup, data is imported once by a single process - the _failIfUpdating_ flag is used in the GATK-4 import tool to ensure this.; * If an importer process crashes in the middle of execution, only the fragments that are fully completed will be visible to downstream queries/reads. Partially written fragments are on disk but ignored. This is achieved by renaming the fragment directory from _.<fragment_id>_ to _fragment_id_. While this is not an atomic operation, under the current use case, I cannot think of any way that will cause issues. At most, you waste some cycles re-importing data. What's not protected:; * Mapping data - sample name to TileDB row id, chromosome name to TileDB column interval. No plans to protect this in the current implementation. Once we have the PostgreSQL based storage for the mapping data, we will be able to detect and prevent inconsistencies.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2536#issuecomment-305557716:965,detect,detect,965,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2536#issuecomment-305557716,1,['detect'],['detect']
Safety,"There are params in Extract Cohort that can be tightened up. Extract Tool has two params that are not used by Extract Features and are _already_ in Extract Cohort. The filter_set_name is used in the BQ filtering tables and looks like we can set it to be completely required for any type of filtering. There are 3 BQ filter tables -- 2 are needed for filtering (no matter what?) and 1 (tranches) is needed for thresholding and sensitivity calculations?. Genotype level filtering is true by default, but this doesn't seem like it should effect things after this cleanup. Though technically it should only be true if a filter_set_name has been specified. I will add another comment in the body of the code, but I would like to add this safety gate explicitly. Disable gnarly doesn't need to be a passed in param---so we'll rip it out for now. SNP and INDEL truth sensitivity and SNP and INDEL Lod scores are cumbersome to have to worry about passing in, but I dont see a better alternative. Should there be additional validation on these (where if they are specified, but no filter_set_name is, then they throw an error?)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7293:733,safe,safety,733,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7293,1,['safe'],['safety']
Safety,There are places in the genome where IUPAC bases can actually be decoded because the amino acid code is partially redundant. Add this logic into getMitochondrialAminoAcidByCodon and getEukaryoticAminoAcidByCodon.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6777:114,redund,redundant,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6777,1,['redund'],['redundant']
Safety,"There has been no activity on this for two years, and the two classes already inherit from different superclasses, and the current ""has a"" implementation avoids code duplication nicely.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4580#issuecomment-592146189:154,avoid,avoids,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4580#issuecomment-592146189,1,['avoid'],['avoids']
Safety,"There have been a few instances like this though. This one was obviously accidental, but things like the correct spelling of @magicDGS actual name seem like reasonable things to be able to include in the source. Also, testing non-ascii characters seems like something that is going to be increasingly common as we support new versions of the spec so it seems like we should learn to avoid this problem...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5936#issuecomment-492761095:383,avoid,avoid,383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5936#issuecomment-492761095,2,['avoid'],['avoid']
Safety,"There should be an option to inform the user when reads do not pass the WellFormedReadFilter. This could be by logging the number of reads failing this filter or exploding (user-specified). Ideally, it would also report which part of the filter they failed. There are a lot of simple ""gotchas"" that can cause reads to fail, like not adding read groups with sample names. To a lay user, this could be very frustrating. In Spark tools that perform their own additional filtering, it can be impossible to tell even when a substantial subset of the input is silently lost this way (very scary stuff!). A tool to detect reads that are not Wellformed (akin to ValidateSamFile) would be helpful, although not for catching bugs like #3453. @lbergelson suggested creating a WellFormedOrExplodeReadFilter, which would allow tool developers to handle this issue at their discretion. I will work on something like this because PathSeq is especially susceptible to the problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3454:608,detect,detect,608,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3454,1,['detect'],['detect']
Safety,"There were a couple of things I needed to do to get the new Spark code running on a cluster:. i. Go back to using Spark's version of Kryo. Using a different version of Kryo is not actually needed (2.21 used by Spark passes the tests), and actually caused errors on the cluster when run with `--conf spark.driver.userClassPathFirst=true` (which is needed to avoid other library conflicts, like with jopt-simple). ii. Exclude Spark from the JAR file to avoid library conflicts. It's normal to exclude Spark and Hadoop from JAR files since they are supplied by `spark-submit`. Since Gradle doesn't have a 'provided' dependency (see https://github.com/broadinstitute/hellbender/issues/836), I had to do a bit of a workaround with the `shadowJar` target, which is now `sparkJar`. . Here's the command I ran:. ``` bash; NAMENODE=...; SPARK_MASTER=yarn-client; HELLBENDER_HOME=...; spark-submit \; --master $SPARK_MASTER \; --conf spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; build/libs/hellbender-all-*-spark.jar ReadsPipelineSpark \; --input hdfs://$NAMENODE/user/$USER/bam/NA12878.chr17_69k_70k.dictFix.bam \; --output hdfs://$NAMENODE/user/$USER/out/spark-reads-pipeline \; --reference hdfs://$NAMENODE/user/$USER/fasta/human_g1k_v37.chr17_1Mb.fasta \; --baseRecalibrationKnownVariants $HELLBENDER_HOME/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf \; --sparkMaster $SPARK_MASTER ; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/882:357,avoid,avoid,357,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/882,2,['avoid'],['avoid']
Safety,There's no reason to even risk it becoming a problem in the future -- let's just include the fully-packaged jars in the docker image (since that is our standard binary distribution format anyway).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4409#issuecomment-365993526:26,risk,risk,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4409#issuecomment-365993526,1,['risk'],['risk']
Safety,"These are the changes relevant to gatk -; * Allow changes for allele specific and other annotations to vid file via GenomicsDBImporter without hardcoding them in genomicsdb. See [GenomicsDB Fix 39](https://github.com/GenomicsDB/GenomicsDB/pull/39). Thanks @mlathara.; * [Fix](https://github.com/GenomicsDB/GenomicsDB/pull/54) for ; BUGreportGATK-07-19-19 reported by @bshifaw where a large ploidy + number of genotypes was leading to math overflow. The overflow is now caught and GenomicsDB stops enumerating genotypes for this case. Thanks @kgururaj.; * [Fix](https://github.com/GenomicsDB/GenomicsDB/pull/66) for missing libcurl in the native GenomicsDB library - Issue #6122 ; * [Fix](https://github.com/GenomicsDB/GenomicsDB/pull/67) to avoid crashing when vcfbufferstream from htslib happens to be invalid. This check was put in response to the [Forum Issue 59667](https://gatkforums.broadinstitute.org/gatk/discussion/comment/59667#Comment_59667). Note that the test vcfs [sample2](https://github.com/broadinstitute/gatk/tree/master/src/test/resources/org/broadinstitute/hellbender/tools/mutect/createpon/sample2.vcf), [sample3](https://github.com/broadinstitute/gatk/tree/master/src/test/resources/org/broadinstitute/hellbender/tools/mutect/createpon/sample3.vcf) and [sample4](https://github.com/broadinstitute/gatk/tree/master/src/test/resources/org/broadinstitute/hellbender/tools/mutect/createpon/sample4.vcf) had to be changed to be htslib compliant for importing and to run `org.broadinstitute.hellbender.tools.walkers.mutect.CreateSomaticPanelOfNormalsIntegrationTest` successfully.; * Allow for native GenomicsDBExceptions to be propagated as java IOExceptions to allow gatk to gracefully handle the exception by printing out relevant information. See [GenomicsDB Fix 68](https://github.com/GenomicsDB/GenomicsDB/pull/68).; * [Fix](https://github.com/GenomicsDB/GenomicsDB/pull/70) for issues using vid protobuf interface to pass vid information and there is more than one config. Thank",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6188:741,avoid,avoid,741,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6188,1,['avoid'],['avoid']
Safety,These initial results suggest that the savings from a pure-Spark pipeline are in the 15-30% range. @tomwhite Do you attribute these savings mostly to avoiding writing/reading intermediate outputs?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3395#issuecomment-341788281:150,avoid,avoiding,150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3395#issuecomment-341788281,1,['avoid'],['avoiding']
Safety,"These measurement are useful when tuning performance (or hunting down performance anomalies), but they have a measurable overhead (10% difference on a test with 1000 intervals, 5x the standard deviation on 10 runs). So turn them off by default. Also refactor a few of those into a try-finally to avoid repetition and its associated risks on correctness.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2391:296,avoid,avoid,296,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391,2,"['avoid', 'risk']","['avoid', 'risks']"
Safety,"These tools use fread + grep preprocessing (`fread(""grep ..."")`) to quickly read in large TSVs in the backend R scripts. Unfortunately, because of https://github.com/Rdatatable/data.table/issues/1139 and the fact that /dev/shm is limited to 64MB in a standard GATK Docker container, this can yield the following error when running within Docker:. ````; Stderr: grep: write error: No space left on device; Error in fread(sprintf(""grep -v ^@ %s"", tsv_file), sep = ""\t"", stringsAsFactors = FALSE, : ; Expected sep ('	') but new line, EOF (or other non printing character) ends field 2 when detecting types from point 10: 2	229515751	229516; Calls: source ... eval -> eval -> WriteModeledSegmentsPlot -> ReadTSV. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:80); 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:19); 	at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:131); 	at org.broadinstitute.hellbender.tools.copynumber.plotting.PlotModeledSegments.writeModeledSegmentsPlot(PlotModeledSegments.java:289); 	at org.broadinstitute.hellbender.tools.copynumber.plotting.PlotModeledSegments.doWork(PlotModeledSegments.java:206); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); 	at org.broadinstitute.hellbender.Main.main(Main.java:275); ````. Starting a Docker container with a sufficiently large `--shm-size` resolves this, but I am not sure if we ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4140:587,detect,detecting,587,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4140,1,['detect'],['detecting']
Safety,"They are retried. You're seeing this message because it failed more than `maxChannelReopens` times. The new version which you really want to use for any test at this point is described there:; https://github.com/broadinstitute/gatk/issues/2685#issuecomment-302798685. Among other things, this new version puts in a message about 'retry failed' when it runs out of retries to eliminate the very confusion that you ran into. GenomicsDBImport opens a large number of parallel connections and as a result is getting throttled fairly heavily (by the host GCE machine if nothing else). This results in timeouts and dropped connections. One way forward is to increase the retry delays, another is to find a way to do the same work with fewer parallel open connections.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-304123753:596,timeout,timeouts,596,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-304123753,1,['timeout'],['timeouts']
Safety,"They discuss that add more ram.; I try with 8 cpu and 500 Go of RAM, but still not working. Error for one bam file:. ```. 15:47:36.554 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 28, 2019 3:47:37 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:47:37.239 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.240 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.4.0; 15:47:37.240 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:47:37.240 INFO Mutect2 - Executing as jpollet@cl1n031.genouest.org on Linux v3.10.0-693.21.1.el7.x86_64 amd64; 15:47:37.240 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 15:47:37.246 INFO Mutect2 - Start Date/Time: 28 novembre 2019 15:47:36 CET; 15:47:37.246 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.246 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.246 INFO Mutect2 - HTSJDK Version: 2.20.3; 15:47:37.246 INFO Mutect2 - Picard Version: 2.21.1; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:47:37.247 INFO Mutect2 - Deflater: IntelDeflater; 15:47:37.247 INFO Mutect2 - Inflater: IntelInflater; 15:47:37.247 INFO Mutect2 - GCS max retries/reopens: 20; 15:47:37.247 INFO Mutect2 - Requester pays: disabled; 15:47:37.247 INFO Mutect2 - Initializing engine; 15:47:41.204 INFO Mutect2 - Done initializi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558:489,detect,detect,489,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558,1,['detect'],['detect']
Safety,"They now take about the same time, we avoid paying the cost of R installation but pay a cost to build the docker instead.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2965#issuecomment-349742450:38,avoid,avoid,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2965#issuecomment-349742450,1,['avoid'],['avoid']
Safety,"This ""fix"" to the timeout issue doesn't work -- closing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2805#issuecomment-305931466:18,timeout,timeout,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2805#issuecomment-305931466,1,['timeout'],['timeout']
Safety,"This PR addresses spec-ops issue #235 - Use -m flag in final gsutil mv of files in ImportGenomes. . Additionally, this PR adds branch filters to the dockstore.yml file that will help with development. The filter for each workflow indicates which branch(es) will show up for that workflow in dockstore. If we don't include these filters, dockstore will run checks of ALL workflows on ALL branches, which causes timeouts. We could remove these filters later (before merging to master) or not, but for now this could help us develop on ah_var_store. Note that we'll need to add feature branches to that file as we work on them. This workflow was tested in Terra and the upload succeeded. Also confirmed that if one file fails, the entire process throws an error code (i.e. -m flag will not cause failures to silently pass) - in example below, `test_file_list.txt` was a list of 6 files, including 1 file that did not exist.; ```; ➜ cat test_file_list.txt | gsutil cp -I gs://dsp-fieldeng-dev/test_cp/; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; CommandException: No URLs matched: test4.txt; ➜ cat test_file_list.txt | gsutil -m cp -I gs://dsp-fieldeng-dev/test_cp/; If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o ""GSUtil:parallel_process_count=1""`. Note that multithreading is still available even if you disable multiprocessing. CommandException: No URLs matched: test4.txt; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test5.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; Copying file://test6.txt [Content-Type=text/plain]...; - [5/5 files][ 37.0 B/ 37.0 B] 100% Done; Ope",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7104:410,timeout,timeouts,410,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7104,1,['timeout'],['timeouts']
Safety,This PR expands the GermlineCNVCaller integration test suite and addresses #6893 and #4375. The tests that were added are: . - Numerical accuracy test that checks for changes of gCNV model posterior values as compared to a previously computed model. This test is meant to detect Python library updates that affect gCNV results and unintentional consequences of minor gCNV model changes.; - A test that runs gCNV in the COHORT mode with a pre-trained model as a starting point.; - A test that runs gCNV with an annotated intervals file that contain GC content column. As @samuelklee suggested we should consider adding functionality to the GermlineCNVCallerIntegrationTest to regenerate test files when there is a discrepancy in gCNV model outputs and we are okay with that discrepancy. See example of it in the HaplotypeCallerSparkIntegrationTest class -- specifically note UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS flag. @mwalker174 let me know what you think.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7889:272,detect,detect,272,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7889,1,['detect'],['detect']
Safety,"This PR is an attempt at improving SelectVariants by. - Rewriting unclear code;; - Adding documentation where needed; and; - Adding tests. The initial motivation for this code change (#7497) was to improve performance of SelectVariants by adding an option to do the ""INFO-level filtering"" before doing ""genotype filtering."" Our assumption was that this would improve performance because then we would avoid the expensive genotype ""fully-decode"" operation, which turns string format fields into appropriate object/types (int, array, etc.). This is (we think) done in `VariantContext.fullyDecode().`. This turned out not to be possible for the following reasons. First, there are roughly four types of genotype subsetting you could do:. a) By the sample names (`--sample-name NA12878`); b) JEXL (`--select GQ > 0`); c) JEXL by accessing the variant context object (`--select vc.getGenotype('NA12878').getGQ() > 1`); d) Others (e.g. `--remove-fraction-genotype`). a) does not need ""fully-decode."" It turns out b) was never supported (GATK currently removes all variants and succeed.) And from my experiments, c) does not seem to ever trigger calling `VariantContext.fullyDecode().` In fact the only code path I can see that calls fullyDecode() is by setting the `fully-decode` SelectVariants argument, which seems to just call fullyDecode at the beginning just for the sake of calling it (or so it appears to me. The utility of this command line argument is highly dubious.) . It's possible that apache code does something similar to fully decoding that could affect performance. All that is to say that we cannot achieve performance improvement with our original blueprint simply because this expensive ""fullyDecode"" operation seems to be a mythical operation that is never used in reality. So while I could not speed up SelectVariants, I cleaned up the code and added the following new arguments:. * `--select-genotype`: with this new genotype-specific JEXL argument, we support filtering by genotype f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8092:401,avoid,avoid,401,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092,1,['avoid'],['avoid']
Safety,"This PR modifies `HaplotypeCaller` so that it can output and genotype spanning deletion alleles represented by the `*` allele. . Currently, the output of `HaplotypeCaller` will not include spanning deletion alleles when run in single sample VCF mode or in genotype given alleles mode, even when that genotype would be more appropriate. In the joint calling workflow `GenotypeGVCFs` adds genotypes for spanning deletions, although the input likelihoods will not be broken out to specifically account for spanning deletion alleles. Some implementation notes:. - I also fixed some behavior specific to GGA mode that I encountered while testing this bug. In particular, when GGA mode was run with multiple variants with the same start position or with spanning events, `HaplotypeCaller` used to emit the warning `""Multiple valid VCF records detected in the alleles input file at site "" + loc + "", only considering the first record""` for each such site. This was a bit of a misleading message, since the other variants were in fact taken into account UNLESS HC decided to emit an empty variant context, for example due to zero coverage.; - I rewrote the `createAlleleMapper` method in `AssemblyBasedCallerGenotypingEngine`. The old version had a very brittle mapping scheme that depended heavily on the ordering of alleles in the variant context created by `AssemblyBasedCallerUtils.makeMergedVariantContext` and `getEventsAtThisLoc`. This proved to be difficult to ensure when spanning deletions were added in, and there was an ominous TODO in the old method saying that the logic was not good enough, so I ended up re-writing it from scratch. The new version is longer but I hope it is easier to read and less fragile, but let me know if I've missed anything. Test currently fail on this branch and therefore it should not be merged. To make them pass we need a fix to https://github.com/broadinstitute/gatk/issues/4716 which is currently being worked on in https://github.com/broadinstitute/gatk/pull/46",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4963:837,detect,detected,837,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963,1,['detect'],['detected']
Safety,"This PR reimplements the overlap detector used in WeighedSplitIntervals in a much faster form for our particular use case. It also involved preprocessing the weighted bed input file in a new way, so the previous weights files will no longer work. As such, there's a new weights file uploaded and referred to as part of this pr. I pulled down the documentation and rationale for the original process from the git issue to a markdown file that can live in our repo, and made python scripts out of the necessary bits of python logic there (as well as a new one to do the further preprocessing step that I added). The motivation for this was the inability of the previous WeightedSplitIntervals task to complete when run against an exome interval list. This new one does, and it does so quickly. The link referenced below is not a ""successful"" run in the Terra sense because it was 190k exomes and that was simply too much for Terra to handle, but it DOES show a successful WeightedSplitIntervals run before the real extract started and I believe that is sufficient to merge. Delaying while ticket VS-189 gets figured out will create an unnecessary delay. Successful integration run: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/294fd6a8-15ed-4722-a63e-bdf089c1c52a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8507:33,detect,detector,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8507,1,['detect'],['detector']
Safety,This PR tries to solve several issues:; - Refactoring constructors for LIBS (#1879); - Adding maximum depth per sample argument to `LocusWalker` to avoid memory overload; - Fixing `Pileup`/`CheckPileup` tools for command line read filters; - Method for fix overlaps in `ReadPileup` in the same way as samtools (#2034),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2154:148,avoid,avoid,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2154,1,['avoid'],['avoid']
Safety,This brings in an important fix for auto-detection of indices; for CRAM files.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/800:41,detect,detection,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/800,1,['detect'],['detection']
Safety,"This bug has become part of a bigger effort to address how configure the gatk. We're working on a general solution to avoid this sort of issue in the future. We haven't addressed this specific subcase yet though. For now the workaround I described above should work for you. If it doesn't, let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-274899565:118,avoid,avoid,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-274899565,1,['avoid'],['avoid']
Safety,"This change enables SAMRecordToGATKReadAdapterSerializer, which has been in the codebase for a while now, just not explicitly enabled. We've been using it for manual testing and haven't seen any problems with it. All unit tests pass. The performance improvement is striking: running mark duplicates locally went from ~120s to ~36s (https://github.com/broadinstitute/gatk/issues/1047). In terms of the change this makes, it means that the header is not present on SAMRecord, but since operations on reads go through the GATKRead interface (which does not need the header), the change is safe. Note also that SAMRecordToGATKReadAdapterSerializer explicitly serializes the reference name (and the mate reference name) so that the round trip serialization/deserialization works.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1127:586,safe,safe,586,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1127,1,['safe'],['safe']
Safety,"This code wraps around BaseRecalibrator and presents a very basic interface (set up, add reads, teardown) that's going to be used at each Dataflow worker. Challenges here:; (1) I need to convert the intervals to Features because that's what the BaseRecalibrator class uses, and SimpleInterval is not a subclass of Feature. This may change in the future.; (2) BaseRecalibrator takes Features as inputs - the only simple Feature class I found I could reuse is ArtificialTestFeature. Please let me know if there is a better choice (solving (1) also solves this); (3) I didn't find code to test overlap between a SimpleInterval and a Feature. Rather than roll my own I chose to use the SimpleInterval overlap test and convert to Feature lazily instead of eagerly. This may cause an interval to be converted more than once. So please consider this the start of a discussion on ""here is something that works, but surely there's a better way?"" I'm not so much looking for every performance opportunity, but ideally I'd like to avoid using ArtificialTestFeature if a better candidate is available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/511:1020,avoid,avoid,1020,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511,1,['avoid'],['avoid']
Safety,"This feature was initially opened in this PR: https://github.com/broadinstitute/gatk/pull/8750, after which @lbergelson and @droazen made comments here; https://github.com/broadinstitute/gatk/pull/8752. . The driving use-case is that we took over the GATK3 MergeVariantsAndGenotypes tool at DISCVRseq and users have been requesting the older behavior on VCF merges, such as: https://github.com/BimberLab/DISCVRSeq/issues/313. . The original PR has been languishing since March and I'm hoping to finalize this feature. Because I cant write to the GATK repo and b/c @lbergelson made some suggestions on a GATK-based branch I am going to put every together into one clean PR, which responds to the code review from the thread above. . To recap background: . - In GATK3, when merging variants, the IDs of all the source VCFs were retained. The GATK4 code path seems like it intended to do this, since the variantSources set is generated, but that variant isnt used for anything (I assume GATK3 code was partially carried forward to incompletely refactored?). . - This PR is designed to allow code to opt-in to the old GATK behavior of retaining the IDs of source VCFs in the ID field. It will not change the default behavior for existing code. - I dont think I can kick off the test suite, but these tests did pass here: https://github.com/broadinstitute/gatk/pull/8752. Again, @lbergelson and @droazen both reviewed the original PR and seemed fine with it in principle. The primary concern raised by @droazen was to avoid changing the current defaults and to not create additional burden (such as adding sorts). I believe this addresses both of those concerns. @jamesemery commented on the thread at one point as well. . Is there anything I can do to help move this forward? Thanks for your time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9032:1513,avoid,avoid,1513,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9032,1,['avoid'],['avoid']
Safety,"This fixes a bug that @meganshand found. Here's the background:. By construction we do not assemble graphs with cycles (although @kvg has something to say about this). However, in rare cases recovering a dangling end may create a cycle. Although it's debatable whether this is an issue for the new, Dijkstra's algorithm-based best haplotype finding algorithm, we remove cycles before finding best haplotypes. It seems that the code for removing cycles can go into an infinite loop when, as in Mutect2's mitochondria mode, we allow for the recovery of forked dangling ends. This PR deletes a single line. `parentVertices` is the set of previously visited vertices in the depth-first search. When an edge is incident on one of these vertices it creates a cycle and we mark it for removal. My best guess (@ldgauthier could you be an extra set of brain? @droazen you're welcome to look, too.) is that the idea behind removing a `currentVertex` from `parentVertices` once all its edges were processed was to optimize the O(log n) cost of subsequent `parentVertices.contains` calls. Since it's a depth-first search, you would think that `currentVertex` will never be seen again and that this is innocuous. However, if some other branch of the depth-first search that is not descended from `currentVertex` also leads to a cycle that goes through `currentVertex`, forgetting that it has been visited creates a huge problem. I believe that forked dangling ends create this possibility. . Removing the line in question will incur a tiny performance cost, if any. By the time we get here the graph has been zipped into a `SeqGraph`, so it doesn't have very many vertices. In any case, `Set.contains` is not an expensive operation. We might even save runtime by eliminating all the `Set.remove`. I have tested this on several WGS samples and it does no harm.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5786:191,recover,recovering,191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5786,2,['recover'],"['recovering', 'recovery']"
Safety,"This is a prototype of the basic infrastructure that must go in to make the junction tree based Haplotype finding work. I have pulled out a toggle for the HaplotypeCaller that that enables a separate ReadThreadingAssembler codepath for haplotype finding. Right now when this mode is enabled `ExperimentalReadThreadingAssembler` is used in conjunction with `JuncitonTreeKBestHalotypeFinder` to extract only haplotypes that show up in our junction trees with evidence of > 3 reads. This still poses problems with dangling end recovery as definitionally those branches never include complete junction tree data. . I will continue to work on this branch (as it is in a somewhat rough state still) but I would like to at least get some eyes on it before i get too deep in the weeds to at least validate the structural approach I have chosen. . Currently known issues in this branch: ; - Tests are failing due to resolution of non-unique reference sink vertexes, I would solicit help as to how best to resolve the case where junction trees point to both a reference stop allele and a continued path.; - There is at least one very degenerate edge case that might cause the code to hang, I would also ask after what is the best way to close out of looping assembly structures that never have reads to close them (i.e. a ""dangling end"" hom-var that happens to point to a non-unique reference base). ; - Probably after discussion the threshold for discarding junction trees will be changed to instead use paths from the discarded tree first. . Resolves #5925",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6034:524,recover,recovery,524,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6034,1,['recover'],['recovery']
Safety,"This is achieved by i) using HadoopBAM to read BAM files from HDFS, and; ii) adding a shadowJar target to build app JAR suitable for running with Spark. Note that this uses Spark's spark-submit script to run on the cluster - this is to avoid including the Spark classes in the hellbender JAR itself, so that it can run on different versions of Spark. We might consider making a runnable JAR at some point in the future though. I tested this on a standalone cluster (running with Java 8) as follows:. ``` bash; export SPARK_HOME=~/sw/spark-1.3.1-bin-hadoop2.6/ ; PATH=$PATH:$SPARK_HOME/bin ; $SPARK_HOME/sbin/start-all.sh. export HADOOP_HOME=~/sw/hadoop-2.7.0; export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop.pseudo/; export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin; (cd $HADOOP_HOME; start-hdfs2); hadoop fs -mkdir -p /user/$USER ; hadoop fs -put ~/workspace/hellbender/src/test/resources/org/broadinstitute/hellbender/tools/flag_stat.bam flag_stat.bam; hadoop fs -rmr out. SPARK_MASTER=spark://localhost:7077; spark-submit \; --class org.broadinstitute.hellbender.Main \; --master $SPARK_MASTER \; build/libs/hellbender-GATK.4.alpha-*-all.jar CountBasesDataflow \; --input hdfs://localhost/user/$USER/flag_stat.bam \; --output hdfs://localhost/user/$USER/out/countbases \; --runner SPARK \; --sparkMaster $SPARK_MASTER; hadoop fs -getmerge out out; cat out; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/553:236,avoid,avoid,236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/553,1,['avoid'],['avoid']
Safety,"This is an issue from the forum where an user detected uneven number of fields generated after Funcotator operation. Here is the post. Original post is here. [https://gatk.broadinstitute.org/hc/en-us/community/posts/26089048930715-Funcotator-gnomAD-incoherent-number-of-output-fields](https://gatk.broadinstitute.org/hc/en-us/community/posts/26089048930715-Funcotator-gnomAD-incoherent-number-of-output-fields). --------------------------------------------------------------------------------------------------------------------------------------------------------------------------. a) GATK version used: 4.5.0.0. Funcotator germline with gnomAD is producing incoherent number of output fields. I am showing an example for a variant at chr1 598934. GATK version used: 4.5.0.0. I have called germline variants and performed Variant Quality Score Recalibration using GATK as por best practices, I then filter only allowing PASS variants and indexing it with:. ```; bcftools view -f 'PASS,.' -O z --threads 8 -o output_filtered.vcf.gz VQSR.dir/INDEL_SNPS.applyModel.vcf.gz && tabix -p vcf output_filtered.vcf.gz; Generating a file output_filtered.vcf.gz containing for example this line:; ```. `chr1 598934 . CGGG CG,C,* 374.02 PASS AC=2,3,5;AF=0.083,0.125,0.208;AN=24;BaseQRankSum=0.674;DP=45;ExcessHet=0.0031;FS=0;InbreedingCoeff=0.3671;MLEAC=3,6,9;MLEAF=0.125,0.25,0.375;MQ=48.38;MQRankSum=-1.96;NEGATIVE_TRAIN_SITE;QD=17;ReadPosRankSum=-0.126;SOR=0.724;VQSLOD=-4.486;culprit=DP GT:AD:DP:GQ:PGT:PID:PL:PS ./.:0,0,0,0:0:0:.:.:0,0,0,0,0,0,0,0,0,0:. 0/0:5,0,0,0:5:9:.:.:0,9,135,9,135,135,9,135,135,135:. ./.:0,0,0,0:0:0:.:.:0,0,0,0,0,0,0,0,0,0:. 0/0:1,0,0,0:1:3:.:.:0,3,11,3,11,11,3,11,11,11:. 2|2:0,0,2,0:2:6:1|1:598934_CGGG_C:90,90,90,6,6,0,90,90,6,90:598934 ./.:0,0,0,0:0:0:.:.:0,0,0,0,0,0,0,0,0,0:. ./.:0,0,0,0:0:0:.:.:0,0,0,0,0,0,0,0,0,0:. ./.:1,0,0,0:1:0:.:.:0,0,0,0,0,0,0,0,0,0:. ./.:0,0,0,0:0:0:.:.:0,0,0,0,0,0,0,0,0,0:. ./.:0,0,0,0:0:0:.:.:0,0,0,0,0,0,0,0,0,0:. ./.:0,0,0,0:0:0:.:.:0,0,0,0,0,0,",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8858:46,detect,detected,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8858,1,['detect'],['detected']
Safety,This is the beta version of the pileup based haplotype generation and variant detection code.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7432:78,detect,detection,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7432,1,['detect'],['detection']
Safety,"This looks promising, at a minimum we should try setting up the docker with hadoop native libraries so the performance gains can be extended to most use cases. This might also include adding some magic to the gatk launch script inside the docker to detect and run with the correct version of the hadoop libraries.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387519906:249,detect,detect,249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387519906,1,['detect'],['detect']
Safety,This might allow us to avoid the expensive final concatenation for outputs in Spark.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4492#issuecomment-370504855:23,avoid,avoid,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4492#issuecomment-370504855,1,['avoid'],['avoid']
Safety,"This might be a documentation request. I’m just wanting to know a bit more about how gatk spark tools shard bam files. How do you choose bins and prevent losing mate pairs when working with paired end data? Also, how do you avoid losing coverage at the edges of bins?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6284:224,avoid,avoid,224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6284,1,['avoid'],['avoid']
Safety,"This request removes all calls to getChr() in the code by replacing calls to getChr() with calls to getContig() and removing a redundant assertion that contains a call to getChr(). To complete the removal of getChr() from the code, we could:. Remove the getChr() method overrides in the TableFeature and ArtificialTestFeature classes and convert these classes from implementing the Feature interface to implementing the Locatable interface. Remove the definition of the Feature interface from htsjdk.tribble (since all Feature does is add getChr() to the Locatable interface), and replace all references to the Feature interface in the tools and libraries with references to the Locatable interface. These steps will also remove the deprecation warnings for getChr() (part of #377).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/478:127,redund,redundant,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/478,1,['redund'],['redundant']
Safety,"This request was created from a contribution made by Francesco Mazzarotto on March 23, 2022 14:16 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4892502642075-FilterMutectCalls-haplotype-filter-value-assigned-to-variants-with-different-PGT-tag](https://gatk.broadinstitute.org/hc/en-us/community/posts/4892502642075-FilterMutectCalls-haplotype-filter-value-assigned-to-variants-with-different-PGT-tag). \--. Hello,. I am using GATK v4.2.5.0 to process tumor-only samples sequenced with WES. In a sample, one variant that has been detected with Sanger sequencing (chr14-45137087-C-T) gets filtered out as non-PASS (also) because of the 'haplotype' filter value. As far as the 'haplotype' filter value is concerned, the 'guilty' variant seems to be another SNP 3bp upstream (chr14-45137084-C-T). There are no other variants called within 100bp of the Sanger-validated one (see below). chr14    45136964    .    C    T    .    haplotype;weak\_evidence    AS\_FilterStatus=weak\_evidence;AS\_SB\_TABLE=3,0|1,0;DP=4;ECNT=2;GERMQ=25;MBQ=41,37;MFRL=360,390;MMQ=60,60;MPOS=69;POPAF=7.30;ROQ=17;TLOD=3.20    GT:AD:AF:DP:F1R2:F2R1:PGT:PID:PS:SB    0|1:3,1:0.333:4:1,0:1,1:0|1:45136962\_C\_T:45136962:3,0,1,0 ; ; chr14    45137084    .    C    T    .    germline;haplotype;panel\_of\_normals    AS\_FilterStatus=SITE;AS\_SB\_TABLE=9,1|12,5;DP=27;ECNT=2;GERMQ=1;MBQ=41,41;MFRL=297,326;MMQ=60,60;MPOS=45;PON;POPAF=0.830;ROQ=90;TLOD=59.93    GT:AD:AF:DP:F1R2:F2R1:PGT:PID:PS:SB    0|1:10,17:0.615:27:4,13:4,4:0|1:45137084\_C\_T:45137084:9,1,12,5 ; ; chr14    45137087    .    C    T    .    germline;haplotype    AS\_FilterStatus=SITE;AS\_SB\_TABLE=12,5|9,1;DP=27;ECNT=2;GERMQ=1;MBQ=41,41;MFRL=326,297;MMQ=60,60;MPOS=44;POPAF=2.33;ROQ=93;TLOD=31.76    GT:AD:AF:DP:F1R2:F2R1:PGT:PID:PS:SB    1|0:17,10:0.385:27:13,4:4,6:1|0:45137084\_C\_T:45137084:12,5,9,1 ; ; chr14    45149295    .    AC    A    .    haplotype;weak\_evidence    AS\_FilterStatus=weak\_evidence;AS\_SB\_TABLE=0,0|0,0;DP=1;ECNT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7809:552,detect,detected,552,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7809,1,['detect'],['detected']
Safety,"This request was created from a contribution made by Matt Johnson on July 05, 2021 21:23 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360067819992-SelectVariants-v4-1-6-0-doesn-t-select-the-variants-as-expected-#community\_comment\_4403173344411](https://gatk.broadinstitute.org/hc/en-us/community/posts/360067819992-SelectVariants-v4-1-6-0-doesn-t-select-the-variants-as-expected-#community_comment_4403173344411). \--. Hello, I am also having this issue. \[This page\](/hc/en-us/articles/360035530752-What-types-of-variants-can-GATK-tools-detect-or-handle-) indicates the following definition of SYMBOLIC:. SYMBOLIC (such as the  `<NON-REF>`  allele used in GVCFs produced by HaplotypeCaller, the  `*`  allele used to signify the presence of a  [spanning deletion](https://gatk.zendesk.com/hc/en-us/articles/360035531912), or undefined events like a very large allele or one that's fuzzy and not fully modeled; i.e. there's some event going on here but we don't know what exactly). Therefore I would expect SelectVariants --select-type-to-exclude SYMBOLIC to not have any calls containing spanning deletions. However, the output VCFs still do (in gatk4 4.2.0.0). This causes problems for downstream tools like FastaAlternateReferenceMaker:. java.lang.IllegalArgumentException: the input sequence contains invalid base calls like: \*. Is there any way to force GATK to exclude spanning deletions when filtering a VCF?<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/168722'>Zendesk ticket #168722</a>)<br>gz#168722</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7341:565,detect,detect-or-handle,565,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7341,1,['detect'],['detect-or-handle']
Safety,"This request was created from a contribution made by tc on February 09, 2022 17:49 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4418364848795-java-lang-IllegalArgumentException-Invalid-interval-in-FuncotateSegments](https://gatk.broadinstitute.org/hc/en-us/community/posts/4418364848795-java-lang-IllegalArgumentException-Invalid-interval-in-FuncotateSegments). \--. Hi,. I tried to annotated a called segment file after following the somatic CNV detection workflow of GATK:. gatk --java-options ""-Xmx10g -Djava.io.tmpdir=/lscratch/$SLURM\_JOBID"" FuncotateSegments \\ ; ; \--data-sources-path funcotator\_dataSources.v1.7.20200521s/ \\ ; ; \--ref-version hg19 \\ ; ; \--output-file-format SEG \\ ; ; \-R hs37d5.fa \\ ; ; \--segments sample.called.seg \\ ; ; \-O sample.seg.funcotated.tsv \\ ; ; \--transcript-list funcotator\_dataSources.v1.7.20200521s/transcriptList.exact\_uniprot\_matches.AKT1\_CRLF2\_FGFR1.txt. But I got the following error message:. 12:37:55.534 INFO  FuncotateSegments - The following datasources support funcotation on segments:  ; ; 12:37:55.535 INFO  FuncotateSegments -  Gencode 34 CANONICAL ; ; 12:37:55.542 INFO  FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode.  Performing conversion. ; ; 12:37:55.542 WARN  FuncotatorEngine - WARNING: You are using B37 as a reference.  Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases.  There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references. ; ; 12:37:55.679 INFO  ProgressMeter - Starting traversal ; ; 12:37:55.679 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Features Processed  Features/Minute ; ; 12:37:56.198 WARN  FuncotatorUtils - Reference allele is different than the reference coding sequence (strand: -, alt = G, ref G != T reference coding seq) @\[chr1:13839497\]!  Substituting given allele for sequence code (TTC-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7676:471,detect,detection,471,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676,1,['detect'],['detection']
Safety,"This set of optimizations brings the GATK4 HaplotypeCaller performance into line; with GATK3.x performance. Note that HaplotypeCallerSpark is not touched by this PR (that is for a future PR). Summary of changes:. * AssemblyRegionWalker: query all intervals on each contig simultaneously, rather than individually; * GATKRead: Cache adaptor boundary, soft start/end, and cigar length; * GATKRead: add getBasesNoCopy() / getBaseQualitiesNoCopy(); * ReadPileup: speed up stratified constructor; * LIBS.lazyLoadNextAlignmentContext(): don't keep pileup elements unnecessarily separated by sample during pileup creation; * Restore faster GATK3 version of ReferenceConfidenceModel.sumMismatchingQualities(); * RefVsAnyResult: nest within ReferenceConfidenceModel, and allow direct field access; * Remove redundant getBases() call in ReadThreadingGraph; * Fix BaseGraph Utils.validateArg() call; * ReadPileup: replace Collections.unmodifiableList(pileupElements).iterator() with direct return of an iterator that forbids removal; * Kill expensive bounds checking in GATKRead getBase()/getBaseQuality()/getCigarElement(); * Kill nonNull checks in PileupElement; * Kill expensive PileupElement and ReadPileup arg validation; * GATKRead adapter: clear cached values upon mutation",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4031:798,redund,redundant,798,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4031,1,['redund'],['redundant']
Safety,"This should add support for reading bam files with CSI indexes, as well as porting the FastaReferenceWriter to htsjdk and a lot of other changes to htsjdk. @samuelklee This includes the overlap detector optimizations you wanted as well as the changes to let you write interval file to paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5812#issuecomment-474098651:194,detect,detector,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5812#issuecomment-474098651,1,['detect'],['detector']
Safety,"This should be straight-forward once the discussion above is resolved. Just detect if the variant is on a known Mitochondrial contig. If so, use the MT codon table, otherwise use the regular codon table.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4863#issuecomment-430312996:76,detect,detect,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4863#issuecomment-430312996,1,['detect'],['detect']
Safety,"This should prevent the confusing case of accidentally corrupting an existing GenomicsDB. The next genomicsDB update is supposed to make this unnecessary, but until it happens lets avoid this problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3975:181,avoid,avoid,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3975,1,['avoid'],['avoid']
Safety,"This symlinks /usr/bin/python to /usr/bin/python3, which can help avoid issues with scripts that reference the python binary directly. Resolves #8402",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8499:66,avoid,avoid,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8499,1,['avoid'],['avoid']
Safety,"This task is to take the training data generated in issue #3092 and learn something from it, for example a regression model that predicts a distribution of artifactual read fractions. Using the learned model in filtering is a separate issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2973#issuecomment-307680993:129,predict,predicts,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2973#issuecomment-307680993,2,['predict'],['predicts']
Safety,"This test case is overkill, since the test just before it in the; DataProvider is sufficient to detect bugs in ReadsSparkSink such; as the one we encountered last week. We also think it is responsible; for the recent intermittent timeouts in travis. Resolves #1342",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1343:96,detect,detect,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1343,2,"['detect', 'timeout']","['detect', 'timeouts']"
Safety,"This user was getting a 'java.lang.IllegalArgumentException: Dictionary cannot have size zero' error message when they submitted a VCF as the -I input instead of a BAM. It would save other users a lot of troubleshooting if we added a check and a better error message. This request was created from a contribution made by Ruiqiao Bai on September 12, 2021 01:06 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-). \--. Hi! I am using GATK4 following the tutorial \[(How to) Call somatic mutations using GATK4 Mutect2 – GATK (broadinstitute.org)\](/hc/en-us/articles/360035531132--How-to-Call-somatic-mutations-using-GATK4-Mutect2) for detecting somatic variants. I have received an error when using GetPileupSummaries. Specifically, the command line I used is: . gatk GetPileupSummaries -I /gatk/my\_data/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -V /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -O /gatk/my\_data/wgs\_BAM/step1\_3/getpileupsummaries\_LP6005115-DNA\_B07.table. The entire error log has been pasted below. May I know what might cause this problem? Thanks for your help!. Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_s amtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_leve l=2 -jar /gatk/gatk-package-4.2.0.0-local.jar GetPileupSummaries -I /gatk/my\_dat a/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7479:963,detect,detecting,963,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7479,1,['detect'],['detecting']
Safety,"This was mentioned already as a comment in #562 but it should be an issue instead. BQSR on Dataflow can _theoretically_ avoid the step of saving the textual report, instead piping the recalibration analysis results directly to the ApplyBQSR phase. In practice it cannot, because it turns out that **saving a report and immediately loading it back is not a no-op**. it actually changes in some necessary way. It would be great if someone familiar with that part of the code could help factor the mutation apart from the saving/loading, so I could just call that in the pipeline. This would speed things up a bit and the code would be that much cleaner.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/799:120,avoid,avoid,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/799,1,['avoid'],['avoid']
Safety,This will be the next bug I look into. It's part of a family of issues that all relate to how the predicted protein change is created.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6289#issuecomment-688996786:98,predict,predicted,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6289#issuecomment-688996786,1,['predict'],['predicted']
Safety,"This will require less tuning of `minimum-interval-median-percentile` to filter out completely uncovered intervals, which will avoid e.g. https://gatkforums.broadinstitute.org/gatk/discussion/11461/gatk-4-0-1-2-no-non-zero-singular-values-were-found-in-creating-a-panel-of-normals-for-somatic-cnv. For example, currently if 10% of my intervals are completely uncovered and I set the relevant parameter to 5%, the 5th percentile is then equal to zero. However, because no intervals are strictly less than zero, none are filtered. Changing this to filter on equality then gets rid of all 10% of the intervals as one would want to do in practice.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4503:127,avoid,avoid,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4503,1,['avoid'],['avoid']
Safety,Tiny improvement that was languishing in an ancient and redundant PR https://github.com/broadinstitute/gatk/pull/6736,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8725:56,redund,redundant,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8725,1,['redund'],['redundant']
Safety,"To add some commentary to why this is happening: It looks like multiple threads are hitting this line simultaneously and based on the overload of `ArrayList.add()` this error could be triggered by multiple calls to `ensureCapacityInternal()` inside the add method:; ```; final List<ReadsPathDataSource> readSources = new ArrayList<>(threads);; final ThreadLocal<ReadsPathDataSource> threadReadSource = ThreadLocal.withInitial(; () -> {; final ReadsPathDataSource result = new ReadsPathDataSource(readArguments.getReadPaths(), factory);; readSources.add(result);; return result;; });; ```; The fix should be simple you just have to make sure ti synchronize the initialization or swap out the readSources object to one that is itself thread safe. @vruano",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7403#issuecomment-899732721:739,safe,safe,739,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7403#issuecomment-899732721,2,['safe'],['safe']
Safety,To avoid quota issues when pulling down the base image during tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7102:3,avoid,avoid,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7102,1,['avoid'],['avoid']
Safety,To help avoid user confusion. Resolves #1172,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1173:8,avoid,avoid,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1173,1,['avoid'],['avoid']
Safety,"To-do's for CompareReferences and CheckReferenceCompatibility:. CompareReferences; * add functionality to run base-level comparison modes on specified sequences (not just detected mismatching sequences); * add option to ignore case-level differences in FIND_SNPs mode ; * squash contiguous SNPs in FIND_SNPS mode . CheckReferenceCompatibility; * change wording for missing MD5 compatibility status ('COMPATIBLE' to 'MAYBE_COMPATIBLE,' or something similar)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7988:171,detect,detected,171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7988,1,['detect'],['detected']
Safety,ToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Haplotype count 128; 11:36:56.119 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:56.120 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:06.762 DEBUG Mutect2 - Processing assembly region at chrM:6630-6929 isActive: false numReads: 30053; 11:39:07.547 DEBUG Mutect2 - Processing assembly region at chrM:6930-7229 isActive: false numReads: 0; 11:39:07.574 DEBUG Mutect2 - Processing assembly region at chrM:7230-7493 isActive: false numReads: 359; 11:39:07.584 DEBUG Mutect2 - Processing assembly region at chrM:7494-7771 isActive: true numReads: 718; 11:39:07.668 DEBUG ReadThreadingGraph - Recovered 32 of 33 dangling tails; 11:39:07.713 DEBUG ReadThreadingGraph - Recovered 31 of 50 dangling heads; 11:39:07.996 DEBUG Mutect2Engine - Active Region chrM:7494-7771; 11:39:07.998 DEBUG Mutect2Engine - Extended Act Region chrM:7394-7871; 11:39:07.999 DEBUG Mutect2Engine - Ref haplotype coords chrM:7394-7871; 11:39:08.000 DEBUG Mutect2Engine - Haplotype count 128; 11:39:08.001 DEBUG Mutect2Engine - Kmer sizes count 0; 11:39:08.002 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:12.623 DEBUG Mutect2 - Processing assembly region at chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:14167,Recover,Recovered,14167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Safety,Tool to detect CRAM base corruption caused by GATK issue 8768,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8819:8,detect,detect,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8819,1,['detect'],['detect']
Safety,Tools are:. - DetectCoverageDropout; - DecomposeSingularValues; - CalculatePulldownPhasePosteriors. Nine related docs are:. - CalculatePulldownPhasePosteriors.java			; - CoverageDropoutDetectorTest.java			; - DecomposeSingularValuesIntegrationTest.java; - CalculatePulldownPhasePosteriorsIntegrationTest.java	; - CoverageDropoutResult.java				; - DetectCoverageDropout.java; - CoverageDropoutDetector.java				; - DecomposeSingularValues.java				; - DetectCoverageDropoutIntegrationTest.java,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2809#issuecomment-305998381:14,Detect,DetectCoverageDropout,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2809#issuecomment-305998381,3,['Detect'],"['DetectCoverageDropout', 'DetectCoverageDropoutIntegrationTest']"
Safety,"Tools can set the timeout used by the AsynchronousStreamWriterService, but the timeout used by the StreamingPythonScriptExecutor itself is currently fixed. In order to enable use of a command line argument that allows experimentation with the performance tradeoffs of various batch transfer sizes, we need to be able to scale the timeout with the batch transfer size.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4221:18,timeout,timeout,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4221,3,['timeout'],['timeout']
Safety,Tools touched:. Mutect2; CreateSomaticPanelOfNormals; GetPileupSummaries; CalculateContamination; FilterMutectCalls; Picard CollectSequencingArtifactMetrics; FilterByOrientationBias; Concordance . The Mutect2 (How to) tutorial and various supporting documents are now public and so I've cleaned up now redundant information we temporarily placed in the tool docs. I also made a pass at unifying this set of tool docs in terms of presentation elements.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4310:302,redund,redundant,302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4310,1,['redund'],['redundant']
Safety,"Travis test suites runtimes have been creeping upwards in both gatk and gatk-protected, and have now reached painful levels. Let's parallelize the test suite in a safe way that won't require us to make all test classes thread-safe. In the past, we successfully parallelized by test class, with one process per test class, and aggregated the results at the end.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1769:163,safe,safe,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1769,2,['safe'],['safe']
Safety,"Two high level questions/concerns about the implementation… . 1. I think when things are parallelized, we then lose the retries. In order to do that, we might need to get a little more python async-y than I was originally thinking. The retries are pretty important to recover from transient errors in BQ. 2. I think perhaps the ordering/dependecies are lost between queries? It looks like we fire them all off and then wait for them all to complete. But really we need to wait for the vet_new queries to finish before launching the pet queries since they depend on data from the vet_new. If vet_new hasn't started this would fail… but if vet_new has created the table we'll just get the wrong results",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7505#issuecomment-944301753:268,recover,recover,268,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7505#issuecomment-944301753,1,['recover'],['recover']
Safety,"Two more things:. 1) attached is the error log. There error is:. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fb23d5aba19, pid=147396, tid=140403504084736; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode linux-amd64 ); # Problematic frame:; # C [libtiledbgenomicsdb6950059158101672698.so+0x3e3a19] ArraySchema::tile_num(void const*) const+0x79; #; # Core dump written. Default location: /home/groups/MgapGenomicsDb/@files/sequenceOutputPipeline/SequenceOutput_2020-11-04_09-17-57/Job44/core or core.147396; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #. ```. 2) I was mistaken on when the error happens. The last line logged by GenomicsDBImport is ""GenomicsDBImport - Importing batch 4 with 33 samples"". There are 4 total batches, with a batchSize of 50. I dont know whether batch 4 finished or if this error is mid-batch import. [hs_err_pid147396.log.txt](https://github.com/broadinstitute/gatk/files/5496539/hs_err_pid147396.log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722594280:98,detect,detected,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722594280,1,['detect'],['detected']
Safety,Update AsynchronousStreamWriterService unit test timeouts.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4028:49,timeout,timeouts,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4028,1,['timeout'],['timeouts']
Safety,Update CRAM detector output files.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8971:12,detect,detector,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8971,1,['detect'],['detector']
Safety,"Update to GKL 0.5.8, which fixes bug in AVX detection, which was beha…",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3513:44,detect,detection,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3513,1,['detect'],['detection']
Safety,"Updated plan. ----------; ## Small improvements in new interpretation tool; ; - [x] Output bam instead of sam for assembly alignments; - [x] Instead of creating directory, new interpretation tool writes files (behavior consistent with current interpretation tool); - [x] Prefix with sample name for output files' names; - [x] Add `INSLEN` annotation when there's `INSSEQ`; - [x] Clarify the boundary between `AlignedContig` and `AssemblyContigWithFineTunedAlignments`; - [x] Increase test coverage for `AssemblyContigAlignmentsConfigPicker`; ; ----------; ## Consolidate logic, bump test coverage and update how variants are represented. ### consolidate logic; When initially prototyped, there's redundancy in logic for simple variants, now it's time to consolidate. - [x] `AssemblyContigWithFineTunedAlignments`; - [x] `hasIncompletePicture()`. - [x] `AssemblyContigAlignmentSignatureClassifier`; - [x] Don't make so many splits; - [x] Reduce `RawTypes` into fewer cases; ; - [x] `ChimericAlignment`; - [x] update documentation; - [x] implement a `getCoordinateSortedRefSpans()`, and use in `BreakpointsInference`; - [x] `isNeitherSimpleTranslocationNorIncompletePicture()`; - [x] `extractSimpleChimera()`. ### bump test coverage; Once code above is consolidated, bump test coverage, particularly for the classes above and the following poorly-covered classes; - [x] `ChimericAlignment`; - [x] `isForwardStrandRepresentation()`; - [x] `splitPairStrongEnoughEvidenceForCA()` ; - [x] `parseOneContig()` (needs testing because we need it for simple-re-interpretation for CPX variants) Note that `nextAlignmentMayBeInsertion()` is currently broken in the sense that when using this to filter out alignments whose ref span is contained by another, check if the two alignments involved are head/tail. - [x] `BreakpointsInference` & `BreakpointComplications`. - [x] `NovelAdjacencyAndAltHaplotype`; - [x] `toSimpleOrBNDTypes()`. - [x] `SimpleNovelAdjacencyAndChimericAlignmentEvidence`; - [x] serialization ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4111#issuecomment-375438021:696,redund,redundancy,696,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4111#issuecomment-375438021,2,['redund'],['redundancy']
Safety,"Use Cromwell's [noAddress](https://support.terra.bio/hc/en-us/community/posts/360060020871-noAddress-true-results-in-stalling-jobs) feature to avoid the unnecessary use of external IPs on Cromwell worker VMs on wide scatters that cause us Google quota problems. For most of the GVS code this only involves adding `noAddress: true` to the existing runtime attributes. In the PGEN code this was slightly more work to change away from `docker: ""ubuntu:22.04""` which is implicitly pulled from Docker Hub. Since `noAddress: true` means the VM can only interact with Google services, we have to switch to a GCR-hosted image as Docker Hub has become unreachable. - [Mostly successful integration test](https://job-manager.dsde-prod.broadinstitute.org/jobs/32b9e2b5-3c56-4bf8-ab5e-66fa72c7cadb), delta some existing issues with cost discrepancies documented in VS-1324.; - [Successful PGEN extract](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/1f19bea2-a9b9-4ec5-b741-e9f64bbfa35a)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8764:143,avoid,avoid,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8764,1,['avoid'],['avoid']
Safety,Use per-group sample info to avoid scanning refs/vets,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8676:29,avoid,avoid,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8676,1,['avoid'],['avoid']
Safety,"User Report:. ```; gatk --java-options ""-Xmx6g"" CombineGVCFs -R $ref -O all.g.vcf --variant gvcf.list; Using GATK jar /mnt/d/software_lin/gatk/gatk-package-4.1.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6g -jar /mnt/d/software_lin/gatk/gatk-package-4.1.4.0-local.jar CombineGVCFs -R /mnt/d/data/gatk_bundle/hg38//Homo_sapiens_assembly38.fasta.gz -O all.g.vcf --variant gvcf.list; 22:49:52.449 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/d/software_lin/gatk/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Dec 19, 2019 10:49:53 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 22:49:53.492 INFO CombineGVCFs - ------------------------------------------------------------; 22:49:53.494 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.1.4.0; 22:49:53.494 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:49:53.495 INFO CombineGVCFs - Executing as jiehuang@DESKTOP-POF1PJ4 on Linux v4.4.0-17763-Microsoft amd64; 22:49:53.495 INFO CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v11.0.4+11-post-Ubuntu-1ubuntu218.04.3; 22:49:53.496 INFO CombineGVCFs - Start Date/Time: December 19, 2019 at 10:49:52 PM GMT; 22:49:53.496 INFO CombineGVCFs - ------------------------------------------------------------; 22:49:53.497 INFO CombineGVCFs - ------------------------------------------------------------; 22:49:53.498 INFO CombineGVCFs - HTSJDK Version: 2.20.3; 22:49:53.502 INFO CombineGVCFs - Picard Version: 2.21.1; 22:49:53.502 INFO CombineGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:49:53.523 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:49:53.524 INFO CombineGVCFs - HTSJDK Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6340:841,detect,detect,841,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6340,1,['detect'],['detect']
Safety,"User is seeing non-ref blocks with GQ=0 even though bamout shows reads covering that region. Example: chr1	14155446	.	C	<NON_REF>	.	.	END=14155446	GT:DP:GQ:MIN_DP:PL	0/0: 100:0: 100 :0,0,0. Discussed this at the gatk office hrs and seems like this might be a bug. . Please find below the user report:; 1) I'm using 4.1.3.0; 2) java -Xmx6g -jar ../gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar HaplotypeCaller -R ~/gatk/hg19.unmasked.fa -G StandardAnnotation -mbq 17 --standard-min-confidence-threshold-for-calling 25 --max-reads-per-alignment-start 0 --disable-read-filter NotDuplicateReadFilter --min-dangling-branch-length 5 --allow-non-unique-kmers-in-ref --kmer-size 30 --kmer-size 10 --kmer-size 15 -ERC GVCF -I read1.sampe.bam -O small.i.vcf -L small.intervals --recover-all-dangling-branches --assembly-region-out test.txt --dont-trim-active-regions --min-assembly-region-size 28. 3) The bamout does seem to include the region of interest where GT=0 (see image https://us.v-cdn.net/5019796/uploads/editor/ye/ty6e3xa8vqom.png """"). 4) this it the relevant region in the VCF:; chr1	14155328	.	G	<NON_REF>	.	.	END=14155401	GT:DP:GQ:MIN_DP:PL	0/0:10:30:10:0,30,385; chr1	14155402	.	G	A,<NON_REF>	414.02	.	DP=10;ExcessHet=3.0103;MLEAC=2,0;MLEAF=1.00,0.00;RAW_MQandDP=36000,10	GT:AD:DP:GQ:PL:SB	1/1:0,10,0:10:30:428,30,0,428,30,428:0,0,10,0; chr1	14155403	.	T	<NON_REF>	.	.	END=14155440	GT:DP:GQ:MIN_DP:PL	0/0:10:30:10:0,30,426; chr1	14155441	.	G	<NON_REF>	.	.	END=14155596	GT:DP:GQ:MIN_DP:PL	0/0:0:0:0:0,0,0. This is a region if I'm altering the read to include a SNP in the region where there is a GT=0:; chr1	14155328	.	G	<NON_REF>	.	.	END=14155401	GT:DP:GQ:MIN_DP:PL	0/0: 100:99:1 00:0,120,1800; chr1	14155402	.	G	A,<NON_REF>	4486.03	.	DP=100;ExcessHet=3.0103;MLEAC=2,0;MLEAF=1.00,0.00;RAW_MQandDP=360000,100	GT:AD:DP:GQ:PL:SB	1/1:0,100,0: 100:99:4500,301,0,4500,301,4500:0,0,100,0; chr1	14155403	.	T	<NON_REF>	.	.	END=14155445	GT:DP:GQ:MIN_DP:PL	0/0: 100:99: 100:0,120,1800; chr1	14155446	.	C	<NON_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6100:767,recover,recover-all-dangling-branches,767,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6100,1,['recover'],['recover-all-dangling-branches']
Safety,"Users (@yfarjoun @jnoms) have been reporting high run times in PathSeq when the samples have a large proportion (on the order of 10%+) of microbial reads. PathSeq was designed to run on samples with low numbers (<1%) microbial reads, but there are two ways users can currently improve performance when that's not the case:. 1) Run the 3 PathSeq tools individually (Filter, Align, and Score) instead of using `PathSeqPipelineSpark`, which simply runs those in series. This will eliminate over-allocation of resources during Filter and Score. This also will reduce the likelihood that Spark will recompute parts of the graph when it is low on memory/disk. ; 2) Enable `--skip-pre-bwa-repartition`, see https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_spark_pathseq_PathSeqPipelineSpark.php#--skip-pre-bwa-repartition; 3) Omit metric file outputs. This may also help Spark to avoid recomputing tasks from earlier parts of the graph. Planned features to help improve this:; 1) Automatically enable `--skip-pre-bwa-repartition` when a large amount of non-host reads is detected.; 2) Option to downsample the input BAM on the fly. This is also useful for estimating the proportion of non-host contamination.; 3) Option to limit the number of non-host reads that are processed. This is essentially equivalent to (2), but the downsampling would be performed after host filtering and could be used when the fraction of non-host reads is unknown a priori.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5780:932,avoid,avoid,932,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5780,2,"['avoid', 'detect']","['avoid', 'detected']"
Safety,"Using a larger split size for loading reads via Hadoop-BAM would improve performance -- @tomwhite has shown that the process of reading the input splits is expensive with a small split size like our current default of 10 MB (`ReadsSparkSource.DEFAULT_SPLIT_SIZE`). Let's see if we can use a larger split size for loading the reads, then re-partition back to a smaller split size that is a factor of the larger size before doing any work to avoid out-of-memory issues in map jobs. Ideally the re-partitioning step should NOT trigger a shuffle -- if it does, we may not want to make this change.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1248:440,avoid,avoid,440,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1248,1,['avoid'],['avoid']
Safety,"Using the latest `master` version of GATK4 as dependency in gradle (using [jitpack](https://jitpack.io/)) to build a shadow jar generetes the following error when a custom walker run using `java -jar shadowJar.jar CustomTool`, but also with public tools. Only if `--use_jdk_deflater true` is provided, it works. . The log is the following (there is no other log):. ```; 14:57:19.102 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/daniel/workspaces/gatk4test/build/libs/shadowJar-0.0.1-SNAPSHOT-all.jar!/com/intel/gkl/native/libIntelGKL.dylib; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x0000000128c014d0, pid=31197, tid=5891; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libIntelGKL8818190486223479934.dylib+0xe4d0] _ZN7ContextIfEC2Ev+0x30; #; # Core dump written. Default location: /cores/core or core.31197; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/gatk4test/hs_err_pid31197.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; Abort trap: 6 (core dumped); ```. To fix it, I tried by excluding `com.intel.gkl` from GATK and add it as a dependency to my program, but it blows up anyway. In addition, I tried a sample program to load the PairHMM fastest implementation by `PairHMM.Implementation.FASTEST_AVAILABLE.makeNewHMM()`, and it also blows up. If I remove completely the dependency in my shadow jar, the command line blows up because the gkl `IntelDeflaterFactory` is not found. I guess that the error in the library is GKL-related, but in the case of the GATK framework I would like to have a way of using the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1985:601,detect,detected,601,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985,1,['detect'],['detected']
Safety,VCF files with different syntax for chromosomes than the reference file are not detected,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7538:80,detect,detected,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7538,1,['detect'],['detected']
Safety,VariantAnnotatorEngineUnitTest.testAllAnnotations doesn't detect when new un-asserted annotations are added,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2798:58,detect,detect,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2798,1,['detect'],['detect']
Safety,"VariantWalker/MultiVariantWalker create two FeatureDataSources for each input, which results in redundant dynamic discovery of which codec to use. For a MultiVariantWalker with a lot of inputs, like VariantRecalibrator, this can be a lot of path/stream opening/closing. Propose this small change to cache the codec in the FeatureInput. Ideally FeatureManager would keep track of this, but thats bigger refactor as not all of the FeatureDataSources are created by FeatureManager.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2737:96,redund,redundant,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2737,1,['redund'],['redundant']
Safety,WARNING: Failed to detect whether we are running on Google Compute Engine.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447:19,detect,detect,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447,1,['detect'],['detect']
Safety,"We acknowledge that in targeted capture the reads will predominantly from one strand at the ends of the targets, but we still try to calculate the strand bias for alt vs. ref. The so-called StrandOddsRatio I wrote when I was young and naive (just don't blame me for the name) adds pseudo counts to the calculations to avoid dividing by zero, but this becomes disastrous at hom-var sites where all the reads are in one direction, e.g. homVar SB:0,0,0,135 (from #5698). The expected behavior should be to return a value that's typical of an unbiased site because this isn't a reason to filter the variant.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5711:318,avoid,avoid,318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5711,1,['avoid'],['avoid']
Safety,"We already added the functionality needed for the gCNV workflow to IntervalListTools in https://github.com/broadinstitute/picard/pull/1208. The issue is that the tool outputs each scattered interval list to a separate directory if the number of scatters is greater than 1, but it just outputs to a file (essentially a noop) if we don't need to scatter. Not sure the reason for this design, but it makes things difficult from the perspective of WDL. Would be easier if the expected output was always `Array[File]+`. I don't really see why IntervalListTools needs to create those intermediate directories (nor why the naming scheme is determined by `DecimalFormat(""0000"")`---don't think this is documented, either), but I am not sure if that behavior is expected by now or if it is safe to modify it. Pretty sure SplitIntervals is just calling the same backend class used by IntervalListTools. Perhaps that tool might've been spun off before we exposed the Picard tools? See e.g. https://github.com/broadinstitute/gatk/pull/5392#issuecomment-435588845. I think we should try to avoid writing such custom/utility GATK tools unless really warranted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6502#issuecomment-599639907:780,safe,safe,780,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6502#issuecomment-599639907,2,"['avoid', 'safe']","['avoid', 'safe']"
Safety,"We are rewriting that tool in java to prevent issues like this. You could pull that branch from here: https://github.com/broadinstitute/gatk/pull/4800.; Otherwise, I think pysam handles gzipped indexed vcfs better than plain text ones. So if you gzip your resource files with ; `; bgzip -c hapmap_3.3.hg19.sites.vcf > hapmap_3.3.hg19.sites.vcf.gz; `; then index them with tabix:; `; tabix -p vcf hapmap_3.3.hg19.sites.vcf.gz; `; you may avoid this crash.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4794#issuecomment-391385377:437,avoid,avoid,437,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4794#issuecomment-391385377,1,['avoid'],['avoid']
Safety,We are timing our intermittently on travis -- let's increase the timeout.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1308:65,timeout,timeout,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1308,1,['timeout'],['timeout']
Safety,"We blow up with the following scary error:. org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.io.IOException: unexpected exception type; at java.io.ObjectStreamClass.throwMiscException(ObjectStreamClass.java:1538); at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1110); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1810); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1315:81,abort,aborted,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315,1,['abort'],['aborted']
Safety,"We can add this as an option, but I think there are a few arguments against outright replacement (which may have led to this design decision in the first place):. 1) I don't believe any CNV tools take in sample name as input at the GATK command line, by design. We instead take the BAM basename as the entity ID during the CollectCounts task; this ID is then passed along at the WDL level and is only used to construct the filenames of output files. This obviates the need for tools like GetSampleName and avoids issues with parsing funky sample names at the command line, handling/passing special characters at all levels (WDL, Java, python), etc. (The implicit assumption is that the BAM basename is more likely to be well formed.). 2) I would argue that specifying a sample index is relatively user friendly, especially if we are typically running on all samples. In that case, all you need to know is the total number of samples and that we use zero indexing, and then you simply iterate over all indices. (If you want to run on a single, particular sample, then perhaps using the sample name might be more friendly, but I'd argue that this use case is not typical.). 3) If you want to go ahead and add this option, I would probably keep the directory structure of the GermlineCNVCaller output the same (i.e., with folders named ""SAMPLE_#""), and just check the sample_name.txt files at the PostprocessGermlineCNVCalls step. I don't think this should require GermlineCNVCaller code changes, right?. 4) We may require additional code at the WDL level if we want to both switch over to primarily using sample names but also get rid of bundling (i.e., by passing only the calls for each sample when needed). Locally, you can always just search all output for directories containing the appropriate sample_name.txt. But on the cloud, you'd want to make sure that the postprocessing step for a particular sample gets only its corresponding directories, which would have to happen at the WDL level; the c",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765:506,avoid,avoids,506,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765,2,['avoid'],['avoids']
Safety,"We can certainly modify IntervalListTools to make the behavior more intuitive (e.g. add a new mode for `INTERVAL_COUNT_WITH_OVREFLOW`), but I'm not sure I understand the issue. We're just trying to avoid calling the GATK command if the scatter is going to be a noop?. The GATK SplitIntervals has slightly different behavior that's helpful in some cases.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6502#issuecomment-599618889:198,avoid,avoid,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6502#issuecomment-599618889,2,['avoid'],['avoid']
Safety,"We can implement this, but first can you explain why your `canDecode()` methods can't unambiguously detect your file formats? The VCF/BCF codecs use a magic value at the start of the file to detect the format -- are your codecs guessing as to the intended format? Is there no way to be sure what the format is?. If we have multiple codecs able to decode a file, and only one produces `Features` that match the type parameter of the `FeatureInput`, would it solve your problem if we selected that codec rather than blow up? This would be a bit simpler/easier than a new annotation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1184#issuecomment-163297503:100,detect,detect,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1184#issuecomment-163297503,2,['detect'],['detect']
Safety,We can simplify it by removing thread safety guarantees and then push it down into htsjdk,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/89:38,safe,safety,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/89,1,['safe'],['safety']
Safety,"We decided to remove the ""conversion"" to AllelicCapseg output from ModelSegments, since this was an ill defined procedure. The models used by AllelicCapseg and AllelicCNV/ModelSegments are simply different, so it's not possible to define a unique conversion between their model parameters. Compounding this, we also had difficulty finding up-to-date documentation about the models used by various versions of both AllelicCapseg and ABSOLUTE. That said, some of this removed functionality can be found in unsupported WDLs at https://github.com/broadinstitute/gatk/tree/master/scripts/unsupported/combine_tracks_postprocessing_cnv (specifically, see the PrototypeACSConversion task in combine_tracks.wdl). These scripts also attempt to perform rudimentary filtering of germline events found in the matched normal; see first link below for some additional caveats. Note that we cannot really answer further questions or otherwise support these scripts (and it's possible that the experimental/beta GATK tools used in the WDLs may be removed in the future), and the developer responsible for them has moved on from the Broad---use them at your own risk. See also https://gatkforums.broadinstitute.org/gatk/discussion/comment/59467 https://github.com/broadinstitute/gatk/pull/5450 https://github.com/broadinstitute/gatk/issues/5804 for additional context.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6685#issuecomment-652407603:1144,risk,risk,1144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6685#issuecomment-652407603,2,['risk'],['risk']
Safety,"We decided to replace SparkGenomeReadCounts with a relatively simple ReadWalker to avoid various bugs we were running into (some of which were due to Hadoop-BAM). We found that these bugs gave rise to a relatively high failure rate---roughly 1 in 50 TCGA BAMs. Like any ReadWalker, you can specify custom read filters using GATK engine arguments such as `--disable-default-read-filters` and `--read-filter ...` However, because we count fragment centers (rather than read starts, as in SparkGenomeReadCounts), disabling filters which check that reads are properly paired may lead to unexpected behavior. In principle, we could write a similar ReadWalkerSpark version of the tool. However, our experience running the tool showed that CollectFragmentCounts was already faster than SparkGenomeReadCounts in Spark local mode, sometimes by a factor of ~5 (and, more importantly, it didn't run into Hadoop-BAM failures). We may do some more careful profiling and roll a ReadWalkerSpark version in the future, but these aren't too high priority at the moment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4185#issuecomment-358660583:83,avoid,avoid,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4185#issuecomment-358660583,2,['avoid'],['avoid']
Safety,"We have [a researcher reporting that even when running locally, they are getting messages related to the GCS](https://gatkforums.broadinstitute.org/gatk/discussion/13015/failed-to-detect-whether-we-are-running-on-google-compute-engine#latest), which they find puzzling. > WARNING: Failed to detect whether we are running on Google Compute Engine. plus what looks like an error stacktrace in the middle of the stdout. Is this intentional?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4369#issuecomment-424004818:180,detect,detect-whether-we-are-running-on-google-compute-engine,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369#issuecomment-424004818,2,['detect'],"['detect', 'detect-whether-we-are-running-on-google-compute-engine']"
Safety,"We have a somewhat urgent need for a Spark partitioner (or something similar) that can handle loading of records for regions that overlap. For example, we'd like to be able to have a partition with all reads in `1:900-2100`, and a separate partition with reads in `1:1900-3100`, and some reads in the overlapping regions would appear in both partitions. . Such a partitioner is needed for several current GATK projects, including:. https://github.com/broadinstitute/gatk/issues/1639 Spark version of HaplotypeCaller (the prototype currently uses an inefficient `groupBy` operation to accomplish this). https://github.com/broadinstitute/gatk/issues/1558 Make pileups in parallel on Spark . It also might be needed for avoiding boundary artifacts in the SV caller.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1988:717,avoid,avoiding,717,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1988,1,['avoid'],['avoiding']
Safety,We have asked the green team to run their pipeline tests on this branch to at least limit the risk of more full sample failures. It will probably be a few more days before we have those results. @gbggrant,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6533#issuecomment-614126868:94,risk,risk,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6533#issuecomment-614126868,1,['risk'],['risk']
Safety,"We need a way to install GATK Python modules onto the docker image, from repo source, in a way that doesn't assume a repo clone is present on the docker image (there currently is one, but we want to remove it to recover space), and that also doesn't make the conda environment dependent on a repo clone. This PR adds a build task that creates a zip archive of the GATK Python source; propagates that to the docker image, and then pip installs the contents of the archive into the conda environment on the docker. Since we don't have an actual python module in the repo at the moment, there is a second, temporary, commit that contains a dummy python module used only to trigger and test that the installation works.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3964:212,recover,recover,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3964,1,['recover'],['recover']
Safety,"We need the ability to store command-line argument definitions in @ArgumentCollections like in the GATK, to avoid duplicate definitions, and to provide a standard way of accessing the argument values.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/96:108,avoid,avoid,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/96,1,['avoid'],['avoid']
Safety,"We need to be able to build docker images for gatk remotely for people who cant run our current build script. For carrot we have already gotten the build working with GoogleCloudBuild using a command like this:; `gcloud builds submit --tag {whatever you want to tag the image and push it as} --timeout=24h --machine-type e2_highcpu_8`; This script should function like our current docker script, it should optionally check out a fresh remote gatk branch/image clone it locally, use that context to do the build, then remove the directory that was cloned when it is done (similar to how `build_docker.sh` works currently)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7949:294,timeout,timeout,294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7949,1,['timeout'],['timeout']
Safety,"We need to help users help themselves either with better checks in tools or with better documentation to avoid the discrepancies observed in this thread, whose answer is recapitulated below. ---; Hi @obigriffith,. I am using GATK v4.0.11.0 and I also see what you are seeing. I've been taking an Android App development course since January (in my free time of course), and I've learned that with multiple expressions, sometimes the Java programming language needs help in parsing expressions. That is, we need to help the tool demarcate where an expression begins and ends. **1. no filtering expected works as expected (but this is misleading)**; ```; --filter-expression ""QD < 2.0 || FS > 60.0 || MQ < 40.0 || MQRandSum < -12.5 || ReadPosRankSum < -8.0 || SOR > 3.0""; ```; ![](https://us.v-cdn.net/5019796/uploads/editor/a8/q0yjdx55d0fz.png """"). **2. should be filtered based on SOR (at 0.608) but is not**; ```; --filter-expression ""QD < 2.0 || FS > 60.0 || MQ < 40.0 || MQRandSum < -12.5 || ReadPosRankSum < -8.0 || SOR > 0.5"" ; ```; ![](https://us.v-cdn.net/5019796/uploads/editor/2s/tft38knytdib.png """"). **3. Using parentheses around each expression allows SOR (and presumably other expressions) to be read correctly**; ```; --filter-expression ""(QD < 2.0) || (FS > 60.0) || (MQ < 40.0) || (MQRankSum < -12.5) || (ReadPosRankSum < -8.0) || (SOR > 0.5)""; ```; This will allow the tool to read the SOR expression unambiguously. Here are results from my testing:; ![](https://us.v-cdn.net/5019796/uploads/editor/o4/5939fiysxmr4.png """"). **4. Providing each expression as a separate parameter also allows SOR (and others) to be read correctly and also provides additional insight**; Separate out each condition into individual filter expressions:; ```; --filter-expression ""QD < 2.0"" --filter-name ""QDlessthan2"" --filter-expression ""FS > 60.0"" --filter-name ""FSgreaterthan60"" --filter-expression ""MQ < 90.0"" --filter-name ""MQlessthan90"" --filter-expression ""MQRankSum < -12.5"" --filter-name ""MQRank",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5362:105,avoid,avoid,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5362,1,['avoid'],['avoid']
Safety,We run contamination checks on the somatic VCF using the AMBER / COBALT / PURPLE pipeline. It does not flag contamination. [PURPLE](https://github.com/hartwigmedical/hmftools/tree/master/purity-ploidy-estimator) is a purity and ploidy estimator but often struggles with low purity samples and erroneously reports a samples such as this as purity 1.00 and ploidy 2.00. I wonder if such a sample could be detected using some tool from GATK?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-649095750:403,detect,detected,403,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-649095750,1,['detect'],['detected']
Safety,We should install the `python-is-python3` package during our docker build to create a symlink from `/usr/bin/python` to `/usr/bin/python3`. This would help avoid problems such as https://github.com/broadinstitute/gatk/issues/8402,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8497:156,avoid,avoid,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8497,1,['avoid'],['avoid']
Safety,"We somehow missed some new deprecation warnings that gradle gives when we updated to 7.3.2. . When running `bundle`; ex:; ```; Task :sparkJar; Execution optimizations have been disabled for task ':sparkJar' to ensure correctness due to the following reasons:; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/classes/java/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/resources/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/tmp/sparkJar/MANIFEST.MF'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; ```. ```; Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7625:269,detect,detected,269,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7625,2,['detect'],['detected']
Safety,"We test against 8 and 11 so I would expect either of those to work with out any weird issues. . I can't guarantee that 17 is without problems, but I think if you're running it and the output looks reasonable than there's nothing I know of that would cause subtle errors. You'll definitely have issues with things like building the documentation, but I suspect that if something runs without crashing it's probably ok. It's not officially supported right now though, so it's a bit of use at your own risk.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7842#issuecomment-1122762387:499,risk,risk,499,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7842#issuecomment-1122762387,1,['risk'],['risk']
Safety,"We use the `machine_mem`/`command_mem` framework for most tasks; others are unlikely to need any special memory considerations. There were some tasks for which `machine_mem` and `command_mem` seemed to be switched in the original WDL. @jsotobroad was there any reason for this? I'm guessing they were just typos. I'm also not sure that some of the tasks would've actually run even if they were switched, since they would've resulted in non-integer -Xmx arguments. I changed everything over to MB to avoid this. Closes #4092.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4193:499,avoid,avoid,499,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4193,1,['avoid'],['avoid']
Safety,"We used to call this `--unsafe`, but I feel like you've just put lipstick on the pig. Usually you're more conservative than I am, so if you're okay with this then so am I.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6589#issuecomment-625427966:24,unsafe,unsafe,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6589#issuecomment-625427966,1,['unsafe'],['unsafe']
Safety,"We will certainly output VCFs for our final calls, but we also have useful intermediate file formats that would benefit from having the sample name (and sequence dictionary, if appropriate) attached---for example, our read-count and allelic-count files. I see no reason that these shouldn't have a header with the appropriate tags generated from the input BAM and be read/written in the Tribble framework. However, there are a few exceptions in the CNV pipeline for files that not collections of Features but are still associated with a sample, so I think it would be nice to extract out the sample-name related code across the various tools. This is not only to avoid duplicating effort, but also to standardize how we store this information in our various file formats.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3726#issuecomment-338288560:663,avoid,avoid,663,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3726#issuecomment-338288560,1,['avoid'],['avoid']
Safety,We're having issues with all of our tests today. Github is refusing our git-lfs requests because they're over quota and we need to figure out how to either authenticate our requests in a safe way from travis or figure out why we're suddenly going over quota. It happened very suddenly and I suspect there might be an issue on github's end..,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2506#issuecomment-292005363:187,safe,safe,187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2506#issuecomment-292005363,1,['safe'],['safe']
Safety,We've been noticing a number of sporadic wdl test failures recently. They typically resolve after rerunning. Some of them seem to be the travis timeout after 10 minutes of inactivity. It would be good if we could make that happen less often. Here's an example of a mutect2 wdl failure [travis log.txt](https://github.com/broadinstitute/gatk/files/1623864/travis.log.txt),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4129:144,timeout,timeout,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4129,1,['timeout'],['timeout']
Safety,"Well, I definitely like the safety and generality of this design better, thought as it stands it wastes the first turn of the (`ReadsDataSource`) crank, which would be nice to avoid. Let me take a closer look. Maybe we can replace `TwoPassReadWalker` with this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5985#issuecomment-499482109:28,safe,safety,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5985#issuecomment-499482109,2,"['avoid', 'safe']","['avoid', 'safety']"
Safety,"Well, you're welcome to use gatk-launch as a launch script if you'd like (and feel free to rename to whatever you like...) A. There are a few reasons we have spark and non-spark versions of the tools. . 1. We wanted to port and validate certain tools as quickly as possible and doing a direct port from gatk3 -> gatk4 was easier than making them sparkified at the same time. 2. There's a tradeoff in using spark where you end up spending more total cpu hours in order to finish a job faster. Ideally this would be 1:1, double the number of cores and you halve the time to finish a job. It never scales perfectly though, there's always some overhead for being parallel. Our production pipelines are extremely sensitive to cost and not very sensitive to runtime, so they prefer we have a version that's optimized to use the least cpu hours even if that means a longer runtime. Other users prefer to be able to finish a job quickly and are willing to pay slightly more to do so, so we also have a spark version. . 3. Some tool are complicated to make work well spark. Spark works best when you can divide the input data into independent shards and then process them separately. This is complicated for things like the AssemblyRegion walker where you need context around each location of interest. We had to do things like add extra overlapping padding and things like that to avoid boundary issues where there are shard divisions. We don't yet fully understand spark performance and it's caveats, we're looking into that actively now. We hope that we'll be able to optimize our tools so that a spark pipeline of several tools in series is faster than running the individual non-spark versions, since it lets us avoid doing things like loading the bam file multiple times from disk. Whether or not we can achieve this is still and open question though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273318100:1373,avoid,avoid,1373,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273318100,2,['avoid'],['avoid']
Safety,"What do you mean by more automated? It looks like you're allocating space based on the input file sizes and some padding, which is already more automated than the user adjusting disk size by hand. Do you mean that Cromwell should allocate space appropriately given the inputs? The issue is that you also need space for the outputs, which is harder to predict unless you have a sense of what the task is doing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4737#issuecomment-386594579:351,predict,predict,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4737#issuecomment-386594579,1,['predict'],['predict']
Safety,"What is the best Sparkified way to recapitulate our germline Best Practices workflow? Is there one pipeline that encompasses all steps from BWA alignment to HaplotypeCaller calling?. Please see forum thread as I have answered the user question tentatively with these two options:. [1] BwaSpark --> SortReadFileSpark --> ReadsPipelineSpark; [2] BwaAndMarkDuplicatesPipelineSpark --> SortReadFileSpark --> BQSRPipelineSpark --> HaplotypeCallerSpark. Thanks. ---; Hi @shlee ,; I am really sorry for the delay but I was busy in the last weeks. Anyway I will try to be clearer with this picture:. ![](https://us.v-cdn.net/5019796/uploads/editor/3x/9bu9fsvbgjrh.png """"). as you can see I would like to combine the tools `BwaAndMarkDuplicatesPipelineSpark` and `BQSRPipelineSpark` in one single tool, in order to improve efficiency of the pipeline (avoiding for example a disk writing). ; I tried to do it with [this](https://pastebin.com/XEqvpKmG ""this"") naive approach as I reported in previous comments, but executing this code I obtain this error (as you can see at the end of this [stack-trace](https://paste.ee/p/dMod1 ""stack-trace"") ) : ; ```; 17/11/03 13:02:14 ERROR Utils: Aborting task; java.lang.IllegalArgumentException: Reference index for 'chr11' not found in sequence dictionary.; ```. Do you think is better if I speak directly with developers in the GitHub repository?. Best regards,; Nicholas. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/44143#Comment_44143",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3878:842,avoid,avoiding,842,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3878,2,"['Abort', 'avoid']","['Aborting', 'avoiding']"
Safety,When I am trying to run gatk HallotypeCaller I got error below. Can someone give me some advices?. INFO: Failed to detect whether we are running on Google Compute Engine.; 12:55:36.413 INFO HaplotypeCaller - ------------------------------------------------------------; 12:55:36.413 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.0.0; 12:55:36.414 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:55:36.414 INFO HaplotypeCaller - Executing as linux@Paulina on Linux v5.4.0-67-generic amd64; 12:55:36.414 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v11.0.10+9-Ubuntu-0ubuntu1.18.04; 12:55:36.414 INFO HaplotypeCaller - Start Date/Time: 24 April 2021 at 12:55:36 CEST; 12:55:36.414 INFO HaplotypeCaller - ------------------------------------------------------------; 12:55:36.414 INFO HaplotypeCaller - ------------------------------------------------------------; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Version: 2.24.0; 12:55:36.414 INFO HaplotypeCaller - Picard Version: 2.25.0; 12:55:36.414 INFO HaplotypeCaller - Built for Spark Version: 2.4.5; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:55:36.415 INFO HaplotypeCaller - Deflater: IntelDeflater; 12:55:36.415 INFO HaplotypeCaller - Inflater: IntelInflater; 12:55:36.415 INFO HaplotypeCaller - GCS max retries/reopens: 20; 12:55:36.415 INFO HaplotypeCaller - Requester pays: disabled; 12:55:36.415 INFO HaplotypeCaller - Initializing engine; 12:55:36.508 INFO IntervalArgumentCollection - Processing 1 bp from intervals; 12:55:36.511 INFO HaplotypeCaller - Done initializing engine; 12:55:36.515 INFO HaplotypeCallerEngine - Disabling physical p,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7229:115,detect,detect,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7229,1,['detect'],['detect']
Safety,"When `GenotypeGVCFs` is sharded by `-L` interval, as it typically is in production, it needs a way to avoid emitting the same call across shards in the case of indels that span interval boundaries. The simplest way to do this is to only emit records that start in the current interval. It might make sense to do this at the VCF writer level, and hook it up to an engine-wide argument to toggle this behavior. `HaplotypeCaller` and `Mutect` currently do something similar internally, and could possibly leverage this functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2735:102,avoid,avoid,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2735,1,['avoid'],['avoid']
Safety,"When extracting the overlapped bases of two reads, FragmentUtils miscalculates the starting position of the second read if there are softclipped bases at its head. This leads to misalignment of the two reads and incorrect updating of the base qualities. In some other cases, it may fail to detect overlapping bases if the first and second read both contain softclips. This pull request updates the position calculation and adds a unit test to demonstrate the issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6824:290,detect,detect,290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6824,1,['detect'],['detect']
Safety,"When running tests with `./gradlew clean test` without the lfs files, an error code is thrown from within BWA causing the test suite to fail. . Error and Stack Trace:. ...; 11:54:40.426 [ERROR] [system.err] [bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!; ... 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':test'.; 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:98); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:271,Abort,Abort,271,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['Abort'],['Abort']
Safety,"When the engine ""marginalizes"" haplotype likelihoods into allele likelihoods it avoids double-counting of both the MNP at 151 and the SNP at 152. That is, the CT->TC MNP haplotype is consistent at 152 with the T->C SNP, but it has a different start position and therefore is not marginalized into the evidence for the SNP. So the fact that the DP in sample1 is much less at 152 than at 151 makes sense. I am also confused about sample2. If we're both still confused tomorrow, let's take a look in IGV. Might even need IntelliJ. Might even be a bug -- if you look in `AssemblyBasedCallerGenotypingEngine.createAlleleMapper`, you'll see that the overlapping event logic assumes we're dealing with upstream spanning deletions. Maybe MNPs need to be treated differently.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5513#issuecomment-447033286:80,avoid,avoids,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5513#issuecomment-447033286,1,['avoid'],['avoids']
Safety,"While testing BwaSpark on a yarn cluster, I noticed that only a single executor (the driver) was being created and running only on the master node. The worker nodes were idle. This happened when running the following command . ```; spark-submit \; --deploy-mode client \; --class org.broadinstitute.hellbender.Main \; --master yarn \; /home/hadoop/gatk-package-4.alpha.2-269-gdce8abc-SNAPSHOT-spark.jar BwaSpark \; --bwamemIndexImage /var/tmp/hs38DH-V.fasta.img \; -I hdfs:///unaligned.bam \; -O hdfs:///aligned.bam \; -R hdfs:///hg38/hs38DH-V.fasta \; --disableSequenceDictionaryValidation true; ```. I checked the spark environment settings in the spark web UI and found that `spark.master` was set to `local` despite the `--master yarn` cmd-line argument. Next I checked the gatk source and found a suspicious call to `SparkConf.setMaster()` at line 138 in ; `src/main/java/org/broadinstitute/hellbender/engine/spark/SparkContextFactory.java`. I commented out this call, recompiled, and tried again, and the issue went away. I suspect that `setMaster()` was overriding the `--master yarn` cmd-line argument. I believe this is a bug and propose somehow avoiding `setMaster()` when the app is run with `spark-submit`. Thanks,; David",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2718:1155,avoid,avoiding,1155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2718,1,['avoid'],['avoiding']
Safety,"While working on a GATK-3 bug, broadinstitute/gsa-unstable#855, @SHuang-Broad noticed that the natural place to avoid an explosion of genotypes (i.e. the size of the GLs vector) for a large number of alleles is right after computing haplotype likelihoods. In GATK-4 this is in `HaplotypeCallerEngine::callRegion` after the line `final ReadLikelihoods<Haplotype> readLikelihoods =; likelihoodCalculationEngine.computeReadLikelihoods(assemblyResult, samplesList, reads)`. . However, at this point there is the following comment:. ``` java; // Note: we used to subset down at this point to only the ""best"" haplotypes in all samples for genotyping, but there; // was a bad interaction between that selection and the marginalization that happens over each event when computing; // GLs. In particular, for samples that are heterozygous non-reference (B/C) the marginalization for B treats the; // haplotype containing C as reference (and vice versa). Now this is fine if all possible haplotypes are included; // in the genotyping, but we lose information if we select down to a few haplotypes. [EB]; ```. If I understand EB's (@eitanbanks?) point right, subsetting here is dangerous only because of our downstream hack to turn multiallelic genotyping into a biallelic problem. The new AF/QUAL model will perform honest multiallelic genotyping, hence it makes sense to revisit whether to subset haplotypes at this point.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1868:112,avoid,avoid,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1868,1,['avoid'],['avoid']
Safety,Without JCenter the retrieval of this artifact is failing like:. ```; #16 21.85 A problem occurred evaluating root project 'gatk'.; #16 21.85 > Could not resolve all files for configuration ':runtimeClasspath'.; #16 21.85 > Could not find biz.k11i:xgboost-predictor:0.3.0.; #16 21.85 Searched in the following locations:; #16 21.85 - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 Required by:; #16 21.85 project :; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7830:256,predict,predictor,256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7830,9,['predict'],"['predictor', 'predictor-']"
Safety,"Wow. How on Earth did we avoid this for so long?. On Wed, Jan 24, 2018 at 4:39 PM droazen <notifications@github.com> wrote:. > @ldgauthier <https://github.com/ldgauthier> @yfarjoun; > <https://github.com/yfarjoun> We have an update on this! We've identified; > the bug:; >; > - When AbstractFeatureReader.getFeatureReader() tries to open a .vcf.gz; > that doesn't have an index, it returns a TribbleIndexedFeatureReader; > instead of a TabixFeatureReader, because methods.isTabix() returns; > false when an index is not present.; > - TribbleIndexedFeatureReader, in turn, opens a Java vanilla; > GZIPInputStream, instead of the BlockCompressedInputStream that gets; > opened when you create a TabixFeatureReader.; > - GZIPInputStream, in turn, has a *confirmed bug* filed against it in; > Oracle's bug tracker (see; > https://bugs.java.com/bugdatabase/view_bug.do?bug_id=7036144#), that; > it inappropriately relies on the available() method to detect; > end-of-file, which is never safe to do given the contract of; > available(); > - As the final piece in the ghastly puzzle, implementations of; > SeekableStream in htsjdk do not implement available() at all, instead; > using the default implementation which always returns 0.; >; > As a result of this combination of bugs in Java's GZIPInputStream itself; > and bugs in htsjdk's SeekableStream classes, end-of-file can be detected; > prematurely when within 26 bytes of the end of a block, due to the; > following code in GZIPInputStream.readTrailer():; >; > if (this.in.available() > 0 || n > 26) {; > ....; > }; > return true; // EOF; >; > Where n is the number of bytes left to inflate in the current block.; >; > The solution is to replace all usages of the bugged GZIPInputStream with; > BlockCompressedInputStream in tribble in htsjdk (at least, for points in; > the code where the input is known to be block-gzipped rather than regular; > gzipped). For due diligence we should also implement available(); > correctly for all implementations",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4224#issuecomment-360304725:25,avoid,avoid,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4224#issuecomment-360304725,1,['avoid'],['avoid']
Safety,Yea ideally we would have a more principled solution like the overall Viterbi sequence likelihood that Sam was trying to get to work. But it has been certainly very useful for now as a basic sanity check.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7150#issuecomment-811364073:191,sanity check,sanity check,191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7150#issuecomment-811364073,1,['sanity check'],['sanity check']
Safety,"Yeah that's one of the reasons we didn't catch this (and one other thread safety bug), we don't run multithreaded in prod and don't test for it systematically.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2326#issuecomment-270439784:74,safe,safety,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2326#issuecomment-270439784,1,['safe'],['safety']
Safety,"Yes @rong923, go with >= 4.1.1.0 and you are safe. I ve been running hundreds of pipelines without a problem now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-487724137:45,safe,safe,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-487724137,1,['safe'],['safe']
Safety,"Yes PathSeq has generally been used with longer reads in the past and also with BWA. I suspect with the shorter reads, novel splice sites are getting missed by PathSeq. You could turn down the match threshold to 16 (half the read length) to help this further, or even hard-filter any aligned read, but you do risk losing some microbial reads at this length. . You're also right that in your case, using the mate information would be useful, but this would not be appropriate, for example, when there are host-pathogen chimeras, such as virus integration events.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6687#issuecomment-652638341:309,risk,risk,309,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6687#issuecomment-652638341,1,['risk'],['risk']
Safety,"Yes please. Where we left it I think Louis was happy, but we wanted to ask Nalini if; she had any suggestions to avoid threading the argument for genotypes all; the way through the engine. On Thu, Mar 28, 2019 at 11:49 AM droazen <notifications@github.com> wrote:. > What's the status of this one @ldgauthier <https://github.com/ldgauthier>; > ? Do you need a new reviewer with @lbergelson; > <https://github.com/lbergelson> out?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4947#issuecomment-477654500>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdJBn0j6n9Dfp-sz763M3mP5b1oyDks5vbOSHgaJpZM4U4KK0>; > .; >. -- ; Laura Doyle Gauthier, Ph.D.; Associate Director, Germline Methods; Data Sciences Platform; gauthier@broadinstitute.org; Broad Institute of MIT & Harvard; 320 Charles St.; Cambridge MA 0214",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-477720872:113,avoid,avoid,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-477720872,1,['avoid'],['avoid']
Safety,"Yes we do, we run on a list of calling intervals that avoids empty/blackhole/timesuck regions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2591#issuecomment-297588499:54,avoid,avoids,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2591#issuecomment-297588499,1,['avoid'],['avoids']
Safety,"Yes, that's the error message. You should see [this part of the GenomicsDB](https://github.com/Intel-HLS/GenomicsDB/blob/stack_ovf_fix/src/main/cpp/src/genomicsdb/variant_storage_manager.cc#L540) code that retries the function when an error is detected. So, you don't need to be concerned by those error messages. They are annoying though and I will disable them from being printed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407440149:244,detect,detected,244,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407440149,1,['detect'],['detected']
Safety,"Yes, we can pretty easily detect this exit code and generate an error message suggesting it might be an OOM issue. I'll make a PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6362#issuecomment-572769179:26,detect,detect,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6362#issuecomment-572769179,1,['detect'],['detect']
Safety,"Yes. We should expect that PCR error shouldn't affect the base-quality, so two high quality, disagreeing bases are an indication of a PCR error, while one low-quality base, and one high quality base that have differing qualities looks more like a sequencing error. We might be able to obtain a data-driven model for that using the overlapping bases themselves (over monomorphic sites). The only problem is that this is only true when the reads haven't been processed by Consensus calling....but if we have a good model for consensus calling within haplotype caller we could avoid doing that upfront and simply deal with everything within haplotype caller. **That** would be ideal!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400815571:574,avoid,avoid,574,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400815571,2,['avoid'],['avoid']
Safety,You can also individually disable tool default read filters using the `--disable-read-filter` or `-DF` argument. . In general if you disable the basic safeguards the tool requires you're likely to run into issues though.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8224#issuecomment-1452506814:151,safe,safeguards,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8224#issuecomment-1452506814,1,['safe'],['safeguards']
Safety,"You can detect whether a tool has failed in bash by checking whether the exit status code is non-zero. In bash, the exit status code of the last command run is stored in the variable `$?`; ; In general, you should ask questions like these on the GATK forum (https://gatkforums.broadinstitute.org/gatk) instead of here, however -- this is for bug reports rather than support requests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4242#issuecomment-359955349:8,detect,detect,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4242#issuecomment-359955349,1,['detect'],['detect']
Safety,"You're right, @cmnbroad - the `@Argument` annotation is unsafe if it is included in an argument collection. Maybe a `@DocumentedField` annotation in barclay will help, by populating its information and include it in the json. This will allow to document also constant fields specific to a tool (e.g., a default value for a BAM tag, explaining why it is the default). That will be nice for several use cases, including the one here...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3947#issuecomment-351299081:56,unsafe,unsafe,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3947#issuecomment-351299081,1,['unsafe'],['unsafe']
Safety,"Your solution doesn't address your third listed drawback to the current; approach, though I'm not sure there's any way to do that that wouldn't; require a pretty dramatic change. It's not obvious to me why we wanted the given alleles in the graph; originally. Maybe the use case was variants from UG that we didn't; necessarily believe were aligned properly?. I don't have any objections, but I'd feel better if we had a better guess; at what the original method was trying to do. On Wed, Apr 3, 2019 at 9:56 PM David Benjamin <notifications@github.com>; wrote:. > In Mutect2 and HaplotypeCaller, we force-call alleles by injecting them; > into the ref haplotype, then threading these constructed haplotypes into; > the assembly graph with a large edge weight. There are several drawbacks to; > this approach:; >; > - The strange edge weights interfere with the AdaptiveChainPruner.; > - The large edge weights may not be large enough to avoid pruning when; > depth is extremely high.; > - The alleles may be lost if assembly fails.; > - If the alleles actually exist but are in phase with another variant; > we end up putting an enormous amount of weight on a false haplotype.; >; > We can get around these issue with the following method:; >; > - assemble haplotypes without regard to the force-called alleles.; > - if an allele is present in these haplotypes, do nothing further.; > - otherwise, add a haplotype in which the allele is injected into the; > reference haplotype.; >; > @LeeTL1220 <https://github.com/LeeTL1220> I prototyped this and it seems; > to resolve the missed forced alleles that Ziao found.; >; > @ldgauthier <https://github.com/ldgauthier> Can you think of any; > objections to making this change in HaplotypeCaller?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/5857>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdMcaTJg47gn",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5857#issuecomment-479916767:938,avoid,avoid,938,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5857#issuecomment-479916767,1,['avoid'],['avoid']
Safety,"Yup, as you might recall from some discussions with @bhanugandham and @mwalker174, getting automated pipeline-level CNV evaluations up and running was highest on my list before I handed over the role and went on paternity leave. I think these tests would be more useful than unit/integration tests for correctness, but they would almost certainly have to run on CARROT. That said, the current level of unit/integration test coverage is a bit different from that for the somatic tools, because 1) it's difficult to run gCNV in any useful way on Travis infrastructure, and 2) we hadn't decided on a framework/convention for python unit tests at the time the production code went in (although I think @ldgauthier has added some python unit tests by now). So we currently only have plumbing WDL/integration tests on very small data for gCNV---and these only test that the tools run, not for correctness. For somatic CNV, we have unit tests for correctness on small simulated data (e.g., for things like segmentation and modeling classes), but integration tests don't cover correctness (and it would be pretty redundant to use the same simulated data for integration, so I'd rather put effort towards pipeline-level tests on real data). It might be good for you and @mwalker174 to review the current level of testing coverage and understand where things need to be shored up---happy to discuss more.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7261#issuecomment-859563124:1105,redund,redundant,1105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7261#issuecomment-859563124,1,['redund'],['redundant']
Safety,[CNNScoreVariants] A timeout ocurred waiting for output from the remote Python command,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4696:21,timeout,timeout,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4696,1,['timeout'],['timeout']
Safety,[DRAFT] [IGNORE] sanity check on broken CI,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8369:17,sanity check,sanity check,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8369,1,['sanity check'],['sanity check']
Safety,[DRAFT] sanity check CI on master,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8390:8,sanity check,sanity check,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8390,1,['sanity check'],['sanity check']
Safety,"\* Opening on behalf of a user on an HPC cluster, my knowledge in this field is a bit limited. ### Affected tool(s) or class(es); gatk HaplotypeCaller. ### Affected version(s); Latest 4.6.0.0 release. ### Description ; When running command, ~16 hours into the run the program crashes. Below is the start of the Java error report file. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f06ed243291, pid=1058615, tid=1058616; #; # JRE version: OpenJDK Runtime Environment (17.0.2+8) (build 17.0.2+8-86); # Java VM: OpenJDK 64-Bit Server VM (17.0.2+8-86, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0xcf291] __memset_avx2_erms+0x11; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h %e"" (or dumping to /bigdata/ramadugulab/luy/SNPcallingBreeding/core.1058615); #; # If you would like to submit a bug report, please visit:; # https://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #. --------------- S U M M A R Y ------------. Command Line: -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /bigdata/operations/pkgadmin/opt/linux/centos/8.x/x86_64/pkgs/gatk/4.6.0.0/gatk-package-4.6.0.0-local.jar HaplotypeCaller -R /rhome/luy/bigdata/genomes/Cclementina_182_v1_2.fa -I AlignedCalToCcl_Scaffolds_MarkDupOut.bam -O AlignedCalToCcl_Scaffolds.vcf.gz -ERC GVCF. Host: Intel(R) Xeon(R) CPU E5-2683 v4 @ 2.10GHz, 64 cores, 20G, Rocky Linux release 8.8 (Green Obsidian); Time: Sat Sep 28 04:11:19 2024 PDT elapsed time: 58592.788414 seconds (0d 16h 16m 32s). --------------- T H R E A D ---------------. Current thread (0x00007f06e4025b70): JavaThread ""main""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8988:368,detect,detected,368,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8988,1,['detect'],['detected']
Safety,"_10.g.vcf.gz -V output/B8_11.g.vcf.gz -V output/F_A.g.vcf.gz -V output/B7_3.g.vcf.gz -V output/B8_19.g.vcf.gz -V output/A8.g.vcf.gz -V output/B7_4.g.vcf.gz -V output/B8_4.g.vcf.gz -V output/B8_4g.g.vcf.gz -V output/B8_16.g.vcf.gz -V output/A9_1.g.vcf.gz -V output/TRZ.g.vcf.gz -V output/RAINBOW.g.vcf.gz -V output/7_5_2.g.vcf.gz -V output/Athens.g.vcf.gz -V output/C1.g.vcf.gz -V output/C2.g.vcf.gz -V output/C3.g.vcf.gz -V output/A9-10.g.vcf.gz -V output/P3.g.vcf.gz -V output/P4.g.vcf.gz -V output/Spg.g.vcf.gz -V output/EtAB.g.vcf.gz -V output/TuR.g.vcf.gz -V output/TuG.g.vcf.gz --genomicsdb-workspace-path ABchroneALL --intervals pseudochromosome_1 --batch-size 6; 07:56:24.538 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xxxxxx/miniconda3/share/gatk4-4.1.6.0-0/gatk-package-4.1.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 27, 2020 7:56:24 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 07:56:24.849 INFO GenomicsDBImport - ------------------------------------------------------------; 07:56:24.850 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.6.0; 07:56:24.850 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.oAB/gatk/; 07:56:24.850 INFO GenomicsDBImport - Executing as xxxxxx@galaxy on Linux v4.4.0-133-generic amd64; 07:56:24.850 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 07:56:24.850 INFO GenomicsDBImport - Start Date/Time: 27 May 2020 07:56:24 CEST; 07:56:24.850 INFO GenomicsDBImport - ------------------------------------------------------------; 07:56:24.850 INFO GenomicsDBImport - ------------------------------------------------------------; 07:56:24.851 INFO GenomicsDBImport - HTSJDK Version: 2.21.2; 07:56:24.851 INFO GenomicsDBImport - Picard Version: 2.21.9; 07:56:24.851 INFO GenomicsDBImport - HTSJDK Defau",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6616:3520,detect,detect,3520,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6616,1,['detect'],['detect']
Safety,"_Ab.passed.vcf.gz; Using GATK jar /gatk/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx12G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /gatk/gatk-package-4.1.8.1-local.jar SelectVariants -R /scratch/DBC/BCRBIOIN/SHARED/genomes/homo_sapiens/GRCh38/dna/GRCh38.d1.vd1.fa -V /scratch/DBC/BCRBIOIN/SHARED/analysis/######/######/data/wgs/data/mutect2//tumour_samples/ML13_Ab/ML13_Ab.orientation_filtered.vcf.gz --exclude-filtered -O /scratch/DBC/BCRBIOIN/SHARED/analysis/######/######/data/wgs/data/mutect2//tumour_samples/ML13_Ab/ML13_Ab.passed.vcf.gz; 10:52:40.838 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Sep 02, 2020 10:52:41 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:52:41.263 INFO SelectVariants - ------------------------------------------------------------; 10:52:41.263 INFO SelectVariants - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:52:41.264 INFO SelectVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:52:41.264 INFO SelectVariants - Executing as ######@dav002.prv.davros.compute.estate on Linux v3.10.0-327.3.1.el7.x86_64 amd64; 10:52:41.264 INFO SelectVariants - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 10:52:41.265 INFO SelectVariants - Start Date/Time: September 2, 2020 10:52:40 AM GMT; 10:52:41.265 INFO SelectVariants - ------------------------------------------------------------; 10:52:41.265 INFO SelectVariants - ------------------------------------------------------------; 10:52:41.266 INFO SelectVariants - HTSJDK Version: 2.23.0; 10:52:41.266 INFO SelectVariants - Picard Version: 2.22.8; 10",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6446#issuecomment-685695328:2194,detect,detect,2194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6446#issuecomment-685695328,1,['detect'],['detect']
Safety,"_READ_FOR_SAMTOOLS : false; 13:40:59.660 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 13:40:59.661 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:40:59.661 INFO GenotypeGVCFs - Deflater: IntelDeflater; 13:40:59.661 INFO GenotypeGVCFs - Inflater: IntelInflater; 13:40:59.661 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 13:40:59.662 INFO GenotypeGVCFs - Requester pays: disabled; 13:40:59.663 INFO GenotypeGVCFs - Initializing engine; 13:40:59.909 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/groups/OE0540/internal/users/gleixner/cropseq_uli/rep_ex/test3/output.g.vcf; 13:40:59.951 INFO GenotypeGVCFs - Done initializing engine; 13:41:00.006 INFO ProgressMeter - Starting traversal; 13:41:00.007 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 13:41:00.406 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr19:55910646 the annotation MLEAC=[1, 0] was not a numerical value and was ignored; 13:41:00.476 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr19:55910646 and possibly subsequent; at least 10 samples must have called genotypes; 13:41:00.528 INFO ProgressMeter - unmapped 0.0 37 4277.5; 13:41:00.528 INFO ProgressMeter - Traversal complete. Processed 37 total variants in 0.0 minutes.; 13:41:00.580 INFO GenotypeGVCFs - Shutting down engine; [October 26, 2023 at 1:41:00 PM CEST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=285212672; ```; ```; ##source=HaplotypeCaller; ##bcftools_viewVersion=1.16+htslib-1.16; ##bcftools_viewCommand=view -c1 output.vcf; Date=Thu Oct 26 13:41:08 2023; ##bcftools_annotateVersion=1.16+htslib-1.16; ##bcftools_annotateCommand=annotate -x INFO,FORMAT/SB,FORMAT/PL; Date=Thu Oct 26 13:41:08 2023; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT CGAAG",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195:19312,Detect,Detected,19312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195,1,['Detect'],['Detected']
Safety,"_READ_FOR_SAMTOOLS : false; 14:10:28.309 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:10:28.309 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:10:28.310 INFO GenotypeGVCFs - Deflater: IntelDeflater; 14:10:28.310 INFO GenotypeGVCFs - Inflater: IntelInflater; 14:10:28.310 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 14:10:28.310 INFO GenotypeGVCFs - Requester pays: disabled; 14:10:28.311 INFO GenotypeGVCFs - Initializing engine; 14:10:28.532 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/groups/OE0540/internal/users/gleixner/cropseq_uli/rep_ex/test3/output.g.vcf; 14:10:28.566 INFO GenotypeGVCFs - Done initializing engine; 14:10:28.620 INFO ProgressMeter - Starting traversal; 14:10:28.622 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 14:10:29.048 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr19:55910646 the annotation MLEAC=[1, 0] was not a numerical value and was ignored; 14:10:29.118 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr19:55910646 and possibly subsequent; at least 10 samples must have called genotypes; 14:10:29.175 INFO ProgressMeter - unmapped 0.0 37 4021.7; 14:10:29.175 INFO ProgressMeter - Traversal complete. Processed 37 total variants in 0.0 minutes.; 14:10:29.236 INFO GenotypeGVCFs - Shutting down engine; [October 29, 2023 at 2:10:29 PM CET] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=285212672; ```; results in a vcf file that still has called genotypes:. ```; $ bcftools view output.vcf -c1 | bcftools annotate -x INFO,FORMAT/SB | tail; ##source=HaplotypeCaller; ##bcftools_viewVersion=1.16+htslib-1.16; ##bcftools_viewCommand=view -c1 output.vcf; Date=Sun Oct 29 14:09:42 2023; ##bcftools_annotateVersion=1.16+htslib-1.16; ##bcftools_anno",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8569:3363,Detect,Detected,3363,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8569,1,['Detect'],['Detected']
Safety,"_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar Mutect2 -R /home/proj/stage/cancer/reference/GRCh37/genome/human_g1k_v37_decoy.fasta -L /home/proj/stage/cancer/reference/target_capture_bed/production/balsamic/gicfdna_3.1_hg19_design.bed -I consensus/concatenated_ACC5611A1_XXXXXX_consensusalign_ss_r2.bam -O mutect2/concatenated_ACC5611A1_XXXXXX_mutect2_unfiltered_ss_r2.vcf.gz; 09:39:55.358 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 03, 2020 9:39:55 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 09:39:55.559 INFO Mutect2 - ------------------------------------------------------------; 09:39:55.559 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.8.0; 09:39:55.559 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:39:55.559 INFO Mutect2 - Executing as ashwini.jeggari@hasta.scilifelab.se on Linux v3.10.0-1062.4.1.el7.x86_64 amd64; 09:39:55.560 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 09:39:55.560 INFO Mutect2 - Start Date/Time: July 3, 2020 9:39:55 AM CEST; 09:39:55.560 INFO Mutect2 - ------------------------------------------------------------; 09:39:55.560 INFO Mutect2 - ------------------------------------------------------------; 09:39:55.560 INFO Mutect2 - HTSJDK Version: 2.22.0; 09:39:55.561 INFO Mutect2 - Picard Version: 2.22.8; 09:39:55.561 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:39:55.561 INFO Mutect2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695:1336,detect,detect,1336,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695,1,['detect'],['detect']
Safety,"_all_nio_fixes; 12:18:42.093 INFO HaplotypeCaller - Initializing engine; 12:18:42.597 INFO FeatureManager - Using codec VCFCodec to read file file:///beegfs/work/zxmai83/Reference/dbs/b37/dbsnp_138.b37.vcf; 12:18:42.723 INFO HaplotypeCaller - Done initializing engine; 12:18:42.732 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 12:18:42.732 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 12:18:43.546 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/usr/bin/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:18:43.549 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/usr/bin/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 12:18:43.599 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00002b5f92e39fab, pid=85482, tid=0x00002b5f56e60ae8; #; # JRE version: OpenJDK Runtime Environment (8.0_151-b12) (build 1.8.0_151-b12); # Java VM: OpenJDK 64-Bit Server VM (25.151-b12 mixed mode linux-amd64 compressed oops); # Derivative: IcedTea 3.6.0; # Distribution: Custom build (Tue Nov 21 11:22:36 GMT 2017); # Problematic frame:; # C [libgomp.so.1+0x7fab] omp_get_max_threads+0xb; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /beegfs/work/iiipe01/Exome-Test/work/1e/fc972c6b14c8006857230849630a49/hs_err_pid85482.log; #; # If you would like to submit a bug report, please include; # instructions on how to reproduce the bug and visit:; # http://icedtea.classpath.org/bugzilla; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:3840,detect,detected,3840,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['detect'],['detected']
Safety,"_calling/bbv18h27rm.fa --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_1to128.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_1280to18.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_180to25.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_250to35.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_350to5.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_50to69.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_690to999.g.vcf.gz -O /db_students1/cc/gatk_out/raw_new52_off_xL4_70.g.vcf.gz; 17:52:22.080 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/cc/gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 11, 2020 5:52:23 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:52:23.841 INFO CombineGVCFs - ------------------------------------------------------------; 17:52:23.841 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.1.3.0-25-g8d88f6e-SNAPSHOT; 17:52:23.841 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:52:23.842 INFO CombineGVCFs - Executing as cc@hr18b on Linux v3.10.0-957.el7.x86_64 amd64; 17:52:23.842 INFO CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_212-b04; 17:52:23.842 INFO CombineGVCFs - Start Date/Time: January 11, 2020 5:52:22 PM CST; 17:52:23.842 INFO CombineGVCFs - ------------------------------------------------------------; 17:52:23.842 INFO CombineGVCFs - ------------------------------------------------------------; 17:52:23.843 INFO CombineGVCFs - HTSJDK Version: 2.20.3; 17:52:23.843 INFO CombineGVCFs - Picard Version: 2.20.5; 17:52:23.843 INFO CombineGVCFs - HTSJDK Defaults.COMPRESSION_LE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6368:1704,detect,detect,1704,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6368,1,['detect'],['detect']
Safety,"_comment\_4414746059419](https://gatk.broadinstitute.org/hc/en-us/community/posts/360073212811-GenotypeGVCFs-stalls-while-using-all-sites#community_comment_4414746059419). ```; Using GATK jar /nas/longleaf/apps/gatk/4.2.4.1/gatk-4.2.4.1/gatk-package-4.2.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx190g -jar /nas/longleaf/apps/gatk/4.2.4.1/gatk-4.2.4.1/gatk-package-4.2.4.1-local.jar GenotypeGVCFs -R /proj/matutelb/projects/drosophila/melanogaster/dmel6_ref.fasta -V gendb://all_mels_chr2L -L chr2L -O all_mels_chr2L.vcf.gz; 19:40:48.803 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/nas/longleaf/apps/gatk/4.2.4.1/gatk-4.2.4.1/gatk-package-4.2.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 13, 2022 7:40:50 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:40:50.088 INFO GenotypeGVCFs - ------------------------------------------------------------; 19:40:50.089 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.4.1; 19:40:50.090 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:40:50.102 INFO GenotypeGVCFs - Executing as adagilis@t0601.ll.unc.edu on Linux v3.10.0-1160.2.2.el7.x86_64 amd64; 19:40:50.103 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_292-b10; 19:40:50.103 INFO GenotypeGVCFs - Start Date/Time: January 13, 2022 7:40:48 PM EST; 19:40:50.103 INFO GenotypeGVCFs - ------------------------------------------------------------; 19:40:50.103 INFO GenotypeGVCFs - ------------------------------------------------------------; 19:40:50.104 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 19:40:50.104 INFO GenotypeGVCFs - Picard Version: 2.25.4; 19:40:50.104 INFO GenotypeGVCFs - Built for Spark Ve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7639:1402,detect,detect,1402,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639,1,['detect'],['detect']
Safety,"_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar Mutect2 -R /home/proj/stage/cancer/reference/GRCh37/genome/human_g1k_v37_decoy.fasta -L /home/proj/stage/cancer/reference/target_capture_bed/production/balsamic/gicfdna_3.1_hg19_design.bed -I consensus/concatenated_ACC5611A5_XXXXXX_consensusalign_ss_r2.bam --germline-resource /home/proj/stage/cancer/reference/GRCh37/variants/dbsnp_grch37_b138.vcf.gz -O mutect2/concatenated_ACC5611A5_XXXXXX_mutect2_unfiltered_ss_r2.vcf.gz; 11:47:50.850 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 02, 2020 11:47:51 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:47:51.054 INFO Mutect2 - ------------------------------------------------------------; 11:47:51.055 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.8.0; 11:47:51.055 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:47:51.055 INFO Mutect2 - Executing as ashwini.jeggari@compute-0-0.local on Linux v3.10.0-1062.4.1.el7.x86_64 amd64; 11:47:51.055 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 11:47:51.055 INFO Mutect2 - Start Date/Time: July 2, 2020 11:47:50 AM CEST; 11:47:51.056 INFO Mutect2 - ------------------------------------------------------------; 11:47:51.056 INFO Mutect2 - ------------------------------------------------------------; 11:47:51.056 INFO Mutect2 - HTSJDK Version: 2.22.0; 11:47:51.056 INFO Mutect2 - Picard Version: 2.22.8; 11:47:51.056 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:47:51.056 INFO Mutect2 ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482:1653,detect,detect,1653,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482,1,['detect'],['detect']
Safety,"_replicate_one_small/debug/' \; HaplotypeCallerSpark \; --reference /projects/rdocking_prj/software/bcbio-nextgen/data/genomes/Hsapiens/hg19/ucsc/hg19.2bit \; --annotation MappingQualityRankSumTest --annotation MappingQualityZero \; --annotation QualByDepth --annotation ReadPosRankSumTest \; --annotation RMSMappingQuality --annotation BaseQualityRankSumTest \; --annotation FisherStrand --annotation MappingQuality \; --annotation DepthPerAlleleBySample --annotation Coverage \; -I /projects/karsanscratch/rdocking/KARSANBIO-1390_rna_seq_runs/molm13_replicate_one_small/work/align/MOLM13_rep1/MOLM13_rep1-dedup.splitN.bam \; -L /projects/karsanlab/rdocking/KARSANBIO-1254_pipeline/KARSANBIO-1390_rna_seq_runs/data/gatk_debug/chr1_70k.bed \; --interval-set-rule INTERSECTION \; --spark-master local[12] \; --conf spark.local.dir=/projects/karsanscratch/rdocking/KARSANBIO-1390_rna_seq_runs/molm13_replicate_one_small/debug \; --conf spark.driver.host=localhost \; --conf spark.network.timeout=800 \; --conf spark.executor.heartbeatInterval=100 \; --annotation ClippingRankSumTest --annotation DepthPerSampleHC \; --emit-ref-confidence GVCF -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 60 -GQB 80 \; --output MOLM13_rep1-chr1-70k-gatk-haplotype.vcf; ```. When I run this command on a single chromosome with `-Xmx94349m`, the command completes successfully, but the resulting VCF header does not contain this expected header line:. ```; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ```. (along with most of the other header lines associated with gVCF output). When I up the memory request to 110g for the same input files, the proper VCF header is present. I discovered this in the context of running GATK within the bcbio pipeline, the original descriptions are at: https://github.com/bcbio/bcbio-nextgen/issues/2375. On the linked issue, I have examples of GATK output from runs that produced correct and incorrect output - please let me know if there's any other infor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4821:1404,timeout,timeout,1404,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4821,1,['timeout'],['timeout']
Safety,"_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_180to25.g.vcf.gz; 17:52:24.680 INFO FeatureManager - Using codec VCFCodec to read file file:///db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_250to35.g.vcf.gz; 17:52:24.744 INFO FeatureManager - Using codec VCFCodec to read file file:///db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_350to5.g.vcf.gz; 17:52:24.795 INFO FeatureManager - Using codec VCFCodec to read file file:///db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_50to69.g.vcf.gz; 17:52:24.841 INFO FeatureManager - Using codec VCFCodec to read file file:///db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_690to999.g.vcf.gz; 17:53:24.067 INFO CombineGVCFs - Done initializing engine; 17:53:24.121 INFO ProgressMeter - Starting traversal; 17:53:24.122 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 17:53:24.189 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location scaffold1159:34 the annotation MLEAC=[2, 0] was not a numerical value and was ignored; 17:53:31.218 INFO CombineGVCFs - Shutting down engine; [January 11, 2020 5:53:31 PM CST] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 1.15 minutes.; Runtime.totalMemory()=2739404800; java.lang.IllegalStateException: The elements of the input Iterators are not sorted according to the comparator htsjdk.variant.variantcontext.VariantContextComparator; 	at htsjdk.samtools.util.MergingIterator.next(MergingIterator.java:107); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.eva",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6368:4581,Detect,Detected,4581,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6368,1,['Detect'],['Detected']
Safety,"`BCF2Codec` is out-of-date, and the way it's being used in TileDB is possibly unsafe as a result.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2056:78,unsafe,unsafe,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2056,1,['unsafe'],['unsafe']
Safety,"`GencodeFuncotationFactory.createFuncotationsOnVariant()` (and possibly other places as well) needs to detect symbolic alleles and handle them appropriately. Should add a good test with, eg., the output of GCNV.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5402:103,detect,detect,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5402,1,['detect'],['detect']
Safety,"`MarkDuplicatesSparkIntegrationTest.testMDOrder()` _always_ passes when run locally, but fails about 50% of the time when run in travis. As a result, it's currently disabled. Some things I've tried to fix the test on travis, without success:; -Run MD with `--parallelism 1` instead of `--parallelism 0` (which doesn't guarantee ordering at all!) -- the original version of the test was running with `--parallelism 0`; -Converted test inputs from sam to bam; -Cleared all of travis's caches. This may be purely a travis issue or a flaw in the test itself, or it may indicate that there remain issues with `MarkDuplicatesSpark` output and ordering despite the recent fixes. Note, however, that the original test from @davidadamsphd that first detected the ordering issue with MD on spark now passes, as do the new `ReadsSparkSink` ordering tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1258:741,detect,detected,741,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1258,1,['detect'],['detected']
Safety,"`SAMRecord` has functionality for transient attribute fields. Specifically they are attributes on a read that can be used to cache expensive/useful information for later stages of a tool that we want to avoid writing into the output. Currently `MarkDuplicatesSpark` uses this functionality by casting its `GATKRead` objects to `SAMRecord` objects to get at the field, which is less than ideal for a number of reasons. This would also be useful for the HaplotypeCaller engine, specifically the changes made in #5607.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5653:203,avoid,avoid,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5653,1,['avoid'],['avoid']
Safety,"`VariantsSparkSink` will always sort variants before writing them out. However, `HaplotypeCallerSpark` always processes reads in coordinate-sorted order, and produces variants in the same order, so there is no need for `VariantsSparkSink` to sort variants. (In fact, in GVCF mode the sort is prohibitive since the engine creates a variant for every locus over the interval of interest, which go through the sort step before being merged into GVCF bands.). This PR removes the sort step for `HaplotypeCallerSpark` (and `PrintVariantsSpark`, which doesn't need it either). All of the concordance unit tests pass, and as an additional sanity check I compared the GVCF output from running regular `HaplotypeCaller` on a large input BAM to `HaplotypeCallerSpark` (with and without variant sorting). Removing variant sorting actually made the GVCF output more similar to regular `HaplotypeCaller` - it reduced the number of differences from three to one. (The one difference is a minor difference in QUAL due to a boundary artifact.) See VCFs in [vcfs.zip](https://github.com/broadinstitute/gatk/files/3134046/vcfs.zip).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5909:632,sanity check,sanity check,632,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5909,1,['sanity check'],['sanity check']
Safety,"```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost): ; java.lang.IllegalArgumentException: String tag does not have length() == 2:. at htsjdk.samtools.SAMTagUtil.makeBinaryTag(SAMTagUtil.java:100); at htsjdk.samtools.SAMRecord.setAttribute(SAMRecord.java:1364); at htsjdk.samtools.SAMLineParser.parseTag(SAMLineParser.java:436); at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:346); at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:213); at org.broadinstitute.hellbender.tools.spark.bwa.BwaSparkEngine.lambda$alignWithBWA$464b6154$1(BwaSparkEngine.java:75); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:42); at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:261); at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:259); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$22.apply(RDD.scala:745); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$22.apply(RDD.scala:745); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2039:42,abort,aborted,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2039,1,['abort'],['aborted']
Safety,"a bug fix suggested by Louis.; @lbergelson I don't know how to add a test. Without this you'd see. ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 45 in stage 9.0 failed 8 times, most recent failure: Lost task 45.7 in stage 9.0 (TID 734, shuang-svdps-ceu-w-1.c.broad-dsde-methods.internal, executor 2): java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<init>(BreakEndVariantType.java:253); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.getOrderedMates(BreakEndVariantType.java:261); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes(NovelAdjacencyAndAltHaplotype.java:246); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.inferType(SimpleNovelAdjacencyInterpreter.java:129); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.lambda$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6070:141,abort,aborted,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070,1,['abort'],['aborted']
Safety,a few metrics that are spark equivalents of SinglePassSamProgram should perhaps have a common superclass with common functionality to avoid code duplication (or the duplication should be achieved in another way). For example the read filter that gets applied is very similar from tool to tool.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1027:134,avoid,avoid,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1027,1,['avoid'],['avoid']
Safety,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2707:2914,safe,safe,2914,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707,1,['safe'],['safe']
Safety,a:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:6290,abort,abortStage,6290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['abort'],['abortStage']
Safety,a:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:19089,abort,abortStage,19089,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['abort'],['abortStage']
Safety,"a:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. 17/10/11 14:19:38 ERROR scheduler.TaskSetManager: Task 0 in stage 1.0 failed 4 times; aborting job; 17/10/11 14:19:38 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 17/10/11 14:19:38 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.; 17/10/11 14:19:38 INFO storage.BlockManagerMaster: Removal of executor 2 requested; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Asked to remove non-existent executor 2; 17/10/11 14:19:38 INFO cluster.YarnScheduler: Cancelling stage 1; 17/10/11 14:19:38 INFO scheduler.DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:203) failed in 10.702 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4, com2, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_1507683879816_0006_01_000003 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000003; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(C",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:29130,abort,aborted,29130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['abort'],['aborted']
Safety,aW9ycy5qYXZh) | `85.366% <0%> (ø)` | `8% <0%> (?)` | |; | [...detectcoveragedropout/CoverageDropoutDetector.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9kZXRlY3Rjb3ZlcmFnZWRyb3BvdXQvQ292ZXJhZ2VEcm9wb3V0RGV0ZWN0b3IuamF2YQ==) | `91.803% <0%> (ø)` | `20% <0%> (?)` | |; | [...e/detectcoveragedropout/DetectCoverageDropout.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9kZXRlY3Rjb3ZlcmFnZWRyb3BvdXQvRGV0ZWN0Q292ZXJhZ2VEcm9wb3V0LmphdmE=) | `84% <0%> (ø)` | `4% <0%> (?)` | |; | [...ellbender/tools/exome/DecomposeSingularValues.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9EZWNvbXBvc2VTaW5ndWxhclZhbHVlcy5qYXZh) | `89.474% <0%> (ø)` | `5% <0%> (?)` | |; | [...e/detectcoveragedropout/CoverageDropoutResult.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9kZXRlY3Rjb3ZlcmFnZWRyb3BvdXQvQ292ZXJhZ2VEcm9wb3V0UmVzdWx0LmphdmE=) | `92.593% <0%> (ø)` | `17% <0%> (?)` | |; | [.../broadinstitute/hellbender/utils/tsv/DataLine.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90c3YvRGF0YUxpbmUuamF2YQ==) | `91.453% <0%> (+0.855%)` | `63% <0%> (+1%)` | :arrow_up: |; | [...itute/hellbender/tools/walkers/SplitIntervals.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL1NwbGl0SW50ZXJ2YWxzLmphdmE=) | `90.625% <0%> (+2.39%)` | `12% <0%> (+6%)` | :arrow_up: |; | [...ols/exome/alleliccount/AllelicCountCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3043#issuecomment-306585614:2144,detect,detectcoveragedropout,2144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3043#issuecomment-306585614,1,['detect'],['detectcoveragedropout']
Safety,"ab"" as of scales 0.3.0. This parameter is used repeatedly in the generated R-script via. ```R; scale_fill_gradient(high=""green"", low=""red"", space=""rgb""); ```. #### Steps to reproduce. ```shell; $ R --version; R version 4.1.2 (2021-11-01) -- ""Bird Hippie""; $ rm -rf ~/R; $ R; > install.packages(""ggplot2"", repos=""https://cloud.r-project.org/""); > packageVersion(""scales""); [1] ‘1.3.0’; > quit(); $ gatk --version; The Genome Analysis Toolkit (GATK) v4.5.0.0; HTSJDK Version: 4.1.0; Picard Version: 3.1.1; $ gatk VariantRecalibrator [arguments omitted for brevity]; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.9339186078473502558';source('/path/to/rscript.r');; Stdout: ; Stderr: Error:; ! The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as; of scales 0.3.0.; Backtrace:; ▆; 1. ├─base::source(""/path/to/rscript.r""); 2. │ ├─base::withVisible(eval(ei, envir)); 3. │ └─base::eval(ei, envir); 4. │ └─base::eval(ei, envir); 5. └─ggplot2::scale_fill_gradient(high = ""green"", low = ""red"", space = ""rgb""); 6. ├─ggplot2::continuous_scale(...); 7. │ └─ggplot2::ggproto(...); 8. │ └─rlang::list2(...); 9. └─scales::seq_gradient_pal(low, high, space); 10. └─scales::pal_gradient_n(c(low, high), space = space); 11. └─lifecycle::deprecate_stop(""0.3.0"", ""pal_gradient_n(space = 'only supports be \""Lab\""')""); 12. └─lifecycle:::deprecate_stop0(msg); 13. └─rlang::cnd_signal(...); Execution halted; $ R; > install.packages(""remotes"", repos=""https://cloud.r-project.org/""); > library(remotes); > install_version(""scales"", version=""1.2.1"", repos=""https://cloud.r-project.org/""); > packageVersion(""scales""); [1] ‘1.2.1’; > quit(); $ gatk VariantRecalibrator [arguments omitted for brevity]; $; ```. #### Expected behavior; The output rscript file is used to generate a PDF. #### Actual behavior; Generation of the PDF fails due to an deprecation in the `scales` library causing the `Rscript` command to abort.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8664:2472,abort,abort,2472,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8664,1,['abort'],['abort']
Safety,"adPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 05:09:00.455 WARN TaskSetManager:66 - Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayIndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 05:09:00.456 ERROR TaskSetManager:70 - Task 8 in stage 1.0 failed 1 times; aborting job; 05:09:10.808 ERROR MapOutputTrackerMaster:91 - Error communicating with MapOutputTracker; java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:202); at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:142); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3019:2204,abort,aborting,2204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019,1,['abort'],['aborting']
Safety,"adPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:50 INFO TaskSetManager:54 - Starting task 178.1 in stage 5.0 (TID 1142, scc-q06.scc.bu.edu, executor 23, partition 178, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Finished task 12.0 in stage 5.0 (TID 957) in 30736 ms on scc-q15.scc.bu.edu (executor 15) (117/189); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Removed taskresult_957 on scc-q15.scc.bu.edu:35739 in memory (size: 5.2 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Lost task 181.3 in stage 5.0 (TID 1139) on scc-q02.scc.bu.edu, executor 24: java.lang.IllegalArgumentException (provided start is negative: -1) [duplicate 3]; 2019-02-17 16:25:50 ERROR TaskSetManager:70 - Task 181 in stage 5.0 failed 4 times; aborting job; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Cancelling stage 5; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Stage 5 was cancelled; 2019-02-17 16:25:50 INFO DAGScheduler:54 - ResultStage 5 (collect at FindBreakpointEvidenceSpark.java:963) failed in 30.887 s due to Job aborted due to stage failure: Task 181 in stage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.sp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:39051,abort,aborting,39051,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['abort'],['aborting']
Safety,add detection of build status of protected,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1793:4,detect,detection,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1793,1,['detect'],['detection']
Safety,adding timeout protection,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1322:7,timeout,timeout,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1322,1,['timeout'],['timeout']
Safety,adding two new parameters which work together to allow passing through files from azure too genomicsDB; `--header <vcf>` which lets you specify a vcf file to use the header from as your merged header. Do not mess this up or you will likely be doomed.; `--avoid-nio` which disables GATK sanity checks that involve reading the files since this would require opening them on azure. This needs tests but I wanted to put it here for @meganshand to try.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8438:255,avoid,avoid-nio,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8438,2,"['avoid', 'sanity check']","['avoid-nio', 'sanity checks']"
Safety,addresses problem detected when working on #1413,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1521:18,detect,detected,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1521,1,['detect'],['detected']
Safety,adingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 dangling heads; 11:55:47.787 DEBUG Mutect2Engine - Active Region chrM:9302-9584; 11:55:47.792 DEBUG Mutect2Engine - Extended Act Region chrM:9202-9684; 11:55:47.796 DEBUG Mutect2Engine - Ref haplotype coords chrM:9202-9684; 11:55:47.800 DEBUG Mutect2Engine - Haplotype count 128; 11:55:47.803 DEBUG Mutect2Engine - Kmer sizes count 0; 11:55:47.807 DEBUG Mutect2Engine - Kmer sizes values []; 12:05:48.002 DEBUG Mutect2 - Processing assembly region at chrM:9585-9884 isActive: false numReads: 125080; 12:05:51.435 DEBUG Mutect2 - Processing assembly region at chrM:9885-10184 isActive: false numReads: 0; 12:05:51.448 DEBUG Mutect2 - Processing assembly region at chrM:10185-10484 isActive: false numReads: 0; 12:05:51.460 INFO ProgressMeter - chrM:10185 30.2 40 1.3; 12:05:51.465 DEBUG Mutect2 - Processing assembly region at chrM:10485-10784 isActive: false numReads: 0; 12:05:51.476 DEBUG Mutect2 - Processing assembly region at chrM:10785-11084 isActive: false numReads: 0; 12:05:51.48,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:17326,Recover,Recovered,17326,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Safety,adinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4458:8109,abort,abortStage,8109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458,1,['abort'],['abortStage']
Safety,alls.java	no	https://github.com/broadinstitute/gatk-protected/pull/1129	yes	; 10	ConvertGSVariantsToSegments	internal	5/30	https://github.com/broadinstitute/gatk-protected/blob/86e725d850d129822ef4b5024f8cc46ac9ff580a/src/main/java/org/broadinstitute/hellbender/tools/exome/eval/ConvertGSVariantsToSegments.java	no	https://github.com/broadinstitute/gatk-protected/pull/1117	yes	needs review in 2nd round; 30	XHMMSegmentCaller	internal	5/30	https://github.com/broadinstitute/gatk-protected/blob/3e6142ad4eb23d4d9227fafd8e52b498263b4369/src/main/java/org/broadinstitute/hellbender/tools/exome/germlinehmm/xhmm/XHMMSegmentCaller.java	no	https://github.com/broadinstitute/gatk-protected/pull/1129	yes	; 31	XHMMSegmentGenotyper	internal	5/30	https://github.com/broadinstitute/gatk-protected/blob/86e725d850d129822ef4b5024f8cc46ac9ff580a/src/main/java/org/broadinstitute/hellbender/tools/exome/germlinehmm/xhmm/XHMMSegmentGenotyper.java	no	https://github.com/broadinstitute/gatk-protected/pull/1129	yes	; 15	DetectCoverageDropout	ARCHIVE	5/30	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/detectcoveragedropout/DetectCoverageDropout.java	no	https://github.com/broadinstitute/gatk-protected/pull/1130		1; 14	DecomposeSingularValues	ARCHIVE	5/30	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/DecomposeSingularValues.java	no	https://github.com/broadinstitute/gatk-protected/pull/1130		; 3	CalculatePulldownPhasePosteriors	ARCHIVE	5/30	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/CalculatePulldownPhasePosteriors.java	no	https://github.com/broadinstitute/gatk-protected/pull/1130	; 46	Mutect2		6/4/2017	https://github.com/broadinstitute/gatk-protected/blob/91336c9aefb077d1dc7daf7aaae3a8dc3e007,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3055:9206,Detect,DetectCoverageDropout,9206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3055,1,['Detect'],['DetectCoverageDropout']
Safety,alse numReads: 55070; 12:09:01.757 DEBUG Mutect2 - Processing assembly region at chrM:13621-13636 isActive: false numReads: 55240; 12:09:02.341 DEBUG Mutect2 - Processing assembly region at chrM:13637-13936 isActive: true numReads: 110273; 12:09:09.957 DEBUG ReadThreadingGraph - Recovered 24 of 26 dangling tails; 12:09:10.041 DEBUG ReadThreadingGraph - Recovered 6 of 14 dangling heads; 12:09:10.602 DEBUG Mutect2Engine - Active Region chrM:13637-13936; 12:09:10.608 DEBUG Mutect2Engine - Extended Act Region chrM:13537-14036; 12:09:10.613 DEBUG Mutect2Engine - Ref haplotype coords chrM:13537-14036; 12:09:10.617 DEBUG Mutect2Engine - Haplotype count 128; 12:09:10.621 DEBUG Mutect2Engine - Kmer sizes count 0; 12:09:10.625 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:51.290 DEBUG Mutect2 - Processing assembly region at chrM:13937-13944 isActive: true numReads: 54773; 12:13:53.989 DEBUG ReadThreadingGraph - Recovered 29 of 59 dangling tails; 12:13:54.004 DEBUG ReadThreadingGraph - Recovered 0 of 35 dangling heads; 12:13:54.432 DEBUG Mutect2Engine - Active Region chrM:13937-13944; 12:13:54.440 DEBUG Mutect2Engine - Extended Act Region chrM:13837-14044; 12:13:54.447 DEBUG Mutect2Engine - Ref haplotype coords chrM:13837-14044; 12:13:54.452 DEBUG Mutect2Engine - Haplotype count 128; 12:13:54.456 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:54.462 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:55.715 DEBUG Mutect2 - Processing assembly region at chrM:13945-14244 isActive: false numReads: 54745; 12:13:56.962 DEBUG Mutect2 - Processing assembly region at chrM:14245-14544 isActive: false numReads: 0; 12:13:56.973 DEBUG Mutect2 - Processing assembly region at chrM:14545-14844 isActive: false numReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false numReads: 0; 12:13:57.009 DEBUG Mutect2 - Processing assembly region at ch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:20842,Recover,Recovered,20842,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Safety,also trim reads to avoid calling past the end of short fragments,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5920:19,avoid,avoid,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5920,1,['avoid'],['avoid']
Safety,"aluated. Using a small set of WGS SV tandem-duplication calls from @mwalker174 as a truth set, I did some experimenting with changing the count-collection strategy. (We initially thought we were missing some of these simply due to over-denoising/filtering by the PoN, but as we'll see below, the count-collection strategy plays a non-trivial role.). Subsetting to chr3, I built a small PoN of 12 normals (including the case normal) at 100bp and denoised using bin medians only (i.e., `--number-of-eigensamples 0`) to avoid denoising away common events. In chr3, the case sample had three events:. ````; chr3	8559423		8560126; chr3	64547471	64549936; chr3	90414457	90415989; ````. I tried the following, running `ModelSegments` using fairly sensitive parameters (`--number-of-changepoints-penalty-factor 0.1 --maximum-number-of-segments-per-chromosome 10000 --window-size 16 --window-size 32 --maximum-number-of-smoothing-iterations 0` in copy-ratio-only mode:. 1) CollectFragmentCounts. This only recovered event 2.; 2) CollectReadCounts - same as CollectFragmentCounts, but removing the properly-paired and first-of-pair filters and adding a count for each read to the bin containing its start. This recovered all 3 events.; 3) CollectFragmentOverlaps - same filters as CollectFragmentCounts, but adding counts to all bins overlapping each fragment. Note that we need to implement a filter on maximum fragment length, otherwise we get some strange artifacts from (incorrectly mapped?) extremely long fragments; I arbitrarily chose a cutoff of 10000bp. This recovered events 1 and 2. Event 3 seemed to be the most difficult to recover. Plotting the copy ratios surrounding this event (which spans ~15 100bp bins) yields some insights:. CollectFragmentCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244188-317a7f1e-2453-11e8-937d-f7239354316e.png). CollectReadCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244228-ad24908c-2453-11e8-91dd-a978578e77f4.png",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4519:1734,recover,recovered,1734,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519,1,['recover'],['recovered']
Safety,"an edge case where ""=="" is to be distinguished from ""<="" or "">="". The reason is that when two quantities are equal, the value to be deduced from their difference is simply zero and an expensive computation could be avoided (the method performing the expensive computation has a check for such optimization opportunity and caught this).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3751:215,avoid,avoided,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3751,1,['avoid'],['avoided']
Safety,"anager:54 - Starting task 178.1 in stage 5.0 (TID 1142, scc-q06.scc.bu.edu, executor 23, partition 178, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Finished task 12.0 in stage 5.0 (TID 957) in 30736 ms on scc-q15.scc.bu.edu (executor 15) (117/189); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Removed taskresult_957 on scc-q15.scc.bu.edu:35739 in memory (size: 5.2 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Lost task 181.3 in stage 5.0 (TID 1139) on scc-q02.scc.bu.edu, executor 24: java.lang.IllegalArgumentException (provided start is negative: -1) [duplicate 3]; 2019-02-17 16:25:50 ERROR TaskSetManager:70 - Task 181 in stage 5.0 failed 4 times; aborting job; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Cancelling stage 5; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Stage 5 was cancelled; 2019-02-17 16:25:50 INFO DAGScheduler:54 - ResultStage 5 (collect at FindBreakpointEvidenceSpark.java:963) failed in 30.887 s due to Job aborted due to stage failure: Task 181 in stage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:39333,abort,aborted,39333,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['abort'],['aborted']
Safety,"ank you for your assistance! . #### Steps to reproduce. - Command used (omitting paths to 1000 samples for brevity) for one of the failed shards. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8g -jar /gpfs/gpfs_de6000/home/dalegre/miniconda3/envs/GOASTv4.0/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar GenomicsDBImport -V [samples 1-1002] --genomicsdb-workspace-path results/jointcalling/genomicsDB/temp_0882_of_2000_DB --merge-input-intervals false --bypass-feature-reader --tmp-dir temp --max-num-intervals-to-import-in-parallel 10 --batch-size 50 --intervals results/germline/interval/temp_0882_of_2000/scattered.interval_list --genomicsdb-shared-posixfs-optimizations true; ```. #### Expected behavior; All shards are imported into the GenomicsDB successfully. . #### Actual behavior; _Tell us what happens instead_. job dies with this error:. `malloc(): unaligned tcache chunk detected`. ```; 23:45:26.793 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gpfs/gpfs_de6000/home/dalegre/miniconda3/e; nvs/GOASTv4.0/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:45:26.822 INFO GenomicsDBImport - ------------------------------------------------------------; 23:45:26.824 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.4.0.0; 23:45:26.824 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:45:26.824 INFO GenomicsDBImport - Executing as dalegre@amd4103.hpc.eu.lenovo.com on Linux v5.14.0-284.11.1.el9_2.x86_64 amd6; 4; 23:45:26.824 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v17.0.3-internal+0-adhoc..src; 23:45:26.824 INFO GenomicsDBImport - Start Date/Time: February 6, 2024 at 11:45:26 PM CET; 23:45:26.824 INFO GenomicsDBImport - ------------------------------------------------------------; 23:45",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8683:2123,detect,detected,2123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683,1,['detect'],['detected']
Safety,"antWalker` that inputs a bam and a vcf and outputs the bases of all alt reads in the `ReadsContext` at each variant; 2) send these to an external alignment program; 3) read in the alignments in a GATK tool and filter accordingly. The ambitious version is to write our own simple aligner, eg a kmer-based method like BLAT or BBMap but with all the messy parts for handling big indels, RNA, and proteins removed. Writing our own BWA aligner would be wildly impractical. @takutosato @LeeTL1220 keeping you in the loop. ---. @davidbenjamin commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-275483449). *Even better*: rely on someone else in the group, such as Ted, to write a Java binding for BWA in memory. See broadinstitute/gatk#2367. ---. @davidbenjamin commented on [Sun Apr 23 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296515266). So. . . given that our pipeline aligns with BWA, it might seem like this is just a redundant and laborious rehashing of the mapping quality score. *However*, the mapping quality only considers multi-mapping within the reference, and therefore doesn't account for mapping errors due to incompleteness of the reference. That is, reads from genomic regions that are not part of the reference (because they're hard to assemble, like centromeres etc) might map well to a unique regions within the reference, and therefore will have fine mapping quality even though they are artifacts. There are published ""decoy genomes"" -- essentially pseudo-contigs of regions missing from the reference, and mapping with BWA in memory to *those* might be very helpful. So, we need to: 1) get our hands on a decoy genome that will play nicely with BWA, and 2) talk to the SV team. ---. @ldgauthier commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296675311). To be pedantic, the mapping quality also considers how well the read aligns; to its",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2930:1390,redund,redundant,1390,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930,1,['redund'],['redundant']
Safety,apPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6070:3853,abort,abortStage,3853,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070,1,['abort'],['abortStage']
Safety,apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1457); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1445); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1444); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1444); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1668); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1627); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1616); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1862); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1875); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:33877,abort,abortStage,33877,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['abort'],['abortStage']
Safety,"aplotypeCaller to create the gvcf just the reblock. . ```; Using GATK jar /share/pkg.7/gatk/4.2.2.0/install/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.2.2.0/install/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O g1.test.reblock.g.vcf.gz; 00:54:40.318 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.2.0/install/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 12:54:40 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:54:40.501 INFO ReblockGVCF - ------------------------------------------------------------; 00:54:40.501 INFO ReblockGVCF - The Genome Analysis Toolkit (GATK) v4.2.2.0; 00:54:40.501 INFO ReblockGVCF - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:54:40.501 INFO ReblockGVCF - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 00:54:40.502 INFO ReblockGVCF - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 00:54:40.502 INFO ReblockGVCF - Start Date/Time: August 25, 2021 12:54:40 AM EDT; 00:54:40.502 INFO ReblockGVCF - ------------------------------------------------------------; 00:54:40.502 INFO ReblockGVCF - ------------------------------------------------------------; 00:54:40.503 INFO ReblockGVCF - HTSJDK Version: 2.24.1; 00:54:40.503 INFO ReblockGVCF - Picard Version: 2.25.4; 00:54:40.503 INFO ReblockGVCF - Built for Spark Version: 2.4.5",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7334#issuecomment-905183643:1086,detect,detect,1086,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7334#issuecomment-905183643,1,['detect'],['detect']
Safety,"aplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```; Based on the discussion surrounding #4963 and the [test VCF](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/testGenotypeGivenAllelesMode_givenAlleles.vcf), I gather that cases like these are intended to work, without crashing. I was trying to figure out what was unique in these problematic cases, compared to the spanning deletion in the aforementioned test VCF. I noticed that the problematic cases both have the SNP at the very last base of the spanning deletion. I'm just speculating here, but maybe the issue is related to some sort of ""off-by-one"" bug?. This is based on testing with version 4.0.9.0.; I also tried with 4.0.5.1, and it didn't crash, but rather displayed warnings of the type discussed in #4963:; `00:02:10.995 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16137302-16137302, only considering the first record`; `00:03:08.220 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16464051-16464051, only considering the first record`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5337:5726,detect,detected,5726,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5337,2,['detect'],['detected']
Safety,"aps pushing a fresh branch to this repo might make it a little easier for us to check it out for review---again, not a big deal, so I'll leave it up to you. 2) We try to adhere to the Google style guide https://google.github.io/styleguide/javaguide.html, so the review may yield a lot of seemingly minor and nitpicky change requests. Don't take these personally---the goal is just to make the code base as uniform and easy to maintain as possible! If you prefer, I'm sure we can find a GATK developer to take a quick once over of your branch and make these minor changes. 3) Since the new tool borrows so heavily from CollectAllelicCounts, I think it might be worth consolidating shared code and reducing code duplication---again, with the goal of making future maintenance more straightforward. I'll try to identify some places this can be done during my review. Again, we can make these changes on our end during the once over, or you can address them after the review (or we could also do this on our end in a separate PR after this one goes in). 4) In the near future, I think we should finally make the effort to replace both GetPileupSummaries and CollectAllelicCounts with this new tool. As mentioned in our email thread, @davidbenjamin and I discussed this long ago, e.g. https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926. From a methods perspective, we'd simply need expand the current functionality of your tool to also report the reference allele and do some quick sanity checks to make sure that the differences in count definition and read filtering don't have any undesired downstream effects. However, as we also discussed, this will come with some additional overhead---we'll need to update documentation, workshop slides, tutorials, WDLs, and make sure that any changes in output formats are clearly highlighted in the release notes. I'll leave this effort to @davidbenjamin and @mwalker174. Thanks again for doing this. Let us know how you'd like to proceed!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6543#issuecomment-610462293:1851,sanity check,sanity checks,1851,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6543#issuecomment-610462293,2,['sanity check'],['sanity checks']
Safety,"are all suffering from the same/similar stacktrace. We've had BaseRecalibratorSpark work fine on other bam files. Additionally, changing the number of threads still results in the same stacktrace. I've also tried running the BQSRPipelineSpark to see if that would suffer the same issue and it fails in the same manner. Additionally, I've run ValidateSamFile. There are some reads missing their mates, but this hasn't presented an issue in other tools (including vanilla BaseRecalibrator). Searching thru the forum, I found an old issue with a similar stacktrace, but that issue appears to occur in GATK 2.4: https://gatkforums.broadinstitute.org/gatk/discussion/3265/bqsrgatherer-exception. In the below stacktrace, I've bolded the error message that seems to occur in each of these samples. `Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5854:1433,abort,abortStage,1433,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854,1,['abort'],['abortStage']
Safety,"are working with Dragen data (version 3.6.3); The error:. <details><summary>OPEN ERROR HERE</summary>; <p>. + gatk --java-options -Xms8g GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L /tmp/scratch/cromwell-dragen-us-west-2/cromwell-execution/GatkJointGenotyping/7dd18ebe-29ca-47b1-b71a-56b99c362789/call-SplitIntervalList/glob-d928cd0f5fb17b6bd5e635f48c18ccfb/0073-scattered.interval_list --sample-name-map sample_name_map --reader-threads 5 --merge-input-intervals --consolidate; --; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/tmp/scratch/cromwell-dragen-us-west-2/cromwell-execution/GatkJointGenotyping/7dd18ebe-29ca-47b1-b71a-56b99c362789/call-ImportGVCFs/shard-73/tmp.9a65c1fc; 18:46:55.750 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Dec 01, 2021 6:46:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:46:55.894 INFO GenomicsDBImport - ------------------------------------------------------------; 18:46:55.894 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.3.0; 18:46:55.895 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:46:55.895 INFO GenomicsDBImport - Executing as root@ip-10-10-156-13.us-west-2.compute.internal on Linux v4.14.243-185.433.amzn2.x86_64 amd64; 18:46:55.895 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 18:46:55.895 INFO GenomicsDBImport - Start Date/Time: December 1, 2021 6:46:55 PM GMT; 18:46:55.895 INFO GenomicsDBImport - ------------------------------------------------------------; 18:46:55.895 INFO GenomicsDBImport - ------------------------------------------------------------; 18:46:55.895 INFO GenomicsDBImport - HTSJDK Version: 2.24.1; 18:46:55.895 INFO GenomicsDBImpor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7589:1296,detect,detect,1296,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7589,1,['detect'],['detect']
Safety,"arity exec /fs/scratch/PHS0338/appz/GVCF/gatk_latest.sif \; gatk CombineGVCFs -R /users/PHS0338/jpac1984/data/Autosome.fasta \; --variant PA113.vcf.gz --variant PA113corr.vcf.gz --variant PA112.vcf.gz --variant PA112corr.vcf.gz --variant IN33.vcf.gz\; --variant IN33corr.vcf.gz --variant AL82.vcf.gz \; -O test.vcf.gz; It has all the parameters as mentioned in the website: https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs. #### Expected behavior; _Tell us what should happen_; According to the website (https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs) about combineGVCF, it should have worked fine without any problems... I got the following error log:. 20:11:34.701 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 13, 2021 8:11:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 20:11:35.527 INFO CombineGVCFs - ------------------------------------------------------------; 20:11:35.527 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 20:11:35.527 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 20:11:35.528 INFO CombineGVCFs - Executing as jpac1984@p0002.ten.osc.edu on Linux v3.10.0-1160.21.1.el7.x86_64 amd64; 20:11:35.529 INFO CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 20:11:35.529 INFO CombineGVCFs - Start Date/Time: June 13, 2021 8:11:34 PM GMT; 20:11:35.529 INFO CombineGVCFs - ------------------------------------------------------------; 20:11:35.529 INFO CombineGVCFs - ------------------------------------------------------------; 20:11:35.530 INFO CombineGVCFs - HTSJDK Version: 2.24.0; 20:11:35.530 INFO CombineGVCFs - Picard Version: 2.25.0; 20:11:35.530 INFO CombineGVCFs - Built",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7311:1764,detect,detect,1764,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7311,1,['detect'],['detect']
Safety,ark tools does not seems to recognize it.; When running the following command:; > /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input maprfs://spark-ics/user/axverdier/data/710-PE-G1.bam --output maprfs://spark-ics/user/axverdier/testOutGATK_CountReadsSpark --sparkRunner SPARK --sparkMaster yarn --javaOptions -Dmapr.library.flatclass; I got the following error!. > Driver stacktrace:; > 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1436); > 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1424); > 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); > 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); > 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); > 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1423); > 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); > 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); > 	at scala.Option.foreach(Option.scala:257); > 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); > 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1651); > 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1606); > 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1595); > 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); > 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); > 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); > 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); > 	at org.apache.spark.SparkContext.run,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936:1067,abort,abortStage,1067,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936,1,['abort'],['abortStage']
Safety,"arkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 18/05/01 14:30:35 WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:09 WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:10 INFO ShutdownHookManager: Shutdown hook called; 18/05/01 14:31:15 INFO ShutdownHookManager: Deleting directory /tmp/abd30/spark-3f28d2e3-59d7-40f9-bba3-42d61eff6c6a; 18/05/01 14:31:20 ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; Using GATK jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --input /data/shenlab/abd/TCGA_microbiome/tmp_WXS_colorectal_all/TCGA-AH-6643-11A-01D-1826-10_hg19_Illumina_gdc_realn.bam --kmer-file /data/shenlab/abd/TCGA_microbiome/pathseq_bundle/host_ref/pathse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4725:5319,Timeout,TimeoutException,5319,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725,1,['Timeout'],['TimeoutException']
Safety,art:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4458:8012,abort,abortStage,8012,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458,1,['abort'],['abortStage']
Safety,"ary order"", when i use all parts.; [scattered.1-10.interval_list.txt](https://github.com/broadinstitute/gatk/files/5457466/scattered.1-10.interval_list.txt); [scattered.2-10.interval_list.txt](https://github.com/broadinstitute/gatk/files/5457467/scattered.2-10.interval_list.txt); [scattered.3-10.interval_list.txt](https://github.com/broadinstitute/gatk/files/5457468/scattered.3-10.interval_list.txt); [scattered.4-10.interval_list.txt](https://github.com/broadinstitute/gatk/files/5457469/scattered.4-10.interval_list.txt); [hg19.dict.txt](https://github.com/broadinstitute/gatk/files/5457474/hg19.dict.txt). But if you use the first three, the program works out. - ; `12:43:27.310 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/lmbs02/bio/biosoft/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 29, 2020 12:43:27 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:43:27.439 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 12:43:27.439 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.1.8.1; 12:43:27.439 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:43:27.439 INFO PostprocessGermlineCNVCalls - Executing as lmbs02@Lmbs01 on Linux v5.4.0-48-generic amd64; 12:43:27.439 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_265-8u265-b01-0ubuntu2~18.04-b01; 12:43:27.439 INFO PostprocessGermlineCNVCalls - Start Date/Time: October 29, 2020 12:43:27 PM MSK; 12:43:27.439 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 12:43:27.439 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 12:43:27.440 INFO PostprocessGermlineCNVCalls - HTSJ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924:1685,detect,detect,1685,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924,1,['detect'],['detect']
Safety,"ase sample had three events:. ````; chr3	8559423		8560126; chr3	64547471	64549936; chr3	90414457	90415989; ````. I tried the following, running `ModelSegments` using fairly sensitive parameters (`--number-of-changepoints-penalty-factor 0.1 --maximum-number-of-segments-per-chromosome 10000 --window-size 16 --window-size 32 --maximum-number-of-smoothing-iterations 0` in copy-ratio-only mode:. 1) CollectFragmentCounts. This only recovered event 2.; 2) CollectReadCounts - same as CollectFragmentCounts, but removing the properly-paired and first-of-pair filters and adding a count for each read to the bin containing its start. This recovered all 3 events.; 3) CollectFragmentOverlaps - same filters as CollectFragmentCounts, but adding counts to all bins overlapping each fragment. Note that we need to implement a filter on maximum fragment length, otherwise we get some strange artifacts from (incorrectly mapped?) extremely long fragments; I arbitrarily chose a cutoff of 10000bp. This recovered events 1 and 2. Event 3 seemed to be the most difficult to recover. Plotting the copy ratios surrounding this event (which spans ~15 100bp bins) yields some insights:. CollectFragmentCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244188-317a7f1e-2453-11e8-937d-f7239354316e.png). CollectReadCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244228-ad24908c-2453-11e8-91dd-a978578e77f4.png). CollectFragmentOverlaps:; ![image](https://user-images.githubusercontent.com/11076296/37244230-b25b9cee-2453-11e8-8646-f9c95365b355.png). The increased statistical noise in the CollectFragmentCounts result (due to the lower overall count because of the pairing of reads) probably causes us to miss this event. Also, although CollectFragmentOverlaps initially looks pretty good, I think the bin-to-bin correlations that are evident here negatively affect segmentation. This is not an extremely rigorous evaluation, but it suggests that we should consider switching ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4519:2295,recover,recovered,2295,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519,1,['recover'],['recovered']
Safety,"aseq/gatk_output/CDL-164-04P/log/CDL-164-04P-1_0_249250621_genomicsdb; 11:49:26.339 INFO GenomicsDBImport - Vid Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/vidmap.json; 11:49:26.339 INFO GenomicsDBImport - Callset Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/callset.json; 11:49:26.339 INFO GenomicsDBImport - Complete VCF Header will be written to CDL-164-04P-1_0_249250621_genomicsdb/vcfheader.vcf; 11:49:26.339 INFO GenomicsDBImport - Importing to array - CDL-164-04P-1_0_249250621_genomicsdb/genomicsdb_array; 11:49:26.339 INFO ProgressMeter - Starting traversal; 11:49:26.340 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 11:49:26.697 INFO GenomicsDBImport - Importing batch 1 with 1 samples; Buffer resized from 22726bytes to 32529; Buffer resized from 32529bytes to 32693; Buffer resized from 32693bytes to 32738; Buffer resized from 32738bytes to 32741; Buffer resized from 32741bytes to 32756; Buffer resized from 32756bytes to 32768; Buffer resized from 32768bytes to 32769; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f9935bac359, pid=8320, tid=0x00007f99881bf700; #; # JRE version: OpenJDK Runtime Environment (8.0_171-b10) (build 1.8.0_171-b10); # Java VM: OpenJDK 64-Bit Server VM (25.171-b10 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libtiledbgenomicsdb1661720680664773125.so+0x155359] BufferVariantCell::set_cell(void const*)+0x99; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/log/hs_err_pid8320.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045:4916,detect,detected,4916,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045,1,['detect'],['detected']
Safety,"ast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. [Stage 21:> (0 + 60) / 3539]18/12/21 16:08:30 ERROR org.apache.spark.scheduler.TaskSetManager: Task 26 in stage 21.0 failed 4 times; aborting job; 18/12/21 16:08:30 ERROR org.apache.spark.internal.io.SparkHadoopMapReduceWriter: Aborting job job_20181221160412_0054.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 26 in stage 21.0 failed 4 times, most recent failure: Lost task 26.3 in stage 21.0 (TID 2498, readpipeline-w-4.c.broad-gatk-test.internal, executor 21): java.io.IOException: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.disq_bio.disq.impl.form",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5545:4915,abort,aborted,4915,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545,1,['abort'],['aborted']
Safety,"asta -O ref.hss** ; ; **'''**.  . And then I ran PathSeq with the following command.  . **'''** ; ; **gatk --java-options ""-Xmx200G"" PathSeqPipelineSpark \** ; **--input sample.bam \** ; **--filter-bwa-image ref.fasta.img \** ; **--kmer-file ref.hss \** ; **--is-host-aligned true \** ; **--min-clipped-read-length 70 \** ; **--microbe-fasta pathseq\_microbe.fa \** ; **--microbe-bwa-image pathseq\_microbe.fa.img \** ; **--taxonomy-file pathseq\_taxonomy.db \** ; **--output sample.pathseq.bam \** ; **--scores-output sample.pathseq.txt** ; ; ; **'''**.  . and unfortunately it was shut down by this error message. **09:27:43.974 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/mnt/clinix1/Analysis/mongol/phenomata/Tools/Anaconda3/envs/gatk4/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so** ; **Mar 05, 2020 9:27:44 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine** ; **INFO: Failed to detect whether we are running on Google Compute Engine.** ; **09:27:44.733 INFO PathSeqPipelineSpark - ------------------------------------------------------------** ; **09:27:44.733 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.4.1** ; **09:27:44.734 INFO PathSeqPipelineSpark - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/)** ; **09:27:44.734 INFO PathSeqPipelineSpark - Executing as phenomata@cm132 on Linux v2.6.32-573.18.1.el6.x86\_64 amd64** ; **09:27:44.734 INFO PathSeqPipelineSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_152-release-1056-b12** ; **09:27:44.734 INFO PathSeqPipelineSpark - Start Date/Time: 2020년 3월 5일 (목) 오전 9시 27분 43초** ; **09:27:44.734 INFO PathSeqPipelineSpark - ------------------------------------------------------------** ; **09:27:44.734 INFO PathSeqPipelineSpark - ------------------------------------------------------------** ; **09:27:44.735 INFO",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:2119,detect,detect,2119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['detect'],['detect']
Safety,ationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5545:10071,abort,abortStage,10071,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545,1,['abort'],['abortStage']
Safety,"atk version 4.1.4.0; Steps followed:; 1. bwa alignment; 2. Sorting; 3. Mark duplicate; 4. BaseRecalibrator; 5. ApplyBQSR; 6. HaplotypeCaller. Here is the haplotype caller log. Using GATK jar /home/administrator/IGIB/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/administrator/IGIB/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar HaplotypeCaller -R /home/administrator/IGIB/samples/Homo_sapiens_assembly38.fasta -I data_gatk/aligned.sort.dup.rg.recal.bam -O data_gatk/aligned.sort.dup.rg.recal.bam.vcf; 02:07:46.602 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/administrator/IGIB/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 28, 2019 2:07:51 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 02:07:51.772 INFO HaplotypeCaller - ------------------------------------------------------------; 02:07:51.773 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.4.0; 02:07:51.773 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 02:07:51.773 INFO HaplotypeCaller - Executing as mvasimud@uarch-compression on Linux v4.4.0-142-generic amd64; 02:07:51.773 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v9-internal+0-2016-04-14-195246.buildd.src; 02:07:51.773 INFO HaplotypeCaller - Start Date/Time: 28 November 2019 at 2:07:46 AM IST; 02:07:51.773 INFO HaplotypeCaller - ------------------------------------------------------------; 02:07:51.773 INFO HaplotypeCaller - ------------------------------------------------------------; 02:07:51.774 INFO HaplotypeCaller - HTSJDK Version: 2.20.3; 02:07:51.774 INFO HaplotypeCaller - Picard Version: 2.21.1; 02:07:51.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6292:1062,detect,detect,1062,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6292,1,['detect'],['detect']
Safety,"atk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /shar; ed/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar SplitNCigarReads -R /home/regmvcr/Scratch/reference/sarek/resources_broad_hg38_v0_Homo_sapiens_assemb; ly38.fasta -I /home/regmvcr/Scratch/workspace/JSBF/star_salmon/I3O-MC-JSBF-100-1003.markdup.sorted.bam -O /home/regmvcr/Scratch/workspace/JSBF/SplitNCigarReads/I3O-MC-JSBF-10; 0-1003_split.bam; 19:40:24.551 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/lustre/shared/ucl/apps/gatk-bsd/4.2.5.0/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar!/com; /intel/gkl/native/libgkl_compression.so; Sep 14, 2023 7:40:24 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:40:24.716 INFO SplitNCigarReads - ------------------------------------------------------------; 19:40:24.716 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.2.5.0; 19:40:24.716 INFO SplitNCigarReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:40:24.717 INFO SplitNCigarReads - Executing as regmvcr@node-h00a-012.myriad.ucl.ac.uk on Linux v3.10.0-1160.53.1.el7.x86_64 amd64; 19:40:24.717 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_322-b06; 19:40:24.717 INFO SplitNCigarReads - Start Date/Time: September 14, 2023 7:40:24 PM BST; 19:40:24.717 INFO SplitNCigarReads - ------------------------------------------------------------; 19:40:24.717 INFO SplitNCigarReads - ------------------------------------------------------------; 19:40:24.718 INFO SplitNCigarReads - HTSJDK Version: 2.24.1; 19:40:24.718 INFO SplitNCigarReads - Picard Version: 2.25.4; 19:40:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8522:2274,detect,detect,2274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8522,1,['detect'],['detect']
Safety,"automate detection of ""snapshot"" status of build",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1791:9,detect,detection,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1791,1,['detect'],['detection']
Safety,"ava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). 16/11/16 23:25:11 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Cancelling stage 1; 16/11/16 23:25:11 INFO DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) failed in 0.276 s; 16/11/16 23:25:11 INFO DAGScheduler: Job 0 failed: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 1.029776 s; 16/11/16 23:25:11 INFO SparkContext: SparkContext already stopped.; [November 16, 2016 11:25:11 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2058354688; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:16971,abort,aborted,16971,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['abort'],['aborted']
Safety,avaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951); at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:6532,abort,abortStage,6532,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['abort'],['abortStage']
Safety,avaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:19331,abort,abortStage,19331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['abort'],['abortStage']
Safety,avoid error due to finite precision error in M2 pon creation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5797:0,avoid,avoid,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5797,1,['avoid'],['avoid']
Safety,avoid materializing a partitions worth of assembly regions in HaplotypeCallerSpark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4301:0,avoid,avoid,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4301,1,['avoid'],['avoid']
Safety,"b given intervals. ~197/200 jobs finished, but several gave an odd error. I'm probably not giving you enough information to debug, but maybe this is enough to ask questions. the command is below:. ```. java8 -Xmx48g -Xms48g -Xss2m \; -jar GenomeAnalysisTK4.jar GenotypeGVCFs \; -R REF.fasta \; --variant gendb://<genomicsdb_path> \; -O OUTPUT.vcf.gz \; --annotate-with-num-discovered-alleles \; -stand-call-conf 30 --max-alternate-alleles 12 \; -L <Repeated ~40 times for small contigs>. ```; ; The error is the following:. ```. 21:58:51.873 WARN  MinimalGenotypingEngine - Attempting to genotype more than 50 alleles. Site will be skipped at location QNVO02001146.1:1343; --; 21:59:01.308 INFO  ProgressMeter -  QNVO02001146.1:1679            425.0               1264000           2974.3; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),6.825391631999999,Cpu time(s),6.825079531999995; #; # A fatal error has been detected by the Java Runtime Environment:; #; #  SIGSEGV (0xb) at pc=0x00007fcca9a2ea19, pid=36873, tid=140574431450880; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode linux-amd64 ); # Problematic frame:; # C  [libtiledbgenomicsdb3086049122144672414.so+0x3e3a19]  ArraySchema::tile_num(void const*) const+0x79; #; # Core dump written. Default location: /home/groups/MgapGenomicsDb/@files/sequenceOutputPipeline/SequenceOutput_2020-10-06_16-46-33/Job734/core or core.36873; #; # An error report file with more information is saved as:; # /home/groups/MgapGenomicsDb/@files/sequenceOutputPipeline/SequenceOutput_2020-10-06_16-46-33/Job734/hs_err_pid36873.log; #; # If you would like to submit a bug report, please visit:; #   http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #. ```. Do you have an debugging suggestions based on this? Than",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910:1107,detect,detected,1107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910,1,['detect'],['detected']
Safety,b-joint/bunny_work/main-giab-joint-2017-10-03-104521.457/root/variantcall/8/variantcall_batch_region/16/bcbiotx/tmp7exzBH/NA24631-chr15_68578892_84670250-block.vcf.gz --emitRefConfidence GVCF -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 60 -GQB 80; ```; and the full traceback is:; ```; 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3659:3115,abort,abortStage,3115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3659,1,['abort'],['abortStage']
Safety,b/samples/D1CLVACXX.1.Solexa-125092.aligned.bam -R scripts/microbial/mtb/Mycobacterium_tuberculosis_H37Rv.fasta -O test.vcf --num-matching-bases-in-dangling-end-to-recover 1 --max-reads-per-alignment-start 75. ### Affected version(s); Latest master branch as of 2/18/21. ### Description ; java.lang.ArrayIndexOutOfBoundsException: Index 25 out of bounds for length 25; 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.extendDanglingPathAgainstReference(AbstractReadThreadingGraph.java:913); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.mergeDanglingHead(AbstractReadThreadingGraph.java:646); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.recoverDanglingHead(AbstractReadThreadingGraph.java:542); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.AbstractReadThreadingGraph.recoverDanglingHeads(AbstractReadThreadingGraph.java:447); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.getAssemblyResult(ReadThreadingAssembler.java:685); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.createGraph(ReadThreadingAssembler.java:664); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assemble(ReadThreadingAssembler.java:549); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assembleKmerGraphsAndHaplotypeCall(ReadThreadingAssembler.java:195); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.runLocalAssembly(ReadThreadingAssembler.java:160); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:289); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7085:1090,recover,recoverDanglingHeads,1090,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7085,1,['recover'],['recoverDanglingHeads']
Safety,"bble=false -Dsamjdk.compression_level=2 -jar /app/gatk-package-4.1.8.0-local.jar FilterAlignmentArtifacts -V /output/sample.FilterMutectCalls.vcf.gz -R /db/hs37d5.fa --bwa-mem-index-image /db/hg38.fa.img -I /output/sample.Mutect2.bam -O sample.somatic_filter.test.vcf.gz --use-jdk-inflater true; 19:11:56.929 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 19:11:56.943 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 19:11:56.944 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 19:11:57.168 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 19, 2020 7:11:57 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:11:57.324 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 19:11:57.324 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.0; 19:11:57.325 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:11:57.325 INFO FilterAlignmentArtifacts - Executing as foo@bar.local on Linux v2.6.32-696.6.3.el6.x86_64 amd64; 19:11:57.325 INFO FilterAlignmentArtifacts - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_261-b12; 19:11:57.325 INFO FilterAlignmentArtifacts - Start Date/Time: July 19, 2020 7:11:57 PM CST; 19:11:57.325 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 19:11:57.325 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 19:11:57.325 INFO FilterAlignmentArtifacts - HTSJDK Version: 2.22.0; 19:11:57.325 INFO ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:1602,detect,detect,1602,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['detect'],['detect']
Safety,"bender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assembleKmerGraphsAndHaplotypeCall(ReadThreadingAssembler.java:195); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.runLocalAssembly(ReadThreadingAssembler.java:160); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:289); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:233); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:299); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). #### Steps to reproduce; Get gs bucket with files from @bhanugandham ; To get error, do not use interval list (i.e. run the command as shown above), but we were able to avoid the error by using this interval list:; @HD VN:1.6; @SQ SN:gi|395136682|gb|CP003248.1| LN:4411708 M5:26f1f5c8a8a8c6e33c79d3fc6d40373e UR:file:/Users/bgandham/variant/mtb/Mycobacterium_tuberculosis_H37Rv.fasta; gi|395136682|gb|CP003248.1| 2074300 2074800 + target_1. #### Expected behavior; The tool should produce a vcf. #### Actual behavior; Fails with an ArrayIndexOutOfBounds Error",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7085:3230,avoid,avoid,3230,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7085,1,['avoid'],['avoid']
Safety,biz.k11i:xgboost-predictor:0.3.0 is now ported to the h2oai repo and released to Maven Central?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7839:17,predict,predictor,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7839,1,['predict'],['predictor']
Safety,"breakpoint}, where the complication is mainly used for two purposes: 1) adjusting the exact locations of the breakpoints, and 2) useful for downstream annotations for the associated VCF records.; * NARL is constructed from an input CA, which in turn has two ways of being constructed: 1) deliberate construction with two neighboring alignments on the contig, 2) construction from an input contig whose alignments are scanned through in a semi pair-wise fashion. The second way is the master version currently in use in out pipeline, and is planned to be phased out eventually. Note that the second way extracts neighboring alignments that it considers good quality and send the information to the first version for actual construction. ; * The ctor for CA that takes two neighboring alignments also takes in a string representation of mapping locations of suspected insertions. In master version, this information is extracted from the alignments that are considered not strong enough, e.g. lower MQ, shorter alignment length, but the actual inserted sequence is not extracted in CA, but rather in BC.; * The BC field in NARL, holding inserted sequence, micro-homology and other information (e.g. for duplication), does not contain where the inserted sequence, if any, is mapped, and the inserted sequence is extracted from the distance on the contig between two neighboring alignments stored in the input CA.; * The variant, after its NARL locations are pinned down, is annotated by information stored in both CA and BC, where the information for BC is critical for reconstructing the alt haplotype and those in CA mainly for evaluating evidence strength. . Hence we should put the suspected inserted sequence and mapping in the same class, proposed to be in BC. But I think this PR is getting bigger than it should be, and the cigar operation is what I now need for the graph-based cpx sv detection. So I am thinking of creating a ticket for this issue, and follow up in a month. What you do think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3464#issuecomment-331931810:2326,detect,detection,2326,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3464#issuecomment-331931810,1,['detect'],['detection']
Safety,broadinstitute/gatk-protected/blob/86e725d850d129822ef4b5024f8cc46ac9ff580a/src/main/java/org/broadinstitute/hellbender/tools/exome/eval/ConvertGSVariantsToSegments.java	no	https://github.com/broadinstitute/gatk-protected/pull/1117	yes	needs review in 2nd round; 30	XHMMSegmentCaller	internal	5/30	https://github.com/broadinstitute/gatk-protected/blob/3e6142ad4eb23d4d9227fafd8e52b498263b4369/src/main/java/org/broadinstitute/hellbender/tools/exome/germlinehmm/xhmm/XHMMSegmentCaller.java	no	https://github.com/broadinstitute/gatk-protected/pull/1129	yes	; 31	XHMMSegmentGenotyper	internal	5/30	https://github.com/broadinstitute/gatk-protected/blob/86e725d850d129822ef4b5024f8cc46ac9ff580a/src/main/java/org/broadinstitute/hellbender/tools/exome/germlinehmm/xhmm/XHMMSegmentGenotyper.java	no	https://github.com/broadinstitute/gatk-protected/pull/1129	yes	; 15	DetectCoverageDropout	ARCHIVE	5/30	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/detectcoveragedropout/DetectCoverageDropout.java	no	https://github.com/broadinstitute/gatk-protected/pull/1130		1; 14	DecomposeSingularValues	ARCHIVE	5/30	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/DecomposeSingularValues.java	no	https://github.com/broadinstitute/gatk-protected/pull/1130		; 3	CalculatePulldownPhasePosteriors	ARCHIVE	5/30	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/CalculatePulldownPhasePosteriors.java	no	https://github.com/broadinstitute/gatk-protected/pull/1130	; 46	Mutect2		6/4/2017	https://github.com/broadinstitute/gatk-protected/blob/91336c9aefb077d1dc7daf7aaae3a8dc3e007ffe/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java	scripts/mutect2_wdl/mutect2_multi_sample.wdl and more	https,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3055:9392,detect,detectcoveragedropout,9392,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3055,2,"['Detect', 'detect']","['DetectCoverageDropout', 'detectcoveragedropout']"
Safety,"bs and total single-core run time. These bad regions are characterized by large numbers of false positives with bad mapping quality and very large normal artifact lods. The depth is often high due to mapping issues, which aggravates the problem. We should be able to modify our active region determination so that these bad sites don't trigger the assembly and likelihoods engines. ---. @ldgauthier commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/997#issuecomment-296196072). Have you localized the regions yet? Dave Shiga did some work for us on; slow regions that we causing problems for GenotypeGVCFs in production (not; quite apples to apples) and he found that the centromeres cause a lot of; problems and our MPG collaborators don't trust calls there anyway. In; production for HaplotypeCaller we use an interval list for genomes too; (/seq/references/Homo_sapiens_assembly19/v1/variant_calling/wgs_calling_regions.v1.interval_list); to avoid centromeres, telomeres, and gaps in the reference. That is to say, don't waste too much effort optimizing regions where we; won't trust any calls anyway. On Thu, Apr 20, 2017 at 11:43 PM, David Benjamin <notifications@github.com>; wrote:. > Mutect 2 spends a disproportionate amount of time in certain nasty; > regions. For example, on average a 500,000 bp chunk of DREAM challenge 3; > usually takes 30 seconds on a single core, but in some cases takes hours.; > This is very bad both for scattered jobs and total single-core run time.; >; > These bad regions are characterized by large numbers of false positives; > with bad mapping quality and very large normal artifact lods. The depth is; > often high due to mapping issues, which aggravates the problem. We should; > be able to modify our active region determination so that these bad sites; > don't trigger the assembly and likelihoods engines.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, v",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2975:1342,avoid,avoid,1342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2975,1,['avoid'],['avoid']
Safety,"by T. Li on January 25, 2021 04:37 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360076815852-Error-Running-IndexFeatureFile-on-Ensembl-Mouse-GTF-file-](https://gatk.broadinstitute.org/hc/en-us/community/posts/360076815852-Error-Running-IndexFeatureFile-on-Ensembl-Mouse-GTF-file-). #### Error Log. ```; Using GATK jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar IndexFeatureFile -I gencode/mm10/gencode.vM25.annotation.gtf ; ; 04:33:13.081 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Jan 25, 2021 4:33:13 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 04:33:13.195 INFO IndexFeatureFile - ------------------------------------------------------------ ; ; 04:33:13.195 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.1.9.0-SNAPSHOT ; ; 04:33:13.195 INFO IndexFeatureFile - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 04:33:13.195 INFO IndexFeatureFile - Executing as root@b4c480938d0d on Linux v5.4.0-1029-aws amd64 ; ; 04:33:13.195 INFO IndexFeatureFile - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_242-8u242-b08-0ubuntu3~18.04-b08 ; ; 04:33:13.195 INFO IndexFeatureFile - Start Date/Time: January 25, 2021 4:33:13 AM GMT ; ; 04:33:13.195 INFO IndexFeatureFile - ------------------------------------------------------------ ; ; 04:33:13.195 INFO IndexFeatureFile - ------------------------------------------------------------ ; ; 04:33:13.196 INFO IndexFeatureFile - HTSJDK Version: 2.2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7054:1406,detect,detect,1406,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7054,1,['detect'],['detect']
Safety,"came up in review of https://github.com/broadinstitute/gatk/pull/890 - we dont support detection of overlap with arbitrary feature source, only DBSNP. This needs to be enabled and tested.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1485:87,detect,detection,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1485,1,['detect'],['detection']
Safety,"ce #6616 , but I think we have different errors, And I checked my environment variable, the parameter is displayed as ' declare -x TILEDB_DISABLE_FILE_LOCKING=""1"" '. Hope you can give me some help, thanks in advance. Exact GATK commands used : gatk GenotypeGVCFs -R path/hg38ncbi.fa -V gendb://mydatabase -O rawvariants.vcf. > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home//miniconda3/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar GenotypeGVCFs -R /home//workdir/data_single_cell/sperm/hg38ncbi.fa -V gendb://mydatabase -O rawvariants.vcf; > 21:14:29.330 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/miniconda3/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; > May 25, 2020 9:14:29 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; > INFO: Failed to detect whether we are running on Google Compute Engine.; > 21:14:29.494 INFO GenotypeGVCFs - ------------------------------------------------------------; > 21:14:29.495 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.7.0; > 21:14:29.495 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; > 21:14:29.495 INFO GenotypeGVCFs - Executing as lbjiang@mu01 on Linux v3.10.0-327.el7.x86_64 amd64; > 21:14:29.495 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; > 21:14:29.495 INFO GenotypeGVCFs - Start Date/Time: May 25, 2020 9:14:29 PM CST; > 21:14:29.495 INFO GenotypeGVCFs - ------------------------------------------------------------; > 21:14:29.495 INFO GenotypeGVCFs - ------------------------------------------------------------; > 21:14:29.496 INFO GenotypeGVCFs - HTSJDK Version: 2.21.2; > 21:14:29.496 INFO GenotypeGVCFs - Picard Version: 2.21.9; > 21:14:29.496 INFO GenotypeGVCFs - HTSJDK Defaults.C",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6627:1202,detect,detect,1202,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6627,1,['detect'],['detect']
Safety,ceback is:; ```; 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3659:3360,abort,abortStage,3360,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3659,1,['abort'],['abortStage']
Safety,"ception exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. 17/10/11 14:19:38 ERROR scheduler.TaskSetManager: Task 0 in stage 1.0 failed 4 times; aborting job; 17/10/11 14:19:38 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 17/10/11 14:19:38 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.; 17/10/11 14:19:38 INFO storage.BlockManagerMaster: Removal of executor 2 requested; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Asked to remove non-existent executor 2; 17/10/11 14:19:38 INFO cluster.YarnScheduler: Cancelling stage 1; 17/10/11 14:19:38 INFO scheduler.DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:203) failed in 10.702 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4, com2, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_1507683879816_0006_01_000003 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:28499,abort,aborting,28499,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['abort'],['aborting']
Safety,"changes to build.gradle; R package installation is now part of the gradle build; install_R_packages.R no longer reinstalls existing packages; a warning will be emitted if this fails. compilation no longer depends on R installation, installation does. test run in parallel now; this is set to use 2 cores on travis and 4 locally. adding a note about our R dependency to the readme. travis changes; adding caching to travis for dramatic R installation speedup; updating gradle download because it was using an out of date link. misc changes:; adding an additional flag to mark duplicates to avoid the garbage collection statistics while integration testing; tagging tests that depend on R for future use",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/296:589,avoid,avoid,589,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/296,1,['avoid'],['avoid']
Safety,che.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **Driver stacktrace:** ; **at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)** ; **at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)** ; **at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)** ; **at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)** ; **at scala.Option.foreach(Option.scala:257)** ; **at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)** ; **at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)** ; **at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)** ; **at org.apache.spark.scheduler.DA,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:45921,abort,abortStage,45921,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['abort'],['abortStage']
Safety,ckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:18765,abort,abortStage,18765,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['abort'],['abortStage']
Safety,cluster timed self-termination (beta) feature.; Fixes #3574 considering that several people are suggesting beta from gcloud is safe enough.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3579:127,safe,safe,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3579,1,['safe'],['safe']
Safety,converted to a draft to make sure I don't merge before it's safe (due to call caching),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8930#issuecomment-2258545439:60,safe,safe,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8930#issuecomment-2258545439,1,['safe'],['safe']
Safety,"cords; 00:05:56.230 info NativeGenomicsDB - pid=40375 tid=40376 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; 00:05:56.230 info NativeGenomicsDB - pid=40375 tid=40376 No valid combination operation found for INFO field QD - the field will NOT be part of INFO fields in the generated VCF records; 00:05:56.230 info NativeGenomicsDB - pid=40375 tid=40376 No valid combination operation found for INFO field SOR - the field will NOT be part of INFO fields in the generated VCF records; 00:05:56.776 INFO IntervalArgumentCollection - Processing 105581 bp from intervals; 00:05:56.847 INFO GenotypeGVCFs - Done initializing engine; 00:05:57.036 INFO ProgressMeter - Starting traversal; 00:05:57.036 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 00:07:26.967 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr16:10185 the annotation AS_VarDP=59|115|0 was not a numerical value and was ignored; 00:07:26.967 WARN ReferenceConfidenceVariantContextMerger - Reducible annotation 'AS_VarDP' detected, add -G StandardAnnotation -G AS_StandardAnnotation to the command to annotate in the final VC with this annotation.; 00:07:26.991 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.02938786500000001,Cpu time(s),0.029037034000000003; [August 25, 2021 12:07:27 AM EDT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 1.55 minutes.; Runtime.totalMemory()=1807745024; java.lang.NullPointerException; at java.util.HashMap.putMapEntries(HashMap.java:500); at java.util.HashMap.putAll(HashMap.java:784); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.combineAnnotations(VariantAnnotatorEngine.java:211); at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVari",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7437:7261,Detect,Detected,7261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437,1,['Detect'],['Detected']
Safety,"ct is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, most recent failure: Lost task 15.3 in stage 0.0 (TID 59, 172.31.77.139, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:4770,abort,aborted,4770,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['abort'],['aborted']
Safety,currently HaplotypeCallerSpark is sorting the reads even if the header says they're coordinate sorted. This is very expensive and should be avoided.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4296:140,avoid,avoided,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4296,1,['avoid'],['avoided']
Safety,"d should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 19:11:57.326 INFO FilterAlignmentArtifacts - Initializing engine; 19:11:57.666 INFO FeatureManager - Using codec VCFCodec to read file file:///output/sample.FilterMutectCalls.vcf.gz; 19:11:57.757 INFO FilterAlignmentArtifacts - Done initializing engine; 19:11:57.827 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 19:11:57.861 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 19:11:57.862 INFO IntelPairHmm - Available threads: 4; 19:11:57.862 INFO IntelPairHmm - Requested threads: 4; 19:11:57.862 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 19:11:57.862 INFO ProgressMeter - Starting traversal; 19:11:57.862 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; *** glibc detected *** /for/bar/bin/java: double free or corruption (out): 0x00007f450af58700 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x3d01675dee)[0x7f45058afdee]; /lib64/libc.so.6(+0x3d01678c80)[0x7f45058b2c80]; /tmp/libgkl_smithwaterman410767516409374085.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x7f4499f4cfa8]; /tmp/libgkl_smithwaterman410767516409374085.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)[0x7f4499f4cbf8]; [0x7f44f58be6a2]; ======= Memory map: ========; ```. Then we **disabled** AVX2 in the newer cluster using Intels [sde64](https://software.intel.com/en-us/articles/intel-software-development-emulator) with `-ivb`, which directed GATK to use the Java implementation, and the filter worked without core dump. ```; sde64 -ivb -- faa.sh; Using GATK jar /app/gatk-package-4.1.8.0-local.jar; Running:; /bin/java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_le",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:4472,detect,detected,4472,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['detect'],['detected']
Safety,"dLineProgram.java:31); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:149); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:190); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); at org.broadinstitute.hellbender.testutils.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:107); at org.broadinstitute.hellbender.tools.HaplotypeCallerSparkIntegrationTest.testVCFModeIsConcordantWithGATK3_8Results(HaplotypeCallerSparkIntegrationTest.java:143); Caused by:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 11.0 failed 1 times, most recent failure: Lost task 1.0 in stage 11.0 (TID 26, localhost, executor driver): java.util.ConcurrentModificationException; at java.util.ArrayList.sort(ArrayList.java:1464); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerArgumentCollection.createReadThreadingAssembler(AssemblyBasedCallerArgumentCollection.java:36); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.initialize(HaplotypeCallerEngine.java:231); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.<init>(HaplotypeCallerEn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513:3926,abort,aborted,3926,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513,1,['abort'],['aborted']
Safety,"da3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar MergeVcfs -I data/calling/erc_prod2.SM_V7_1.vcf.gz -I data/calling/cerc_prod2.SM_V7_ZW.vcf.gz -O out.vcf.gz; 17:06:37.645 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Mon Jun 22 17:06:37 CDT 2020] MergeVcfs --INPUT data/calling/erc_prod2.SM_V7_1.vcf.gz --INPUT data/calling/cerc_prod2.SM_V7_ZW.vcf.gz --OUTPUT out.vcf.gz --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX true --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Jun 22, 2020 5:06:37 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Mon Jun 22 17:06:37 CDT 2020] Executing as xxxxxxx@yyyyyy on Linux 3.10.0-693.11.1.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_152-release-1056-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.7.0; [Mon Jun 22 17:06:37 CDT 2020] picard.vcf.MergeVcfs done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=1249378304; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp; htsjdk.samtools.SAMException: Cannot read non-existent file: file:///data/infectious/schistosome/tmp/test%20a/data/calling/erc_prod2.SM_V7_1.vcf.gz; at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:498); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:485); at picard.vcf.MergeVcfs.doWork(MergeVcfs.java:173); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramEx",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241:5825,detect,detect,5825,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241,1,['detect'],['detect']
Safety,dd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3679:4498,abort,abortStage,4498,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3679,1,['abort'],['abortStage']
Safety,"de true -R /nobackup/lnsingh/MTRNA/lib/rCRS.fa -V /nobackup/lnsingh/MTRNA/out/COVSUBJ_0121_1_N_HA_filtered.humanspliced.gvcf.gz -L MT -O /nobackup/lnsingh/MTRNA/out/COVSUBJ_0121_1_N_HA_filtered.humanspliced.filtered.gvcf.gz. 07:33:14.927 WARN GATKReadFilterPluginDescriptor - Disabled filter (MappingQualityReadFilter) is not enabled by this tool. 07:33:14.928 WARN GATKReadFilterPluginDescriptor - Disabled filter (MappingQualityNotZeroReadFilter) is not enabled by this tool. 07:33:14.928 WARN GATKReadFilterPluginDescriptor - Disabled filter (MappingQualityAvailableReadFilter) is not enabled by this tool. 07:33:15.003 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/nobackupp16/swbuild/hsp/COVID19/anaconda3/envs/COVIRT_GATK/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so. Sep 20, 2020 7:33:15 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine. INFO: Failed to detect whether we are running on Google Compute Engine. 07:33:15.360 INFO FilterMutectCalls - ------------------------------------------------------------. 07:33:15.361 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.7.0. 07:33:15.361 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/. 07:33:15.361 INFO FilterMutectCalls - Executing as lnsingh@pfe26 on Linux v4.12.14-122.23.1.20200609-nasa amd64. 07:33:15.361 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12. 07:33:15.361 INFO FilterMutectCalls - Start Date/Time: September 20, 2020 7:33:14 AM PDT. 07:33:15.361 INFO FilterMutectCalls - ------------------------------------------------------------. 07:33:15.361 INFO FilterMutectCalls - ------------------------------------------------------------. 07:33:15.362 INFO FilterMutectCalls - HTSJDK Version: 2.21.2. 07:33:15.362 INFO FilterMutectCalls - Picard Version: 2.21.9. 07:33:15.362 INFO FilterMutect",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6850:1962,detect,detect,1962,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6850,1,['detect'],['detect']
Safety,"dec to read file file:///orange/reed/nhouse/Raw_seqs/SEQ1_samples/SEQ1_gvcf/renamed_seq1trimq10_LHA_AS02_1.raw_variants.g.vcf; 11:30:53.805 INFO FeatureManager - Using codec VCFCodec to read file file:///orange/reed/nhouse/Raw_seqs/SEQ1_samples/SEQ1_gvcf/renamed_seq1trimq10_LHA_AS184_1.raw_variants.g.vcf; 11:30:53.894 INFO FeatureManager - Using codec VCFCodec to read file file:///orange/reed/nhouse/Raw_seqs/SEQ1_samples/SEQ1_gvcf/renamed_seq1trimq10_LHA_AS201_1.raw_variants.g.vcf; 11:30:54.000 INFO FeatureManager - Using codec VCFCodec to read file file:///orange/reed/nhouse/Raw_seqs/SEQ1_samples/SEQ1_gvcf/renamed_seq1trimq10_LHA_AS209_1.raw_variants.g.vcf; 11:34:25.030 INFO CombineGVCFs - Done initializing engine; 11:34:25.154 INFO ProgressMeter - Starting traversal; 11:34:25.155 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 11:34:25.473 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location DS235882:44 the annotation MLEAC=[1, 0] was not a numerical value and was ignored; 11:34:25.944 INFO CombineGVCFs - Shutting down engine; [October 26, 2020 11:34:25 AM EDT] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 3.59 minutes.; Runtime.totalMemory()=3738173440; java.lang.NumberFormatException: empty String; 	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1842); 	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); 	at java.lang.Double.parseDouble(Double.java:538); 	at htsjdk.variant.vcf.VCFUtils.parseVcfDouble(VCFUtils.java:262); 	at htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:808); 	at htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:121); 	at htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158); 	at htsjdk.variant.variantcontext.LazyGenotypesContext.ensureSampleNameMap(LazyGenotypesContex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6913:6941,Detect,Detected,6941,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6913,1,['Detect'],['Detected']
Safety,"dec to read file file:///orange/reed/nhouse/Raw_seqs/SEQ1_samples/SEQ1_gvcf/renamed_seq1trimq10_LHA_AS02_1.raw_variants.g.vcf; 11:30:53.805 INFO FeatureManager - Using codec VCFCodec to read file file:///orange/reed/nhouse/Raw_seqs/SEQ1_samples/SEQ1_gvcf/renamed_seq1trimq10_LHA_AS184_1.raw_variants.g.vcf; 11:30:53.894 INFO FeatureManager - Using codec VCFCodec to read file file:///orange/reed/nhouse/Raw_seqs/SEQ1_samples/SEQ1_gvcf/renamed_seq1trimq10_LHA_AS201_1.raw_variants.g.vcf; 11:30:54.000 INFO FeatureManager - Using codec VCFCodec to read file file:///orange/reed/nhouse/Raw_seqs/SEQ1_samples/SEQ1_gvcf/renamed_seq1trimq10_LHA_AS209_1.raw_variants.g.vcf; 11:34:25.030 INFO CombineGVCFs - Done initializing engine; 11:34:25.154 INFO ProgressMeter - Starting traversal; 11:34:25.155 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 11:34:25.473 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location DS235882:44 the annotation MLEAC=[1, 0] was not a numerical value and was ignored; 11:34:25.944 INFO CombineGVCFs - Shutting down engine; [October 26, 2020 11:34:25 AM EDT] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 3.59 minutes.; Runtime.totalMemory()=3738173440; java.lang.NumberFormatException: empty String; 	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1842); 	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); 	at java.lang.Double.parseDouble(Double.java:538); 	at htsjdk.variant.vcf.VCFUtils.parseVcfDouble(VCFUtils.java:262); 	at htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:808); 	at htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:121); 	at htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158); 	at htsjdk.variant.variantcontext.LazyGenotypesContext.ensureSampleNameMap(LazyGenotypesContex",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-716640444:6704,Detect,Detected,6704,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-716640444,1,['Detect'],['Detected']
Safety,"ded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### ate of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; **GenotypeGVCFs stuck at Starting traversal for coulple of days:**. Using GATK jar /public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR4_gvcf_database -G StandardAnnotation -O fat_ALL_MATERIALS_chr4.g.vcf.gz; 11:58:13.194 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 11:58:14.522 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 20, 2022 11:58:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:58:19.059 INFO GenotypeGVCFs - ------------------------------------------------------------; 11:58:19.060 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.9.0; 11:58:19.060 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:58:19.060 INFO GenotypeGVCFs - Executing as gaoshibin@fat1 on Linux v3.10.0-693.el7.x86_64 amd64; 11:58:19.060 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_211-b12; 11:58:19.060 INFO GenotypeGVCFs - Start Date/Time: 2022年5月20日 上",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7866:2052,Redund,Redundant,2052,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866,1,['Redund'],['Redundant']
Safety,deleted redundant method lnToLog10. Closes #2166.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2650:8,redund,redundant,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2650,1,['redund'],['redundant']
Safety,"denceVariantContextMerger - Reducible annotation 'AS_VarDP' detected, add -G StandardAnnotation -G AS_StandardAnnotation to the command to annotate in the final VC with this annotation.; ```. 3. java.lang.NullPointerException occurs. ; 4. No variants output into VCF. This is the log:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:1-105581 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-1-105581.vcf.gz; 00:05:54.259 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 00:05:54.319 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 12:05:54 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:05:54.582 INFO GenotypeGVCFs - ------------------------------------------------------------; 00:05:54.583 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 00:05:54.583 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:05:54.583 INFO GenotypeGVCFs - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 00:05:54.583 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 00:05:54.584 INFO GenotypeGVCFs - Start Date/Tim",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7437:1470,Redund,Redundant,1470,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437,1,['Redund'],['Redundant']
Safety,"der.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:219); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.sendSynchronousCommand(StreamingPythonScriptExecutor.java:153); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:260); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:891); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.util.concurrent.TimeoutException; at java.util.concurrent.FutureTask.get(FutureTask.java:205); at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.getProcessOutput(StreamingProcessController.java:278); at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.getOutputSynchronizedBy(StreamingProcessController.java:192); at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.getProcessOutputByPrompt(StreamingProcessController.java:163); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:209); ... 9 more; ```. While this execution, continue watching the processes on GPU. $ watch -n 0.5 sh -c 'nvidia-smi | tail'; ```; +-----------------------------------------------------------------------------+; | Processes: GPU Memory |; | GPU PID Type Process name Usage |; |=============================================================================|; | 0 63485 C python 15422MiB |; | 1 63485 C",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4696:2632,Timeout,TimeoutException,2632,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4696,1,['Timeout'],['TimeoutException']
Safety,"different in HC invocation was that the first one used `--dont-trim-active-regions true`:. ```; chr11 6411935 rs3838786 TGCTGGC CGCTGGC,T,<NON_REF> 4029.06 . DB;DP=118;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQandDP=424800,118;REF_BASES=ATGGGCCTGGTGCTGGCGCTG GT:AD:DP:F1R2:F2R1:GQ:PL:SB 1/2:0,62,40,0:102:0,31,23,0:0,31,17,0:99:4046,1646,1982,2435,0,2437,4113,1933,2560,4431:0,0,54,48; ```. and the second one didn't:. ```; chr11 6411935 rs3838786 TGCTGGC T,CGCTGGC,<NON_REF> 2308.64 . BaseQRankSum=-1.312;ClippingRankSum=0.877;DB;DP=119;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQandDP=428400,119;REF_BASES=ATGGGCCTGGTGCTGGCGCTG;ReadPosRankSum=0.255 GT:AD:DP:F1R2:F2R1:GQ:PL:SB 0/2:7,0,65,0:72:1,0,34,0:6,0,31,0:99:2316,2364,2996,0,269,1897,2506,2977,1274,3798:1,6,34,31; ```. Note how in the second case, there are two alts in the gVCF, but only one of them has depth!. The only way to recover these cases is to run with `--dont-trim-active-regions`, but that make the HC run approximately 5 times slower, which is obviously not ideal. What I'd like to suggest is that the HC have some automated way to detect when this kind of error is likely to happen or has happened, and work around it. My suggestion(s) would be:. 1. I _think_ this really only happens in repetitive regions. I wonder if it would be possible to have the HC automatically trim active regions when assembly at kmer size 10 works, and disable it when it has to escalate to a higher kmer size? . 2. Trim the active region, but retain the untrimmed active region also. Genotype using the trimmed region. If any allele receives count=0, re-genotype using the untrimmed regions. My thought here is that I think not trimming the active regions really only makes a difference at a small fraction of sites, on the order of 1/1000, but to rescue those sites we have to pay a 5x performance penalty at every site. It would be great if trimming could be auto-disabled at only those sites that a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5791:1932,recover,recover,1932,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5791,1,['recover'],['recover']
Safety,"dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; 	at java.util.ArrayList.sort(ArrayList.java:1464); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerArgumentCollection.createReadThreadingAssembler(AssemblyBasedCallerArgumentCollection.java:36); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.initialize(HaplotypeCallerEngine.java:231); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.<init>(HaplotypeCal",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:7873,abort,aborted,7873,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['abort'],['aborted']
Safety,"dk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 160972515, span 170618, expected MD5 0cc5e1f5ec5c1b06d5a4bd5fff11b77f) [duplicate 1]; 2019-01-07 11:34:12 INFO TaskSetManager:54 - Starting task 3.3 in stage 0.0 (TID 11, scc-q21.scc.bu.edu, executor 1, partition 3, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:12 INFO TaskSetManager:54 - Lost task 1.3 in stage 0.0 (TID 9) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 3]; 2019-01-07 11:34:12 ERROR TaskSetManager:70 - Task 1 in stage 0.0 failed 4 times; aborting job; 2019-01-07 11:34:12 INFO YarnScheduler:54 - Cancelling stage 0; 2019-01-07 11:34:12 INFO YarnScheduler:54 - Stage 0 was cancelled; 2019-01-07 11:34:12 INFO DAGScheduler:54 - ResultStage 0 (count at CountReadsSpark.java:80) failed in 9.293 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 9, scc-q21.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:33466,abort,aborted,33466,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['abort'],['aborted']
Safety,"ds but `VariantAnnotator` only has the reads. Several annotations had fallback code to annotate a likelihoods object that had no likelihoods. @vruano This is the class that you disliked so much in your recent code review of #5783 . A few issues with this state of things:. * Torturing the definition of `AlleleLikelihoods`, which forced the class to have methods like `hasLikelihoods()`.; * `VariantAnnotator` only applied the few annotations that had custom pileup-based fallback code.; * Lots more annotation code for the fallback mode. So the first step was the option that @lbergelson and @jamesemery liked most: create a regular likelihoods object in `VariantAnnotator` by hard-assigning of each read to the allele it best matches. This is exactly what all the custom fallback modes were doing in effect, but now it's implemented in one place instead of six or so. This lets us delete `UnfilledLikelihoods` and also lets `VariantAnnotator` apply any annotation. @ldgauthier Since the most non-trivial aspect is the new integration test I'm inclined to assign you the review, but a case could be made for someone on the engine team. This completely broke the `VariantAnnotator` tests, which were based on exact matches. This had been an issue before and has always been a bit of a nuisance, but now overhauling the tests became completely unavoidable. So, I rewrote all the tests and wrote a rigorous test based on concordance with annotations from `Mutect2`. If I were reviewing I would start with the new code in `VariantAnnotator` that constructs the likelihoods object from the reads and verify that it is just a more polished version of the fallback code that several annotations used to have. Then I would look at the new `VariantAnnotator` integration tests. Some of the tolerances are fairly liberal but it's worth noting that much of the old exact match ""truth"" annotations were completely bogus. This is better than what we had before by a long shot but it's still use at your own risk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6172:2240,risk,risk,2240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6172,1,['risk'],['risk']
Safety,"due to recursive implementation of Legendre abscissas in Apache Commons. @vdauwera @takutosato this is very simple; it just caps the number of subdivisions of the integral to avoid the recursive stack overflow. I tested it on absurdly high coverage (100,000) and reproduced the error with the old code. Whichever one of you gets to this first should review. While this isn't the most beautiful thing in the world, it will work reasonably while new integration code is pending.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3335:175,avoid,avoid,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3335,1,['avoid'],['avoid']
Safety,e coords chrM:7394-7871; 11:39:08.000 DEBUG Mutect2Engine - Haplotype count 128; 11:39:08.001 DEBUG Mutect2Engine - Kmer sizes count 0; 11:39:08.002 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:12.623 DEBUG Mutect2 - Processing assembly region at chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8672-8829 isActive: false numReads: 148658; 11:39:47.277 DEBUG IntToDoubleFunctionCache - cache miss 92836 > 47638 expanding to 95278; 11:40:02.830 DEBUG Mutect2 - Processing assembly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:15455,Recover,Recovered,15455,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Safety,"e deleted, and its use `support = range(lo, hi)` should become `support = new IndexRange(lo, hi)`. Then `IntStream.of(support).mapToDouble(___).toArray()` becomes `range.mapToDouble(___)` and `apply(promote(support), ___)` becomes `support.mapToDouble(___)`. * `final double relErr = 1 + pow(10, -7)` should become a `static` constant. * The steps; ```java; final double maxD1 = arrayMax(d1);; final double[] d2 = apply(apply(d1, d -> d - maxD1), d -> exp(d));; final double sumD2 = sum(d2);; return apply(d2, d -> d/sumD2);; ```; inside `dnHyper` are just a home-brewed way to normalize the log-space array `d1`. Let's go throught the steps: 1) find the max. 2) subtract the max -- subtracting a constant in log-space is equivalent to dividing by a constant in real space, and since we're normalizing in the end this constant is arbitrary. It's done for numerical stability. 3) exponentiate to get an unnormalized real-space array. 4) find the sum. 5) divide by the sum to get the normalized result. The log-10 version of this is `MathUtils::normalizeFromLog10ToLinearSpace(d1)`. You could either calculate `d1` in log-10 space or convert it, replacing the above line with `return MathUtils.normalizeFromLog10ToLinearSpace(MathUtils.applyToArrayInPlace(d1, MathUtils::logToLog10))`. The latter option is simpler. * Import static should be avoided except to escape horrible clutter, which is not the case here. * The second argument of `Utils.validateArg(condition, calculated string expression)` should become `Utils.validateArg(condition, () -> calculated string expression)`. In the first version, the expression is calculated *even if* the condition is satisfied, whereas in the second it is only calculated as needed. It's not critical here but it's a good habit to get into. PS I am a zealot of the Boy Scout Rule (always leave the camp site cleaner than you found it). It is a great way to get familiar with a large code base and a great way to make the code more readable for the next person.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155:1960,avoid,avoided,1960,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266263155,2,['avoid'],['avoided']
Safety,"e issue that intermittently affects only AVX operations, b) that the Intel native PairHMM doesn't handle that situation gracefully but instead returns an empty likelihoods map and c) that's causing the warnings I'm seeing the discrepancies in the gVCFs. I'm at a bit of a loss for what to do here since I've tried multiple times to reproduce the issue and cannot. And therefore also can't try running with different GATK versions or options etc. But at the same time if it's possible for a hardware issue to cause these problems without crashing the GATK that's very scary. The following is the logging prior to traversal so you can see which versions of various things are in use:. ```; 03:15:01.986 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:conda/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 09, 2020 3:15:02 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 03:15:02.169 INFO HaplotypeCaller - ------------------------------------------------------------; 03:15:02.170 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.4.1; 03:15:02.170 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 03:15:02.170 INFO HaplotypeCaller - Executing as <redacted> on Linux v4.4.0-1114-aws amd64; 03:15:02.170 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_144-b01; 03:15:02.170 INFO HaplotypeCaller - Start Date/Time: October <redacted>; 03:15:02.170 INFO HaplotypeCaller - ------------------------------------------------------------; 03:15:02.170 INFO HaplotypeCaller - ------------------------------------------------------------; 03:15:02.170 INFO HaplotypeCaller - HTSJDK Version: 2.21.0; 03:15:02.170 INFO HaplotypeCaller - Picard Version: 2.21.2; 03:15:02.170 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6889:3074,detect,detect,3074,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6889,1,['detect'],['detect']
Safety,"e mapping quality also considers how well the read aligns; to its best mapping. In places where a sample has a lot of nearby SNPs; compared to the reference the mapping qualities of the reads are low; compared to reads that contain fewer SNPs. I've been mulling over the; conflation of these two aspects of mapping quality for a while because it; biases our VQSR results, but maybe the new filtering models will be able to; figure it out. The b37 reference with decoy contigs is here:; /humgen/1kg/reference/human_g1k_v37_decoy.fasta.I believe that the; reference issue that required the decoy in the b37 1000G work was resolved; in the hg38 reference. This is an excellent topic to discuss with Heng; during his office hours when he gets back from China in a few weeks, but I; expect the SV team will also be helpful in the meantime. On Sun, Apr 23, 2017 at 11:14 PM, David Benjamin <notifications@github.com>; wrote:. > So. . . given that our pipeline aligns with BWA, it might seem like this; > is just a redundant and laborious rehashing of the mapping quality score.; >; > *However*, the mapping quality only considers multi-mapping within the; > reference, and therefore doesn't account for mapping errors due to; > incompleteness of the reference. That is, reads from genomic regions that; > are not part of the reference (because they're hard to assemble, like; > centromeres etc) might map well to a unique regions within the reference,; > and therefore will have fine mapping quality even though they are artifacts.; >; > There are published ""decoy genomes"" -- essentially pseudo-contigs of; > regions missing from the reference, and mapping with BWA in memory to; > *those* might be very helpful.; >; > So, we need to: 1) get our hands on a decoy genome that will play nicely; > with BWA, and 2) talk to the SV team.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-p",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2930:3327,redund,redundant,3327,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930,1,['redund'],['redundant']
Safety,"e or directory)) [duplicate 1]; 02:34 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.25, executor 6, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:41:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.xx:45142 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:42:02 INFO TaskSetManager: Lost task 0.3 in stage 2.0 (TID 8) on xx.xx.xx.xx, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 18/04/24 17:42:02 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:42:02 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 117.782 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:33857,abort,aborted,33857,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['abort'],['aborted']
Safety,"e to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoised copy ratio and/or allelic counts. If only one input is provided, then we only model only the corresponding quantity.; - There is no separate allele-fraction workflow. Unlike the old approach, we do not perform any genotyping or modeling before doing kernel segmentation.; - [x] Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I've added this, but there could be some minor improvements. Right now, only use one het per copy-ratio interval and throw away those off-target/bin. There are a few percent of targets/bins that have more than one het, and we could potentially rescue the off-target/bin hets as well with some care.; - Old code and models are used for modeling. Since the old allele-fraction model only models hets, we perform a `GetHetCoverage`-like binomial genotyping step (and output the results) before modeling. However, instead of assuming the null hypothesis of het (f = 0.5) and accepting a site when we cannot reject the null, we assume the null hypothesis of hom (f = error rate or 1 - error rate) and accept a site when we can reject the null. This entire process is very similar to what @davidbenjamin does in https://github.com/broadinstitute/gatk/pull/3638. We should co",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:4036,detect,detection,4036,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,2,['detect'],['detection']
Safety,"e two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. While the marginalized model is free of such spurious local minima, it does not yield discrete posteriors in a tractable way; hence, the necessity of ultimately decoupling in the ""main"" stage. - Capped phred-scaled qualities to maximum values permitted by machine precision in order to avoid NaNs and overflows. - Took a first step toward tracking and logging parameters during inference, starting with the ELBO history. In the future, it may be desirable to allow tracking of arbitrary RVs and deterministics via command line args (for debugging and exploratory work). Notes:. - We still need to decide about `GermlineCNVCaller` default arguments. See issue #4719.; - The case denoising and calling is unlikely to benefit from t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4720:1690,avoid,avoid,1690,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720,1,['avoid'],['avoid']
Safety,"e whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. 03:51:59.035 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 13, 2023 3:51:59 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 03:51:59.220 INFO GenotypeGVCFs - ------------------------------------------------------------; 03:51:59.220 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 03:51:59.220 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 03:51:59.220 INFO GenotypeGVCFs - Executing as dingrj@localhost.localdomain on Linux v4.18.0-348.7.1.el8_5.x86_64 amd64; 03:51:59.220 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v11.0.11+9-Ubuntu-0ubuntu2.18.04; 03:51:59.220 INFO GenotypeGVCFs - Start Date/Time: July 13, 2023 at 3:51:58 AM UTC; 03:51:59.221 INFO GenotypeGVCFs - ------------------------------------------------------------; 03:51:59.221 INFO GenotypeGVCFs - ------------------------------------------------------------; 03:51:59.221 INFO GenotypeGVCFs - HTSJDK Version: 2.24.0; 03:51:59.221 INFO GenotypeGVCFs - Picard Version: 2.25.0; 03:51:59.221 INFO GenotypeGVCF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8415:1490,detect,detect,1490,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8415,1,['detect'],['detect']
Safety,"e(Channels.java:172); at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81); at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:220). Error type2:. gatk --java-options ""-Xmx32G -XX:ParallelGCThreads=8 -Djava.io.tmpdir=/group/zhougrp2/dguan/tmp"" SplitNCigarReads --spark-runner LOCAL -I 10_mkdup/SAMN05828173_mkdup.bam -R /group/zhougrp2/dguan/00_ref/Gallus_gallus.GRCg6a.dna.toplevel.fa -L /group/zhougrp2/dguan/00_ref/chicken_chr.list -O 11_cigar/SAMN05828173_cigar.bam --create-output-bam-index true --max-reads-in-memory 5000. 00:01:27.003 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/dguan/anaconda3/envs/Chicken_GTEx/share/gatk4-4.1.9.0-0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 21, 2021 12:01:27 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:01:27.275 INFO SplitNCigarReads - ------------------------------------------------------------; 00:01:27.276 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.1.9.0; 00:01:27.276 INFO SplitNCigarReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:01:27.276 INFO SplitNCigarReads - Executing as dguan@bigmem6 on Linux v4.15.0-122-generic amd64; 00:01:27.276 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 00:01:27.277 INFO SplitNCigarReads - Start Date/Time: February 21, 2021 at 12:01:26 AM PST; 00:01:27.277 INFO SplitNCigarReads - ------------------------------------------------------------; 00:01:27.277 INFO SplitNCigarReads - ------------------------------------------------------------; 00:01:27.279 INFO SplitNCigarReads - HTSJDK Version: 2.23.0; 00:01:27.279 INFO SplitNCigarReads - Picard Version: 2.23.3; 00:01:27.279 INFO SplitNCigarReads - HTSJ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7091:62591,detect,detect,62591,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7091,1,['detect'],['detect']
Safety,"e. Elapsed time: 2,531.64 minutes.; Runtime.totalMemory()=9711910912; Tool returned:; true; **Calling Variants Attempt**; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Xmx32g -jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar GenotypeGVCFs --genomicsdb-shared-posixfs-optimizations --reference /data1/EquCab/_ECA30/Equus_caballus.EquCab3.0.dna_sm.toplevel.fa/ -V gendb://ECA3_GenomicsDB_260/3 -O ECA3_GenomicsDB_260.3.g.vcf.gz; 21:16:35.251 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 17, 2021 9:16:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:16:35.496 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:16:35.497 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 21:16:35.497 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:16:35.497 INFO GenotypeGVCFs - Executing as ccastane9@andersserver-01.cvm.tamu.edu on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 21:16:35.497 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_275-b01; 21:16:35.497 INFO GenotypeGVCFs - Start Date/Time: January 17, 2021 9:16:35 PM CST; 21:16:35.497 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:16:35.497 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 21:16:35.498 INFO GenotypeGVCFs - Picard Version: 2.22.8; 21:16:35.498 INFO GenotypeGVCFs - HTSJ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839:2716,detect,detect,2716,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839,1,['detect'],['detect']
Safety,"e; 19/02/18 16:58:29 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@45c90a05{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 16:58:29.970 INFO PrintVariantsSpark - Shutting down engine; [February 18, 2019 4:58:29 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVariantsSpark done. Elapsed time: 0.34 minutes.; Runtime.totalMemory()=1106771968; org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: com.esotericsoftware.kryo.KryoException: java.lang.NullPointerException; Serialization trace:; genotypes (htsjdk.variant.variantcontext.VariantContext); interval (org.broadinstitute.hellbender.engine.spark.SparkSharder$PartitionLocatable); 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:9647,abort,abortStage,9647,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,1,['abort'],['abortStage']
Safety,"eManager - Using codec VCFCodec to read file file:///mnt/OutData/proj1/CRR108494/CRR108494.g.vcf.gz; 06:26:19.874 INFO  FeatureManager - Using codec VCFCodec to read file file:///mnt/OutData/proj1/CRR108508/CRR108508.g.vcf.gz; 06:26:20.052 INFO  FeatureManager - Using codec VCFCodec to read file file:///mnt/OutData/proj1/CRR108528/CRR108528.g.vcf.gz; 06:26:20.221 INFO  FeatureManager - Using codec VCFCodec to read file file:///mnt/OutData/proj1/CRR108547/CRR108547.g.vcf.gz; 06:26:20.417 INFO  FeatureManager - Using codec VCFCodec to read file file:///mnt/OutData/proj1/CRR108592/CRR108592.g.vcf.gz; 06:26:22.363 INFO  IntervalArgumentCollection - Processing 51272880 bp from intervals; 06:26:22.388 INFO  CombineGVCFs - Done initializing engine; 06:26:22.401 INFO  ProgressMeter - Starting traversal; 06:26:22.401 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Variants Processed  Variants/Minute; 06:26:24.884 WARN  ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location NC_038255.2:36 the annotation MLEAC=[1, 0] was not a numerical value and was ignored; 06:26:32.408 INFO  ProgressMeter -   NC_038255.2:132104              0.2                702000        4209474.3; 06:26:42.411 INFO  ProgressMeter -   NC_038255.2:329020              0.3               1725000        5172413.8; 06:26:52.415 INFO  ProgressMeter -   NC_038255.2:548109              0.5               2781000        5559405.6; 06:27:02.421 INFO  ProgressMeter -   NC_038255.2:735845              0.7               3727000        5587706.1; 06:27:12.426 INFO  ProgressMeter -   NC_038255.2:934042              0.8               4757000        5705547.2; 06:27:22.431 INFO  ProgressMeter -  NC_038255.2:1124882              1.0               5747000        5744127.9; 06:27:32.437 INFO  ProgressMeter -  NC_038255.2:1314436              1.2               6761000        5792164.0; 06:27:42.439 INFO  ProgressMeter -  NC_038255.2:1509338       ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8735:7419,Detect,Detected,7419,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735,1,['Detect'],['Detected']
Safety,"eSampleOrdering(LazyGenotypesContext.java:205); 	at htsjdk.variant.variantcontext.GenotypesContext.add(GenotypesContext.java:353); 	at htsjdk.variant.variantcontext.GenotypesContext.add(GenotypesContext.java:46); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:134); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:40); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 18 more; 19/02/18 16:58:29 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@45c90a05{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 16:58:29.970 INFO PrintVariantsSpark - Shutting down engine; [February 18, 2019 4:58:29 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVariantsSpark done. Elapsed time: 0.34 minutes.; Runtime.totalMemory()=1106771968; org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: com.esotericsoftware.kryo.KryoException: java.lang.NullPointerException; Serialization trace:; genotypes (htsjdk.variant.variantcontext.VariantContext); interval (org.broadinstitute.hellbender.engine.spark.SparkSharder$PartitionLocatable); 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:9050,abort,aborted,9050,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,1,['abort'],['aborted']
Safety,"e_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:1-105581 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-1-105581.vcf.gz; 00:05:54.259 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 00:05:54.319 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 12:05:54 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:05:54.582 INFO GenotypeGVCFs - ------------------------------------------------------------; 00:05:54.583 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 00:05:54.583 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:05:54.583 INFO GenotypeGVCFs - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 00:05:54.583 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 00:05:54.584 INFO GenotypeGVCFs - Start Date/Time: August 25, 2021 12:05:54 AM EDT; 00:05:54.584 INFO GenotypeGVCFs - ------------------------------------------------------------; 00:05:54.584 INFO GenotypeGVCFs - ------------------------------------------------------------; 00:05:54.584 INFO GenotypeGVCFs - HTSJDK Version: 2.24.0; 00:05:54.585 INFO GenotypeGVCFs - Picard Version: 2.25.0; 00:05:54.585 INFO GenotypeGVCFs - Built fo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7437:1891,detect,detect,1891,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437,1,['detect'],['detect']
Safety,ead.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:41051,abort,abortStage,41051,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['abort'],['abortStage']
Safety,ead.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:41797,abort,abortStage,41797,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['abort'],['abortStage']
Safety,"eaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; 21/04/13 07:32:25 INFO DAGScheduler: Job 2 failed: runJob at SparkHadoopWriter.scala:78, took 0.365288 s; 21/04/13 07:32:25 ERROR SparkHadoopWriter: Aborting job job_20210413073224_0026.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:13976,abort,aborted,13976,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['abort'],['aborted']
Safety,eads: 4726; 11:36:09.094 DEBUG Mutect2 - Processing assembly region at chrM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine - Active Region chrM:3703-3943; 11:36:16.348 DEBUG Mutect2Engine - Extended Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isA,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:10910,Recover,Recovered,10910,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Safety,"ealign.bqsr.cram; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:44:55.228 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:44:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:44:55.456 INFO ComposeSTRTableFile - ------------------------------------------------------------; 13:44:55.458 INFO ComposeSTRTableFile - The Genome Analysis Toolkit (GATK) v4.2.0.0; 13:44:55.458 INFO ComposeSTRTableFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:44:55.459 INFO ComposeSTRTableFile - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 13:44:55.459 INFO ComposeSTRTableFile - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:44:55.460 INFO ComposeSTRTableFile - Start Date/Time: April 4, 2021 1:44:55 PM EDT; 13:44:55.460 INFO ComposeSTRTableFile - ------------------------------------------------------------; 13:44:55.460 INFO ComposeSTRTableFile - ------------------------------------------------------------; 13:44:55.461 INFO ComposeSTRTableFile - HTSJDK Version: 2.24.0; 13:44:55.461 INFO ComposeSTRTableFile - Picard Ve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182:4429,detect,detect,4429,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182,1,['detect'],['detect']
Safety,ect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:50026,abort,abortStage,50026,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['abort'],['abortStage']
Safety,ecution.ExecuteActionsTaskExecuter$TaskExecution.execute(ExecuteActionsTaskExecuter.java:237); at org.gradle.internal.execution.steps.ExecuteStep.lambda$execute$1(ExecuteStep.java:33); at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:33); at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:26); at org.gradle.internal.execution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:58); at org.gradle.internal.execution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:35); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:32); at org.gradle.internal.execution.steps.RecordOutp,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:8632,Timeout,TimeoutStep,8632,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Timeout'],['TimeoutStep']
Safety,"ed tool(s) or class(es); StructuralVariationDiscoveryPipelineSpark. ### Affected version(s); GATK 4.1.0.0. ### Description . Running SV program generates a Java exception...; java.lang.IllegalArgumentException: provided start is negative: -1. #### Steps to reproduce; ```; gatk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv//$SAMPLE.contig-sam-file\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.sv.vcf \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode client \; --executor-memory 80G\; --driver-memory 30g\; --num-executors 40\; --executor-cores 4\; --conf spark.yarn.submit.waitAppCompletion=false\; --name ""$SAMPLE"" \; --files $REF.img,$KMER \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120. ```. ```; Running:; /share/pkg/spark/2.3.0/install/bin/spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --deploy-mode client --executor-memory 80G --driver-memory 30g --num-executors 40 --executor-cores 4 --conf spark.yarn.submit.waitAppCompletion=false -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:1011,timeout,timeout,1011,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['timeout'],['timeout']
Safety,ed=false disableDithering=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 baq=OFF baqGapOpenPenalty=40.0 refactor_NDN_cigar_string=false fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false useOriginalQualities=false defaultBaseQualities=-1 performanceLog=null BQSR=null quantize_quals=0 static_quantized_quals=null round_down_quantized=false disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 globalQScorePrior=-1.0 validation_strictness=SILENT remove_program_records=false keep_program_records=false sample_rename_mapping_file=null unsafe=null disable_auto_index_creation_and_locking_when_reading_rods=false no_cmdline_in_header=false sites_only=false never_trim_vcf_format_field=false bcf=false bam_compression=null simplifyBAM=false disable_bam_indexing=false generate_md5=false num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false variant_index_type=DYNAMIC_SEEK variant_index_parameter=-1 reference_window_stop=0 logging_level=INFO log_to_file=null help=false version=false variant=(RodBinding name=variant source=/humgen/gsa-hpprojects/dev/gauthier/AS_GTEx/GTEx_AS/GTEx_AS.recal.multialleleics.AS.recalibrated.vcf) discordance=(RodBinding name= source=UNBOUND) concordance=(RodBinding name= source=UNBOUND) out=/humgen/gsa-hpprojects/dev/gauthier/AS_GTEx/GTEx_AS/GTEx_AS.recal.multialleleics.AS.recalibrated.mixed.vcf sample_name=[] sample_expressions=null sample_file=null exclude_sample_name=[] exclude_sample_file=[] exclude_sample_expressions=[] selectexpressions=[] invertselect=false excludeNonVariants=false excludeFiltered=false preserveAlleles=false removeUnusedAlternates=false restrictAllelesTo=ALL keepOriginalAC=false ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834:4392,unsafe,unsafe,4392,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834,2,['unsafe'],['unsafe']
Safety,ed=false disableDithering=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 baq=OFF baqGapOpenPenalty=40.0 refactor_NDN_cigar_string=false fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false useOriginalQualities=false defaultBaseQualities=-1 performanceLog=null BQSR=null quantize_quals=0 static_quantized_quals=null round_down_quantized=false disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 globalQScorePrior=-1.0 validation_strictness=SILENT remove_program_records=false keep_program_records=false sample_rename_mapping_file=null unsafe=null disable_auto_index_creation_and_locking_when_reading_rods=false no_cmdline_in_header=false sites_only=true never_trim_vcf_format_field=false bcf=false bam_compression=null simplifyBAM=false disable_bam_indexing=false generate_md5=false num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false variant_index_type=DYNAMIC_SEEK variant_index_parameter=-1 reference_window_stop=0 logging_level=INFO log_to_file=null help=false version=false variant=(RodBinding name=variant source=/humgen/gsa-hpprojects/dev/gauthier/AS_GTEx/GTEx_AS/GTEx_AS.recal.multialleleics.AS.recalibrated.mixed.vcf) discordance=(RodBinding name= source=UNBOUND) concordance=(RodBinding name= source=UNBOUND) out=/humgen/gsa-hpprojects/dev/gauthier/AS_GTEx/VQSR.AStest.input.vcf sample_name=[] sample_expressions=null sample_file=null exclude_sample_name=[] exclude_sample_file=[] exclude_sample_expressions=[] selectexpressions=[] invertselect=false excludeNonVariants=false excludeFiltered=false preserveAlleles=false removeUnusedAlternates=false restrictAllelesTo=ALL keepOriginalAC=false keepOriginalDP=false mendelianViola,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834:1285,unsafe,unsafe,1285,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834,2,['unsafe'],['unsafe']
Safety,"edulerBackend: Shutting down all executors; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Asking each executor to shut down; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Stopped; 17/10/11 14:19:38 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 17/10/11 14:19:38 INFO storage.MemoryStore: MemoryStore cleared; 17/10/11 14:19:38 INFO storage.BlockManager: BlockManager stopped; 17/10/11 14:19:38 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 17/10/11 14:19:38 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 17/10/11 14:19:38 INFO spark.SparkContext: Successfully stopped SparkContext; 14:19:38.600 INFO PrintReadsSpark - Shutting down engine; [October 11, 2017 2:19:38 PM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.48 minutes.; Runtime.totalMemory()=986185728; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4, com2, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_1507683879816_0006_01_000003 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000003; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:31959,abort,aborted,31959,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['abort'],['aborted']
Safety,"efaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:55:48.867 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:55:48.867 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:55:48.867 INFO GenotypeGVCFs - Deflater: IntelDeflater; 09:55:48.867 INFO GenotypeGVCFs - Inflater: IntelInflater; 09:55:48.867 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 09:55:48.867 INFO GenotypeGVCFs - Requester pays: disabled; 09:55:48.867 INFO GenotypeGVCFs - Initializing engine; 09:55:49.015 INFO FeatureManager - Using codec VCFCodec to read file file:///share/org/YZWL/yzwl_hanxt/leizhou/variant/H-4/H-4.g.vcf.gz; 09:55:49.190 INFO GenotypeGVCFs - Done initializing engine; 09:55:49.215 INFO ProgressMeter - Starting traversal; 09:55:49.216 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 09:55:49.310 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr1_1-157403528:5512 the annotation MLEAC=[1, 0] was not a numerical value and was ignored; 09:55:49.336 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr1_1-157403528:5512 and possibly subsequent; at least 10 samples must have called genotypes; 09:55:50.064 INFO GenotypeGVCFs - Shutting down engine; [September 3, 2024 at 9:55:50 AM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=1241513984; java.lang.RuntimeException: Invalid deflate block found.; at com.intel.gkl.compression.IntelInflater.inflateNative(Native Method); at com.intel.gkl.compression.IntelInflater.inflate(IntelInflater.java:176); at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:145); at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:561); at htsjdk.samtools.util.BlockC",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8969:2889,Detect,Detected,2889,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8969,1,['Detect'],['Detected']
Safety,embly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 danglin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:16345,Recover,Recovered,16345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Safety,"emove alt alleles, just when certain conditions are met, and are independent. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260781482). I don't think https://github.com/broadinstitute/gatk/pull/1918 has been backported, no. . ---. @SHuang-Broad commented on [Mon Dec 19 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-268016064). Looking more closely, isn't this done already in #1377 ? @vruano ?. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287822737). @SHuang-Broad @vruano Status update on this?. ---. @vruano commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287916452). @SHuang-Broad this is not fixed by #1377 as this makes reference to the selection executed by the AFCalculator... it might be that @davidbenjamin now AF calculator addressed the issue, but is also possible that he avoid it it entirely and just focused in the new QUAL calculation. . ---. @vruano commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287921195). Looking at the code I made reference in GATK3, it seem that it is still faulty... I guess we need to take a look on whether in GATK4 has been fixed and then back-ported if people are interested. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287952396). Alright, thanks for the update. At this point we don't care too much about fixing it in GATK3; we're all about moving forward with GATK4. Do you want me to move this issue to GATK or do you already have an issue for this there?. ---. @davidbenjamin commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/950#issuecomment-288598906). The new qual score doesn't subset alleles at all because it doesn't need to. `AlleleSubsettingUt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2958:4241,avoid,avoid,4241,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2958,1,['avoid'],['avoid']
Safety,"emove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136.1; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:56:43.998 INFO ProgressMeter - Pf3D7_13_v3:2730055 100.9 114840 1138.3; 15:56:59.911 INFO ProgressMeter - Pf3D7_13_v3:2798281 101.2 115210 1139.0; 15:59:27.062 INFO ProgressMeter - Pf3D7_13_v3:2861780 103.6 115460 1114.4; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:59:37.457 INFO ProgressMeter - Pf3D7_13_v3:2869697 103.8 115500 1112.9. real 671m24.770s; user 777m30.923s; sys 6m13.682s. $ echo $?; 247; ```. Here is my command-line invocation:; ```; ./gatk --java-options ""-Xmx100000m -Xms25000m"" \; HaplotypeCaller \; -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta \; -I ${WORKING_DIR}/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam \; -O ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz \; --bam-output ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam \; -contamination 0 \; --sample-ploidy 2 \; --linked-de-bruijn-graph \; --pileup-detection true \; --pileup-detection-enable-indel-pileup-calling true \; --max-reads-per-alignment-start 20 \; --annotate-with-num-discovered-alleles \; -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 \; -G StandardAnnotation -G StandardHCAnnotation \; -ERC GVCF \; --verbosity INFO \; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8440:6431,detect,detection,6431,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440,2,['detect'],"['detection', 'detection-enable-indel-pileup-calling']"
Safety,"en the input was very small ; 2. FilterAlignmentArtifacts finished run at different variant (chrX:73769127) when analyzing the smaller input (`test.vcf.gz`) . The log issue looks very similar to that described here [#7162)](https://github.com/broadinstitute/gatk/issues/7162), but the *Problematic frame* information is different. ; As suggested in this issue [#5690](https://github.com/broadinstitute/gatk/issues/5690), the problem disappears when using gatk 4.1.3.0 on the same inputs. . log:; ```bash; 17:37:20.674 INFO ProgressMeter - chr20:43968267 10.6 44000 4132.2; 17:37:38.646 INFO ProgressMeter - chr22:22736335 10.9 45000 4110.5; 17:37:52.672 INFO ProgressMeter - chrX:7000139 11.2 46000 4113.9; 17:38:05.421 INFO ProgressMeter - chrX:26360893 11.4 47000 4125.0; 17:38:17.207 INFO ProgressMeter - chrX:44917184 11.6 48000 4141.4; 17:38:29.312 INFO ProgressMeter - chrX:77681733 11.8 49000 4155.3; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fc0ccec5cdb, pid=15987, tid=15988; #; # JRE version: OpenJDK Runtime Environment (11.0.11+9) (build 11.0.11+9-Ubuntu-0ubuntu2.18.04); # Java VM: OpenJDK 64-Bit Server VM (11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0x97cdb] cfree+0x31b; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P %E"" (or dumping to /home/kt/core.15987); #; # An error report file with more information is saved as:; # /home/kt/hs_err_pid15987.log; ```. #### Steps to reproduce; My commands:; ```bash; gatk --java-options ""-Xmx11g"" \; FilterAlignmentArtifacts \; -R GRCh38.no_alt_analysis_set.fa \; -V in.vcf.gz \; -I bamout.bam \; --bwa-mem-index-image Homo_sapiens_assembly38.fa.img \; --num-regular-contigs 194 \; --max-reasonable-fragment-length 2000 \; --drop-ratio 0.1 \; --indel-start-tolerance 8 \; -O out.vcf.gz; ```; I copied the input vcfs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7247:1446,detect,detected,1446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7247,1,['detect'],['detected']
Safety,"endb:///tmp/tmp.6QEyWPGpWs/vcf\_database/Interval\_6 -O /tmp/tmp.6QEyWPGpWs/gentaumix\_interval\_6\_raw.vcf.gz -D /shared/projects/gentaumix/Ressources/known\_sites/Homo\_sapiens\_assembly38.dbsnp138.vcf --sequence-dictionary /shared/projects/gentaumix/Ressources/known\_sites/Homo\_sapiens\_assembly38.dict -L /shared/projects/gentaumix/Ressources/interval\_genomicsdbi/temp\_6/interval.interval\_list -G StandardAnnotation -G AS\_StandardAnnotation --merge-input-intervals ; ; 14:17:35.171 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default ; ; 14:17:35.232 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Sep 10, 2021 2:17:35 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:17:35.492 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 14:17:35.492 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 14:17:35.492 INFO GenotypeGVCFs - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 14:17:35.493 INFO GenotypeGVCFs - Executing as quentin67100@cpu-node-9 on Linux v3.10.0-1160.6.1.el7.x86\_64 amd64 ; ; 14:17:35.493 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_282-b08 ; ; 14:17:35.493 INFO GenotypeGVCFs - Start Date/Time: 10 septembre 2021 14:17:35 CEST ; ; 14:17:35.493 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 14:17:35.494 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 14:17:35.495 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1 ; ; 14:17:35.495 INFO GenotypeGVCFs - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7465:3123,detect,detect,3123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7465,1,['detect'],['detect']
Safety,"ent/60577#Comment_60577) on two different bams. Here is the latest error below. . If possible, can the error message improve? The user was able to continue after creating a new index and dictionary for their reference:. ./gatk --java-options ""-Dsamjdk.sra_libraries_download=true"" Mutect2 -R hg19.fa -I test3.bam -O unfiltered.vcf; Using GATK jar /c/linux/gatk/gatk-package-4.1.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Dsamjdk.sra_libraries_download=true -jar /c/linux/gatk/gatk-package-4.1.3.0-local.jar Mutect2 -R hg19.fa -I test3.bam -O unfiltered.vcf; 10:33:35.145 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/c/linux/gatk/gatk-package-4.1.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 28, 2019 10:33:37 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:33:37.506 INFO Mutect2 - ------------------------------------------------------------; 10:33:37.507 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.3.0; 10:33:37.507 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:33:37.508 INFO Mutect2 - Executing as ascott3@LAPTOP-L1C565MP on Linux v4.4.0-17763-Microsoft amd64; 10:33:37.508 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v11.0.4+11-post-Ubuntu-1ubuntu218.04.3; 10:33:37.508 INFO Mutect2 - Start Date/Time: August 28, 2019 at 10:33:35 AM GMT; 10:33:37.509 INFO Mutect2 - ------------------------------------------------------------; 10:33:37.509 INFO Mutect2 - ------------------------------------------------------------; 10:33:37.510 INFO Mutect2 - HTSJDK Version: 2.20.1; 10:33:37.510 INFO Mutect2 - Picard Version: 2.20.5; 10:33:37.510 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:33:37.511 INFO Mutect2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6142:1233,detect,detect,1233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6142,1,['detect'],['detect']
Safety,"ential additional information: /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/stderr.; Could not retrieve content: Could not read from /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/stderr: /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/stderr; [2020-07-14 05:09:46,38] [info] WorkflowManagerActor WorkflowActor-968be82c-eef3-4bdb-a1ab-3d4e2ca70674 is in a terminal state: WorkflowFailedState; [2020-07-14 05:09:51,97] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2020-07-14 05:09:55,28] [info] Workflow polling stopped; [2020-07-14 05:09:55,30] [info] 0 workflows released by cromid-ca5c695; [2020-07-14 05:09:55,30] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2020-07-14 05:09:55,30] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2020-07-14 05:09:55,30] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2020-07-14 05:09:55,31] [info] JobExecutionTokenDispenser stopped; [2020-07-14 05:09:55,31] [info] Aborting all running workflows.; [2020-07-14 05:09:55,31] [info] WorkflowStoreActor stopped; [2020-07-14 05:09:55,31] [info] WorkflowLogCopyRouter stopped; [2020-07-14 05:09:55,31] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2020-07-14 05:09:55,32] [info] WorkflowManagerActor All workflows finished; [2020-07-14 05:09:55,32] [info] WorkflowManagerActor stopped; [2020-07-14 05:09:55,53] [info] Connection pools shut down; [2020-07-14 05:09:55,53] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,53] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,53] [info] SubWorkflowStoreActor stopped; [2020-07-14 05:09:55,54] [info] JobStoreActor stopped; [2020-07-14 05:0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:8040,Timeout,Timeout,8040,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,4,"['Abort', 'Timeout']","['Aborting', 'Timeout']"
Safety,"er approach to select best haplotypes that can handle complex graph we might well not need to prune low supported hap early as they seemly they won't be selected if the are not amongst the best haplotypes. . B.1 Now that still would produce a considerable number of unlikely haplotypes that would cause a CPU burden. That can be changed by imposing another kinds of limit, For example we include all haplotypes with scores (likelihoods) that are Q0 - Q40 or we include haplotypes until the sum of their likelihoods is larger than the 99.99% probability mass. . B.2 This could provide a downstream solution to the problem caused by ranging heads recovery (explained above in A.2). B.3 If pruning is to be maintained, it makes more sense to do it at the very end after all dangling ends hav been recovered and the edges supports are finalized. Of course I assuming here that dangling end recovery does the sensible think of updating those supports are the graphs is modified. C. The use of Smith-Waterman in dangling end recovery does not seem totally optimal or even needed. . C.1 Recovering tails quite often this finish with the same sequence as the reference path because in fact they are supposed to end like that by construction (reads are trimmed by AR coordinates). For example, this can be cause because due to the k-mer size there is not enough based after variation for the paths to merge back. In this case you can simply merge the last vertices of the tail and the reference, faster and potentially more accurate. . C.2 Similarly dangling heads, at least part of the sequence of those dangling heads are clearly threadable back into the graph without the need of SW. For example look at the AA…AAAAAGA sequence in the picture below. . C.3 PairHMM runs in effect are performing SW kind of computations and so it is totally possible to use its partial result to find good alignment of dangling ends back to other parts of the graph without the need of running a separate SW thus saving time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/264:2035,recover,recovery,2035,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/264,2,"['Recover', 'recover']","['Recovering', 'recovery']"
Safety,"er/tools/spark/pipelines/FlagStatSpark, and the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) for the method's defining class, org/broadinstitute/hellbender/tools/FlagStat$FlagStatus, have different Class objects for the type org/broadinstitute/hellbender/utils/read/GATKRead used in the signature; at java.lang.invoke.MethodHandleNatives.resolve(Native Method); at java.lang.invoke.MemberName$Factory.resolve(MemberName.java:965); at java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:990); at java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:1387); at java.lang.invoke.MethodHandles$Lookup.linkMethodHandleConstant(MethodHandles.java:1739); at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:442); ... 41 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1315:5237,abort,abortStage,5237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315,3,['abort'],['abortStage']
Safety,"es. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in LocusIteratorByState) but keep all undownsampled reads in memory anyway (defeating the main purpose of that first pass), then do a second downsampling pass per active region that does not respect dcov (uses the hardcoded per-region limit).; - If we find that we can't avoid storing all of the undownsampled reads in memory at once for some reason, then perhaps the right thing to do would be to completely disable the downsampling pass in LocusIteratorByState for active region traversals, and disallow the -dcov argument for active region walkers. Downsampling would then by controlled solely by the new argument to set the max # of reads per active region.; - Clarifying the meaning of the per-locus DP annotation for the HC given things like realignment of reads to haplotypes and region-based downsampling. ---. Eventually it was agreed that this would never be fixed in Classic GATK but should be taken into account in designing Hellbender's downsampling.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:6173,avoid,avoid,6173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345,1,['avoid'],['avoid']
Safety,"es/Human/GATK/b37/human_g1k_v37_decoy.fasta""; gvcfs=$(find hapcall -maxdepth 1 -name ""*_hapcall.g.vcf.gz"" -type f | xargs ls | sed 's/^/-V /' | xargs); /data1/software/gatk/4.1.8.1/gatk --java-options ""-XX:ParallelGCThreads=30 -Xms100g -Xmx100g -Djava.io.tmpdir=tmp"" CombineGVCFs -R ${ref_fasta} -O combine/human_combine.g.vcf.gz ${gvcfs} ${intervals} -G StandardAnnotation -G AS_StandardAnnotation --create-output-variant-index true > combine/human_combine.log 2>&1. ---. Here the log:; [human_combine.log](https://github.com/broadinstitute/gatk/files/5165640/human_combine.log). 09:10:26.647 INFO IntervalArgumentCollection - Processing 3095677412 bp from intervals; 09:10:26.694 INFO CombineGVCFs - Done initializing engine; 09:10:26.713 INFO ProgressMeter - Starting traversal; 09:10:26.714 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 09:10:30.685 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location 1:13021 the annotation MLEAC=[1, 0] was not a numerical value and was ignored; 09:10:39.543 INFO ProgressMeter - 1:232994 0.2 1000 4676.9; 09:10:51.253 INFO ProgressMeter - 1:688469 0.4 2000 4890.4; 09:11:01.889 INFO ProgressMeter - 1:809005 0.6 3000 5117.3; 09:11:13.838 INFO ProgressMeter - 1:818424 0.8 5000 6366.2; 09:11:16.811 INFO CombineGVCFs - Shutting down engine; [September 3, 2020 at 9:11:16 AM CST] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 1.20 minutes.; Runtime.totalMemory()=107374182400; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.StrandBiasUtils.encode(StrandBiasUtils.java:52); 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1624); 	at java.base",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6790:1169,Detect,Detected,1169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6790,1,['Detect'],['Detected']
Safety,"esktop/programlar/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/detagen/Desktop/programlar/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar AnalyzeCovariates --before-report-file /home/detagen/Desktop/pipeline/playground//BACKUP/FMF-248_Backup/before.recal.FMF-248.table --after-report-file /home/detagen/Desktop/pipeline/playground//BACKUP/FMF-248_Backup/after.recal.FMF-248.table --plots-report-file /home/detagen/Desktop/pipeline/playground//NECESSARY/FMF-248/AnalyzeCovariates.FMF-248.pdf; 12:57:16.643 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/detagen/Desktop/programlar/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Dec 17, 2020 12:57:16 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:57:16.774 INFO AnalyzeCovariates - ------------------------------------------------------------; 12:57:16.775 INFO AnalyzeCovariates - The Genome Analysis Toolkit (GATK) v4.1.8.1; 12:57:16.775 INFO AnalyzeCovariates - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:57:16.775 INFO AnalyzeCovariates - Executing as detagen@detagen on Linux v5.4.0-58-generic amd64; 12:57:16.775 INFO AnalyzeCovariates - Java runtime: OpenJDK 64-Bit Server VM v11.0.9.1+1-Ubuntu-0ubuntu1.18.04; 12:57:16.775 INFO AnalyzeCovariates - Start Date/Time: December 17, 2020 at 12:57:16 PM TRT; 12:57:16.775 INFO AnalyzeCovariates - ------------------------------------------------------------; 12:57:16.775 INFO AnalyzeCovariates - ------------------------------------------------------------; 12:57:16.776 INFO AnalyzeCovariates - HTSJDK Version: 2.23.0; 12:57:16.776 INFO AnalyzeCovariates - Picard Version: 2.22.8; 12:57:16",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7006:1440,detect,detect,1440,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7006,1,['detect'],['detect']
Safety,"et --reference_window_stop >= 220 ; INFO 21:38:54,237 LeftAlignAndTrimVariants - Reference allele is too long (262) at position chr2_KI270894v1_alt:207863; skipping that record. Set --reference_window_stop >= 262 ; 0 variants were aligned; INFO 21:38:54,554 ProgressMeter - done 3.31246907E8 31.8 m 5.0 s 99.7% 31.8 m 5.0 s ; INFO 21:38:54,554 ProgressMeter - Total runtime 1905.29 secs, 31.75 min, 0.53 hours ; ------------------------------------------------------------------------------------------; Done. There were 4 WARN messages, the first 4 are repeated below.; WARN 17:39:57,688 IndexDictionaryUtils - Track variant doesn't have a sequence dictionary built in, skipping dictionary validation ; WARN 18:13:42,039 SimpleTimer - Clock drift of -1,503,348,737,016,211,299 - -1,503,346,772,578,127,937 = 1,964,438,083,362 nanoseconds detected, vs. max allowable drift of 5,000,000,000. Assuming checkpoint/restart event. ; WARN 20:14:18,043 SimpleTimer - Clock drift of -1,503,355,916,564,964,097 - -1,503,348,737,015,111,124 = 7,179,549,852,973 nanoseconds detected, vs. max allowable drift of 5,000,000,000. Assuming checkpoint/restart event. ; WARN 21:10:35,064 SimpleTimer - Clock drift of -1,503,359,203,412,549,926 - -1,503,355,916,564,817,209 = 3,286,847,732,717 nanoseconds detected, vs. max allowable drift of 5,000,000,000. Assuming checkpoint/restart event. ; ------------------------------------------------------------------------------------------; WMCF9-CB5:Mutect2 shlee$ ; ```. ### Notice the following line from above. > 0 variants were aligned. Also, it would be great if the tool, which appears to keep track of the lengths of reference alleles that are too long, could give me the **maximum length** reference allele so that I can go back and set the `--reference_window_stop` argument appropriately in a second round so that I can left-align _all_ of my variants. . ### MD5 and looking into the files, we see input and output are different and in fact the tool did change al",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487:7840,detect,detected,7840,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487,1,['detect'],['detected']
Safety,"executor 1: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 02:34 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.25, executor 6, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:41:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.xx:45142 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:42:02 INFO TaskSetManager: Lost task 0.3 in stage 2.0 (TID 8) on xx.xx.xx.xx, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 18/04/24 17:42:02 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:42:02 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 117.782 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.ca",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:33597,abort,aborting,33597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['abort'],['aborting']
Safety,"explain what the splitting index is a bit better and then it will make more sense I think. Spark works by splitting files up into similar sized chunks and passing those chunks to different worker machines. ; Bam files are hard to split nicely into chunks. The way they're structured makes it hard to identify where safe boundaries are to split on. If you don't have a splitting index, we have an algorithm to start reading at essentially random locations and look for safe splitting points, but we've had some issues in the past where you can misidentify a split (which results in a crash) or miss good splits. The splitting index is a precomputed list of split points, which works around the problem of having to find the splits again next time. It's only used by spark tools that load bams, so it won't benefit Mutect2 because that's not built on spark. . We should add some documentation about this somewhere... #4235",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4219#issuecomment-359826965:315,safe,safe,315,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4219#issuecomment-359826965,2,['safe'],['safe']
Safety,"f these simply due to over-denoising/filtering by the PoN, but as we'll see below, the count-collection strategy plays a non-trivial role.). Subsetting to chr3, I built a small PoN of 12 normals (including the case normal) at 100bp and denoised using bin medians only (i.e., `--number-of-eigensamples 0`) to avoid denoising away common events. In chr3, the case sample had three events:. ````; chr3	8559423		8560126; chr3	64547471	64549936; chr3	90414457	90415989; ````. I tried the following, running `ModelSegments` using fairly sensitive parameters (`--number-of-changepoints-penalty-factor 0.1 --maximum-number-of-segments-per-chromosome 10000 --window-size 16 --window-size 32 --maximum-number-of-smoothing-iterations 0` in copy-ratio-only mode:. 1) CollectFragmentCounts. This only recovered event 2.; 2) CollectReadCounts - same as CollectFragmentCounts, but removing the properly-paired and first-of-pair filters and adding a count for each read to the bin containing its start. This recovered all 3 events.; 3) CollectFragmentOverlaps - same filters as CollectFragmentCounts, but adding counts to all bins overlapping each fragment. Note that we need to implement a filter on maximum fragment length, otherwise we get some strange artifacts from (incorrectly mapped?) extremely long fragments; I arbitrarily chose a cutoff of 10000bp. This recovered events 1 and 2. Event 3 seemed to be the most difficult to recover. Plotting the copy ratios surrounding this event (which spans ~15 100bp bins) yields some insights:. CollectFragmentCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244188-317a7f1e-2453-11e8-937d-f7239354316e.png). CollectReadCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244228-ad24908c-2453-11e8-91dd-a978578e77f4.png). CollectFragmentOverlaps:; ![image](https://user-images.githubusercontent.com/11076296/37244230-b25b9cee-2453-11e8-8646-f9c95365b355.png). The increased statistical noise in the CollectFragmentCounts result ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4519:1938,recover,recovered,1938,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519,1,['recover'],['recovered']
Safety,"f8f4c8aa8)]; 0x000056487672d800 JavaThread ""C1 CompilerThread1"" daemon [_thread_blocked, id=85492, stack(0x00002b5f8f2c7000,0x00002b5f8f3c7aa8)]; 0x000056487672a000 JavaThread ""C2 CompilerThread0"" daemon [_thread_blocked, id=85491, stack(0x00002b5f8f1c5000,0x00002b5f8f2c5aa8)]; 0x0000564876728000 JavaThread ""Signal Dispatcher"" daemon [_thread_blocked, id=85490, stack(0x00002b5f8f0c3000,0x00002b5f8f1c3aa8)]; 0x00005648766f3800 JavaThread ""Finalizer"" daemon [_thread_blocked, id=85489, stack(0x00002b5f8efc2000,0x00002b5f8f0c2aa8)]; 0x00005648766f0800 JavaThread ""Reference Handler"" daemon [_thread_blocked, id=85488, stack(0x00002b5f8eec1000,0x00002b5f8efc1aa8)]; =>0x00005648765c2000 JavaThread ""main"" [_thread_in_native, id=85483, stack(0x00002b5f56d60000,0x00002b5f56e60aa8)]. Other Threads:; 0x00005648766e6800 VMThread [stack: 0x00002b5f8edc0000,0x00002b5f8eec0aa8] [id=85487]; 0x00005648767b7000 WatcherThread [stack: 0x00002b5f8f4ca000,0x00002b5f8f5caaa8] [id=85494]. VM state:not at safepoint (normal execution). VM Mutex/Monitor currently owned by a thread: None. Heap:; PSYoungGen total 1003520K, used 156121K [0x000000066ab00000, 0x00000006aef00000, 0x00000007c0000000); eden space 946688K, 10% used [0x000000066ab00000,0x0000000670bff978,0x00000006a4780000); from space 56832K, 99% used [0x00000006a5900000,0x00000006a9076d70,0x00000006a9080000); to space 85504K, 0% used [0x00000006a9b80000,0x00000006a9b80000,0x00000006aef00000); ParOldGen total 1497088K, used 20019K [0x00000003c0000000, 0x000000041b600000, 0x000000066ab00000); object space 1497088K, 1% used [0x00000003c0000000,0x00000003c138ceb0,0x000000041b600000); Metaspace used 36791K, capacity 37258K, committed 37504K, reserved 1081344K; class space used 5023K, capacity 5176K, committed 5248K, reserved 1048576K. Card table byte_map: [0x00002b5f67df9000,0x00002b5f69dfa000] byte_map_base: 0x00002b5f65ff9000. Marking Bits: (ParMarkBitMap*) 0x00002b5f57e71fa0; Begin Bits: [0x00002b5f6b656000, 0x00002b5f7b656000); End Bits",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:14264,safe,safepoint,14264,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['safe'],['safepoint']
Safety,"fasta -O /media/glier_ubuntu/4TB/Javad_Final/5-trinity/Fastajavad_Trinity/Trinity.dict; 15:46:56.167 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/glier_ubuntu/gatk-4.1.1.0/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Wed May 13 15:46:56 EDT 2020] CreateSequenceDictionary --OUTPUT /media/glier_ubuntu/4TB/Javad_Final/5-trinity/Fastajavad_Trinity/Trinity.dict --REFERENCE /media/glier_ubuntu/4TB/Javad_Final/5-trinity/Fastajavad_Trinity/Trinity.fasta --TRUNCATE_NAMES_AT_WHITESPACE true --NUM_SEQUENCES 2147483647 --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; May 13, 2020 3:46:57 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Wed May 13 15:46:57 EDT 2020] Executing as glier_ubuntu@glierubuntu-Precision-7920-Tower on Linux 4.15.0-99-generic amd64; OpenJDK 64-Bit Server VM 11.0.7+10-post-Ubuntu-2ubuntu218.04; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.1.0; [Wed May 13 15:46:57 EDT 2020] picard.sam.CreateSequenceDictionary done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2107637760; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp; picard.PicardException: /media/glier_ubuntu/4TB/Javad_Final/5-trinity/Fastajavad_Trinity/Trinity.dict already exists. Delete this file and try again, or specify a different output file.; 	at picard.sam.CreateSequenceDictionary.doWork(CreateSequenceDictionary.java:209); 	at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:295); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(Pic",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6604:1825,detect,detect,1825,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6604,1,['detect'],['detect']
Safety,faster GATKRead.numCigarElements - avoid object allocation for unmodifable list. Speeds up HC runs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1796:35,avoid,avoid,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1796,1,['avoid'],['avoid']
Safety,fixes #1140 (extracts file names to name constants to avoid errors and indicate where same outputs are expected). for @droazen,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1152:54,avoid,avoid,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1152,1,['avoid'],['avoid']
Safety,fixes #1448. MarkDuplicates spark was not writing an output bam when running with a standalone spark instance if the output path was a relative file path.; This fixes the problem by making any relative file paths into absolute file paths.; Added a check to see if no part files can be found so that errors like this will crash in the future instead of silently failing. No tests are added because it's still unclear how to test errors that only occur in certain spark configurations. `makeFilePathAbsolute()` should be redundant once #958 is finished,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1450:519,redund,redundant,519,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1450,1,['redund'],['redundant']
Safety,"fo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2020-07-14 05:09:55,30] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2020-07-14 05:09:55,31] [info] JobExecutionTokenDispenser stopped; [2020-07-14 05:09:55,31] [info] Aborting all running workflows.; [2020-07-14 05:09:55,31] [info] WorkflowStoreActor stopped; [2020-07-14 05:09:55,31] [info] WorkflowLogCopyRouter stopped; [2020-07-14 05:09:55,31] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2020-07-14 05:09:55,32] [info] WorkflowManagerActor All workflows finished; [2020-07-14 05:09:55,32] [info] WorkflowManagerActor stopped; [2020-07-14 05:09:55,53] [info] Connection pools shut down; [2020-07-14 05:09:55,53] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,53] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,53] [info] SubWorkflowStoreActor stopped; [2020-07-14 05:09:55,54] [info] JobStoreActor stopped; [2020-07-14 05:09:55,53] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,54] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,54] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2020-07-14 05:09:55,54] [info] CallCacheWriteActor stopped; [2020-07-14 05:09:55,54] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,54] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2020-07-14 05:09:55,54] [info] IoProxy stopped; [2020-07-14 05:09:55,54] [info] DockerHashActor stopped; [2020-07-14 05:09:55,55] [info] Shutting down connection pool: curAllocated=1 idleQueues.size=1 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2020-07-14 05:09:55,55] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2020-07-14 05:09:55,55] [info] WriteMetadataActor Shutting down: 0 queued messag",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:8573,Timeout,Timeout,8573,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,7,['Timeout'],['Timeout']
Safety,"gAlignmentsModifier` (refactor `AlnModType` into it), `GappedAlignmentSplitter`, `StrandSwitch`, `FilterLongReadAlignmentsSAMSpark` (factor out the major methods in the new alignment filter by score into a 1st level class). ### type & location inference (sub package). * imprecise: refactor out methods from to-be-deprecated `DiscoverVariantsFromContigAlignmentsSAMSpark`. * alignment classification: `ChimericAlignment` and `NovelAdjacencyReferenceLocations` (very tricky to decouple the functionalities because both have over 50 uses), `AssemblyContigAlignmentSignatureClassifier`, `VariantDetectorFromLocalAssemblyContigAlignments`. * simple: `SimpleSVType`, `SvTypeInference`, `InsDelVariantDetector`, `BreakpointComplications` (rename to `BreakpointComplicationsForSimpleTypes`). * complex: `BreakEndVariantType`, `SuspectedTransLocDetector`, `SimpleStrandSwitchVariantDetector`. ### deprecated. `DiscoverVariantsFromContigAlignmentsSAMSpark` . It currently provides 3 groups of functionalities:. * novel adjacency detection (for ins, del, small dup, inversion only) by delegating to `ChimericAlignment.parseOneContig` and `NovelAdjacencyReferenceLocations(ChimericAlignment chimericAlignment, byte[] contigSequence, SAMSequenceDictionary)`; this should be deprecated; * exact variant type inference (delegated to `SvTypeInference.inferFromNovelAdjacency()`) and annotation (delegated to `AnnotatedVariantProducer.produceAnnotatedVcFromInferredTypeAndRefLocations()`); this should be deprecated; * imprecise variants detection; this should be kept and factored out. --------; --------. ## Planed steps. 1. repackaging & refactoring (no logic change, see #3934 ); 2. bring in some valuable changes made in PR #3668; 3. **more test coverage** (ticket #3431); 4. switch; make `StructuralVariationDiscoveryPipelineSpark` call into `SvDiscoverFromLocalAssemblyContigAlignmentsSpark` by default and optionally into `DiscoverVariantsFromContigAlignmentsSAMSpark`, i.e. opposite of what we currently do.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4111:2060,detect,detection,2060,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4111,2,['detect'],['detection']
Safety,"gatk --java-options ""-Xmx8G -XX:ParallelGCThreads=16 -Djava.io.tmpdir=/group/zhougrp2/dguan/tmp"" BaseRecalibrator --spark-runner LOCAL -I 11_cigar/SAMN06242676_cigar.bam --known-sites /group/zhougrp2/dguan/00_ref/gallus_gallus.vcf.gz -L /group/zhougrp2/dguan/00_ref/chicken_chr.list -O 12_bqsr/SAMN06242676_bqsr.table -R /group/zhougrp2/dguan/00_ref/Gallus_gallus.GRCg6a.dna.toplevel.fa --tmp-dir /group/zhougrp2/dguan/tmp. 00:59:52.106 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/dguan/anaconda3/envs/Chicken_GTEx/share/gatk4-4.2.0.0-0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 22, 2021 12:59:52 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:59:52.360 INFO BaseRecalibrator - ------------------------------------------------------------; 00:59:52.361 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.0.0; 00:59:52.361 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:59:52.361 INFO BaseRecalibrator - Executing as dguan@c11-95 on Linux v4.15.0-122-generic amd64; 00:59:52.361 INFO BaseRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 00:59:52.362 INFO BaseRecalibrator - Start Date/Time: February 22, 2021 at 12:59:52 AM PST; 00:59:52.362 INFO BaseRecalibrator - ------------------------------------------------------------; 00:59:52.362 INFO BaseRecalibrator - ------------------------------------------------------------; 00:59:52.363 INFO BaseRecalibrator - HTSJDK Version: 2.24.0; 00:59:52.363 INFO BaseRecalibrator - Picard Version: 2.25.0; 00:59:52.363 INFO BaseRecalibrator - Built for Spark Version: 2.4.5; 00:59:52.363 INFO BaseRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 00:59:52.363 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:59:52.363 INFO BaseRecalibrator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7092:776,detect,detect,776,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7092,1,['detect'],['detect']
Safety,gatk GenotypeGVCFs error：A USER ERROR has occurred: Bad input: Presence of '-RAW_MQ' annotation is detected.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574:99,detect,detected,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574,1,['detect'],['detected']
Safety,"gatk-4.4.0.0 out there and still, `gatk HaplotypeCaller --sample-name 7-Wu-FF ...` bails on me with:. `A USER ERROR has occurred: Argument --sample_name has a bad value: Specified name does not exist in input bam files`. Indeed, there is not `@RG` line at all, how how about adding an extra error/warning message somewhere more above in the code path?. ```; $ samtools view -H 7-Wu-FF.sorted.bam; @HD VN:1.6 SO:coordinate; @SQ SN:7-Wu-FF LN:443; @PG ID:bwa PN:bwa VN:0.7.17-r1188 CL:bwa mem -t 32 -o exact_matches/7-Wu-FF.sam 7-Wu-FF.fasta 7-Wu-FF_R1.fastq.gz 7-Wu-FF_R2.fastq.gz; @PG ID:samtools PN:samtools PP:bwa VN:1.17 CL:samtools sort 7-Wu-FF.sorted.bam 7-Wu-FF.sam; @PG ID:samtools.1 PN:samtools PP:samtools VN:1.17 CL:samtools view -H 7-Wu-FF.sorted.bam; $; ```. Like others I agree that if there are no samples found by GATK then *all* data should be used. I also think that `@SQ SN:` is enough for a sanity check. No need for `@RG SN:` tag. Finally, fix the `--sample_name` with `--sample-name` in the error message output, the old syntax with an underscore is not accepted anymore.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6501#issuecomment-1832051700:910,sanity check,sanity check,910,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6501#issuecomment-1832051700,1,['sanity check'],['sanity check']
Safety,"gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/cc/gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar HaplotypeCaller -R /db_students1/genetic_map/snp_calling/bbv18h27rm.fa -ERC GVCF --alleles /db_students1/gatk_out/db_raw_call_bbe_6largest.vcf -I /db_students1/genetic_map/reseqData_mapping_bam/bam/bbe_off_xL3_68.concordant_withRG.bam -O /db_students1/gatk_out/test_call_off_xL3_68_allele_46samples.vcf.gz; 17:04:13.440 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/cc/gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 12, 2019 5:04:15 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:04:15.137 INFO HaplotypeCaller - ------------------------------------------------------------; 17:04:15.138 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.3.0-25-g8d88f6e-SNAPSHOT; 17:04:15.138 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:04:15.138 INFO HaplotypeCaller - Executing as cc@hr18b on Linux v3.10.0-957.el7.x86_64 amd64; 17:04:15.138 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_212-b04; 17:04:15.138 INFO HaplotypeCaller - Start Date/Time: November 12, 2019 5:04:13 PM CST; 17:04:15.139 INFO HaplotypeCaller - ------------------------------------------------------------; 17:04:15.139 INFO HaplotypeCaller - ------------------------------------------------------------; 17:04:15.139 INFO HaplotypeCaller - HTSJDK Version: 2.20.3; 17:04:15.140 INFO HaplotypeCaller - Picard Version: 2.20.5; 17:04:15.140 INFO HaplotypeCalle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6260:1292,detect,detect,1292,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6260,1,['detect'],['detect']
Safety,"genomicsdbarray-at-workspace-from-GenotypeGVCFs-in-GATK-4000) , but except for using the latest version of GATK, it seems like there are no other solutions. I was wondering that how do I fix the issues with GATK 4.0.3.0? Does anyone have a better solution?. I also tried GenotypeGVCFs in GATK 4.2.1.0, but there is a problem in terms of MQ calculation. So I think it's better to stick to the same GATK version in the whole workflow. A USER ERROR has occurred: Bad input: Presence of '-RAW\_MQ' annotation is detected. ; ; This GATK version expects key RAW\_MQandDP with a tuple of sum of squared MQ values and total reads over variant genotypes as the value. ; ; This could indicate that the provided input was produced with an older version of GATK. ; ; Use the argument '--allow-old-rms-mapping-quality-annotation-data' to override and attempt the deprecated MQ calculation. ; ; There may be differences in how newer GATK versions calculate DP and MQ that may result in worse MQ results. Use at your own risk. Another question is related to the fasta file:. I downloaded the reference data in the link of [https://console.cloud.google.com/storage/browser/gatk-legacy-bundles/b37](https://console.cloud.google.com/storage/browser/gatk-legacy-bundles/b37) , when I noticed that this is an old database, I have already generated GVCF files. It seems like GenotypeGVCFs does not understand the FAI index file. error informaion; ================. \[E::fai\_read\] Could not understand FAI /home/users/nus/bizszl/scratch/WES-new/reference\_hg19/b37\_human\_g1k\_v37\_decoy.fasta.fai line 1 ; ; \[E::fai\_load3\] Failed to read FASTA index /home/users/nus/bizszl/scratch/WES-new/reference\_hg19/b37\_human\_g1k\_v37\_decoy.fasta.fai. FAI file; ========. 1 dna:chromosome chromosome:GRCh37:1:1:249250621:1 249250621 52 60 61 ; ; 2 dna:chromosome chromosome:GRCh37:2:1:243199373:1 243199373 253404903 60 61 ; ; 3 dna:chromosome chromosome:GRCh37:3:1:198022430:1 198022430 500657651 60 61 ; ; 4 dna:chromosom",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7442:6501,risk,risk,6501,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7442,1,['risk'],['risk']
Safety,"gets separated from garbage in the 20 threshold graph: ![Screen Shot 2022-01-25 at 4 13 52 PM](https://user-images.githubusercontent.com/16102845/151060728-5a0d4d95-2eb4-4777-a0e9-34b07b2e6196.png). And here is that spot in the 60 threshold graph:; ![Screen Shot 2022-01-25 at 4 16 43 PM](https://user-images.githubusercontent.com/16102845/151061165-fb803312-59b8-48c4-b196-b0e97d2e00ea.png). And here it is in the 1 threshold ; <img width=""303"" alt=""Screen Shot 2022-01-25 at 4 22 17 PM"" src=""https://user-images.githubusercontent.com/16102845/151061909-25d41a3d-39c2-461e-8fd3-938e859ef3d7.png"">; graph:. This seems to have caused the two thresholds to assemble different haplotypes after dangling end recovery (since all of these are dangling ends because the assembly engine can't do anything else because there is not enough padding provided) and it just so happens this failed assembly misses the correct haplotype in that 20 threshold graph and we end up throwing away most of the reads as incongruent with assembly as a result which is why the depth drops out so low at that site. This is a pretty rare edge case and I happened to be able to recover the 20 mq threshold variant with reasonable correct coverage by playing with the `--min-pruning 4` argument. In general though this issue might or might not have existed if the bam snippet provided (and especially the calling interval you provided of chr7:145945238-145945238) were not centered on one single point since assembly works best and is most likely to succeed when it has a few hundred bases of padding around the variant in question (typically for a SNP we end up with at least 100 bases of active window plus another 300 bases of padding on either side for assembly) which cuts down on the risk of assembly failures like this one. I'm curious if you observed this behavior on the initial variantcalling run of HaplotypeCaller on this data or only when you try scalpel out the SNP at chr7:145945238 for testing in this experiment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7124#issuecomment-1021629139:1564,recover,recover,1564,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7124#issuecomment-1021629139,2,"['recover', 'risk']","['recover', 'risk']"
Safety,"gsutil working is not a good predictor of gatk working. It's possible for default credentials to be wrong but gsutil credentials to be fine at the same time. Here is an example of how to get into this situation:. ```; // set application credentials; gcloud auth login; // unset default credentials (alternatively, forget to set them in the first place); gcloud auth application-default revoke; // gsutil works; gsutil cat $VCF > /dev/null; // GATK does not work; ./gatk-launch SelectVariants --verbosity=DEBUG -V $VCF -L 1:1000-2000 -O /tmp/foo.vcf; A USER ERROR has occurred: Couldn't read file (...); ```. Please make sure to set up Google Cloud access as follows:; ```; $ gcloud auth application-default login; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281526964:29,predict,predictor,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281526964,1,['predict'],['predictor']
Safety,"gz \; -V /home/xuql/copyNAM/CML277/CML277ToB73.gvcf.gz \; -V /home/xuql/copyNAM/CML52/CML52ToB73.gvcf.gz \; -V /home/xuql/copyNAM/Il14H/Il14HToB73.gvcf.gz \; -V /home/xuql/copyNAM/Ky21/Ky21ToB73.gvcf.gz \; -V /home/xuql/copyNAM/Mo18W/Mo18WToB73.gvcf.gz \; -V /home/xuql/copyNAM/NC358/NC358ToB73.gvcf.gz \; -V /home/xuql/copyNAM/P39/P39ToB73.gvcf.gz \; -V /home/xuql/copyNAM/CML228/CML228ToB73.gvcf.gz \; -V /home/xuql/copyNAM/CML322/CML322ToB73.gvcf.gz \; -V /home/xuql/copyNAM/CML69/CML69ToB73.gvcf.gz \; -V /home/xuql/copyNAM/Ki11/Ki11ToB73.gvcf.gz \; -V /home/xuql/copyNAM/M162W/M162WToB73.gvcf.gz \; -V /home/xuql/copyNAM/Ms71/Ms71ToB73.gvcf.gz \; -V /home/xuql/copyNAM/Oh43/Oh43ToB73.gvcf.gz \; -V /home/xuql/copyNAM/Tx303/Tx303ToB73.gvcf.gz \; --batch-size 1 \; --genomicsdb-workspace-path /home/xuql/copyNAM/NAM_out_gatk9_1 \; --genomicsdb-segment-size 1048576 --genomicsdb-vcf-buffer-size 50000000 -L 9. #Elapsed time: 52.78 minutes. Runtime.totalMemory()=6761218048; ```. I run `LeftAlignAndTrimVariants` at default parameter, ordered all length of > 200 indels from long to short for all chromosomes. the result is as followed, we found the aborted location nearby the super-indel.; ```; ['10:56:14.335 INFO LeftAlignAndTrimVariants - Indel is too long (34461688) at position 9:3695105; skipping that record. Set --max-indel-length >= 34461688\n',; '10:56:30.429 INFO LeftAlignAndTrimVariants - Indel is too long (10668738) at position 10:33212598; skipping that record. Set --max-indel-length >= 10668738\n',; '10:56:28.937 INFO LeftAlignAndTrimVariants - Indel is too long (9101264) at position 10:14179; skipping that record. Set --max-indel-length >= 9101264\n',; '10:56:30.038 INFO LeftAlignAndTrimVariants - Indel is too long (7918835) at position 10:22996027; skipping that record. Set --max-indel-length >= 7918835\n',; '11:31:49.968 INFO LeftAlignAndTrimVariants - Indel is too long (7154442) at position 6:16715313; skipping that record. Set --max-indel-length >= 7154442\n',; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7976#issuecomment-1367332059:2743,abort,aborted,2743,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7976#issuecomment-1367332059,1,['abort'],['aborted']
Safety,"h htsjdk). ### Affected version(s); - GATK version: 4.1.9.0. ### Description ; Running `ApplyVQSR` on a VCF with a pre-existing SB field makes it overwrite it with an incorrect header. Original header:. ```; ##INFO=<ID=SB,Number=.,Type=Integer,Description="""">; ``` . New header:. ```; ##INFO=<ID=SB,Number=1,Type=Float,Description=""Strand Bias"">; ```. Even though the `INFO` values stay in the format of an array of integers: `SB=5,2,18,29`. My understanding is the replacement of the header actually [happens in htsjdk](https://github.com/samtools/htsjdk/blob/7719274fe370a51a24e6067de21bbe7e18c160a9/src/main/java/htsjdk/variant/vcf/VCFStandardHeaderLines.java#L187), as it has SB in a set of [standard fields](https://github.com/samtools/htsjdk/blob/7719274fe370a51a24e6067de21bbe7e18c160a9/src/main/java/htsjdk/variant/vcf/VCFStandardHeaderLines.java#L175). I see that [many](https://github.com/broadinstitute/gatk/search?q=Per-sample+component+statistics+which+comprise+the+Fisher%27s+Exact+Test+to+detect+strand+bias) GATK tools add SB with Number=1,Type=Integer, and the [VCF spec](https://samtools.github.io/hts-specs/VCFv4.2.pdf) doesn't actually say what format should the SB field be, so overriding it seems to be a bug in htsjdk?. #### Steps to reproduce; ```; cat sb-good-tiny.vcf; ##fileformat=VCFv4.2; ##INFO=<ID=SB,Number=.,Type=Integer,Description="""">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38>; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO; chr1	10146	.	AC	A	.	.	SB=5,2,18,29. gatk ApplyVQSR -O sb-recalibrated-tiny.vcf -V sb-good-tiny.vcf --recal-file recalibration. cat sb-recalibrated-tiny.vcf; ##fileformat=VCFv4.2; ##FILTER=<ID=LOW_VQSLOD,Description=""VQSLOD < 0.0"">; ##FILTER=<ID=PASS,Description=""Site contains at least one allele that passes filters"">; ##GATKCommandLine=<ID=ApplyVQSR,CommandLine=""ApplyVQSR --recal-file /Users/vlad/tmp/sb/recalibration --output sb-recalibrated-tiny-renamed4.vcf --variant sb-good-tiny-renamed4.vcf --use-allele-specific-annotations fals",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7280:1127,detect,detect,1127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7280,1,['detect'],['detect']
Safety,"h the code, but I do have some concerns about the design:. In the SV group's pipeline, we distribute this multi-gig file from its home in the cloud once at cluster-creation time, and then reuse it for multiple client executions. There are no superfluous copies lying about anywhere, and no redundant copying operations. We can give it any name we wish, and put it anywhere we desire (except that the path must be the same on every worker). This code, if I'm reading it correctly, will redistribute the file from a non-permanent home on the master's local file system or on the HDFS (to which it must be copied redundantly at least once per cluster instantiation), and then it will further be redundantly copied to a temporary location on each worker's local file system with every client execution. I don't know if that's overhead that we can live with, or whether that might prevent us from writing clients with brief execution times. I'm just opening the issue for discussion. We also lose a little flexibility in that the image must live in the same directory as the reference, though I don't think that's a serious drawback -- it's a perfectly logical place for it. However, since we're just appending a fixed extension ("".img"") to the reference name we can only have one image file per reference, which may be a problem because different images need to be created for different versions of bwa and for various options such as the list of alt contigs. We can handle the first problem by insisting that all clients on a particular cluster stick to one version of bwa, which is probably a good idea, anyway, but I think we're stuck if clients need to specify various alt contig lists. It might be better to provide a default path of ""ref-name""+"".img"", but allow that default to be overridden. Also, just to twist the knife a bit, it's too bad we never reviewed my PR for gatk-bwamem-jni, which version-stamped the images for safety. It's now languished since July, and has suffered serious bit rot.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3643#issuecomment-333598350:1960,safe,safety,1960,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3643#issuecomment-333598350,1,['safe'],['safety']
Safety,hashtable lookups are expensive in Kryo and they add up to 15% or more of runtime (top hotspot on Xprof). https://twitter.com/aphyr/status/478638361150636032. This PR turns off reference tracking in Kryo which speeds things up ~7.2mins vs 7.4mins on MarkDuplicatesSpark but it's a bit risky because I think it may result in an infinite loop for cyclic object graphs. We do not have any cyclic object graphs now and so it's fine. The PR is to open a convo about this.; @tomwhite @droazen @laserson wdyt?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1734:285,risk,risky,285,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1734,1,['risk'],['risky']
Safety,"hat have very clear read support as seen in IGV. I have used the –bam-output option to show the output bam and in IGV, it shows that there is no assembly in this region and no mutation event was detected. I am showing the IGV screenshot for one of such calls (chr12:25398285). ![](https://gatk.broadinstitute.org/hc/user_images/46GjRo3tH-Y456j6ApIsqw.png). I am using the latest version GATK 4.2.0.0 and the following is the full Mutect2 command from the log file. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Mutect2 -R ../resources/hg19.fa -L ../resources/coding\_regions.bed -I bam\_files/sample1.bam --pon ../resources/pon.vcf.gz --germline-resource ../resources/af-only-gnomad.raw.sites.hg19.vcf.gz --bam-output sample1.mutect2\_out.bam --recover-all-dangling-branches true -min-pruning 1 --min-dangling-branch-length 2 --debug --max-reads-per-alignment-start 0 --genotype-pon-sites True --f1r2-tar-gz vcf\_files/f1r2.sample1.tar.gz -O vcf\_files/unfiltered.sample1.vcf  . In the debug mode, the following log messages are generated for this region. 08:01:26.086 INFO  Mutect2Engine - Assembling chr12:**2539**8242-**2539**8320 with 14298 reads:    (with overlap region = chr12:**2539**8142-**2539**8420). I have another call with similar VAF that is detected in the vcf output(chr12:25380275). **chr12** 25380275   .    T    G    .    .     AS\_SB\_TABLE=3911,5343|26,21;DP=9485;ECNT=1;MBQ=36,36;MFRL=0,0;MMQ=42,42;MPOS=18;POPAF=7.30;TLOD=53.53     GT:AD:AF:DP:F1R2:F2R1:SB   0/1:9254,47:4.970e-03:9301:5321,21:3867,26:3911,5343,26,21. The input and the output BAMs show this call with the variant. ![](https://gatk.broadinstitute.org/hc/user_images/FVlI3WhNIzYK7NB7PakCmw.png). In the logs, it shows the detection of an active region here:. 08:01:23.642 INFO  Mutect2Engine - Assembling chr12:**2538**0238-**2538**0327 with ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7232:1610,recover,recover-all-dangling-branches,1610,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232,1,['recover'],['recover-all-dangling-branches']
Safety,he - cache miss 1 > -1 expanding to 11; 11:35:42.619 DEBUG Mutect2 - Processing assembly region at chrM:301-600 isActive: false numReads: 0; 11:35:42.757 DEBUG IntToDoubleFunctionCache - cache miss 18 > 11 expanding to 28; 11:35:42.758 DEBUG IntToDoubleFunctionCache - cache miss 2649 > 28 expanding to 2659; 11:35:42.766 DEBUG IntToDoubleFunctionCache - cache miss 2666 > 11 expanding to 2676; 11:35:42.789 DEBUG IntToDoubleFunctionCache - cache miss 2667 > 2659 expanding to 5320; 11:35:42.790 DEBUG IntToDoubleFunctionCache - cache miss 2679 > 2676 expanding to 5354; 11:35:43.244 DEBUG Mutect2 - Processing assembly region at chrM:601-900 isActive: false numReads: 0; 11:35:43.823 DEBUG Mutect2 - Processing assembly region at chrM:901-1153 isActive: false numReads: 2725; 11:35:44.025 DEBUG Mutect2 - Processing assembly region at chrM:1154-1397 isActive: true numReads: 5446; 11:35:45.183 DEBUG ReadThreadingGraph - Recovered 0 of 0 dangling tails; 11:35:45.190 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 11:35:45.409 DEBUG IntToDoubleFunctionCache - cache miss 0 > -1 expanding to 10; 11:35:45.413 DEBUG Mutect2Engine - Active Region chrM:1154-1397; 11:35:45.413 DEBUG Mutect2Engine - Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Ref haplotype coords chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Haplotype count 1; 11:35:45.413 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:45.414 DEBUG Mutect2Engine - Kmer sizes values []; 11:35:45.737 DEBUG Mutect2 - Processing assembly region at chrM:1398-1697 isActive: false numReads: 2722; 11:35:45.837 DEBUG Mutect2 - Processing assembly region at chrM:1698-1997 isActive: false numReads: 0; 11:35:45.999 DEBUG Mutect2 - Processing assembly region at chrM:1998-2297 isActive: false numReads: 0; 11:35:46.219 DEBUG Mutect2 - Processing assembly region at chrM:2298-2543 isActive: false numReads: 2555; 11:35:46.674 DEBUG Mutect2 - Processing assembly region at chrM:2544-2841 isActive: true numReads: 5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:8141,Recover,Recovered,8141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Safety,"he 99.9 -tranche 99.5 -tranche 99.0 -tranche 97.0 -tranche 96.0 -tranche 95.0 -tranche 94.0 -tranche 93.5 -tranche 93.0 -tranche 92.0 -tranche 91.0 -tranche 90.0 -an DP -an FS -an MQRankSum -an QD -an ReadPosRankSum -an SOR -mode INDEL --max-gaussians 4 -resource:mills,known=false,training=true,truth=true,prior=12 ~/db/mutect2_support/b37/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=2 ~/db/mutect2_support/b37/hg19_v0_dbsnp_138.b37.vcf.gz --use-allele-specific-annotations -resource:axiomPoly,known=false,training=true,truth=false,prior=10 ~/db/mutect2_support/b37/Axiom_Exome_Plus.genotypes.all_populations.poly.b37.vcf.gz. 14:58:10.389 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 12, 2020 2:58:10 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:58:10.555 INFO VariantRecalibrator - ------------------------------------------------------------; 14:58:10.555 INFO VariantRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.9.0; 14:58:10.555 INFO VariantRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:58:10.555 INFO VariantRecalibrator - Executing as y@c001 on Linux v3.10.0-957.el7.x86_64 amd64; 14:58:10.555 INFO VariantRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 14:58:10.556 INFO VariantRecalibrator - Start Date/Time: November 12, 2020 2:58:10 PM CST; 14:58:10.556 INFO VariantRecalibrator - ------------------------------------------------------------; 14:58:10.556 INFO VariantRecalibrator - ------------------------------------------------------------; 14:58:10.556 INFO VariantRecalibrator - HTSJDK Version: 2.23.0; 14:58:10.556 INFO VariantRecalibrator - Picard Version: 2.23.3; 14",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6963:3238,detect,detect,3238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6963,1,['detect'],['detect']
Safety,"he 99.9 -tranche 99.5 -tranche 99.0 -tranche 97.0 -tranche 96.0 -tranche 95.0 -tranche 94.0 -tranche 93.5 -tranche 93.0 -tranche 92.0 -tranche 91.0 -tranche 90.0 -an DP -an FS -an MQRankSum -an QD -an ReadPosRankSum -an SOR -mode INDEL --max-gaussians 4 -resource:mills,known=false,training=true,truth=true,prior=12 ~/db/mutect2_support/b37/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=2 ~/db/mutect2_support/b37/hg19_v0_dbsnp_138.b37.vcf.gz --use-allele-specific-annotations -resource:axiomPoly,known=false,training=true,truth=false,prior=10 ~/db/mutect2_support/b37/Axiom_Exome_Plus.genotypes.all_populations.poly.b37.vcf.gz. 14:58:10.389 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 12, 2020 2:58:10 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:58:10.555 INFO VariantRecalibrator - ------------------------------------------------------------; 14:58:10.555 INFO VariantRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.9.0; 14:58:10.555 INFO VariantRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:58:10.555 INFO VariantRecalibrator - Executing as y@c001 on Linux v3.10.0-957.el7.x86_64 amd64; 14:58:10.555 INFO VariantRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 14:58:10.556 INFO VariantRecalibrator - Start Date/Time: November 12, 2020 2:58:10 PM CST; 14:58:10.556 INFO VariantRecalibrator - ------------------------------------------------------------; 14:58:10.556 INFO VariantRecalibrator - ------------------------------------------------------------; 14:58:10.556 INFO VariantRecalibrator - HTSJDK Version: 2.23.0; 14:58:10.556 INFO VariantRecalibrator - Picard Version: 2.23.3; 14",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6701#issuecomment-726406532:1890,detect,detect,1890,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6701#issuecomment-726406532,1,['detect'],['detect']
Safety,he miss 1 > -1 expanding to 11; 11:35:42.520 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.619 DEBUG Mutect2 - Processing assembly region at chrM:301-600 isActive: false numReads: 0; 11:35:42.757 DEBUG IntToDoubleFunctionCache - cache miss 18 > 11 expanding to 28; 11:35:42.758 DEBUG IntToDoubleFunctionCache - cache miss 2649 > 28 expanding to 2659; 11:35:42.766 DEBUG IntToDoubleFunctionCache - cache miss 2666 > 11 expanding to 2676; 11:35:42.789 DEBUG IntToDoubleFunctionCache - cache miss 2667 > 2659 expanding to 5320; 11:35:42.790 DEBUG IntToDoubleFunctionCache - cache miss 2679 > 2676 expanding to 5354; 11:35:43.244 DEBUG Mutect2 - Processing assembly region at chrM:601-900 isActive: false numReads: 0; 11:35:43.823 DEBUG Mutect2 - Processing assembly region at chrM:901-1153 isActive: false numReads: 2725; 11:35:44.025 DEBUG Mutect2 - Processing assembly region at chrM:1154-1397 isActive: true numReads: 5446; 11:35:45.183 DEBUG ReadThreadingGraph - Recovered 0 of 0 dangling tails; 11:35:45.190 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 11:35:45.409 DEBUG IntToDoubleFunctionCache - cache miss 0 > -1 expanding to 10; 11:35:45.413 DEBUG Mutect2Engine - Active Region chrM:1154-1397; 11:35:45.413 DEBUG Mutect2Engine - Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Ref haplotype coords chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Haplotype count 1; 11:35:45.413 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:45.414 DEBUG Mutect2Engine - Kmer sizes values []; 11:35:45.737 DEBUG Mutect2 - Processing assembly region at chrM:1398-1697 isActive: false numReads: 2722; 11:35:45.837 DEBUG Mutect2 - Processing assembly region at chrM:1698-1997 isActive: false numReads: 0; 11:35:45.999 DEBUG Mutect2 - Processing assembly region at chrM:1998-2297 isActive: false numReads: 0; 11:35:46.219 DEBUG Mutect2 - Processing assembly region at chrM:2298-2543 isActive: false numReads: 2555; 11:35:46.674 DEBUG Mutect2 ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:8068,Recover,Recovered,8068,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Safety,he.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607); at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:31990,abort,abortStage,31990,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['abort'],['abortStage']
Safety,"he.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). 11:00:53.977 INFO DAGScheduler - Job 1 failed: runJob at SparkHadoopWriter.scala:83, took 3.799268 s; 11:00:53.979 ERROR SparkHadoopWriter - Aborting job job_202408111100502620487673658411251_0021.; org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.OutOfMemoryError: Required array length 2147483639 + 798 is too large; java.lang.OutOfMemoryError: Required array length 2147483639 + 798 is too large; at java.base/jdk.internal.util.ArraysSupport.hugeLength(ArraysSupport.java:649); at java.base/jdk.internal.util.ArraysSupport.newLength(ArraysSupport.java:642); at java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:100); at java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:130); at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41); at java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1862); at java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:714); at org.apache.spark.util.Utils$$anon$2.write(Utils.scala:160); at com.esotericsoftware.kryo.io.Output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:8532,Abort,Aborting,8532,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['Abort'],['Aborting']
Safety,"he.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(50,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(52,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(34,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(60,WrappedArray()); 20:38:37.897 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [January 12, 2018 8:38:37 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 42.74 minutes.; Runtime.totalMemory()=16692805632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 284 in stage 25.0 failed 4 times, most recent failure: Lost task 284.3 in stage 25.0 (TID 43224, cw-test-w-6.c.broad-dsde-methods.internal, executor 7): java.lang.IllegalArgumentException: two input alignments' overlap on read consumes completely one of them.	1_1097_chrUn_JTFH01000492v1_decoy:501-1597_+_1097M6H_60_1_1092_O	483_612_chr17:26962677-26962806_-_482S130M491S_60_-1_281_S; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.ContigAlignmentsModifier.removeOverlap(ContigAlignmentsModifier.java:36); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.AssemblyContigAlignmentSignatureClassifier.lambda$processContigsWithTwoAlignments$e28aa838$1(AssemblyContigAlignmentSignatureClassifier.java:114); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:6215,abort,aborted,6215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['abort'],['aborted']
Safety,"here are some reads missing their mates, but this hasn't presented an issue in other tools (including vanilla BaseRecalibrator). Searching thru the forum, I found an old issue with a similar stacktrace, but that issue appears to occur in GATK 2.4: https://gatkforums.broadinstitute.org/gatk/discussion/3265/bqsrgatherer-exception. In the below stacktrace, I've bolded the error message that seems to occur in each of these samples. `Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scal",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5854:1776,abort,abortStage,1776,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854,1,['abort'],['abortStage']
Safety,"hi @KevinCLydon, @davidbenjamin , thanks for this fix. I am preparing a custom version of 4.1.4.1 with just this patch on top of it. One question I had is: is this a safe patch to have on its own, or does it need to go in tandem with any other patch to form a proper release (and not break anything else)? Given this fix is now ~7 months old, I'm hoping insiders can maybe tell me if any other patch ended up being needed after this one, either to deal with the same issue, or to deal with any new issues introduced by it (if any).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6327#issuecomment-659117371:166,safe,safe,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6327#issuecomment-659117371,1,['safe'],['safe']
Safety,"how/67601310; ```. B. Low support chain pruning might not be longer needed. Now we have a newer approach to select best haplotypes that can handle complex graph we might well not need to prune low supported hap early as they seemly they won't be selected if the are not amongst the best haplotypes. . B.1 Now that still would produce a considerable number of unlikely haplotypes that would cause a CPU burden. That can be changed by imposing another kinds of limit, For example we include all haplotypes with scores (likelihoods) that are Q0 - Q40 or we include haplotypes until the sum of their likelihoods is larger than the 99.99% probability mass. . B.2 This could provide a downstream solution to the problem caused by ranging heads recovery (explained above in A.2). B.3 If pruning is to be maintained, it makes more sense to do it at the very end after all dangling ends hav been recovered and the edges supports are finalized. Of course I assuming here that dangling end recovery does the sensible think of updating those supports are the graphs is modified. C. The use of Smith-Waterman in dangling end recovery does not seem totally optimal or even needed. . C.1 Recovering tails quite often this finish with the same sequence as the reference path because in fact they are supposed to end like that by construction (reads are trimmed by AR coordinates). For example, this can be cause because due to the k-mer size there is not enough based after variation for the paths to merge back. In this case you can simply merge the last vertices of the tail and the reference, faster and potentially more accurate. . C.2 Similarly dangling heads, at least part of the sequence of those dangling heads are clearly threadable back into the graph without the need of SW. For example look at the AA…AAAAAGA sequence in the picture below. . C.3 PairHMM runs in effect are performing SW kind of computations and so it is totally possible to use its partial result to find good alignment of dangling ends",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/264:1902,recover,recovery,1902,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/264,1,['recover'],['recovery']
Safety,https://github.com/broadinstitute/gatk/commit/9a4fb6d4e5fd2a226db820a8b4062c139b66e2ef; There is a standard math variant of log(x) for calculating the common log(1+x). It avoids numerical error for small x.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402623148:171,avoid,avoids,171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402623148,1,['avoid'],['avoids']
Safety,id 20 --likelihood-calculation-engine PairHMM --ba; se-quality-score-threshold 18 --dragstr-het-hom-ratio 2 --dont-use-dragstr-pair-hmm-scores false --pair-hmm-gap-continuation-penalty 10 --expected-mismatch-rate-for-read-disqualification 0; .02 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --disable-symmetric-hmm-normalizing false --disable-cap-base-qu; alities-to-map-quality false --enable-dynamic-read-disqualification-for-genotyping false --dynamic-read-disqualification-threshold 1.0 --native-pair-hmm-threads 4 --native-pair-hmm-use-dou; ble-precision false --flow-hmm-engine-min-indel-adjust 6 --flow-hmm-engine-flat-insertion-penatly 45 --flow-hmm-engine-flat-deletion-penatly 45 --pileup-detection false --pileup-detection-; enable-indel-pileup-calling false --num-artificial-haplotypes-to-add-per-allele 5 --artifical-haplotype-filtering-kmer-size 10 --pileup-detection-snp-alt-threshold 0.1 --pileup-detection-i; ndel-alt-threshold 0.5 --pileup-detection-absolute-alt-depth 0.0 --pileup-detection-snp-adjacent-to-assembled-indel-range 5 --pileup-detection-bad-read-tolerance 0.0 --pileup-detection-pro; per-pair-read-badness true --pileup-detection-edit-distance-read-badness-threshold 0.08 --pileup-detection-chimeric-read-badness true --pileup-detection-template-mean-badness-threshold 0.0; --pileup-detection-template-std-badness-threshold 0.0 --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --override-fragment-softclip-check false --min-base-quality-s; core 10 --smith-waterman JAVA --max-mnp-distance 0 --force-call-filtered-alleles false --reference-model-deletion-quality 30 --soft-clip-low-quality-ends false --allele-informative-reads-o; verlap-margin 2 --smith-waterman-dangling-end-match-value 25 --smith-waterman-dangling-end-mismatch-penalty -50 --smith-waterman-dangling-end-gap-open-penalty -110 --smith-waterman-danglin; g-end-gap-extend-penalty -6 --smith-waterman-haplo,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:6777,detect,detection-i,6777,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,1,['detect'],['detection-i']
Safety,"iedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. 8/02/23 23:06:24 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 18/02/23 23:06:24 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/02/23 23:06:24 INFO spark.SparkContext: Successfully stopped SparkContext; 23:06:24.240 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 23, 2018 11:06:24 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 73.92 minutes.; Runtime.totalMemory()=10458497024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 27 in stage 15.0 failed 4 times, most recent failure: Lost task 27.3 in stage 15.0 (TID 29483, scc-q15.scc.bu.edu, executor 13): org.broadinstitute.hellbender.exc eptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4458:4718,abort,aborted,4718,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458,1,['abort'],['aborted']
Safety,if doWork or onStartup in CommandLineProgram throw an exception then suppress any exception throw in onShutdown; fixes #528 . I still think every effort should be made to avoid an exception in onShutdown though.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/563:171,avoid,avoid,171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/563,1,['avoid'],['avoid']
Safety,il.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1457); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1445); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1444); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1444); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1668); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1627); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:33534,abort,abortStage,33534,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['abort'],['abortStage']
Safety,"ile file:///paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/A200314.HC.g.vcf.gz; 12:01:38.062 INFO FeatureManager - Using codec VCFCodec to read file file:///paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/A200315.HC.g.vcf.gz; 12:01:38.184 INFO FeatureManager - Using codec VCFCodec to read file file:///paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/PID20-006.HC.g.vcf.gz; 12:01:38.311 INFO FeatureManager - Using codec VCFCodec to read file file:///paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/PID20-007.HC.g.vcf.gz; 12:01:38.417 INFO FeatureManager - Using codec VCFCodec to read file file:///paedwy/disk1/yangyxt/wes/backup_gvcfs/all_wes_samples.g.vcf; 12:01:49.097 INFO CombineGVCFs - Done initializing engine; 12:01:49.113 INFO ProgressMeter - Starting traversal; 12:01:49.113 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 12:01:49.492 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chrM:63 the annotation MLEAC=[2, 0] was not a numerical value and was ignored; 12:01:49.505 INFO CombineGVCFs - Shutting down engine; [August 24, 2020 12:01:49 PM HKT] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=6277824512; java.lang.NullPointerException; at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.StrandBiasUtils.encode(StrandBiasUtils.java:52); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6766:5152,Detect,Detected,5152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6766,1,['Detect'],['Detected']
Safety,"ile will be written to gdbworkspace-gatk/callset.json; 17:00:54.418 INFO GenomicsDBImport - Complete VCF Header will be written to gdbworkspace-gatk/vcfheader.vcf; 17:00:54.418 INFO GenomicsDBImport - Importing to array - gdbworkspace-gatk/genomicsdb_array; 17:00:54.470 INFO ProgressMeter - Starting traversal; 17:00:54.470 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 17:00:54.488 INFO GenomicsDBImport - Importing batch 1 with 1 samples; terminate called after throwing an instance of 'FileBasedVidMapperException'; what(): FileBasedVidMapperException : type_index_iter != VidMapper::m_typename_string_to_type_index.end() && ""Unhandled field type""; [login1:01909] *** Process received signal ***; [login1:01909] Signal: Aborted (6); [login1:01909] Signal code: (-6); [login1:01909] [ 0] /lib64/libpthread.so.0[0x30bfa0f7e0]; [login1:01909] [ 1] /lib64/libc.so.6(gsignal+0x35)[0x30bf6325e5]; [login1:01909] [ 2] /lib64/libc.so.6(abort+0x175)[0x30bf633dc5]; [login1:01909] [ 3] /apps/GCC/6.3.0/lib64/libstdc++.so.6(_ZN9__gnu_cxx27__verbose_terminate_handlerEv+0x15d)[0x7f507f7018ed]; [login1:01909] [ 4] /apps/GCC/6.3.0/lib64/libstdc++.so.6(+0x8e8a6)[0x7f507f6ff8a6]; [login1:01909] [ 5] /apps/GCC/6.3.0/lib64/libstdc++.so.6(+0x8e8f1)[0x7f507f6ff8f1]; [login1:01909] [ 6] /apps/GCC/6.3.0/lib64/libstdc++.so.6(+0x8eb08)[0x7f507f6ffb08]; [login1:01909] [ 7] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x151df1)[0x7f507fb56df1]; [login1:01909] [ 8] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x1434d9)[0x7f507fb484d9]; [login1:01909] [ 9] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x1489f9)[0x7f507fb4d9f9]; [login1:01909] [10] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x12c78e)[0x7f507fb3178e]; [login1:01909] [11] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x12d7b4)[0x7f507fb327b4]; [login1:01909] [12] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x11f80d)[0x7f507fb2480d]; [login1:01909] [13] /apps/GENOMICSD",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4514:3933,abort,abort,3933,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4514,1,['abort'],['abort']
Safety,ileup - Defaults.CUSTOM_READER_FACTORY : ; 15:04:36.349 INFO Pileup - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 15:04:36.349 INFO Pileup - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 15:04:36.349 INFO Pileup - Defaults.REFERENCE_FASTA : null; 15:04:36.349 INFO Pileup - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 15:04:36.349 INFO Pileup - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:04:36.349 INFO Pileup - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 15:04:36.350 INFO Pileup - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:04:36.350 INFO Pileup - Defaults.USE_CRAM_REF_DOWNLOAD : false; 15:04:36.350 INFO Pileup - Deflater IntelDeflater; 15:04:36.350 INFO Pileup - Initializing engine; WARNING: BAM index file /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bai is older than BAM /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bam; 15:04:38.560 INFO IntervalArgumentCollection - Processing 999914 bp from intervals; 15:04:38.630 INFO Pileup - Done initializing engine; 15:04:38.635 INFO ProgressMeter - Starting traversal; 15:04:38.636 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; JProfiler> Protocol version 49; JProfiler> Using JVMTI; JProfiler> JVMTI version 1.1 detected.; JProfiler> 64-bit library; JProfiler> Listening on port: 31757.; JProfiler> Attach mode initialized; JProfiler> Instrumenting native methods.; JProfiler> Can retransform classes.; JProfiler> Can retransform any class.; JProfiler> Retransforming 8 base class files.; JProfiler> Base classes instrumented.; JProfiler> Native library initialized; JProfiler> Using dynamic instrumentation; JProfiler> Time measurement: elapsed time; JProfiler> CPU profiling enabled; JProfiler> Initializing configuration.; JProfiler> Retransforming 3697 class files.; JProfiler> Configuration updated. ```. ![oncobuntu_mk3](https://cloud.githubusercontent.com/assets/2152339/22307273/583f61a8-e310-11e6-87ef-e87eaba7cf93.png),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2356:3437,detect,detected,3437,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2356,1,['detect'],['detected']
Safety,"imVariants - Reference allele is too long (212) at position chr2_KI270894v1_alt:202602; skipping that record. Set --reference_window_stop >= 212 ; INFO 21:38:54,233 LeftAlignAndTrimVariants - Reference allele is too long (220) at position chr2_KI270894v1_alt:204859; skipping that record. Set --reference_window_stop >= 220 ; INFO 21:38:54,237 LeftAlignAndTrimVariants - Reference allele is too long (262) at position chr2_KI270894v1_alt:207863; skipping that record. Set --reference_window_stop >= 262 ; 0 variants were aligned; INFO 21:38:54,554 ProgressMeter - done 3.31246907E8 31.8 m 5.0 s 99.7% 31.8 m 5.0 s ; INFO 21:38:54,554 ProgressMeter - Total runtime 1905.29 secs, 31.75 min, 0.53 hours ; ------------------------------------------------------------------------------------------; Done. There were 4 WARN messages, the first 4 are repeated below.; WARN 17:39:57,688 IndexDictionaryUtils - Track variant doesn't have a sequence dictionary built in, skipping dictionary validation ; WARN 18:13:42,039 SimpleTimer - Clock drift of -1,503,348,737,016,211,299 - -1,503,346,772,578,127,937 = 1,964,438,083,362 nanoseconds detected, vs. max allowable drift of 5,000,000,000. Assuming checkpoint/restart event. ; WARN 20:14:18,043 SimpleTimer - Clock drift of -1,503,355,916,564,964,097 - -1,503,348,737,015,111,124 = 7,179,549,852,973 nanoseconds detected, vs. max allowable drift of 5,000,000,000. Assuming checkpoint/restart event. ; WARN 21:10:35,064 SimpleTimer - Clock drift of -1,503,359,203,412,549,926 - -1,503,355,916,564,817,209 = 3,286,847,732,717 nanoseconds detected, vs. max allowable drift of 5,000,000,000. Assuming checkpoint/restart event. ; ------------------------------------------------------------------------------------------; WMCF9-CB5:Mutect2 shlee$ ; ```. ### Notice the following line from above. > 0 variants were aligned. Also, it would be great if the tool, which appears to keep track of the lengths of reference alleles that are too long, could give me the **ma",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487:7616,detect,detected,7616,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487,1,['detect'],['detected']
Safety,"implementation is different from the one in `ReadWindowWalker`: first, the overlap between windows is only in one direction; second, `SlidingWindowWalker` is more like a reference/interval walker, from the beginning of the reference (or interval) till the end, it walks in overlapping windows. One example is the following (window-size 10, window-step 5, the - represent the window):. ```; Reference: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; Windows1: _ _ _ _ _ _ _ _ _ _; Windows2: _ _ _ _ _ _ _ _ _ _; Windows3: _ _ _ _ _ _ _ _ _ _; Windows4: _ _ _ _ _ _ _ _ _ _; Windows5: _ _ _ _ _ _ _ _ _ _; ```. Of course, after having a look to `ReadWindowWalker` I think that several things could be improved in my implementation for a general `SlidingWindowWalker`:; - Apply function similar to the `ReadWindowWalker`, with `ReadWindow` being empty if reads are not provided.; - Three window options: `windowSize` (the actual size of the window), `windowStep` (how much advance for the following window) and `windowPadding` (how much extend the window in both directions). Using this abstraction, `ReadWindowWalker` could be implemented setting `windowSize=windowStep`, and the problem that I need to solve could be implemented setting `windowPadding=0`. The simplest way to acomplish this is to use the current implementation of `ReadWindowWalker` to develop a `SlidingWindowWalker` adding three abstract methods for the three parameters (`getWindowSize()`, `getWindowStep()` and `getWindowPadding()`, and implement `ReadWindowWalker` as a extension of this interface setting `getWindowStep()` to return `getWindowSize()` and `requiresReads()` to true. I can do this once the PR #1567 is accepted and generate the two interfaces (to be sure that the integration with the HC engine is working as expected with the changes), or just implement the `SlidingWindowWalker` and you can include it in the HC PR, or update afterwards to avoid redundancy in the code. What do you think, @droazen?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775:2524,avoid,avoid,2524,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775,2,"['avoid', 'redund']","['avoid', 'redundancy']"
Safety,"in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar CalibrateDragstrModel -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.Dragstr.model -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:55:30.890 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:55:31 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:55:31.182 INFO CalibrateDragstrModel - ------------------------------------------------------------; 13:55:31.183 INFO CalibrateDragstrModel - The Genome Analysis Toolkit (GATK) v4.2.0.0; 13:55:31.183 INFO CalibrateDragstrModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:55:31.183 INFO CalibrateDragstrModel - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 13:55:31.184 INFO CalibrateDragstrModel - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:55:31.184 INFO CalibrateDragstrModel - Start Date/Time: April 4, 2021 1:55:30 PM EDT; 13:55:31.184 INFO CalibrateDragstrModel - ------------------------------------------------------------; 13:55:31.184 INFO CalibrateDragstrModel - ------------------------------------------------------------; 13:55:31.185 INFO CalibrateDragstrModel - HTSJDK Version: 2.24.0; 13:55:31.185 INFO CalibrateDrag",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182:12268,detect,detect,12268,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182,1,['detect'],['detect']
Safety,"in the count files, and perhaps fail if one is provided for any of the tools (I don’t recall exactly how VCF indexing is triggered by providing one, as seems to be indicated by the tutorial, but hopefully we can disallow external dictionaries while still taking advantage of the relevant engine features for VCF writing). EDIT: Went digging in Slack to try to remind myself of the context of these changes, and found the following PR comment from 1/7 (although it seems to have mysteriously disappeared from GitHub):. > Just so I understand, are we allowing overriding of the sequence dictionary in the shards (and skipping the consistency check) by allowing the parameter --sequence-dictionary to be specified? If so, we might want to document. Otherwise, I'd be inclined to enforce using the sequence dictionary in the shards (and ensuring the consistency check across shards is performed) by changing the null check in getBestAvailableSequenceDictionary to a check that the dictionary has not been set via the command line. EDIT^2: I think I misremembered the details of how #6330 hooked up the sequence dictionary and how getBestAvailableSequenceDictionary in GATKTool works (which probably explains why that comment was deleted...). Now that I actually go back and look, the `--sequence-dictionary` is not hooked up at all, so there is no change to revert in point 4!. Note that after all of this, it will *still* be possible to get into trouble at the gCNV step if you make funky shards (e.g., you could have shard 1 contain intervals from chr1 and chr3, and shard 2 contain intervals from chr2). I don't think it is possible to check for this case early, but you would still fail at PostprocessGermlineCNVCalls as above. Of course, all of these possibilities can be avoided by simply using the WDL, but it will be good to harden checks for those still working at the command line. @ldgauthier @droazen @mwalker174 what do you think? Happy to review later, but OK if I pass this off to you all?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-719576249:3960,avoid,avoided,3960,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-719576249,2,['avoid'],['avoided']
Safety,"inaBasecallingMetrics (Picard) Collects Illumina Basecalling metrics for a sequencing run. ...skipped for brevity... VcfFormatConverter (Picard) Converts VCF to BCF or BCF to VCF.; VcfToIntervalList (Picard) Converts a VCF or BCF file to a Picard Interval List. --------------------------------------------------------------------------------------. Exception in thread ""main"" org.broadinstitute.hellbender.exceptions.UserException: 'FixVcfHead' is not a valid command.; Did you mean this?; FixVcfHeader; 	at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:341); 	at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:172); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:192); 	at org.broadinstitute.hellbender.Main.main(Main.java:275); ```. I expect something without the stack trace and the scary ""Exception"" message. For example:. ```; USAGE: <program name> [-h]. Available Programs:; --------------------------------------------------------------------------------------; Base Calling: Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters; CheckIlluminaDirectory (Picard) Asserts the validity for specified Illumina basecalling data.; CollectIlluminaBasecallingMetrics (Picard) Collects Illumina Basecalling metrics for a sequencing run. ...skipped for brevity... VcfFormatConverter (Picard) Converts VCF to BCF or BCF to VCF.; VcfToIntervalList (Picard) Converts a VCF or BCF file to a Picard Interval List. --------------------------------------------------------------------------------------. Did you mean this?; FixVcfHeader; ```. The same happens with unknown commands. The code that should be changed for that is the following, where the `setupConfigAndExtractProgram` call should be also inside the try block:. https://github.com/broadinstitute/gatk/blob/8ac2f102b303f343c4787ad4e3359335641c5121/src/main/java/org/broadinstitute/hellbender/Main.java#L190-L212",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4256:1605,detect,detect,1605,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4256,1,['detect'],['detect']
Safety,"ing GATK jar /public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR4_gvcf_database -G StandardAnnotation -O fat_ALL_MATERIALS_chr4.g.vcf.gz; 11:58:13.194 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 11:58:14.522 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 20, 2022 11:58:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:58:19.059 INFO GenotypeGVCFs - ------------------------------------------------------------; 11:58:19.060 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.9.0; 11:58:19.060 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:58:19.060 INFO GenotypeGVCFs - Executing as gaoshibin@fat1 on Linux v3.10.0-693.el7.x86_64 amd64; 11:58:19.060 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_211-b12; 11:58:19.060 INFO GenotypeGVCFs - Start Date/Time: 2022年5月20日 上午11时58分13秒; 11:58:19.060 INFO GenotypeGVCFs - ------------------------------------------------------------; 11:58:19.060 INFO GenotypeGVCFs - ------------------------------------------------------------; 11:58:19.061 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 11:58:19.061 INFO GenotypeGVCFs - Picard Version: 2.23.3; 11:58:19.061 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7866:2475,detect,detect,2475,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866,1,['detect'],['detect']
Safety,ingRankSumTest --annotation DepthPerSampleHC --output /mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2017-10-03-104521.457/root/variantcall/8/variantcall_batch_region/16/bcbiotx/tmp7exzBH/NA24631-chr15_68578892_84670250-block.vcf.gz --emitRefConfidence GVCF -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 60 -GQB 80; ```; and the full traceback is:; ```; 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3659:3017,abort,abortStage,3017,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3659,1,['abort'],['abortStage']
Safety,"ion methods. Agree. **EDIT**: done in d4d42444986aae848e98d19dfb8a4d3fd8031775. > I'm not sure we should filter out very small and very large variants, especially very large ones. We should get graded on making those calls. Very small ones I can see excluding because by definition they might not be in the truth set, but I don't think that applies to large ones. I agree it is not optimal, particularly for filtering out the huge variants. ; The huge variants come from 2 sources: `<DEL>`, `<INV>`.; For huge deletions, the 50% (or a custom value) reciprocal overlap should classify almost all of them as FP; actually the PacBio call sets (CHM1, CHM13, and the mixture) do not contain any deletion records that are over 30K in size.; For huge inversions though, I am not sure exactly what to do with them. The scripts currently avoid overlap analysis on the inversions actually because IMO we are overly confident in the `<INV>` variants, and the new variant interpretation methods will not emit <INV> records at all&mdash;they become BND's with appripriate annotations and are submitted to a yet-to-be-implemented unit for further interpretation if it is a dispersed duplication or inversion. > What's the reason for storing compiled Rdata objects in with the scripts? I don't necessarily see anything that needs that, and it will make things very hard to maintain. Historical reason. They were used for storing R functions. Will remove them. **EDIT**: done in 546a36465f7860f8c85e28cf40ca8f3851ba9d4c. > `masterVsFeature.sh` appears to set its working directory in the parent directory of where it is run from. If these scripts are in the gatk repo that will end up being in the scripts/sv/evaluation dir and will look like an untracked directory by git there. Its location should be a parameter just like the other working directories. Will do as suggested. **EDIT**: done in 23da9d41667bc21a978134d92f49b65d9af55b35. > Running masterVsFeature should be optional; sometimes we'll just want to run",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4406#issuecomment-368655983:1528,avoid,avoid,1528,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4406#issuecomment-368655983,1,['avoid'],['avoid']
Safety,"ion. M=10^6, k/K=10^3 ; chr1 10000-99 # from 10000 to 10099... ; # perhaps is best not to accept this as it might silence user input errors.; # but what about instead?; chr1 100[00-99]; chr1 10000+100 # 100 bps starting at 10000 so 10000-10099; chr1 4k # only poistion 4000.; chr20 1M+32K # from position 1 million extending to the following 32Kbps.; chr20 1M1+32K # from position 1 million and 1 instead. (avoiding all those 0s). chr1 *:200 # consecutive 200bp intervals for the entire chromosome; chr1 *:200(100) # 200bp intervals with 100 gaps; chr1 *:200/20 # 200bp intervals with an overlap of 20bp.; chr1 *:20/200 # 200bp starting every 20 positions (so 180bp overlap); chr1 *:200~20 # 200bp intervals truncating down to 20bp if necessary. ; chr1 { # we can combine interval specs in blocks if they apply to the same contig(s).; 1M-2M:150(20) # from 1 to 2Mbp 150 intervals with 20bp gap; 20M-25M # a big interval from 20 to 25M.; 40012451-40023451 # another standalone interval ; } . ```; ## Interval exclusion; We could specify the exclused interval in the same file:; ```; chr20 *:200 exclude *10000 11000000+10000 32510000* # 200bp intervals except telomere and centromere regions. chr20 { # another way using blocks.; *:200; } excl {; *10000 ; 11000000+10000 ; 32510000*; }. ```. ## Arbitrary interval list. Some other tools cannot specify intervals if these are very specific... for example in exome analysis targets do not fall at regular intervals and are tailor to the capture used. In this case explicit listing is not avoidable. However there are ways to gain. For one thing the language above allows to pack intervals on the same contig on the block so saving to specify the name at each line. (e.g. ```chr1 { 100-200 3124-5681 ... }````. . However real gains would come from ""publishing"" those list under some unique identifier or stable URL that reduce the need of marshaling the whole interval-file file every time. These lest could be retrieved and cached locally by the engine.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5702:2821,avoid,avoidable,2821,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5702,1,['avoid'],['avoidable']
Safety,"ire low quality ends to be included in the models for genotyping, I have added the option to softclipLowQualityEnds (as opposed to their current treatment which involves hardclipping). This has resulted in a lot of code revolving around handling soft reads and making sure that the correct bases get used in the correct places, which often manifests as simply re-clipping the soft-clipped bases where necessary. This might seem expensive but low quality ends are fairly rare and consequently this has a negligible effect on runtime. ; (NOTE: this might cause unintended consequences for annotations, which have not been extensively tested thus far); - The `DRAGENGenotypeLikelihoodCalculator` object is actually an instantiation of the regular `GenotypeLikelihoodCalculator` object that is called normally for the standard variant model calculation and then has its computed tables/values reused for the subsequent calculations. This means there is a risk if not careful of using the table values for the wrong reads/sties if we are not strict about the state of the cache.; - Currently in order to lower the mapping quality threshold for HaplotypeCaller two separate arguments must be called. This is because the mapping-quality threshold is checked twice, once for the read filter plugin `getToolDefaultArgumentCollections()` which gets instantiated before the HaplotypeCaller arguments are populated, and again before assembly. While the functionality to be stricter about mapping quality for assembly compared to active region discovery might be important it is unclear if this matters and perhaps the latter check can be done away with? ; - I have added a genotype debugging stream that closely matches the debug output stream from DRAGEN (which itself was a reflection of the GATK3 debug out stream). This involved a lot of threading output writers through the codebase and perhaps this is better handled by the ""--debug"" argument like it used to? Thoughts? . Notes: ; - It should be noted that ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6634:2785,risk,risk,2785,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6634,1,['risk'],['risk']
Safety,is this the answer? https://github.com/broadinstitute/gatk/issues/6370; the doc should then state it to avoid confusion.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6547#issuecomment-612841129:104,avoid,avoid,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6547#issuecomment-612841129,1,['avoid'],['avoid']
Safety,"is time there was: ""cannot load book-keeping: Reading MBR failed"" (output below) ; 3. available memory ~89Gb; 4. I am running -Xmx16g java option. Newest output:; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16g -jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar GenotypeGVCFs --genomicsdb-shared-posixfs-optimizations --reference /data1/EquCab/_ECA30/Equus_caballus.EquCab3.0.dna_sm.toplevel.fa/ -V gendb://ECA3_GenomicsDB_260/3 -O ECA3_GenomicsDB_260.3.g.vcf.gz; 16:26:34.912 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2021 4:26:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:26:35.417 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:26:35.418 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 16:26:35.418 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:26:35.420 INFO GenotypeGVCFs - Executing as ccastane9@andersserver-01.cvm.tamu.edu on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 16:26:35.421 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_275-b01; 16:26:35.421 INFO GenotypeGVCFs - Start Date/Time: January 6, 2021 4:26:34 PM CST; 16:26:35.421 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:26:35.421 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:26:35.422 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 16:26:35.423 INFO GenotypeGVCFs - Picard Version: 2.22.8; 16:26:35.423 INFO GenotypeGVCFs - HTSJD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:1249,detect,detect,1249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,1,['detect'],['detect']
Safety,ite(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:16755,abort,abortStage,16755,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['abort'],['abortStage']
Safety,"ithub.com/broadinstitute/gsa-unstable/issues/855#issuecomment-260714842). Are you planning/working on this in GATK3 or GATK4? Would be good to know where the issue should live. . ---. @vdauwera commented on [Wed Feb 08 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-278478318). @SHuang-Broad ping... ---. @SHuang-Broad commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-280102484). @vdauwera sorry this went off my attention for a while. I did attempt to port a similar change a while back, but discovered that it was not so simple: the fix worked in HC code by removing alt alleles looking at the supporting haplotype scores. Such scores are not available in `GenotypeGVCFs` so either we would have to, like Valentin suggested, make sure the tools handle input without PLs, which is a direction that I looked into and found that the pay/cost is not good (if I recall correctly, most of the places that handles the input does not require valid PL but there are several that's difficult to handle). Then I began wondering how the new QUAL calculating method David Benjamin has put in will make such problems obsolete. So I would say if I find time beyond finishing my SV duty, I would chase down if the new QUAL method indeed will resolve all these, and that will definitely happen in GATK 4. ---. @vdauwera commented on [Mon Feb 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-281073466). Ah, interesting, thanks Steve. Do you have any sense of when you might be able to look further into this? This is not to pressure you, just to estimate the roadmap/timeline. An order of magnitude (weeks, months, more) would be fine. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-287838044). I'm going to move this issue to GATK; feel free to close it there if it is redundant with existing issues describing the same problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2955:11214,redund,redundant,11214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955,1,['redund'],['redundant']
Safety,"java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations. 12:31:14.647 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/exacloud/gscratch/prime-seq/bin/GenomeAnalysisTK4.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 15, 2022 12:31:14 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:31:14.783 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:31:14.783 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.5.0; 12:31:14.783 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:31:14.784 INFO GenotypeGVCFs - Executing as labkey_submit@exanode-6-25 on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 12:31:14.784 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_60-b27; 12:31:14.784 INFO GenotypeGVCFs - Start Date/Time: February 15, 2022 12:31:14 PM PST; 12:31:14.784 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:31:14.784 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:31:14.784 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 12:31:14.784 INFO GenotypeGVCFs - Picard Version: 2.25.4; 12:31:14.784 INFO GenotypeGVCFs - Built ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674:1700,detect,detect,1700,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674,1,['detect'],['detect']
Safety,java:119); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:374); at org.broadinstitute.hellbender.tools.spark.pipelines.SortSamSpark.runTool(SortSamSpark.java:114); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:546); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.OutOfMemoryError: Required array length 2147483639 + 798 is too large; java.lang.OutOfMemoryError: Required array length 2147483639 + 798 is too large; at java.base/jdk.internal.util.ArraysSupport.hugeLength(ArraysSupport.java:649); at java.base/jdk.internal.util.ArraysSupport.newLength(ArraysSupport.java:642); at java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:100); at java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:130); at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41); at java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1862); at java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:714); at org.apache.spark.util.Utils$$anon$2.write(Utils.scala:160); at com.esotericsoftware.kryo.io.Output.flush(Output.java:185); at com.esotericsoftware.kryo.io.Output.close(Output.java:196); at org.apache.sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:27346,abort,aborted,27346,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['abort'],['aborted']
Safety,java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1457); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1445); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1444); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1444); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1668); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1627); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1616); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:33632,abort,abortStage,33632,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['abort'],['abortStage']
Safety,"jects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq436f163.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/neuro337f175.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq530f196.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq608f226.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq621f231.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq639f237.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/neuro442f249.Roche-M.mutect2.vcf -V /nfs/projects/CNV\_WGS/Mutetc2-PON-OUT/Roche-M/diagseq944f346.Roche-M.mutect2.vcf ; ; 11:20:35.249 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/ec3408/GATK-4.2.0.0/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Jul 13, 2021 11:20:35 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:20:35.481 INFO GenomicsDBImport - ------------------------------------------------------------ ; ; 11:20:35.482 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 11:20:35.482 INFO GenomicsDBImport - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:20:35.482 INFO GenomicsDBImport - Executing as [ec3408@dev2.igm.cumc.columbia.edu](mailto:ec3408@dev2.igm.cumc.columbia.edu) on Linux v3.10.0-957.27.2.el7.x86\_64 amd64 ; ; 11:20:35.482 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_222-b10 ; ; 11:20:35.482 INFO GenomicsDBImport - Start Date/Time: July 13, 2021 11:20:35 AM EDT ; ; 11:20:35.482 INFO GenomicsDBImport - ------------------------------------------------------------ ; ; 11:20:35.482 INFO GenomicsDBImport - ------------------------------------------------------------ ; ; 11:20:35.483 INFO G",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7362:17415,detect,detect,17415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7362,1,['detect'],['detect']
Safety,"k CalculateContamination -I gewb.tumor.pileups.table -matched gewb.normal.pileups.table -O gewb.contamination.table`. the following information is displayed:. Using GATK jar /cluster/home/jialu/miniconda3/envs/wes2/share/gatk4-4.2.3.0-0/gatk-package-4.2.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /cluster/home/jialu/miniconda3/envs/wes2/share/gatk4-4.2.3.0-0/gatk-package-4.2.3.0-local.jar CalculateContamination -I gewb.tumor.pileups.table -matched gewb.normal.pileups.table -O gewb.contamination.table; 19:10:31.163 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cluster/home/jialu/miniconda3/envs/wes2/share/gatk4-4.2.3.0-0/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 06, 2022 7:10:31 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:10:31.437 INFO CalculateContamination - ------------------------------------------------------------; 19:10:31.437 INFO CalculateContamination - The Genome Analysis Toolkit (GATK) v4.2.3.0; 19:10:31.437 INFO CalculateContamination - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:10:31.438 INFO CalculateContamination - Executing as haojie@node1 on Linux v3.10.0-957.el7.x86_64 amd64; 19:10:31.438 INFO CalculateContamination - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_302-b08; 19:10:31.438 INFO CalculateContamination - Start Date/Time: March 6, 2022 7:10:31 PM CST; 19:10:31.438 INFO CalculateContamination - ------------------------------------------------------------; 19:10:31.438 INFO CalculateContamination - ------------------------------------------------------------; 19:10:31.439 INFO CalculateContamination - HTSJDK Version: 2.24.1; 19:10:31.439 INFO CalculateContamination - Picard ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7707:1135,detect,detect,1135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7707,1,['detect'],['detect']
Safety,"k-4.1.4.1/gatk-package-4.1.4.1-local.jar GenomicsDBImport --genomicsdb-workspace-path data/interim/combined_database_bpres/0004 --batch-size 50 --reader-threads 6 --sample-name-map data/processed/sample_map --intervals data/processed/scattered_intervals/0004-scattered.intervals --tmp-dir /scratch/sdturner/genomicsdbimport/0004; ```. #### Expected behavior; My understanding is that it may be more efficient to use a small buffer and write the final database in full. . #### Actual behavior; Again my (limited) understanding is that the tool is writing output multiple times and throwing out all but the last write. Here is an example of a log for a 2.6 Mb region and 295 samples: ; ; ```; 07:24:39.198 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/apps/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 28, 2020 7:24:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 07:24:39.616 INFO GenomicsDBImport - ------------------------------------------------------------; 07:24:39.617 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.4.1; 07:24:39.617 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 07:24:39.617 INFO GenomicsDBImport - Executing as sdturner@c6-74 on Linux v4.15.0-65-generic amd64; 07:24:39.617 INFO GenomicsDBImport - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_20-b26; 07:24:39.617 INFO GenomicsDBImport - Start Date/Time: February 28, 2020 7:24:39 AM PST; 07:24:39.617 INFO GenomicsDBImport - ------------------------------------------------------------; 07:24:39.617 INFO GenomicsDBImport - ------------------------------------------------------------; 07:24:39.618 INFO GenomicsDBImport - HTSJDK Version: 2.21.0; 07:24:39.618 INFO GenomicsDBImport - Picard Version: 2.21.2; 07:24:39.618 INFO GenomicsDBImpor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6487:1652,detect,detect,1652,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6487,1,['detect'],['detect']
Safety,"k.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 1, start 93925364, span 266689, expected MD5 54babf05a23e9e88a8738dfbb20a1683) [duplicate 2]; 2019-01-09 13:35:56 INFO TaskSetManager:54 - Starting task 4.3 in stage 0.0 (TID 12, scc-q20.scc.bu.edu, executor 2, partition 4, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:56 INFO TaskSetManager:54 - Lost task 1.3 in stage 0.0 (TID 11) on scc-q20.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 3]; 2019-01-09 13:35:56 ERROR TaskSetManager:70 - Task 1 in stage 0.0 failed 4 times; aborting job; 2019-01-09 13:35:56 INFO YarnScheduler:54 - Cancelling stage 0; 2019-01-09 13:35:56 INFO YarnScheduler:54 - Stage 0 was cancelled; 2019-01-09 13:35:56 INFO DAGScheduler:54 - ResultStage 0 (count at CountReadsSpark.java:80) failed in 12.543 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 11, scc-q20.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:33215,abort,aborted,33215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['abort'],['aborted']
Safety,"ke$class.count(JavaRDDLike.scala:455); 	at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); 	at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 18/05/01 14:30:35 WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:09 WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:10 INFO ShutdownHookManager: Shutdown hook called; 18/05/01 14:31:15 INFO ShutdownHookManager: Deleting directory /tmp/abd30/spark-3f28d2e3-59d7-40f9-bba3-42d61eff6c6a; 18/05/01 14:31:20 ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; Using GATK jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4725:4982,timeout,timeout,4982,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725,1,['timeout'],['timeout']
Safety,"kl_compression.so; [Thu Aug 10 12:49:41 UTC 2023] CollectSequencingArtifactMetrics --FILE_EXTENSION .txt --INPUT NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup.bam --OUTPUT NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup --REFERENCE_SEQUENCE GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz --MINIMUM_QUALITY_SCORE 20 --MINIMUM_MAPPING_QUALITY 30 --MINIMUM_INSERT_SIZE 60 --MAXIMUM_INSERT_SIZE 600 --INCLUDE_UNPAIRED false --INCLUDE_DUPLICATES false --INCLUDE_NON_PF_READS false --TANDEM_READS false --USE_OQ true --CONTEXT_SIZE 1 --ASSUME_SORTED true --STOP_AFTER 0 --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Aug 10, 2023 12:49:43 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Thu Aug 10 12:49:43 UTC 2023] Executing as root@34684eaa046e on Linux 4.15.0-208-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.3.0; [Thu Aug 10 12:49:43 UTC 2023] picard.analysis.artifacts.CollectSequencingArtifactMetrics done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2076049408; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp; htsjdk.samtools.SAMException: Cannot read non-existent file: file:///gatk/data/Continuum/WES/vcf/NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup.bam; at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:483); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:470); at picard.analysis.SinglePassSamProgram.makeItSo(SinglePassSamProgram.java:95); at picard.analysis.SinglePassSamProgram.doWork(SinglePassSamProgram.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8462:1591,detect,detect,1591,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8462,1,['detect'],['detect']
Safety,kpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6070:3951,abort,abortStage,3951,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070,1,['abort'],['abortStage']
Safety,"l, but there are situations where the model can get stuck in incorrect, degenerate solutions. Going to try adding some MH steps that will swap populations to see if these can help get the model unstuck.; - Need to add outlier absorption to the model, which appears to be critical for inference of subclonal populations from real data (i.e., ACNV output), which may have spurious segments, oversegmentation, etc. Simple clonal models appear to work reasonably well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model, i.e., purity is recovered within credible intervals (question: what are the error bars on the purities of the samples?). Subclonal performance is a little less clear due to 1) no real ground truth, 2) events in the normal, and 3) lack of outlier absorption.; - Can we get a hold of some cleaner purity series?; - [ ] Document algorithm in technical whitepaper. ---. @samuelklee commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/750#issuecomment-265798051). The first release of this tool will most likely include the following:. - Some refactoring to MCMC package and addition of an EnsembleSampler, which implements affine-invariant ensemble sampling from Goodman & Weare 2010 (this is the same method used by the emcee python package). This method is critical for sampling our highly multimodal posterior well. - Output of 1) all population fraction / ploidy MCMC samples, and 2) average variant profile and 3) posterior summaries at the posterior mode (determined by naive binning of samples). - No plotting. Early next qu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2909:2019,recover,recovered,2019,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909,1,['recover'],['recovered']
Safety,la:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513:7336,abort,abortStage,7336,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513,1,['abort'],['abortStage']
Safety,la:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690:7347,abort,abortStage,7347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690,1,['abort'],['abortStage']
Safety,la:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:8878,abort,abortStage,8878,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['abort'],['abortStage']
Safety,"laptop, in the 4.0.11.0 Docker. How can I deal with this WARN?. ```; (gatk) root@3231a24c7afb:/gatk/my_data/3-somatic# gatk LearnReadOrientationModel -alt-table 13_tumor-alt.tsv -ref-hist 13_tumor-ref.metrics -alt-hist 13_tumor-alt-depth1.metrics -O tumor-artifact-prior-table.tsv ; Using GATK jar /gatk/gatk-package-4.0.11.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.0.11.0-local.jar LearnReadOrientationModel -alt-table 13_tumor-alt.tsv -ref-hist 13_tumor-ref.metrics -alt-hist 13_tumor-alt-depth1.metrics -O tumor-artifact-prior-table.tsv; 16:20:57.885 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.0.11.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 26, 2018 4:20:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused (Connection refused); 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); 	at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); 	at sun.net.www.http.HttpClient.New(HttpClient.java:339); 	at sun.net.www.http.HttpClient.New(HttpClient.java:357); 	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnectio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447:1351,detect,detect,1351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447,1,['detect'],['detect']
Safety,leMapTask.scala:79); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3679:4837,abort,abortStage,4837,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3679,1,['abort'],['abortStage']
Safety,lect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018:13929,abort,abortStage,13929,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018,1,['abort'],['abortStage']
Safety,ler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.immutable.List.foreach(List.scala:431) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventPr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:13523,abort,abortStage,13523,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['abort'],['abortStage']
Safety,lets avoid those 5-10 seconds of waiting for cran on every build,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/287:5,avoid,avoid,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/287,1,['avoid'],['avoid']
Safety,"li Chromosome we can see that there are 873 alignments overlapping that position:; `samtools view gatk_bams/Escherichia_coli_complete_genome.fasta.SRR12324251_1.fastq.bam.bam Escherichia_coli_chromosome:97201-97201 | less -S`. Applying the above filters to these alignments reports 871 alignments. However, GATK HaplotypeCaller reports only 743 valid alignments (before variant calling). Below is some code where I added debug messages to retrieve the read names that HaplotypeCaller determines to be valid:. *~line 476 of `HaplotypeCallerEngine.java`*; ```; public ActivityProfileState isActive( final AlignmentContext context, final ReferenceContext ref, final FeatureContext features ) {; if ( forceCallingAllelesPresent && features.getValues(hcArgs.alleles, ref).stream().anyMatch(vc -> hcArgs.forceCallFiltered || vc.isNotFiltered())) {; return new ActivityProfileState(ref.getInterval(), 1.0);; }. if( context == null || context.getBasePileup().isEmpty() ) {; // if we don't have any data, just abort early; return new ActivityProfileState(ref.getInterval(), 0.0);; }. final boolean debug = (context.getPosition() - 1) % 100000 == 0 || (context.getPosition() >= 97200 && context.getPosition() <= 97350) || (context.getPosition() >= 641000 && context.getPosition() <= 650000);; if (debug) {; System.out.println(""Position "" + context.getPosition() + "" Reads at position "" + context.size());; if (context.getPosition() == 97201 && context.getContig().equals(""Escherichia_coli_chromosome"")) {; System.out.println(""##READS"");; context.getBasePileup().getReads().forEach(read -> System.out.println(read.getName()));; System.out.println(""##END READS"");; }; }; ...; ...; }; ```. So HaplotypeCaller is filtering more reads which is fine, but the problem appears when we look at which alignments are actually being filtered. Below is a table with four example alignments at the previously described position on the E. coli genome. The main output from samtools view is included as well as a column describ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7873:3187,abort,abort,3187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7873,1,['abort'],['abort']
Safety,"like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and does not support VCFv4.1, for input source: /gss1/home/hjb20181119/panyongpeng/NN1138-2/RIL_genotype/mapping/w-1.raw.g.vcf; ##### ERROR ------------------------------------------------------------------------------------------. I checked my GVCF file and the header is :. ##fileformat=VCFv4.1; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FORMAT=<ID=AD,Number=.,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias."">. Could you tell me how to fix it and why the VCF4.1 format generated by HaplotypeCaller didn't work on the same version of GenotypeGVCFs . ; Thank you very much.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7315:3550,detect,detect,3550,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315,1,['detect'],['detect']
Safety,"like you can get around the compiler lock issues by pointing each invocation of GermlineCNVCaller to a different compilation directory. For example, invoke `gatk` by; > ; > `THEANORC=PATH/TO/THEANORC_# gatk GermlineCNVCaller ...`; > ; > This uses the `THEANORC` environment variable to set the `.theanorc` configuration file to `PATH/TO/THEANORC_#` for this instance of GATK (where you should fill in `#` appropriately). Each `PATH/TO/THEANORC_#` should be a file containing the following:; > ; > ```; > [global]; > base_compiledir = PATH/TO/COMPILEDIR_#; > ```; > ; > Where again, `#` is filled in appropriately. The goal is to point each GermlineCNVCaller instance to a different compilation directory. @xysj1989 can you let me know if this works for you?; > ; > This is a bit of a hack. We could probably avoid this by changing the GATK code to use a specified or temporary directory for the theano directory without too much effort.; > ; > However, there is an upside to using a non-temporary directory to avoid recompilation of the model upon subsequent runs. In this case, we'd just want to let the user be able to specify the theano directory (rather than dump things in `~/.theano` unexpectedly). We should think about whether this should be opt-in, i.e., should we preserve the original behavior of using `~/.theano` by default?; > ; > @mwalker174 opinions? @droazen or engine team, thoughts on what the policy should be for python/R scripts doing this sort of thing? Is it generally true that the GATK leaves no trace, other than producing the expected output?. Dear samuelklee,. Thank you very much for you reply. I also found this problem last night. It seems that the problem is originally from Theano and Pymc3, rather than GATK 4.0. Some similar problems have been reported just like (1) https://github.com/pymc-devs/pymc3/issues/1463 (2) https://stackoverflow.com/questions/52270853/how-to-get-rid-of-theano-gof-compilelock and (3) https://groups.google.com/forum/#!topic/theano-users",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548557073:1022,avoid,avoid,1022,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548557073,1,['avoid'],['avoid']
Safety,llection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:49929,abort,abortStage,49929,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['abort'],['abortStage']
Safety,"llowing location: '/Users/louisb/Workspace/gatk/build/classes/java/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/resources/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/tmp/sparkJar/MANIFEST.MF'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; ```. ```; Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.gradle.org/7.3.2/userguide/command_line_interface.html#sec:command_line_warnings. Execution optimizations have been disabled for 4 invalid unit(s) of work during this build to ensure correctness.; Please consult deprecation warnings for more details.; ```; The warnings show up in at least these task",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7625:1252,detect,detected,1252,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7625,1,['detect'],['detected']
Safety,"loads/Reference.fasta' -V '/home/india/Downloads/Galaxy57-[Merged_file.vcf].vcf' -F CHROM -F POS -F REF -F ALT -GF AD -GF DP -GF GQ -GF PL -O bothbulks_new.table. Using GATK jar /home/india/Downloads/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/india/Downloads/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar VariantsToTable -R /home/india/Downloads/Reference.fasta -V /home/india/Downloads/Galaxy57-[Merged_file.vcf].vcf -F CHROM -F POS -F REF -F ALT -GF AD -GF DP -GF GQ -GF PL -O bothbulks_new.table; 16:46:03.294 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/india/Downloads/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 16, 2020 4:46:04 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:46:04.315 INFO VariantsToTable - ------------------------------------------------------------; 16:46:04.316 INFO VariantsToTable - The Genome Analysis Toolkit (GATK) v4.1.9.0; 16:46:04.316 INFO VariantsToTable - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:46:04.317 INFO VariantsToTable - Executing as india@india-HP-ProBook-445-G1 on Linux v5.4.0-26-generic amd64; 16:46:04.317 INFO VariantsToTable - Java runtime: OpenJDK 64-Bit Server VM v11.0.8+10-post-Ubuntu-0ubuntu120.04; 16:46:04.317 INFO VariantsToTable - Start Date/Time: 16 October 2020 at 4:46:02 PM IST; 16:46:04.318 INFO VariantsToTable - ------------------------------------------------------------; 16:46:04.318 INFO VariantsToTable - ------------------------------------------------------------; 16:46:04.320 INFO VariantsToTable - HTSJDK Version: 2.23.0; 16:46:04.320 INFO VariantsToTable - Picard Version: 2.23.3; 16:46:04.320 I",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6897:1290,detect,detect,1290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6897,1,['detect'],['detect']
Safety,"lockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4725:2431,abort,abortStage,2431,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725,1,['abort'],['abortStage']
Safety,ls; 12:06:11.087 DEBUG ReadThreadingGraph - Recovered 7 of 36 dangling heads; 12:06:11.465 DEBUG Mutect2Engine - Active Region chrM:12730-13020; 12:06:11.470 DEBUG Mutect2Engine - Extended Act Region chrM:12630-13120; 12:06:11.474 DEBUG Mutect2Engine - Ref haplotype coords chrM:12630-13120; 12:06:11.478 DEBUG Mutect2Engine - Haplotype count 128; 12:06:11.481 DEBUG Mutect2Engine - Kmer sizes count 0; 12:06:11.485 DEBUG Mutect2Engine - Kmer sizes values []; 12:08:48.420 DEBUG Mutect2 - Processing assembly region at chrM:13021-13320 isActive: false numReads: 44155; 12:08:49.628 INFO ProgressMeter - chrM:13021 33.1 50 1.5; 12:09:01.241 DEBUG Mutect2 - Processing assembly region at chrM:13321-13620 isActive: false numReads: 55070; 12:09:01.757 DEBUG Mutect2 - Processing assembly region at chrM:13621-13636 isActive: false numReads: 55240; 12:09:02.341 DEBUG Mutect2 - Processing assembly region at chrM:13637-13936 isActive: true numReads: 110273; 12:09:09.957 DEBUG ReadThreadingGraph - Recovered 24 of 26 dangling tails; 12:09:10.041 DEBUG ReadThreadingGraph - Recovered 6 of 14 dangling heads; 12:09:10.602 DEBUG Mutect2Engine - Active Region chrM:13637-13936; 12:09:10.608 DEBUG Mutect2Engine - Extended Act Region chrM:13537-14036; 12:09:10.613 DEBUG Mutect2Engine - Ref haplotype coords chrM:13537-14036; 12:09:10.617 DEBUG Mutect2Engine - Haplotype count 128; 12:09:10.621 DEBUG Mutect2Engine - Kmer sizes count 0; 12:09:10.625 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:51.290 DEBUG Mutect2 - Processing assembly region at chrM:13937-13944 isActive: true numReads: 54773; 12:13:53.989 DEBUG ReadThreadingGraph - Recovered 29 of 59 dangling tails; 12:13:54.004 DEBUG ReadThreadingGraph - Recovered 0 of 35 dangling heads; 12:13:54.432 DEBUG Mutect2Engine - Active Region chrM:13937-13944; 12:13:54.440 DEBUG Mutect2Engine - Extended Act Region chrM:13837-14044; 12:13:54.447 DEBUG Mutect2Engine - Ref haplotype coords chrM:13837-14044; 12:13:54.452 DEBUG Mutect2Engine - Haplotype ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:20128,Recover,Recovered,20128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Safety,"lson I am trying latest release, I use following command:. ```; ./gatk-4.1.3.0/gatk --java-options ""-Xmx4g"" FilterMutectCalls -O Filtered.vcf -V Try.vcf.gz -R ~/human.fa/ucsc.hg19.fasta; ```. and got following Info:. ```; Using GATK jar /mnt/md0/DataProcess/Ranshi/Mutect2/gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /mnt/md0/DataProcess/Ranshi/Mutect2/gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar FilterMutectCalls -O Filtered.vcf -V Try.vcf.gz -R /home/imp/human.fa/ucsc.hg19.fasta; 09:44:27.763 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/md0/DataProcess/Ranshi/Mutect2/gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 21, 2019 9:44:29 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 09:44:29.499 INFO FilterMutectCalls - ------------------------------------------------------------; 09:44:29.500 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.3.0; 09:44:29.500 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:44:29.500 INFO FilterMutectCalls - Executing as imp@imp-WorkStation on Linux v4.15.0-55-generic amd64; 09:44:29.500 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_222-8u222-b10-1ubuntu1~16.04.1-b10; 09:44:29.501 INFO FilterMutectCalls - Start Date/Time: 2019年8月21日 上午09时44分27秒; 09:44:29.501 INFO FilterMutectCalls - ------------------------------------------------------------; 09:44:29.501 INFO FilterMutectCalls - ------------------------------------------------------------; 09:44:29.502 INFO FilterMutectCalls - HTSJDK Version: 2.20.1; 09:44:29.502 INFO FilterMutectCalls - Picard Version: 2.20.5; 09:44:29.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6102#issuecomment-523262338:1012,detect,detect,1012,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6102#issuecomment-523262338,1,['detect'],['detect']
Safety,"lure: Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayI; ndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3019:7430,abort,abortStage,7430,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019,1,['abort'],['abortStage']
Safety,ly(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018:14171,abort,abortStage,14171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018,1,['abort'],['abortStage']
Safety,ly(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:14890,abort,abortStage,14890,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['abort'],['abortStage']
Safety,"m.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.nextMatePosition.position == null ? 8 : 0 #mate_unmapped. so it looks like the doc supports the existing code. Should I submit this as an issue? . Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:1749,redund,redundant,1749,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033,1,['redund'],['redundant']
Safety,mReads: 54745; 12:13:56.962 DEBUG Mutect2 - Processing assembly region at chrM:14245-14544 isActive: false numReads: 0; 12:13:56.973 DEBUG Mutect2 - Processing assembly region at chrM:14545-14844 isActive: false numReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false numReads: 0; 12:13:57.009 DEBUG Mutect2 - Processing assembly region at chrM:15445-15744 isActive: false numReads: 0; 12:13:57.027 INFO ProgressMeter - chrM:15445 38.3 60 1.6; 12:13:57.035 DEBUG Mutect2 - Processing assembly region at chrM:15745-15960 isActive: false numReads: 14; 12:13:57.047 DEBUG Mutect2 - Processing assembly region at chrM:15961-16230 isActive: true numReads: 30; 12:13:57.055 DEBUG ReadThreadingGraph - Recovered 1 of 1 dangling tails; 12:13:57.063 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 12:13:57.096 DEBUG ReadThreadingGraph - Recovered 3 of 3 dangling tails; 12:13:57.106 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling heads; 12:13:57.464 DEBUG Mutect2Engine - Active Region chrM:15961-16230; 12:13:57.469 DEBUG Mutect2Engine - Extended Act Region chrM:15861-16299; 12:13:57.472 DEBUG Mutect2Engine - Ref haplotype coords chrM:15861-16299; 12:13:57.476 DEBUG Mutect2Engine - Haplotype count 111; 12:13:57.479 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:57.482 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:58.821 DEBUG Mutect2 - Processing assembly region at chrM:16231-16299 isActive: false numReads: 15; 12:13:58.938 INFO Mutect2 - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityNotZeroReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonChimericOriginalAlignmentReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignment,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:22347,Recover,Recovered,22347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Safety,made callable sites a Long to avoid integer overflow,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6303:30,avoid,avoid,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6303,1,['avoid'],['avoid']
Safety,"many methods in MathUtils are redundant with ApacheCommons/Math or JDK. We should remove those methods.; For example, `MathUtils.lnGamma` should be deleted in favor of `Gamma.logGamma' unless there's a severe speed penalty.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/400:30,redund,redundant,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/400,1,['redund'],['redundant']
Safety,"mark duplicates in dataflow - based on the code by garrickevans . The main work is done in; `private static final class MarkDuplicatesDataflowTransform extends PTransform<PCollection<Read>, PCollection<Read>>` - the sigrature conforms to the main read processing pipeline. Limitations:; - no optical duplicates; - only integration tests (would be good to have unit tests that check dup detection logic on very specific reads - ideally those from picard's tests). @droazen please review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/541:386,detect,detection,386,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/541,1,['detect'],['detection']
Safety,"may have done wrong? I already went over the README and standard documentation, and don't think I missed a step. ### Affected tool(s) or class(es); gvnvkernel, other expected modules. #### Expected behavior; Generate output file from my VCF. #### Actual behavior; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /home/gamer456148/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar GermlineCNVCaller --input var.vcf --run-mode CASE --contig-ploidy-calls X/prefix-calls --output-prefix regular.vcf --output testfile.vcf; 21:21:12.277 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/gamer456148/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 23, 2020 9:21:12 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:21:12.543 INFO GermlineCNVCaller - ------------------------------------------------------------; 21:21:12.544 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.1.4.1; 21:21:12.544 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:21:12.544 INFO GermlineCNVCaller - Executing as gamer456148@gamer456148-Inspiron-15-7579 on Linux v4.15.0-88-generic amd64; 21:21:12.544 INFO GermlineCNVCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_191-b12; 21:21:12.544 INFO GermlineCNVCaller - Start Date/Time: February 23, 2020 9:21:12 PM EST; 21:21:12.544 INFO GermlineCNVCaller - ------------------------------------------------------------; 21:21:12.544 INFO GermlineCNVCaller - ------------------------------------------------------------; 21:21:12.544 INFO GermlineCNVCaller - HTSJDK Version: 2.21.0; 21:21:12.544 INFO GermlineCNVCaller - Picard Version: 2.21",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6467:1245,detect,detect,1245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6467,1,['detect'],['detect']
Safety,"me.log.txt ; ; Using GATK jar /home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar ; ; Running: ; ;    java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Djava.io.tmpdir=/tmp -Xmx3g -jar /home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-packa ; ; ge-4.2.5.0-local.jar HaplotypeCaller -R /home/gvandeweyer/elprep\_streaming/reference/hg19.fasta -I /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam -O results/wesep-229191-f.vcf --alleles ../wesid-226998-m.haplotypecaller.final.vcf.gz -L 0005-scattered.inter ; ; val\_list -bamout results/wesep-229191-f.variants.bam -G StandardAnnotation -G StandardHCAnnotation --dragen-mode --dragstr-params-path /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam.params ; ; 22:06:39.332 WARN  GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default ; ; 22:06:39.337 WARN  GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default ; ; 22:06:39.383 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Mar 12, 2022 10:06:39 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 22:06:39.543 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.543 INFO  HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; 22:06:39.543 INFO  HaplotypeCaller - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 22:06:39.54",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7741:4133,Redund,Redundant,4133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741,1,['Redund'],['Redundant']
Safety,"metimes yield the homologuos sequence to a shorter alignment and take that away from a longer alignment; when it comes to a later filtering step that filters out alignments purely based on its unique read span, because of the deoverlapping step, those short alignment that received the homologuous sequence because of the left-aligning convention now survived, compared to if there were no upfront deoverlap. The drop in number of DUP is due to a similar reason:; the upfront overlap removal made some contigs that would be classified as having incomplete picture unfairly now becomes a contig with complete picture (see [this](http://www.genomeribbon.com/?perma=AvOvbThN4z) for example). IMO removal of the upfront deoverlapping step is good, because the number of calls would not be affected by which convention we follow (convention should only affect representation, not if a variant is real); on the other hand, the convention definitely hurts some other small alignments. -------. The drop in number of variants detected from stable-versioned tool to the experimental tool without this fix are mainly from the imprecise deletion calls that are not hooked up in the experiemental tool yet, and many deletion and duplications are now incoporated into CPX variants. -------. Alternatively, we can turn off the filter based on unique alignment length all together, and annotate the calls with unique alignment length for later stage filter. I did experiment with that, and as expected, the number of BND calls and deletion calls increased, whereas the numer of DUP calls dropped, when comparing only-turn-off-length-filter, turn-off-upfront-deoverlap-and-length-filter. The drop in number of DUP calls are due to the fact that without upfront deoverlap, more contigs are classified as having incomplete picture. -------; Also proposing an ""improvement"" to the script by adding more checks in the pipeline bash script:; the problem it is trying to address is documented in the changes to the script.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4282:5419,detect,detected,5419,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4282,1,['detect'],['detected']
Safety,"mline resource and without PON; - calling with PON only, without germline resource; - calling with germline resource only, without PON; - calling with both germline resource and PON. The results (please note that the labeling conventions are now different compared to WGS experiments, apologies for the inconvenience):. ![FD_1_T_tumor-normal_WES_muTect2_FD_1_tumor-normal_muTect2_PON_FD_1_tumor-normal_muTect2_gnomAD_FD_1_tumor-normal_muTect2_PON_gnomAD](https://user-images.githubusercontent.com/15612230/182358963-97f04d12-94c6-4f77-acef-c3ebcd78a98f.png). 2. The reference is GRCh38.primary_assembly.genome.fa; The benchmark (""gold standard"") call set contains only variants on chr1-chr22, which AFAIK are identical or almost identical between the different b38 versions. 3. In my minimal WES example, most of the new FPs (150/158) from v4.1.9.0 are not present in raw, unfiltered calls from v.4.1.8.1. This was found the following way.; Compare FPs by CHROM, POS, REF, ALT using the scratch output of som.py (ran with --keep-scratch):. `; comm -23 <(bcftools query -f ""%CHROM %POS %REF %ALT\n"" WES_FD_TN_4190_filter_som_py/fp.vcf.gz | sort) <(bcftools query -f ""%CHROM %POS %REF %ALT\n"" WES_FD_TN_4181_filter_som_py/fp.vcf.gz | sort) | sed 's/ /\t/g' > new_FPs.txt; `; `; wc -l new_FPs.txt ; `; `; 158 new_FPs.txt; `. Find new FPs present in the raw output of v.4.1.8.1, again matching by CHROM, POS, REF, ALT:. `; awk 'NR==FNR{a[$1""_""$2""_""$3""_""$4]=1; next;}{if(substr($0,1,1)!=""#"" && a[$1""_""$2""_""$4""_""$5]==1) print $0}' new_FPs.txt WES_FD_TN_4181.vcf | wc -l; `; `; 8; `. For the 8 new FPs that have been detected, but filtered in v.4.1.8.1 (all of which are SNVs), find FILTER classification:; `; awk 'NR==FNR{a[$1""_""$2""_""$3""_""$4]=1; next;}{if(substr($0,1,1)!=""#"" && a[$1""_""$2""_""$4""_""$5]==1) print $0}' new_FPs.txt WES_FD_TN_4181_filtered.vcf | cut -f7 | sort | uniq -c; `; `; 2 clustered_events; `; `; 1 normal_artifact; `; `; 1 strand_bias; `; `; 4 weak_evidence; `. Hope this helps,. Dmitriy",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1202344705:2994,detect,detected,2994,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1202344705,1,['detect'],['detected']
Safety,"more concretely the private method getReferenceBases(SAMSeqRecord) should be syncronized or avoid it calling directly to the syncronized getReferenceBases(SSR, boolean) and getReferenceBasesByRegions should not update the cache fields.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8139#issuecomment-1376313615:92,avoid,avoid,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139#issuecomment-1376313615,1,['avoid'],['avoid']
Safety,more sanity checks and grouping the work by what they are trying to do,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4328:5,sanity check,sanity checks,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4328,1,['sanity check'],['sanity checks']
Safety,"mtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx7000M -Xms7000M -XX:ParallelGCThreads=2 -jar /shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar GenotypeGVCFs -R /shared/projects/gentaumix/Ressources/grch38\_BWA\_2/GCA\_000001405.15\_GRCh38\_no\_alt\_plus\_hs38d1\_analysis\_set.fa -V gendb:///tmp/tmp.6QEyWPGpWs/vcf\_database/Interval\_6 -O /tmp/tmp.6QEyWPGpWs/gentaumix\_interval\_6\_raw.vcf.gz -D /shared/projects/gentaumix/Ressources/known\_sites/Homo\_sapiens\_assembly38.dbsnp138.vcf --sequence-dictionary /shared/projects/gentaumix/Ressources/known\_sites/Homo\_sapiens\_assembly38.dict -L /shared/projects/gentaumix/Ressources/interval\_genomicsdbi/temp\_6/interval.interval\_list -G StandardAnnotation -G AS\_StandardAnnotation --merge-input-intervals ; ; 14:17:35.171 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default ; ; 14:17:35.232 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Sep 10, 2021 2:17:35 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:17:35.492 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 14:17:35.492 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 14:17:35.492 INFO GenotypeGVCFs - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 14:17:35.493 INFO GenotypeGVCFs - Executing as quentin67100@cpu-node-9 on Linux v3.10.0-1160.6.1.el7.x86\_64 amd64 ; ; 14:17:35.493 INFO GenotypeGVCFs - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7465:2646,Redund,Redundant,2646,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7465,1,['Redund'],['Redundant']
Safety,"my pipeline(from fastq ) is 👍 ; . 1. FastqToSam . 2. ConvertHeaderlessHadoopBamShardToBam. 3. BwaAndMarkDuplicatesPipelineSpark; ; ...... but in the step 3 （BwaAndMarkDuplicatesPipelineSpark），the pipeline crash; ```; Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, com1, executor 1): # **java.lang.IllegalArgumentException: Reference name for '1853452901' not found in sequence dictionary.; at htsjdk.samtools.SAMRecord.resolveNameFromIndex(SAMRecord.java:569); at htsjdk.samtools.SAMRecord.setMateReferenceIndex(SAMRecord.java:506); at htsjdk.samtools.BAMRecord.<init>(BAMRecord.java:94); at htsjdk.samtools.DefaultSAMRecordFactory.createBAMRecord(DefaultSAMRecordFactory.java:42); at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:210); at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.<init>(BAMFileReader.java:963); at htsjdk.samtools.BAMFileReader.getIterator(BAMFileReader.java:491)**; at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:182); at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInputFormat.java:211); at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:190); at org.apache.spark.rdd.NewHadoopRDD$$anon$1.liftedTree1$1(NewHadoopRDD.scala:180); at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:179); at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:134); at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:69); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4179:265,abort,aborted,265,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4179,1,['abort'],['aborted']
Safety,n - Processing 48129895 bp from intervals; 18:30:58.188 INFO CNNScoreVariants - Done initializing engine; 18:31:00.188 INFO CNNScoreVariants - Using key:CNN_1D for CNN architecture:.../1d_cnn_mix_train_full_bn.2560984151625233621.json and weights:.../1d_cnn_mix_train_full_bn.2397909300265264152.hd5; 18:31:19.873 INFO ProgressMeter - Starting traversal; 18:31:19.874 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 18:31:50.095 INFO CNNScoreVariants - Shutting down engine; [2018/04/24 18:31:50 JST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.88 minutes.; Runtime.totalMemory()=2141716480; ***********************************************************************; A USER ERROR has occurred: A timeout ocurred waiting for output from the remote Python command.; ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: A timeout ocurred waiting for output from the remote Python command.; at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:219); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.sendSynchronousCommand(StreamingPythonScriptExecutor.java:153); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:260); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:891); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4696:1548,timeout,timeout,1548,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4696,1,['timeout'],['timeout']
Safety,"n called without the MNP output I get a pair of variants as follows (some info removed for clarity), coordinates are HG19:. ```; chr4 5743509 . C T 5903.03 . FS=0.000;QD=25.36;SOR=9.825 GT:AD:DP:GQ:PL 1/1:0,135:135:99:5917,406,0; chr4 5743512 . T C 2766.60 . FS=0.000;QD=21.12;SOR=0.983 GT:AD:DP:GQ:PL 0/1:57,74:131:99:2774,0,2060; ```. I'm trying to get permission to share the BAM over this region, but the key information is that every single read that spans or is in proximity to these variants is on the R strand. There is zero F strand coverage. This seems reasonable. It's a bit odd to me that the first SNP which is hom-var has a SOR value of 9.825, but it's homozygous so it's more or less irrelevant. Looking at the code, I think the problem here is that the code avoids divide-by-zero errors by adding pseudo-counts of `1.0` to the table, which for homozygous variants with no coverage on one strand creates a weird situation. I think it would be better to just detect if _all_ coverage is on one strand and short-circuit the calculation, but I digress. The real problem comes when running with `--max-mnp-distance 5`. Then I get this single variant:. ```; chr4 5743509 . CTAT TTAC,TTAT 5506.10 . FS=0.000;QD=25.36;SOR=9.750 GT:AD:DP:GQ:PL 1/2:0,74,56:130:99:5523,2213,2060,3016,0,2774; ```. Now I have a het variant with an SOR of 9.75. This seems really wrong to me - note how FS is 0.0. Again all coverage of all alleles is on one strand. And the het SNP that forms part of this MNP had an SOR of 0.983 when called independently. Since the first SNP is hom-var and the second is het, I would have expected the SOR value for the MNP call to closely mirror that of the het SNP. My suspicion is that what's going on here is probably that the calculation is being run using the contingency table for the hom-var SNP that's first in the MNP, perhaps filtered to only reads that span the whole MNP, since the value is marginally lower. #### Steps to reproduce; I will try and post a BAM snippe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5698:1435,detect,detect,1435,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5698,1,['detect'],['detect']
Safety,"n't have an index, it returns a TribbleIndexedFeatureReader; > instead of a TabixFeatureReader, because methods.isTabix() returns; > false when an index is not present.; > - TribbleIndexedFeatureReader, in turn, opens a Java vanilla; > GZIPInputStream, instead of the BlockCompressedInputStream that gets; > opened when you create a TabixFeatureReader.; > - GZIPInputStream, in turn, has a *confirmed bug* filed against it in; > Oracle's bug tracker (see; > https://bugs.java.com/bugdatabase/view_bug.do?bug_id=7036144#), that; > it inappropriately relies on the available() method to detect; > end-of-file, which is never safe to do given the contract of; > available(); > - As the final piece in the ghastly puzzle, implementations of; > SeekableStream in htsjdk do not implement available() at all, instead; > using the default implementation which always returns 0.; >; > As a result of this combination of bugs in Java's GZIPInputStream itself; > and bugs in htsjdk's SeekableStream classes, end-of-file can be detected; > prematurely when within 26 bytes of the end of a block, due to the; > following code in GZIPInputStream.readTrailer():; >; > if (this.in.available() > 0 || n > 26) {; > ....; > }; > return true; // EOF; >; > Where n is the number of bytes left to inflate in the current block.; >; > The solution is to replace all usages of the bugged GZIPInputStream with; > BlockCompressedInputStream in tribble in htsjdk (at least, for points in; > the code where the input is known to be block-gzipped rather than regular; > gzipped). For due diligence we should also implement available(); > correctly for all implementations of SeekableStream in htsjdk.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4224#issuecomment-360282461>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0h8AF8wYzkbHSmAu4-8n5TE8GtOUks5tN6MfgaJpZM4RoUzm>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4224#issuecomment-360304725:1376,detect,detected,1376,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4224#issuecomment-360304725,1,['detect'],['detected']
Safety,"n(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4515:3413,abort,abortStage,3413,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515,1,['abort'],['abortStage']
Safety,"n-negative-indices. Command: ; gatk LeftAlignIndels \; -R /projects/beck-lab/walawi/GATK/hg38.fa \; -I /projects/beck-lab/walawi/GATK/sorted_bam/H1-L_GT19_35266_GACCTGAA-TTGGTGAG_S2.withRG.sorted.bam \; -O /projects/beck-lab/walawi/GATK/sorted_bam/leftalignindels/H1-L_GT19_35266_GACCTGAA-TTGGTGAG_S2_leftaligned.withRG.sorted.bam. ```; gatk LeftAlignIndels -R /projects/beck-lab/walawi/GATK/hg38.fa -I /projects/beck-lab/walawi/GATK/sorted_bam/H1-L_GT19_35266_GACCTGAA-TTGGTGAG_S2.withRG.sorted.bam -O /projects/beck-lab/walawi/GATK/sorted_bam/leftalignindels/H1-L_GT19_35266_GACCTGAA-TTGGTGAG_S2_leftaligned.withRG.sorted.bam. 10:43:19.190 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/walawi/miniconda3/envs/gatk4_venv/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so. Aug 18, 2020 10:43:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine. INFO: Failed to detect whether we are running on Google Compute Engine. 10:43:19.667 INFO LeftAlignIndels - ------------------------------------------------------------. 10:43:19.668 INFO LeftAlignIndels - The Genome Analysis Toolkit (GATK) v4.1.7.0. 10:43:19.668 INFO LeftAlignIndels - For support and documentation go to https://software.broadinstitute.org/gatk/. 10:43:19.668 INFO LeftAlignIndels - Executing as walawi@sumner055 on Linux v3.10.0-1062.1.2.el7.x86_64 amd64. 10:43:19.668 INFO LeftAlignIndels - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01. 10:43:19.668 INFO LeftAlignIndels - Start Date/Time: August 18, 2020 10:43:19 AM EDT. 10:43:19.668 INFO LeftAlignIndels - ------------------------------------------------------------. 10:43:19.668 INFO LeftAlignIndels - ------------------------------------------------------------. 10:43:19.668 INFO LeftAlignIndels - HTSJDK Version: 2.21.2. 10:43:19.669 INFO LeftAlignIndels - Picard Version: 2.21.9. 10:43:19.669 INFO LeftAlignIndels - HTSJDK Defaults.COMPRESSION_LE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6765:1647,detect,detect,1647,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6765,1,['detect'],['detect']
Safety,"n_rnaseq/gatk_output/CDL-164-04P/CDL-164-04P-1_0_249250621_genomicsdb; 10:24:57.553 INFO GenomicsDBImport - Vid Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/vidmap.json; 10:24:57.554 INFO GenomicsDBImport - Callset Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/callset.json; 10:24:57.554 INFO GenomicsDBImport - Complete VCF Header will be written to CDL-164-04P-1_0_249250621_genomicsdb/vcfheader.vcf; 10:24:57.554 INFO GenomicsDBImport - Importing to array - CDL-164-04P-1_0_249250621_genomicsdb/genomicsdb_array; 10:24:57.554 INFO ProgressMeter - Starting traversal; 10:24:57.554 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 10:24:57.971 INFO GenomicsDBImport - Importing batch 1 with 1 samples; Buffer resized from 22726bytes to 32529; Buffer resized from 32529bytes to 32693; Buffer resized from 32693bytes to 32738; Buffer resized from 32738bytes to 32741; Buffer resized from 32741bytes to 32756; Buffer resized from 32756bytes to 32768; Buffer resized from 32768bytes to 32769; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f7288295359, pid=68672, tid=0x00007f72dc187700; #; # JRE version: OpenJDK Runtime Environment (8.0_171-b10) (build 1.8.0_171-b10); # Java VM: OpenJDK 64-Bit Server VM (25.171-b10 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libtiledbgenomicsdb8064358042335455262.so+0x155359] BufferVariantCell::set_cell(void const*)+0x99; #; # Core dump written. Default location: /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/core or core.68672; #; # An error report file with more information is saved as:; # /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/hs_err_pid68672.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045:10784,detect,detected,10784,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045,1,['detect'],['detected']
Safety,"na-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar GenotypeGVCFs -R resources/genome.fasta -V gendb://results/genomics_db/chromosomes/CM031199.1 -O results/vcf_parts/CM031199.1.vcf.gz; 22:17:18.737 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 16, 2022 10:17:18 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 22:17:18.863 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:17:18.863 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.4.1; 22:17:18.863 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:17:18.864 INFO GenotypeGVCFs - Executing as eanderson@node34.cluster on Linux v4.18.0-193.28.1.el8_2.x86_64 amd64; 22:17:18.864 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v11.0.9.1-internal+0-adhoc..src; 22:17:18.864 INFO GenotypeGVCFs - Start Date/Time: January 16, 2022 at 10:17:18 PM PST; 22:17:18.864 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:17:18.864 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:17:18.864 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 22:17:18.864 INFO GenotypeGVCFs - Picard Version: 2.25.4; 22:17:18.865 INFO GenotypeGVCF",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014180059:2183,detect,detect,2183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014180059,1,['detect'],['detect']
Safety,"nal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. 21/04/13 07:32:25 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Cancelling stage 5; 21/04/13 07:32:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage cancelled; 21/04/13 07:32:25 INFO DAGScheduler: ResultStage 5 (runJob at SparkHadoopWriter.scala:78) failed in 0.353 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurren",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:11259,abort,aborted,11259,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['abort'],['aborted']
Safety,"nc_io_write_tribble=false -Dsamjdk.compression_level=2 -jar $HOME/local/pckg/python/miniconda3/envs/test/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar MergeVcfs -I data/calling/a.vcf.gz -I data/calling/b.vcf.gz -O c.vcf.gz; 23:25:05.033 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:$HOME/local/pckg/python/miniconda3/envs/test/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Tue Jun 16 23:25:05 CDT 2020] MergeVcfs --INPUT data/calling/a.vcf.gz --INPUT data/calling/b.vcf.gz --OUTPUT c.vcf.gz --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX true --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Jun 16, 2020 11:25:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Tue Jun 16 23:25:05 CDT 2020] Executing as xxxx on Linux 3.10.0-693.11.1.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_152-release-1056-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.7.0; [Tue Jun 16 23:25:05 CDT 2020] picard.vcf.MergeVcfs done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=605028352; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp; htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to create BasicFeatureReader using feature file , for input source: file:///tmp/test%20a/data/calling/a.vcf.gz; at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:124); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:81); at htsjdk.variant.vcf.VCFFileReader.<init>(VCFFileReader.java:148); at htsjdk.variant.vcf.VCFFileReader.<init>(VCFFileReader.java:98); at picard.vc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664:2098,detect,detect,2098,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664,1,['detect'],['detect']
Safety,"nce context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updated test suite to use full, checked-in references. - Updated many tests to use regression test data sources rather than small data; sources specifically for PIK3CA and MUC16. - Removed some now unused data files (primarily old data sources). - Updated regression tests to work on local data sources and references. - Now `FuncotatorIntegrationTest::regressionTest` works on local, checked-out copies of the data sources so that; they can be run from any checkout. - Fixes #4344 ; - Fixes #5295",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5302:4674,safe,safely,4674,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302,1,['safe'],['safely']
Safety,"ncotator_dataSources.v1.7.20200521s \; > --output ./my_data/variants.funcotated.maf \; > --output-file-format MAF \; > --disable-sequence-dictionary-validation; Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Funcotator --variant ./my_data/test_b37.vcf --reference ./my_data/human_g1k_v37.fasta --ref-version hg19 --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s --output ./my_data/variants.funcotated.maf --output-file-format MAF --disable-sequence-dictionary-validation; 12:11:19.732 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 24, 2021 12:11:19 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:11:19.904 INFO Funcotator - ------------------------------------------------------------; 12:11:19.904 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.2.0.0; 12:11:19.904 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:11:19.905 INFO Funcotator - Executing as root@75181703d894 on Linux v4.15.0-132-generic amd64; 12:11:19.905 INFO Funcotator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 12:11:19.905 INFO Funcotator - Start Date/Time: March 24, 2021 12:11:19 PM GMT; 12:11:19.905 INFO Funcotator - ------------------------------------------------------------; 12:11:19.905 INFO Funcotator - ------------------------------------------------------------; 12:11:19.906 INFO Funcotator - HTSJDK Version: 2.24.0; 12:11:19.906 INFO Funcotator - Picard Version: 2.25.0; 12:11:19.906 INFO Funcotator - Built for Spark Version: 2.4.5; 12:11:19.90",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7158:2898,detect,detect,2898,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158,1,['detect'],['detect']
Safety,"nd other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely necessary, as this may cause conflicts with other core dependencies; # verify that this is using numpy compiled against MKL (e.g., by checking tensorflow.pywrap_tensorflow.IsMklEnabled()); - conda-forge::scipy=1.0.0 # do not update, this will break a scipy.misc.logsumexp import (deprecated in scipy=1.0.0) in pymc3=3.1; - conda-forge::pymc3=3.1 # do not update, this will break gcnvkernel; - conda-forge::keras=2.2.4 # updated from pip-installed 2.2.0, which caused various conflicts/clobbers of conda-installed packages; # conda-installed 2.2.4 appears to be the most recent version with a consistent API and without conflicts/clobbers; # if you wish to update, note that versions of conda-forge::keras after 2.2.5; # undesirably set the environment variable KERAS_BACKEND = theano by default; - defaults::intel-openmp=2019.4; - conda-forge::scikit-learn=0.22.2; - conda-forge::matplotlib=3.2.1; - conda-forge::pandas=1.0.3. # core R dependencies; these should only be used for plotting and do not take precedence over core python dependencies!; - r-base=3.6.2; - r-data.table=1.12.8; - r-dplyr=0.8.5; - r-getopt=1.20.3; - r-ggplot2=3.3.0; - r-gplots=3.0.3; - r-gsalib=2.1; - r-optparse=1.6.4. # other python dependencies; these should be removed after functionality is moved into Java code; - biopython=1.76; - pyvcf=0.6.8; - bioconda::pysam=0.15.3 # using older conda-installed versions may result in libcrypto / openssl bugs. # pip installs should be avoided, as pip may not respect the dependencies found by the conda solver; - pip:; - gatkPythonPackageArchive.zip; ```. It seems to successfully create the environment. I'd still recommend updating the information on your README.md and the file.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:3717,avoid,avoided,3717,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,2,['avoid'],['avoided']
Safety,nds 11 --gvcf-gq-bands 12 --gvcf-gq-bands 13 --gvcf-gq-bands 14 --gvcf-gq-bands 15 --gvcf-gq-bands 16 --gvcf-gq-bands 17 --gvc; f-gq-bands 18 --gvcf-gq-bands 19 --gvcf-gq-bands 20 --gvcf-gq-bands 21 --gvcf-gq-bands 22 --gvcf-gq-bands 23 --gvcf-gq-bands 24 --gvcf-gq-bands 25 --gvcf-gq-bands 26 --gvcf-gq-bands 27 --g; vcf-gq-bands 28 --gvcf-gq-bands 29 --gvcf-gq-bands 30 --gvcf-gq-bands 31 --gvcf-gq-bands 32 --gvcf-gq-bands 33 --gvcf-gq-bands 34 --gvcf-gq-bands 35 --gvcf-gq-bands 36 --gvcf-gq-bands 37 -; -gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47; --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-gq-bands ; 57 --gvcf-gq-bands 58 --gvcf-gq-bands 59 --gvcf-gq-bands 60 --gvcf-gq-bands 70 --gvcf-gq-bands 80 --gvcf-gq-bands 90 --gvcf-gq-bands 99 --floor-blocks false --indel-size-to-eliminate-in-re; f-model 10 --disable-optimizations false --dragen-mode false --flow-mode NONE --apply-bqd false --apply-frd false --disable-spanning-event-genotyping false --transform-dragen-mapping-quali; ty false --mapping-quality-threshold-for-genotyping 20 --max-effective-depth-adjustment-for-frd 0 --just-determine-active-regions false --dont-genotype false --do-not-run-physical-phasing ; false --do-not-correct-overlapping-quality false --use-filtered-reads-for-annotations false --use-flow-aligner-for-stepwise-hc-filtering false --adaptive-pruning false --do-not-recover-dan; gling-branches false --recover-dangling-heads false --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 ; --min-dangling-branch-length 4 --recover-all-dangling-branches false --max-num-haplotypes-in-population 128 --min-pruning 2 --adaptive-pruning-initial-error-rate 0.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:4877,recover,recover-dan,4877,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,3,['recover'],"['recover-all-dangling-branches', 'recover-dan', 'recover-dangling-heads']"
Safety,"ne(GATKSparkTool.java:387); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 18/05/01 14:30:35 WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:09 WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:10 INFO ShutdownHookManager: Shutdown hook called; 18/05/01 14:31:15 INFO ShutdownHookManager: Deleting directory /tmp/abd30/spark-3f28d2e3-59d7-40f9-bba3-42d61eff6c6a; 18/05/01 14:31:20 ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; Using GATK jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --input /data/shenlab/abd/TCGA_microbiome/tmp_WXS_colorectal_all/TCGA-AH-6643-11A-01D-1826-10_hg19_Illumina_gdc_realn.bam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4725:5289,timeout,timeout,5289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725,1,['timeout'],['timeout']
Safety,"neGVCFs \; -G StandardAnnotation \; -G AS_StandardAnnotation \; -R ""$ref_gen""/ucsc.hg19.fasta \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/A200272.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/A200273.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/A200274.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/A200313.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/A200314.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/A200315.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/PID20-006.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/8_samples_20200819/gvcfs/PID20-007.HC.g.vcf.gz \; -V /paedwy/disk1/yangyxt/wes/backup_gvcfs/all_wes_samples.g.vcf \; -O /paedwy/disk1/yangyxt/wes/backup_gvcfs/all_wes_samples_plus_${sample_batch}.g.vcf.gz && echo ""Combine_gvcfs done"". Error Log:; ```; 12:01:36.798 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 12:01:36.824 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/yangyxt/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 24, 2020 12:01:37 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:01:37.108 INFO CombineGVCFs - ------------------------------------------------------------; 12:01:37.108 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 12:01:37.108 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:01:37.108 INFO CombineGVCFs - Executing as yangyxt@paedyl01 on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 12:01:37.108 INFO CombineGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 12:01:37.108 INFO CombineGVCFs - Start Date/Time: August 24, 202",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6766:1570,Redund,Redundant,1570,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6766,1,['Redund'],['Redundant']
Safety,"ner. As I'd think that all software dependencies and whatnot should be fine. However, I still get the same error message:. /gatk/./gatk --java-options ""-Xmx25g"" SplitNCigarReads \; > -R Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam \; > --tmp-dir /gatk/my_data/temp -O thing.bam; Using GATK jar /gatk/gatk-package-4.1.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx25g -jar /gatk/gatk-package-4.1.3.0-local.jar SplitNCigarReads -R Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam --tmp-dir /gatk/my_data/temp -O thing.bam. 21:12:14.158 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 02, 2023 9:12:16 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:12:16.383 INFO SplitNCigarReads - ------------------------------------------------------------; 21:12:16.384 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.1.3.0; 21:12:16.384 INFO SplitNCigarReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:12:16.384 INFO SplitNCigarReads - Executing as root@9d399eec0e24 on Linux v5.19.0-32-generic amd64; 21:12:16.384 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; 21:12:16.384 INFO SplitNCigarReads - Start Date/Time: March 2, 2023 9:12:14 PM UTC; 21:12:16.385 INFO SplitNCigarReads - ------------------------------------------------------------; 21:12:16.385 INFO SplitNCigarReads - ------------------------------------------------------------; 21:12:16.385 INFO SplitNCigarReads - HTSJDK Version: 2.20.1; 21:12:16.385 INFO SplitNCigarReads - Picard Version: 2.20.5; 21:12:16.385 IN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452564826:1183,detect,detect,1183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452564826,1,['detect'],['detect']
Safety,new dangling head recovery leads to array index out of bounds,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7085:18,recover,recovery,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7085,1,['recover'],['recovery']
Safety,"ng GATK wrapper script /juffowup/gatk/build/install/gatk/bin/gatk; Running:; /juffowup/gatk/build/install/gatk/bin/gatk HaplotypeCaller -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta -I /juffowup2/malaria/haplotypecaller_arg_testing/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam -O /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz --bam-output /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam -contamination 0 --sample-ploidy 2 --linked-de-bruijn-graph --pileup-detection true --pileup-detection-enable-indel-pileup-calling true --max-reads-per-alignment-start 20 --annotate-with-num-discovered-alleles -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -G StandardAnnotation -G StandardHCAnnotation -ERC GVCF --verbosity INFO; 14:14:15.323 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 14:14:15.328 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 14:14:15.388 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/juffowup/gatk/build/install/gatk/lib/gkl-0.8.11.jar!/com/intel/gkl/native/libgkl_compression.so; 14:14:15.435 INFO HaplotypeCaller - ------------------------------------------------------------; 14:14:15.439 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0-44-g1529aa1-SNAPSHOT; 14:14:15.439 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:14:15.439 INFO HaplotypeCaller - Executing as jonn@dsde-methods-jonn-juffowup on Linux v5.4.0-1104-gcp amd64; 14:14:15.439 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v17.0.7+7; 14:14:15.440 INFO HaplotypeCaller - Start Date/Time: July 26, 2023 at 2:14:15 PM UTC; ...; 22:1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8440:1078,Redund,Redundant,1078,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440,1,['Redund'],['Redundant']
Safety,"ng engine; Created workspace /scratch/production/cluengo/genomicsdb/gdbworkspace-gatk; 17:00:54.418 INFO GenomicsDBImport - Vid Map JSON file will be written to gdbworkspace-gatk/vidmap.json; 17:00:54.418 INFO GenomicsDBImport - Callset Map JSON file will be written to gdbworkspace-gatk/callset.json; 17:00:54.418 INFO GenomicsDBImport - Complete VCF Header will be written to gdbworkspace-gatk/vcfheader.vcf; 17:00:54.418 INFO GenomicsDBImport - Importing to array - gdbworkspace-gatk/genomicsdb_array; 17:00:54.470 INFO ProgressMeter - Starting traversal; 17:00:54.470 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 17:00:54.488 INFO GenomicsDBImport - Importing batch 1 with 1 samples; terminate called after throwing an instance of 'FileBasedVidMapperException'; what(): FileBasedVidMapperException : type_index_iter != VidMapper::m_typename_string_to_type_index.end() && ""Unhandled field type""; [login1:01909] *** Process received signal ***; [login1:01909] Signal: Aborted (6); [login1:01909] Signal code: (-6); [login1:01909] [ 0] /lib64/libpthread.so.0[0x30bfa0f7e0]; [login1:01909] [ 1] /lib64/libc.so.6(gsignal+0x35)[0x30bf6325e5]; [login1:01909] [ 2] /lib64/libc.so.6(abort+0x175)[0x30bf633dc5]; [login1:01909] [ 3] /apps/GCC/6.3.0/lib64/libstdc++.so.6(_ZN9__gnu_cxx27__verbose_terminate_handlerEv+0x15d)[0x7f507f7018ed]; [login1:01909] [ 4] /apps/GCC/6.3.0/lib64/libstdc++.so.6(+0x8e8a6)[0x7f507f6ff8a6]; [login1:01909] [ 5] /apps/GCC/6.3.0/lib64/libstdc++.so.6(+0x8e8f1)[0x7f507f6ff8f1]; [login1:01909] [ 6] /apps/GCC/6.3.0/lib64/libstdc++.so.6(+0x8eb08)[0x7f507f6ffb08]; [login1:01909] [ 7] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x151df1)[0x7f507fb56df1]; [login1:01909] [ 8] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x1434d9)[0x7f507fb484d9]; [login1:01909] [ 9] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x1489f9)[0x7f507fb4d9f9]; [login1:01909] [10] /apps/GENOMICSDB/0.9.2/lib/libtiledbgenomicsdb.so(+0x12c78e)[0x",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4514:3725,Abort,Aborted,3725,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4514,1,['Abort'],['Aborted']
Safety,"ng/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar GenotypeGVCFs -R resources/genome.fasta -V gendb://results/genomics_db/chromosomes/CM031199.1 --max-alternate-alleles 7 -O results/vcf_parts/CM031199.1.vcf.gz; 21:57:11.346 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 16, 2022 9:57:11 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:57:11.476 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:57:11.477 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.4.1; 21:57:11.477 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:57:11.477 INFO GenotypeGVCFs - Executing as eanderson@node34.cluster on Linux v4.18.0-193.28.1.el8_2.x86_64 amd64; 21:57:11.477 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v11.0.9.1-internal+0-adhoc..src; 21:57:11.477 INFO GenotypeGVCFs - Start Date/Time: January 16, 2022 at 9:57:11 PM PST; 21:57:11.477 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:57:11.477 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:57:11.478 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 21:57:11.478 INFO GenotypeGVCFs - Picard Version: 2.25.4; 21:57:11.478 INFO GenotypeGVCFs",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014180059:11500,detect,detect,11500,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014180059,1,['detect'],['detect']
Safety,"ng?. On Wed, Jan 24, 2018 at 4:39 PM droazen <notifications@github.com> wrote:. > @ldgauthier <https://github.com/ldgauthier> @yfarjoun; > <https://github.com/yfarjoun> We have an update on this! We've identified; > the bug:; >; > - When AbstractFeatureReader.getFeatureReader() tries to open a .vcf.gz; > that doesn't have an index, it returns a TribbleIndexedFeatureReader; > instead of a TabixFeatureReader, because methods.isTabix() returns; > false when an index is not present.; > - TribbleIndexedFeatureReader, in turn, opens a Java vanilla; > GZIPInputStream, instead of the BlockCompressedInputStream that gets; > opened when you create a TabixFeatureReader.; > - GZIPInputStream, in turn, has a *confirmed bug* filed against it in; > Oracle's bug tracker (see; > https://bugs.java.com/bugdatabase/view_bug.do?bug_id=7036144#), that; > it inappropriately relies on the available() method to detect; > end-of-file, which is never safe to do given the contract of; > available(); > - As the final piece in the ghastly puzzle, implementations of; > SeekableStream in htsjdk do not implement available() at all, instead; > using the default implementation which always returns 0.; >; > As a result of this combination of bugs in Java's GZIPInputStream itself; > and bugs in htsjdk's SeekableStream classes, end-of-file can be detected; > prematurely when within 26 bytes of the end of a block, due to the; > following code in GZIPInputStream.readTrailer():; >; > if (this.in.available() > 0 || n > 26) {; > ....; > }; > return true; // EOF; >; > Where n is the number of bytes left to inflate in the current block.; >; > The solution is to replace all usages of the bugged GZIPInputStream with; > BlockCompressedInputStream in tribble in htsjdk (at least, for points in; > the code where the input is known to be block-gzipped rather than regular; > gzipped). For due diligence we should also implement available(); > correctly for all implementations of SeekableStream in htsjdk.; >; > —; > You",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4224#issuecomment-360304725:945,detect,detect,945,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4224#issuecomment-360304725,2,"['detect', 'safe']","['detect', 'safe']"
Safety,ngGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine - Active Region chrM:3703-3943; 11:36:16.348 DEBUG Mutect2Engine - Extended Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembly region at chrM:4844-5143 isActive: false numReads: 0; 11:36:40.765 DEBUG,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:11056,Recover,Recovered,11056,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Safety,nonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:14955,abort,abortStage,14955,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['abort'],['abortStage']
Safety,"not necessary anymore for GATK-SV assembly-detected variants because the alt haplotype sequence is always output. still necessary for variants that are re-interpreted in #4189 , and other tools (but may never come true).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2386#issuecomment-361072987:43,detect,detected,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2386#issuecomment-361072987,1,['detect'],['detected']
Safety,"nputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 01:44 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:39 INFO TaskSetManager: Starting task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:35903 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:39 INFO TaskSetManager: Lost task 1.3 in stage 2.0 (TID 10) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 18/04/24 17:56:39 ERROR TaskSetManager: Task 1 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:56:39 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 45.219 s due to Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.ca",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:34716,abort,aborting,34716,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['abort'],['aborting']
Safety,"nquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); GATK CalibrateDragstrModel. ### Affected version(s); - [x] Latest public release version [4.3.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; When running CalibrateDragstrModel in parallel mode, the supplied reference isn't detected correctly causing the following error stack trace:. ```bash; Using GATK jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx72g -jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar CalibrateDragstrModel --input input.cram --output input.txt --reference hg38.fa --str-table-path hg38.zip --threads 12 --intervals fasta_bed.bed --tmp-dir .; 10:24:21.117 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:24:21.289 INFO CalibrateDragstrModel - ------------------------------------------------------------; 10:24:21.289 INFO CalibrateDragstrModel - The Genome Analysis Toolkit (GATK) v4.3.0.0; 10:24:21.289 INFO CalibrateDragstrModel -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8139:1468,detect,detected,1468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139,1,['detect'],['detected']
Safety,"ns ""-Xmx30g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" GenotypeGVCFs -R Reference/File_S16_uT_3_Pseudochromosomes.fasta -V gendb://ABchroneALL -O ABchroneALL.vcf.gz; Using GATK jar /data/xxxxxx/miniconda3/share/gatk4-4.1.6.0-0/gatk-package-4.1.6.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx30g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /data/xxxxxx/miniconda3/share/gatk4-4.1.6.0-0/gatk-package-4.1.6.0-local.jar GenotypeGVCFs -R Reference/File_S16_uT_3_Pseudochromosomes.fasta -V gendb://ABchroneALL -O ABchroneALL.vcf.gz; 09:48:14.426 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xxxxxx/miniconda3/share/gatk4-4.1.6.0-0/gatk-package-4.1.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 27, 2020 9:48:14 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 09:48:14.871 INFO GenotypeGVCFs - ------------------------------------------------------------; 09:48:14.871 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.6.0; 09:48:14.872 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.oAB/gatk/; 09:48:14.872 INFO GenotypeGVCFs - Executing as xxxxxx@galaxy on Linux v4.4.0-133-generic amd64; 09:48:14.872 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 09:48:14.872 INFO GenotypeGVCFs - Start Date/Time: 27 May 2020 09:48:14 CEST; 09:48:14.872 INFO GenotypeGVCFs - ------------------------------------------------------------; 09:48:14.872 INFO GenotypeGVCFs - ------------------------------------------------------------; 09:48:14.873 INFO GenotypeGVCFs - HTSJDK Version: 2.21.2; 09:48:14.873 INFO GenotypeGVCFs - Picard Version: 2.21.9; 09:48:14.873 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:48:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6616:8623,detect,detect,8623,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6616,1,['detect'],['detect']
Safety,nstitute/hellbender/tools/exome/calculatetargetcoverage/exome-read-counts-NA12878.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/exome/calculatetargetcoverage/exome-read-counts-NA12878.output; src/test/resources/org/broadinstitute/hellbender/tools/exome/calculatetargetcoverage/exome-read-counts-sample-NA12878.output; src/test/resources/org/broadinstitute/hellbender/tools/exome/calculatetargetcoverage/test_reference.dict; src/test/resources/org/broadinstitute/hellbender/tools/exome/calculatetargetcoverage/test_reference.fasta.fai; src/test/resources/org/broadinstitute/hellbender/tools/exome/conversion/allelicbalancecaller/cell_line_full-sim-final.seg; src/test/resources/org/broadinstitute/hellbender/tools/exome/create-pon-some-targets.bed; src/test/resources/org/broadinstitute/hellbender/tools/exome/detectcoveragedropout; src/test/resources/org/broadinstitute/hellbender/tools/exome/detectcoveragedropout/HCC1143T-100_27M_37M.seg; src/test/resources/org/broadinstitute/hellbender/tools/exome/detectcoveragedropout/test.tn.HCC1143T-100_27M_37M.tsv; src/test/resources/org/broadinstitute/hellbender/tools/exome/discover-germline-input-to-xhmm-zscores.pl; src/test/resources/org/broadinstitute/hellbender/tools/exome/discover-germline-xhmm-output-4-6-70-3-3.tab; src/test/resources/org/broadinstitute/hellbender/tools/exome/dummy_cov_profile.txt; src/test/resources/org/broadinstitute/hellbender/tools/exome/dupReadsMini.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/exome/eval/eval-calls.vcf; src/test/resources/org/broadinstitute/hellbender/tools/exome/eval/eval-calls.vcf.gz; src/test/resources/org/broadinstitute/hellbender/tools/exome/eval/eval-calls.vcf.gz.tbi; src/test/resources/org/broadinstitute/hellbender/tools/exome/eval/eval-targets.tsv; src/test/resources/org/broadinstitute/hellbender/tools/exome/eval/eval-truth.vcf.gz; src/test/resources/org/broadinstitute/hellbender/tools/exome/eval/eval-truth.vcf.gz.tbi; src/test/resources/org/broadinst,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905:30932,detect,detectcoveragedropout,30932,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905,1,['detect'],['detectcoveragedropout']
Safety,"nt bin* provides equal weight---rather than the counts themselves. As usual, modeling each bin as Poisson is close enough to modeling all bins as multinomial for our purposes. If we directly use the NB likelihood and simply weight the count likelihood by occurrences, occurrences in the peak will strongly affect the result, adversely so if the count likelihood is actually misspecified there. As an example, consider trying to fit a Poisson to data that is actually zero-inflated Poisson---fitting the histogram will actually result in a more robust estimate for the mean. Another benefit is that truncated data (as we have here) is straightforwardly handled in an unbiased way. In the special case of complete, trivially-binned data, the full, unbinned likelihood is recovered. I think this sort of histogram fitting is pretty standard in the astro/particle community. We can certainly change up the model to include strictly quantized + free-floating states as you describe (rather than the ""fuzzily quantized"" states I use here), but I just wanted to avoid having another level of mixtures/logsumexps for this quick prototype. However, note that modeling mosaicism on the autosomes is desirable, but there we also want the strong diploid prior to nail down the depth and per-contig bias. So we will have to be a little careful about how we introduce free-floating states. Also, since I was not using gcnvkernel, I had to integrate out all discrete parameters. It may be that we can write down a nice model with discrete parameters if we use your inference framework. Finally, I did not further bin the counts here (or rather, the bin size is 1), which already yields the maximum information, but I did use a maximum-count cutoff. If we use the same cutoff for all samples, this allows us to simply pass a non-ragged matrix from Java (with dimensions of samples x contigs x maximum count) as a TSV. However, we may run into trouble if we hit a case sample with very high depth. So some sort of spa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522:1159,avoid,avoid,1159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522,2,['avoid'],['avoid']
Safety,"nt context [C*, CT] can you maybe advise whats going on? . java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx30G -jar /run/media/riadh/One Touch1/Analysis/gatk-4.2.4.1/gatk-package-4.2.5.0-local.jar VariantAnnotator -V PE69_chr3.vcf -R /run/media/riadh/One Touch/Reference_data_b38/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta --resource:gnomad /run/media/riadh/One Touch/Reference_data_b38/gnomad.genomes.v3.1.2.sites.chr3.vcf.bgz -E gnomad.nhomalt -E gnomad.ALT -E gnomad.AF -O PE69_ch3_vep_cadd_gnomad.vcf --resource-allele-concordance; 10:58:19.715 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/run/media/riadh/One%20Touch1/Analysis/gatk-4.2.4.1/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 17, 2022 10:58:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:58:19.796 INFO VariantAnnotator - ------------------------------------------------------------; 10:58:19.796 INFO VariantAnnotator - The Genome Analysis Toolkit (GATK) v4.2.5.0; 10:58:19.796 INFO VariantAnnotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:58:19.797 INFO VariantAnnotator - Executing as riadh@ikm-unix-1012.uio.no on Linux v5.16.12-200.fc35.x86_64 amd64; 10:58:19.797 INFO VariantAnnotator - Java runtime: OpenJDK 64-Bit Server VM v11.0.14.1+1; 10:58:19.797 INFO VariantAnnotator - Start Date/Time: March 17, 2022 at 10:58:19 AM CET; 10:58:19.797 INFO VariantAnnotator - ------------------------------------------------------------; 10:58:19.797 INFO VariantAnnotator - ------------------------------------------------------------; 10:58:19.797 INFO VariantAnnotator - HTSJDK Version: 2.24.1; 10:58:19.797 INFO VariantAnnotator - Picard Version: 2.25.4; 10:58:19.798 INFO Varian",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053:1183,detect,detect,1183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053,1,['detect'],['detect']
Safety,"ntBroadcast.scala:293); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); 	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. [Stage 21:> (0 + 60) / 3539]18/12/21 16:08:30 ERROR org.apache.spark.scheduler.TaskSetManager: Task 26 in stage 21.0 failed 4 times; aborting job; 18/12/21 16:08:30 ERROR org.apache.spark.internal.io.SparkHadoopMapReduceWriter: Aborting job job_20181221160412_0054.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 26 in stage 21.0 failed 4 times, most recent failure: Lost task 26.3 in stage 21.0 (TID 2498, readpipeline-w-4.c.broad-gatk-test.internal, executor 21): java.io.IOException: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5545:4744,abort,aborting,4744,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545,1,['abort'],['aborting']
Safety,nternal.tasks.execution.ExecuteActionsTaskExecuter$TaskExecution.execute(ExecuteActionsTaskExecuter.java:237); at org.gradle.internal.execution.steps.ExecuteStep.lambda$execute$1(ExecuteStep.java:33); at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:33); at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:26); at org.gradle.internal.execution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:58); at org.gradle.internal.execution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:35); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:32); at org.gradle.internal.execution.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:8612,Timeout,TimeoutStep,8612,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Timeout'],['TimeoutStep']
Safety,"nts (up from 130 for the 100% tumor alone, as above). Using this joint segmentation for subsequent ModelSegments runs:. For the 100% normal, this yields 88 segments (up from 36):; ![N-SJS modeled](https://user-images.githubusercontent.com/11076296/76632024-ebecb300-6518-11ea-89ff-109c97970ef0.png). For the 100% tumor, this yields 166 segments (up from 130):; ![T-SJS modeled](https://user-images.githubusercontent.com/11076296/76632125-13dc1680-6519-11ea-9901-0c78809d08ba.png). I haven't performed detailed validations, but some spot checking suggests that this actually mitigate oversegmentation while still increasing sensitivity to shared events. For example, there is a small 13-bin deletion in chr19 that is found when running the 100% normal alone, but gets broken up into two adjacent deletions when running the 100% tumor alone (probably just due to statistical noise in the copy ratios). When running jointly, the deletion does not get broken up. However, as discussed over Slack, we should probably run some scenarios with simulated data to check behavior---for example, how robust is the joint segmentation to some of the samples being noisy/oversegmented?. There are lots of options for restructuring the workflow. We could potentially modify ModelSegments to take in the denoised copy ratios from the normal, when available, and add modeling of the normal and germline tagging to that tool. Or we could break things up into separate tools. @fleharty any opinions?. Note that another benefit of using this joint segmentation for germline tagging is that common breakpoints will be shared. This obviates the need for a lot of the idiosyncratic code (in the experimental postprocessing tools) that deals with reconciling segmentations and combining breakpoints. In general, I think such code is extremely prone to off-by-one errors and should be avoided, if possible. See https://github.com/broadinstitute/gatk/pull/5450 for a reminder of some of the remaining issues with that workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598764477:2278,avoid,avoided,2278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598764477,1,['avoid'],['avoided']
Safety,"nts -V INPUT_VCF -R REF -O OUTPUT_VCF. Then, GATK outputs the following:. ```; 18:30:57.468 INFO CNNScoreVariants - Initializing engine; 18:30:57.985 INFO FeatureManager - Using codec VCFCodec to read file ...; 18:30:58.183 INFO IntervalArgumentCollection - Processing 48129895 bp from intervals; 18:30:58.188 INFO CNNScoreVariants - Done initializing engine; 18:31:00.188 INFO CNNScoreVariants - Using key:CNN_1D for CNN architecture:.../1d_cnn_mix_train_full_bn.2560984151625233621.json and weights:.../1d_cnn_mix_train_full_bn.2397909300265264152.hd5; 18:31:19.873 INFO ProgressMeter - Starting traversal; 18:31:19.874 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 18:31:50.095 INFO CNNScoreVariants - Shutting down engine; [2018/04/24 18:31:50 JST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.88 minutes.; Runtime.totalMemory()=2141716480; ***********************************************************************; A USER ERROR has occurred: A timeout ocurred waiting for output from the remote Python command.; ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: A timeout ocurred waiting for output from the remote Python command.; at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:219); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.sendSynchronousCommand(StreamingPythonScriptExecutor.java:153); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:260); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:891); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4696:1349,timeout,timeout,1349,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4696,1,['timeout'],['timeout']
Safety,"o that. Sincerely,; Emily. From: ldgauthier ***@***.***>; Sent: Monday, March 28, 2022 2:39 PM; To: broadinstitute/gatk ***@***.***>; Cc: Emily Elizabeth Puckett (puckett3) ***@***.***>; Mention ***@***.***>; Subject: Re: [broadinstitute/gatk] CombineGVCFs: ERROR input alleles must contain <NON_REF> (Issue #7737). CAUTION: This email originated from outside of the organization. Do not click links or open attachments unless you recognize the sender and trust the content is safe. If I'm reading the process correctly, I don't actually think this should work. CombineGVCFs is specifically for combining GVCFs and it expects GVCFs to have <NON_REF> alleles. If you've already run the data through GenotypeGVCFs then you can't use CombineGVCFs again because the <NON_REF> likelihoods have been applied and those alleles are gone. The vcfcombine tool from bcftools is quite fast if all you want to do is join the samples together. -; Reply to this email directly, view it on GitHub<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fbroadinstitute%2Fgatk%2Fissues%2F7737%23issuecomment-1081062021&data=04%7C01%7CEmily.Puckett%40memphis.edu%7C51db6aa9f41b483e1ce408da10f2aa5d%7Cae145aeacdb2446ab05a7858dde5ddba%7C0%7C0%7C637840931685525269%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=Pxg8joQfE51l5e3cUUbKA9bQEYDZjp0AxdX0aqDG1MY%3D&reserved=0>, or unsubscribe<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FALDFEHAXSKZ7YHSFGISLPUTVCIDGZANCNFSM5RZSK5PA&data=04%7C01%7CEmily.Puckett%40memphis.edu%7C51db6aa9f41b483e1ce408da10f2aa5d%7Cae145aeacdb2446ab05a7858dde5ddba%7C0%7C0%7C637840931685525269%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=6Dkb6rbHDZpS05bYUHhlIRHJitgVtR%2FPB5rNHHFMg%2FQ%3D&reserved=0>.; You are receiving this because you were mentioned.Message ID: ***@***.******@***.***>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7737#issuecomment-1082170127:1022,safe,safelinks,1022,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7737#issuecomment-1082170127,1,['safe'],['safelinks']
Safety,"o update the gvcf file I want to update? what should I do?. ### Command. java -jar $GATK GenomicsDBImport \; -L chr${1} \; -V $File_PATH/4762/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4763/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4764/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4765/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4767/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; --genomicsdb-update-workspace-path $DB_PATH/test_database \; --genomicsdb-shared-posixfs-optimizations true \; --max-num-intervals-to-import-in-parallel 5 \; --reader-threads 10 \; --tmp-dir $Script_PATH/tmp. ### Error message. 10:49:12.018 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/mone/OMICS/Tools/Programs/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 18, 2021 10:49:12 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:12.231 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.232 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:49:12.232 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:12.232 INFO GenomicsDBImport - Executing as chowoo1023@bdcm04 on Linux v3.10.0-514.2.2.el7.x86_64 amd64; 10:49:12.232 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-b12; 10:49:12.232 INFO GenomicsDBImport - Start Date/Time: June 18, 2021 10:49:11 AM KST; 10:49:12.233 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.233 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.233 INFO GenomicsDBImport - HTSJDK Version: 2.23.0; 10:49:12.233 INFO GenomicsDBImport - Picard Version: 2.22.8; 10:49:12.234 INFO GenomicsDBImpor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7324:1488,detect,detect,1488,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324,1,['detect'],['detect']
Safety,"o.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:748); 18/04/24 17:42:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:42:11 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:42:11 INFO BlockManager: BlockManager stopped; 18/04/24 17:42:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:42:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:42:11 INFO SparkContext: Successfully stopped SparkContext; 17:42:11.053 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:42:11 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=866648064; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:39053,abort,aborted,39053,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['abort'],['aborted']
Safety,"o_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:105582-211160 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-105582-211160.vcf.gz; 07:46:18.893 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 07:46:18.944 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 7:46:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 07:46:19.128 INFO GenotypeGVCFs - ------------------------------------------------------------; 07:46:19.128 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 07:46:19.128 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 07:46:19.129 INFO GenotypeGVCFs - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 07:46:19.129 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 07:46:19.129 INFO GenotypeGVCFs - Start Date/Time: August 25, 2021 7:46:18 AM EDT; 07:46:19.129 INFO GenotypeGVCFs - ------------------------------------------------------------; 07:46:19.129 INFO GenotypeGVCFs - ------------------------------------------------------------; 07:46:19.130 INFO GenotypeGVCFs - HTSJDK Version: 2.24.0; 07:46:19.130 INFO GenotypeGVCFs - Picard Version: 2.25.0; 07:46:19.130 INFO GenotypeGVCFs - Built for",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278:1604,detect,detect,1604,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278,1,['detect'],['detect']
Safety,"oadcast_0_piece0 in memory on mpcb006.cm.cluster:46741 (size: 15.5 KB, free: 17.8 GB); 20/10/08 18:35:30 INFO SparkContext: Created broadcast 0 from broadcast at SamSource.java:78; 20/10/08 18:35:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 148.8 KB, free 17.8 GB); 20/10/08 18:35:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 25.4 KB, free 17.8 GB); 20/10/08 18:35:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on mpcb006.cm.cluster:46741 (size: 25.4 KB, free: 17.8 GB); 20/10/08 18:35:30 INFO SparkContext: Created broadcast 1 from newAPIHadoopFile at SamSource.java:108; 20/10/08 18:35:30 INFO BlockManagerInfo: Removed broadcast_1_piece0 on mpcb006.cm.cluster:46741 in memory (size: 25.4 KB, free: 17.8 GB); 20/10/08 18:35:30 INFO BlockManagerInfo: Removed broadcast_0_piece0 on mpcb006.cm.cluster:46741 in memory (size: 15.5 KB, free: 17.8 GB); WARNING	2020-10-08 18:35:30	SamReaderFactory	Unable to detect file format from input URL or stream, assuming SAM format.; WARNING	2020-10-08 18:35:30	SamReaderFactory	Unable to detect file format from input URL or stream, assuming SAM format.; 20/10/08 18:35:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 231.0 KB, free 17.8 GB); 20/10/08 18:35:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.5 KB, free 17.8 GB); 20/10/08 18:35:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on mpcb006.cm.cluster:46741 (size: 15.5 KB, free: 17.8 GB); 20/10/08 18:35:30 INFO SparkContext: Created broadcast 2 from broadcast at SamSource.java:78; 20/10/08 18:35:30 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 148.8 KB, free 17.8 GB); 20/10/08 18:35:30 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 25.4 KB, free 17.8 GB); 20/10/08 18:35:30 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on mpcb00",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6875:6979,detect,detect,6979,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6875,1,['detect'],['detect']
Safety,"ok, thanks. also - the travis build seems to have timed out. from the log i think that's just a travis CI issue, not due to my commit. is this travis timeout problem something you see frequently w/ GATK4?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-384687146:150,timeout,timeout,150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-384687146,1,['timeout'],['timeout']
Safety,ollection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018:13832,abort,abortStage,13832,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018,1,['abort'],['abortStage']
Safety,"ome/platon/Dissertation/Exp/ngs_test/homo_sapiens/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz \; -O /home/platon/Dissertation/Exp/Output; ```. ```; Using GATK jar /home/platon/miniconda3/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/platon/miniconda3/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar CNNScoreVariants -V /dev/stdin -R /home/platon/Dissertation/Exp/ngs_test/homo_sapiens/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz -O /home/platon/Dissertation/Exp/Output; 18:04:27.033 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/platon/miniconda3/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 26, 2020 6:04:27 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:04:27.246 INFO CNNScoreVariants - ------------------------------------------------------------; 18:04:27.246 INFO CNNScoreVariants - The Genome Analysis Toolkit (GATK) v4.1.8.1; 18:04:27.246 INFO CNNScoreVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:04:27.246 INFO CNNScoreVariants - Executing as platon@platon-VivoBook-ASUSLaptop-X712FA-X712FA on Linux v5.4.0-42-generic amd64; 18:04:27.246 INFO CNNScoreVariants - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 18:04:27.247 INFO CNNScoreVariants - Start Date/Time: 26 августа 2020 г. 18:04:27 MSK; 18:04:27.247 INFO CNNScoreVariants - ------------------------------------------------------------; 18:04:27.247 INFO CNNScoreVariants - ------------------------------------------------------------; 18:04:27.247 INFO CNNScoreVariants - HTSJDK Version: 2.23.0; 18:04:27.247 INFO CNNScoreVariants - Picard Version: 2.22.8; 18:04:27.2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6749:1411,detect,detect,1411,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6749,1,['detect'],['detect']
Safety,"oncurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.lang.Thread.run(Thread.java:745); ```; and in case I am missing anything in how I'm calling HaplotypeCallerSpark, here is the full command line we're using:; ```; gatk-launch --java-options '-Xms1000m -Xmx46965m -Djava.io.tmpdir=/mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2018-04-14-195952.723/root/variantcall/2/variantcall_batch_region/3/bcbiotx/tmpno7wyh' HaplotypeCallerSpark --reference /mnt/work/cwl/bcbio_validation_workflows/giab-joint/biodata/collections/hg38/ucsc/hg38.2bit --annotation MappingQualityRankSumTest --annotation MappingQualityZero --annotation QualByDepth --annotation ReadPosRankSumTest --annotation RMSMappingQuality --annotation BaseQualityRankSumTest --annotation FisherStrand --annotation MappingQuality --annotation DepthPerAlleleBySample --annotation Coverage -I /mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2018-04-14-195952.723/root/alignment/1/process_alignment/align/NA24385/NA24385-sort.bam -L /mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2018-04-14-195952.723/root/variantcall/2/variantcall_batch_region/3/gatk-haplotype/chr15/NA24385-chr15_0_101977614-block-regions.bed --interval-set-rule INTERSECTION --spark-master local[16] --conf spark.local.dir=/mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2018-04-14-195952.723/root/variantcall/2/variantcall_batch_region/3/bcbiotx/tmpno7wyh --annotation ClippingRankSumTest --annotation DepthPerSampleHC --emit-ref-confidence GVCF -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 60 -GQB 80 --output /mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2018-04-14-195952.723/root/variantcall/2/variantcall_batch_region/3/bcbiotx/tmpno7wyh/NA24385-chr15_0_101977614-block.vcf; ```; Thanks so much for any clues about how to debug further or avoid the issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4661:8444,avoid,avoid,8444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661,1,['avoid'],['avoid']
Safety,"ontent.com/812850/27811313-9000019c-6097-11e7-82ac-aac557be31db.PNG).; And the program failed eventually:; ```; 18:24:57.885 INFO BwaAndMarkDuplicatesPipelineSpark - Shutting down engine; [July 3, 2017 6:24:57 PM CST] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark done. Elapsed time: 269.29 minutes.; Runtime.totalMemory()=4172283904; org.apache.spark.SparkException: Job aborted due to stage failure: Task 607 in stage 3.0 failed 4 times, most recent failure: Lost task 607.13 in stage 3.0 (TID 14832, 12.9.68.0, executor 24): ExecutorLostFailure (executor 24 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 169939 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312758363:1371,abort,abortStage,1371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312758363,1,['abort'],['abortStage']
Safety,"ools.SAMFormatException: Did not inflate expected amount\] in the other two of the individuals. Please help me! Thank you in advance!. a) GATK version used:. GATK 4.2.0.0. b) Exact command used:. java -jar /home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar \\ ; ; HaplotypeCaller \\ ; ; \-R /media/ngs/NGS0/Database/RefSeq/Homo\_sapiens\_NCBI\_GRCh38Decoy/Homo\_sapiens/NCBI/GRCh38Decoy/Sequence/WholeGenomeFasta/NewIndex/genome.fa \\ ; ; \-I /media/ngs/BAM5T/WGS\_analysis/Data/9\_BQSRBam/Ped-San-3\_merged\_realigned\_bqsr.bam \\ ; ; \-ERC GVCF \\ ; ; \-O /media/ngs/BAM5T/WGS\_analysis/Data/10\_gvcf/Ped-San-3\_merged\_realigned\_bqsr.g.vcf. c) Entire error log:. 14:14:32.075 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Nov 01, 2021 2:14:32 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:14:32.573 INFO HaplotypeCaller - ------------------------------------------------------------ ; ; 14:14:32.573 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 14:14:32.573 INFO HaplotypeCaller - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 14:14:32.573 INFO HaplotypeCaller - Executing as ngs@ngs-linux on Linux v5.8.0-59-generic amd64 ; ; 14:14:32.573 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_292-8u292-b10-0ubuntu1~20.04-b10 ; ; 14:14:32.573 INFO HaplotypeCaller - Start Date/Time: 2021年11月1日 下午02时14分31秒 ; ; 14:14:32.573 INFO HaplotypeCaller - ------------------------------------------------------------ ; ; 14:14:32.573 INFO HaplotypeCaller - ------------------------------------------------------------ ; ; 14:14:32.574 INFO HaplotypeCaller - HTSJDK Version: 2.24.0 ; ; 14:14:32.574 INFO Hap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7582:1728,detect,detect,1728,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582,1,['detect'],['detect']
Safety,ools.spark.pipelines.PrintVariantsSpark done. Elapsed time: 0.34 minutes.; Runtime.totalMemory()=1106771968; org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: com.esotericsoftware.kryo.KryoException: java.lang.NullPointerException; Serialization trace:; genotypes (htsjdk.variant.variantcontext.VariantContext); interval (org.broadinstitute.hellbender.engine.spark.SparkSharder$PartitionLocatable); 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:9892,abort,abortStage,9892,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,1,['abort'],['abortStage']
Safety,"ools/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /technology/software_tools/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar Funcotator --variant P01.mutect2.somatic.filterMutectCalls.indels.vcf.gz --reference /technology/dependent_resource/genome/hsa/ensembl/GRCh37.p13_GATK/genome.fa --ref-version hg19 --data-sources-path /technology/dependent_resource/variation/hg19/funcotator_dataSources.v1.7.20200521s --output P01.mutect2.somatic.filterMutectCalls.indels.funcotator.vcf --output-file-format VCF; 10:25:49.484 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/technology/software_tools/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 09, 2021 10:25:49 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:25:49.665 INFO Funcotator - ------------------------------------------------------------; 10:25:49.665 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.2.0.0; 10:25:49.665 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:25:49.665 INFO Funcotator - Executing as qin_dan@server on Linux v5.4.0-65-generic amd64; 10:25:49.665 INFO Funcotator - Java runtime: OpenJDK 64-Bit Server VM v11.0.10+9-Ubuntu-0ubuntu1.20.04; 10:25:49.666 INFO Funcotator - Start Date/Time: March 9, 2021 at 10:25:49 AM UTC; 10:25:49.666 INFO Funcotator - ------------------------------------------------------------; 10:25:49.666 INFO Funcotator - ------------------------------------------------------------; 10:25:49.666 INFO Funcotator - HTSJDK Version: 2.24.0; 10:25:49.666 INFO Funcotator - Picard Version: 2.25.0; 10:25:49.666 INFO Funcotator - Built for Spark Version: 2.4.5; 10:25:49.666 INFO Fun",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7135:1680,detect,detect,1680,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7135,1,['detect'],['detect']
Safety,or.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:38516,abort,abortStage,38516,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,4,['abort'],['abortStage']
Safety,"ore refinements to the method and have settled on the following procedure:. Assume we have _N_ data points:; ![1](https://user-images.githubusercontent.com/11076296/29580954-bea24562-8745-11e7-9c8c-d68504ba31da.png); To find segments, we:; 1) Select _C<sub>max</sub>_, the maximum number of changepoints to discover. In practice, _C<sub>max</sub> = 100_ per chromosome should more than suffice.; 2) Select a kernel (linear for sensitivity to changes in the distribution mean, Gaussian with a specified variance _σ<sup>2</sup>_ for multimodal data, etc.) and a subsample of _p_ points to approximate it using SVD.; 3) Select window sizes _w<sub>j</sub>_ for which to compute local costs at each point. To be precise, we compute the cost of a changepoint at the point with index _i_, assuming adjacent segments containing the points with indices _[i - w<sub>j</sub> + 1, i]_ and _[i + 1, i + w<sub>j</sub>]_. Selecting a minimum window size and then doubling up to relevant length scales (noting that longer window lengths allow for more subtle changepoints to be detected) works well in practice. For example, here are what the cost functions look like for window sizes of 8, 16, 32, and 64:. ![2](https://user-images.githubusercontent.com/11076296/29582011-210d37b8-8749-11e7-9383-0c657232347e.png). ![3](https://user-images.githubusercontent.com/11076296/29582016-23fbc6a6-8749-11e7-951e-f618e8489a0b.png). ![4](https://user-images.githubusercontent.com/11076296/29582044-3eb20a1e-8749-11e7-84a0-3734bad15e1f.png). ![5](https://user-images.githubusercontent.com/11076296/29582047-410ac490-8749-11e7-8a98-b2098cf1b5ea.png); 4) For each of these cost functions, find (up to) the _C<sub>max</sub>_ most significant local minima. The problem of finding local minima of a noisy function can be solved by using topological persistence (e.g., https://people.mpi-inf.mpg.de/~weinkauf/notes/persistence1d.html and http://www2.iap.fr/users/sousbie/web/html/indexd3dd.html?post/Persistence-and-simplification). ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586:1082,detect,detected,1082,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586,2,['detect'],['detected']
Safety,org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513:7578,abort,abortStage,7578,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513,1,['abort'],['abortStage']
Safety,org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690:7589,abort,abortStage,7589,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690,1,['abort'],['abortStage']
Safety,org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3462:2208,abort,abortStage,2208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3462,1,['abort'],['abortStage']
Safety,"ot sure if this is a bug or something wrong with my bam files. Any help on solving/debugging would be welcomed. Running mutect2 returns >100,000 warnings of more than two reads with the same name found. The bams were processing following best practices. The header of the log file: . Mutect2 -R resources/hg38/genome/d ; 975 efault/genome.fa -L resources/hg38/a.interval_list -I recal/RBL3_diagnostic.bam -I recal/RBL3_germline.bam -I recal/RBL3_diagnostic.bam -I recal/RBL3_relapse1.bam -I recal/RBL ; 976 3_relapse2.bam -I recal/RBL3_PDX.bam -normal RBL3_germline_hg38 --germline-resource resources/hg38/gnomad/af-only-gnomad.hg38.vcf.gz --panel-of-normals resources/hg38/pon/1000 ; 977 g_pon.hg38.vcf.gz --f1r2-tar-gz results/RBL3/f1r2.tar.gz --read-filter NotSupplementaryAlignmentReadFilter --read-filter NotSecondaryAlignmentReadFilter --native-pair-hmm-thre ; 978 ads 20 -O results/RBL3/unfiltered.vcf ; 979 17:07:51.270 WARN GATKReadFilterPluginDescriptor - Redundant enabled filter (NotSecondaryAlignmentReadFilter) is enabled for this tool by default ; 980 17:07:51.313 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/rds/project/rds-cyiwgCzJok8/WES_snakemake/.snakemake/conda/773770bb2edb9f4c58fb17b5017e1f ; 981 be_/share/gatk4-4.5.0.0-0/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so ; 982 17:07:51.633 INFO Mutect2 - ------------------------------------------------------------ ; 983 17:07:51.635 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.5.0.0 ; 984 17:07:51.635 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/ ; 985 17:07:51.635 INFO Mutect2 - Executing as cjs236@cpu-r-25 on Linux v4.18.0-553.16.1.el8_10.x86_64 amd64 ; 986 17:07:51.635 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v17.0.11-internal+0-adhoc..src ; 987 17:07:51.635 INFO Mutect2 - Start Date/Time: August 28, 2024 at 5:07:51 PM BST ; 988 17:07:51.635 INFO Mutect2 - ----------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8966:967,Redund,Redundant,967,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8966,1,['Redund'],['Redundant']
Safety,"otypeGVCF working when only six samples are imported in GenomicsDB; gatk GenotypeGVCFs -R Reference/File_S16_uT_chromosomes.fasta -V gendb://GenomicsDB_wd -O test_chromosome_1_6_samples.vcf; Using GATK jar /data/xxxxx/miniconda3/share/gatk4-4.1.6.0-0/gatk-package-4.1.6.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /data/xxxxx/miniconda3/share/gatk4-4.1.6.0-0/gatk-package-4.1.6.0-local.jar GenotypeGVCFs -R Reference/File_S16_Tetranychus_urticae_3_chromosomes.fasta -V gendb://GenomicsDB_wd -O test_chromosome_1_6_samples.vcf; 21:14:37.817 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xxxxx/miniconda3/share/gatk4-4.1.6.0-0/gatk-package-4.1.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 26, 2020 9:14:38 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:14:38.245 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:14:38.246 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.6.0; 21:14:38.246 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:14:38.246 INFO GenotypeGVCFs - Executing as xxxxx@galaxy on Linux v4.4.0-133-generic amd64; 21:14:38.246 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 21:14:38.246 INFO GenotypeGVCFs - Start Date/Time: 26 May 2020 21:14:37 CEST; 21:14:38.246 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:14:38.246 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:14:38.247 INFO GenotypeGVCFs - HTSJDK Version: 2.21.2; 21:14:38.247 INFO GenotypeGVCFs - Picard Version: 2.21.9; 21:14:38.247 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 21:14:3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6616:13767,detect,detect,13767,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6616,1,['detect'],['detect']
Safety,"park web UI at http://cm132:4040** ; **20/03/05 09:28:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!** ; **20/03/05 09:28:58 INFO NewHadoopRDD: Input split: file:/clinix1/Analysis/mongol/phenomata/04.GC\_CC/01.Alignment/Aligned/17039\_N.bam:1342177280+33554432** ; **20/03/05 09:28:58 INFO MemoryStore: MemoryStore cleared** ; **20/03/05 09:28:58 INFO BlockManager: BlockManager stopped** ; **20/03/05 09:28:58 INFO BlockManagerMaster: BlockManagerMaster stopped** ; **20/03/05 09:28:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!** ; **20/03/05 09:28:58 INFO SparkContext: Successfully stopped SparkContext** ; **09:28:58.889 INFO PathSeqPipelineSpark - Shutting down engine** ; **[2020년 3월 5일 (목) 오전 9시 28분 58초] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 1.25 minutes.** ; **Runtime.totalMemory()=19560660992** ; **org.apache.spark.SparkException: Job aborted due to stage failure: Task 34 in stage 0.0 failed 1 times, most recent failure: Lost task 34.0 in stage 0.0 (TID 34, localhost, executor driver): com.esotericsoftware.kryo.KryoException: Buffer underflow.** ; **at com.esotericsoftware.kryo.io.Input.require(Input.java:199)** ; **at com.esotericsoftware.kryo.io.Input.readLong(Input.java:686)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LongHopscotchSet.<init>(LongHopscotchSet.java:83)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LongHopscotchSet$Serializer.read(LongHopscotchSet.java:527)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LongHopscotchSet$Serializer.read(LongHopscotchSet.java:519)** ; **at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:712)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LargeLongHopscotchSet.<init>(LargeLongHopscotchSet.java:55)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LargeLongHopscotchSet$Serializer.read(LargeLongHopscotchSet.java:172)** ; **at org.b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:42440,abort,aborted,42440,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['abort'],['aborted']
Safety,park.pipelines.PrintVariantsSpark done. Elapsed time: 0.43 minutes.; Runtime.totalMemory()=823132160; org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; Serialization trace:; genotypes (org.seqdoop.hadoop_bam.VariantContextWithHeader); interval (org.broadinstitute.hellbender.engine.spark.SparkSharder$PartitionLocatable); 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840:9035,abort,abortStage,9035,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840,1,['abort'],['abortStage']
Safety,park.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:38613,abort,abortStage,38613,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,4,['abort'],['abortStage']
Safety,"park.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). 16/11/16 23:25:11 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Cancelling stage 1; 16/11/16 23:25:11 INFO DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) failed in 0.276 s; 16/11/16 23:25:11 INFO DAGScheduler: Job 0 failed: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 1.029776 s; 16/11/16 23:25:11 INFO SparkContext: SparkContext already stopped.; [November 16, 2016 11:25:11 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2058354688; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:16269,abort,aborting,16269,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['abort'],['aborting']
Safety,partially addressed https://github.com/broadinstitute/gatk/issues/1427 (let's do this in a few steps to minimize risk). @droazen have a look; (will remove them from protected when this is merged and we're reved up),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1888:113,risk,risk,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1888,1,['risk'],['risk']
Safety,peline.writeAndFlush(DefaultChannelPipeline.java:954); at io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:244); at org.apache.spark.network.server.TransportRequestHandler.respond(TransportRequestHandler.java:138); at org.apache.spark.network.server.TransportRequestHandler.processFetchRequest(TransportRequestHandler.java:110); at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:85); at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:101); at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51); at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846); at io.nett,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1491:3680,timeout,timeout,3680,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491,1,['timeout'],['timeout']
Safety,"plied_ to the reference to generate the SV genome. Paired-end reads w/ equal lengths (100bp) and insert sizes (500bp) were _uniformly_ generated from the SV genome and was mapped to chr22 using BWA-MEM (default arguments). Coverage on 100bp uniform bins were collected using `CollectFragmentCounts` (default arguments: MQ > 30, both mates aligned, and only innies). The coverage was studied case-by-case on a few SVs. **Case-by-base study**:; ; _Balanced translocation:_; <img width=""1440"" alt=""baltr-1-igv"" src=""https://user-images.githubusercontent.com/15305869/37737056-0a811466-2d29-11e8-84ef-4f31f030d05b.png"">. Here, an event in shown where a ~ 3kb region of chr22 is translocated to another region. Ideally, there should be no coverage loss. The IGV inspection shows excess coverage on the left side and depletion on the right side. Upon inspecting the conjugate translocation site, a similar scenario is seen. This situation is hardly avoidable -- depending on the mappability of the two loci, one captures the chimeric fragment of the other with higher probability (right?). The situation is worse for `CollectFragmentCounts` because chimeras are ignored altogether:. ![baltr-1](https://user-images.githubusercontent.com/15305869/37738066-2bb27a5a-2d2c-11e8-905d-ea553b93741b.png). _Deletion:_; For deletions, both coverage collection strategies work well. The deletion region is not quite captured perfectly by either method. <img width=""1440"" alt=""del-1-igv"" src=""https://user-images.githubusercontent.com/15305869/37738404-48dec038-2d2d-11e8-9d7e-625ff8e453e7.png"">. ![del-1](https://user-images.githubusercontent.com/15305869/37739544-ceeb7376-2d30-11e8-9480-3fb2a408e48a.png). _Tandem Duplication_:; For tandem duplications, neglecting FF and RR fagments leads to an underestimation of the size of the duplicated region by `CollectFragmentCounts`. IGV does not seem to get it quite right either (@cwhelan does the IGV plot make sense to you? could it be there's a bug in SVGen in genera",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4551:1413,avoid,avoidable,1413,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4551,1,['avoid'],['avoidable']
Safety,"plotypes rather than alleles at one locus. Unresolved difficulties with this include:. - BQD and FRD are defined with respect to one particular variant position. How would we define them for a haplotype that has no particular locus?; - BQD involves the base qualities at one particular variant locus, how would this be defined for an entire haplotype?; - The above is especially thorny for haplotypes that exhibit multiple variants.; - The FRD prior is only defined for individual events, not haplotypes.; - The BQD and FRD models use reads that overlap a variant site, but it is not clear how to use reads that only partially intersect a haplotype.; - BQD and FRD likelihoods are only defined for homozygous haplotypes, but heterozygous combinations of _haplotypes_ contribute to homozygous genotypes all loci where the distinct haplotypes agree. Clearly, generalizing BQD and FRD to entire haplotypes is not straightforward. Nor does it suffice to produce ""raw"" genotype likelihoods using the joint detection approach and then apply BQD and FRD on variant loci afterwards. Some difficulties with this include:. - BQD and FRD require the read-allele likelihoods matrix. Where are these likelihoods supposed to come from? The pre-joint-detection unrigorous ""marginalization"" where to each allele we assign the maximum likelihood over all haplotypes supporting that allele? Some read-allele likelihoods matrix derived from the read-haplotype likelihoods matrix?; - The drawbacks of the faulty ""marginalization"" actually become more severe with joint detection since genotyping multiple alleles together in a single determined span produces more haplotypes, which in turn increases the risk of the read-allele likelihoods cherry-picking from too many different haplotypes for different reads.; - The BQD and FRD models produce likelihoods on an absolute scale that is only meaningful relative to genotyping likelihoods from the pre-joint-detection approach. They do not inherently ""play nicely"" with th",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8616:1565,detect,detection,1565,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8616,1,['detect'],['detection']
Safety,"pool/src/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx1600g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /home/dan_vanderpool/src/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar GenotypeGVCFs -R /home/dan_vanderpool/Wolf_raw_reads/Wolf_genome/GCA_905319855.2_mCanLor1.2_genomic.fa -V gendb://Wolf_Genome_Variantsdb -O All_Wolf_Samples_Joint_Genotypes_Raw.vcf.gz -L /scratch/dan/Wolf_reads_raw/Wolf_GenCov300_Q20_Merged.interval_list -imr ALL --genomicsdb-max-alternate-alleles 10 --max-alternate-alleles 6; 17:49:29.781 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/dan_vanderpool/src/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 23, 2022 5:49:30 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:49:30.164 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:30.165 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.5.0; 17:49:30.165 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:49:30.165 INFO GenotypeGVCFs - Executing as dan_vanderpool@0e07622619ad on Linux v4.4.0-210-generic amd64; 17:49:30.165 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 17:49:30.166 INFO GenotypeGVCFs - Start Date/Time: February 23, 2022 at 5:49:29 PM UTC; 17:49:30.166 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:30.166 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:30.167 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 17:49:30.167 INFO GenotypeGVCFs - Picard Version: 2.25.4; 17:49:30.167 INFO GenotypeGVCFs - Built for Spark Version: ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1049112454:4475,detect,detect,4475,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1049112454,1,['detect'],['detect']
Safety,pply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **Driver stacktrace:** ; **at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)** ; **at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)** ; **at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)** ; **at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)** ; **at scala.Option.foreach(Option.scala:257)** ; **at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)** ; **at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)** ; **at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)** ; **at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)** ; **at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)** ; **at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)** ; **at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)** ; **at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)** ; **at org.apach,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:46280,abort,abortStage,46280,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['abort'],['abortStage']
Safety,"ptions.defaultCredentials(ServiceOptions.java:304); at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:278); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:83); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:31); at com.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:78); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.setGlobalNIODefaultOptions(BucketUtils.java:382); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:183); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289). Nov 24, 2018 6:05:09 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.NoRouteToHostException: No route to host (Host unreachable); at java.net.PlainSocketImpl.socketConnect(Native Method); at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); at java.net.Socket.connect(Socket.java:589); at sun.net.NetworkClient.doConnect(NetworkClient.java:175); at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); at sun.net.www.http.HttpClient.New(HttpClient.java:339); at sun.net.www.http.HttpClient.New(HttpClient.java:357); at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441873417:3680,detect,detect,3680,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441873417,2,['detect'],['detect']
Safety,"r appears to be related to the reblocking of the gvcfs. ```; gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:105582-211160 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-105582-211160.vcf.gz; 07:46:18.893 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 07:46:18.944 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 7:46:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 07:46:19.128 INFO GenotypeGVCFs - ------------------------------------------------------------; 07:46:19.128 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 07:46:19.128 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 07:46:19.129 INFO GenotypeGVCFs - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 07:46:19.129 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 07:46:19.129 INFO GenotypeGVCFs - Start Date/Time",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278:1184,Redund,Redundant,1184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278,1,['Redund'],['Redundant']
Safety,"r dangling heads yet it seems to have other problems downstream when selecting or pruning haplotypes:. ```; https://www.pivotaltracker.com/story/show/67601310; ```. B. Low support chain pruning might not be longer needed. Now we have a newer approach to select best haplotypes that can handle complex graph we might well not need to prune low supported hap early as they seemly they won't be selected if the are not amongst the best haplotypes. . B.1 Now that still would produce a considerable number of unlikely haplotypes that would cause a CPU burden. That can be changed by imposing another kinds of limit, For example we include all haplotypes with scores (likelihoods) that are Q0 - Q40 or we include haplotypes until the sum of their likelihoods is larger than the 99.99% probability mass. . B.2 This could provide a downstream solution to the problem caused by ranging heads recovery (explained above in A.2). B.3 If pruning is to be maintained, it makes more sense to do it at the very end after all dangling ends hav been recovered and the edges supports are finalized. Of course I assuming here that dangling end recovery does the sensible think of updating those supports are the graphs is modified. C. The use of Smith-Waterman in dangling end recovery does not seem totally optimal or even needed. . C.1 Recovering tails quite often this finish with the same sequence as the reference path because in fact they are supposed to end like that by construction (reads are trimmed by AR coordinates). For example, this can be cause because due to the k-mer size there is not enough based after variation for the paths to merge back. In this case you can simply merge the last vertices of the tail and the reference, faster and potentially more accurate. . C.2 Similarly dangling heads, at least part of the sequence of those dangling heads are clearly threadable back into the graph without the need of SW. For example look at the AA…AAAAAGA sequence in the picture below. . C.3 PairHMM runs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/264:1810,recover,recovered,1810,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/264,1,['recover'],['recovered']
Safety,"r message. It silently fails. BTW, I'm dealing with WES data. This is the code I used:; # For GenomicDBImport, I randomly select 50 samples from our history samples(using the same probe set) along with the current batch.; time ${gatk} --java-options ""-Xmx8g -Xms2g"" GenomicsDBImport \; --tmp-dir /paedyl01/disk1/yangyxt/test_tmp \; --genomicsdb-update-workspace-path ${vcf_dir}/genomicdbimport_chr${1} \; -R ${ref_gen}/ucsc.hg19.fasta \; --batch-size 0 \; --sample-name-map ${gvcf}/batch_cohort.sample_map \; --reader-threads 5; check_return_code. # For GenotypeGVCFs; time ${gatk} --java-options ""-Xmx8g -Xms2g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" GenotypeGVCFs \; -R ${ref_gen}/ucsc.hg19.fasta \; -V gendb://${vcf_dir}/genomicdbimport_chr${1} \; -G StandardAnnotation \; -G AS_StandardAnnotation \; -L chr${1} \; -O ${bgvcf}/all_${seq_type}_samples_plus_${sample_batch}.chr${1}.HC.vcf. # These are log records:; 02:07:51.286 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 02:07:51.321 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/yangyxt/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl; Nov 06, 2020 2:07:56 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 02:07:56.529 INFO GenotypeGVCFs - ------------------------------------------------------------; 02:07:56.529 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 02:07:56.530 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 02:08:01.543 INFO GenotypeGVCFs - Executing as yangyxt@paedyl01 on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 02:08:01.543 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 02:08:01.543 INFO GenotypeGVCFs - Start Date/Time: November 6, 2020 2:07:51",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-722764059:1124,Redund,Redundant,1124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-722764059,1,['Redund'],['Redundant']
Safety,r one-way message.; org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160); at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140); at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655); at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:208); at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113); at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340); at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340); at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340); at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerCon,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:43996,timeout,timeout,43996,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['timeout'],['timeout']
Safety,"r while running GATK.; It is hard for me to say what exactly is wrong, and extensive searching has not been helpul. Thanks in advance!. ```; gatk ValidateVariants -V ../../data/geno/phased/chr1-22.phased.rename.reheader.vcf.gz -R ../../../../index/hg19.fa.gz; Using GATK jar ~/tools/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/zepengmu/tools/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar ValidateVariants -V ../../data/geno/phased/chr1-22.phased.rename.reheader.vcf.gz -R ../../../../index/hg19.fa.gz; 19:53:34.379 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/zepengmu/tools/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 25, 2020 7:53:34 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:53:34.606 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.606 INFO ValidateVariants - The Genome Analysis Toolkit (GATK) v4.1.8.0; 19:53:34.606 INFO ValidateVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:53:34.607 INFO ValidateVariants - Executing as zepengmu@midway2-login1.rcc.local on Linux v3.10.0-1127.8.2.el7.x86_64 amd64; 19:53:34.607 INFO ValidateVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14; 19:53:34.607 INFO ValidateVariants - Start Date/Time: October 25, 2020 7:53:34 PM CDT; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - HTSJDK Version: 2.22.0; 19:53:34.607 INFO ValidateVariants - Picard Version: 2.22.8; 19:53:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911:1041,detect,detect,1041,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911,1,['detect'],['detect']
Safety,"r-round 50 --log-emission-sampling-median-rel-error 0.005 --log-emission-sampling-rounds 10 --max-advi-iter-first-epoch 5000 --max-advi-iter-subsequent-epochs 100 --min-training-epochs 10 --max-training-epochs 100 --initial-temperature 2.0 --num-thermal-advi-iters 2500 --convergence-snr-averaging-window 500 --convergence-snr-trigger-threshold 0.1 --convergence-snr-countdown-window 10 --max-calling-iters 10 --caller-update-convergence-threshold 0.001 --caller-internal-admixing-rate 0.75 --caller-external-admixing-rate 1.00 --disable-annealing false. [2019-02-22 23:49:20,42] [info] WorkflowManagerActor WorkflowActor-098a389e-b298-4324-8a8c-9f46f05708b5 is in a terminal state: WorkflowFailedState; [2019-02-22 23:50:01,65] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-02-22 23:50:02,38] [info] Workflow polling stopped; [2019-02-22 23:50:02,48] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-22 23:50:02,49] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Aborting all running workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting dow",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:30715,Timeout,Timeout,30715,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,4,"['Abort', 'Timeout']","['Aborting', 'Timeout']"
Safety,r.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607); at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:31674,abort,abortStage,31674,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['abort'],['abortStage']
Safety,"r.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /home/glier_ubuntu/gatk-4.1.1.0/gatk-package-4.1.1.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -jar /home/glier_ubuntu/gatk-4.1.1.0/gatk-package-4.1.1.0-local.jar VariantsToTable -V /media/glier_ubuntu/4TB/Javad_Final/6bwa/2/filtered_merged_firstpass.vcf.gz -O /media/glier_ubuntu/4TB/Javad_Final/6bwa/2/filtered_merged_firstpass_table.txt -F CHROM -F POS -F QD -F MQ -F FS -F SOR -F MQRankSum -F ReadPosRankSum -jdk-inflater -jdk-deflater; May 13, 2020 3:47:25 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:47:25.643 INFO VariantsToTable - ------------------------------------------------------------; 15:47:25.644 INFO VariantsToTable - The Genome Analysis Toolkit (GATK) v4.1.1.0; 15:47:25.644 INFO VariantsToTable - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:47:25.644 INFO VariantsToTable - Executing as glier_ubuntu@glierubuntu-Precision-7920-Tower on Linux v4.15.0-99-generic amd64; 15:47:25.644 INFO VariantsToTable - Java runtime: OpenJDK 64-Bit Server VM v11.0.7+10-post-Ubuntu-2ubuntu218.04; 15:47:25.644 INFO VariantsToTable - Start Date/Time: May 13, 2020 at 3:47:24 p.m. EDT; 15:47:25.644 INFO VariantsToTable - ------------------------------------------------------------; 15:47:25.644 INFO VariantsToTable - ------------------------------------------------------------; 15:47:25.645 INFO VariantsToTable - HTSJDK Version: 2.19.0; 15:47:25.646 INFO VariantsToTable - Picard Version: 2.19.0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6604:9116,detect,detect,9116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6604,1,['detect'],['detect']
Safety,r.scala:408); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:8633,abort,abortStage,8633,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['abort'],['abortStage']
Safety,rM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine - Active Region chrM:3703-3943; 11:36:16.348 DEBUG Mutect2Engine - Extended Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:10983,Recover,Recovered,10983,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Safety,"ractJavaRDDLike.count(JavaRDDLike.scala:45); 	at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 18/05/01 14:30:35 WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:09 WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout, java.util.concurrent.TimeoutException; java.util.concurrent.TimeoutException; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67); 18/05/01 14:31:10 INFO ShutdownHookManager: Shutdown hook called; 18/05/01 14:31:15 INFO ShutdownHookManager: Deleting directory /tmp/abd30/spark-3f28d2e3-59d7-40f9-bba3-42d61eff6c6a; 18/05/01 14:31:20 ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; Using GATK jar /gpfs/fs0/home/abd30/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4725:5012,Timeout,TimeoutException,5012,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725,1,['Timeout'],['TimeoutException']
Safety,rator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:8535,abort,abortStage,8535,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['abort'],['abortStage']
Safety,re-assembly-failure-bam false --num-matching-bases-in-dangling-end-to-recover -1 --error-; correction-log-odds -Infinity --error-correct-reads false --kmer-length-for-read-error-correction 25 --min-observations-for-kmer-to-be-solid 20 --likelihood-calculation-engine PairHMM --ba; se-quality-score-threshold 18 --dragstr-het-hom-ratio 2 --dont-use-dragstr-pair-hmm-scores false --pair-hmm-gap-continuation-penalty 10 --expected-mismatch-rate-for-read-disqualification 0; .02 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --disable-symmetric-hmm-normalizing false --disable-cap-base-qu; alities-to-map-quality false --enable-dynamic-read-disqualification-for-genotyping false --dynamic-read-disqualification-threshold 1.0 --native-pair-hmm-threads 4 --native-pair-hmm-use-dou; ble-precision false --flow-hmm-engine-min-indel-adjust 6 --flow-hmm-engine-flat-insertion-penatly 45 --flow-hmm-engine-flat-deletion-penatly 45 --pileup-detection false --pileup-detection-; enable-indel-pileup-calling false --num-artificial-haplotypes-to-add-per-allele 5 --artifical-haplotype-filtering-kmer-size 10 --pileup-detection-snp-alt-threshold 0.1 --pileup-detection-i; ndel-alt-threshold 0.5 --pileup-detection-absolute-alt-depth 0.0 --pileup-detection-snp-adjacent-to-assembled-indel-range 5 --pileup-detection-bad-read-tolerance 0.0 --pileup-detection-pro; per-pair-read-badness true --pileup-detection-edit-distance-read-badness-threshold 0.08 --pileup-detection-chimeric-read-badness true --pileup-detection-template-mean-badness-threshold 0.0; --pileup-detection-template-std-badness-threshold 0.0 --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --override-fragment-softclip-check false --min-base-quality-s; core 10 --smith-waterman JAVA --max-mnp-distance 0 --force-call-filtered-alleles false --reference-model-deletion-quality 30 --soft-clip-low-quality-ends false --allele-informative-reads-o; verlap-marg,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:6563,detect,detection,6563,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,3,['detect'],"['detection', 'detection-snp-alt-threshold']"
Safety,readingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 dangling heads; 11:55:47.787 DEBUG Mutect2Engine - Active Region chrM:9302-9584; 11:55:47.792 DEBUG Mutect2Engine - Extended Act Region chrM:9202-9684; 11:55:47.796 DEBUG Mutect2Engine - Ref haplotype coords chrM:9202-9684; 11:55:47.800 DEBUG Mutect2Engine - Haplotype count 128; 11:55:47.803 DEBUG Mutect2Engine - Kmer sizes count 0; 11:55:47.807 DEBUG Mutect2Engine - Kmer sizes values []; 12:05:48.002 DEBUG Mutect2 - Processing assembly region at chrM:9585-9884 isActive: false numReads: 125080; 12:05:51.435 DEBUG Mutect2 - Processing assembly region at chrM:9885-10184 isActive: false numReads: 0; 12:05:51.448 DEBUG Mutect2 - Processing assembly region at chrM:10185-10484 isActive: false numReads: 0; 12:05:51.460 INFO ProgressMeter - chrM:10185 30.2 40 1.3; 12:05:51.465 DEBUG Mutect2 - Processing assembly region at chrM:10485-10784 isActive: false numReads: 0; 12:05:51.476 DEBUG Mutect2 - Processing as,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:17251,Recover,Recovered,17251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Safety,"reated broadcast 0 from broadcast at SamSource.java:78; 20/10/08 18:35:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 148.8 KB, free 17.8 GB); 20/10/08 18:35:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 25.4 KB, free 17.8 GB); 20/10/08 18:35:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on mpcb006.cm.cluster:46741 (size: 25.4 KB, free: 17.8 GB); 20/10/08 18:35:30 INFO SparkContext: Created broadcast 1 from newAPIHadoopFile at SamSource.java:108; 20/10/08 18:35:30 INFO BlockManagerInfo: Removed broadcast_1_piece0 on mpcb006.cm.cluster:46741 in memory (size: 25.4 KB, free: 17.8 GB); 20/10/08 18:35:30 INFO BlockManagerInfo: Removed broadcast_0_piece0 on mpcb006.cm.cluster:46741 in memory (size: 15.5 KB, free: 17.8 GB); WARNING	2020-10-08 18:35:30	SamReaderFactory	Unable to detect file format from input URL or stream, assuming SAM format.; WARNING	2020-10-08 18:35:30	SamReaderFactory	Unable to detect file format from input URL or stream, assuming SAM format.; 20/10/08 18:35:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 231.0 KB, free 17.8 GB); 20/10/08 18:35:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.5 KB, free 17.8 GB); 20/10/08 18:35:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on mpcb006.cm.cluster:46741 (size: 15.5 KB, free: 17.8 GB); 20/10/08 18:35:30 INFO SparkContext: Created broadcast 2 from broadcast at SamSource.java:78; 20/10/08 18:35:30 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 148.8 KB, free 17.8 GB); 20/10/08 18:35:30 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 25.4 KB, free 17.8 GB); 20/10/08 18:35:30 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on mpcb006.cm.cluster:46741 (size: 25.4 KB, free: 17.8 GB); 20/10/08 18:35:30 INFO SparkContext: Created broadcast 3 from newAPIHadoo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6875:7101,detect,detect,7101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6875,1,['detect'],['detect']
Safety,"reference sequence dictionary. We discussed an improvement with either the filter for this problem or the error message. Complete stack trace:; ```; Using GATK jar GATK/4.1.8.1-GCCcore-9.3.0-Java-1.8/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16g -jar /GATK/4.1.8.1-GCCcore-9.3.0-Java-1.8/gatk-package-4.1.8.1-local.jar Mutect2 -R ref/Homo_sapiens_assembly38.fasta -I SRR_MM10_2pass_recal.bam --germline-resource /af-only-gnomad.hg38.vcf.gz --panel-of-normals ref/1000g_pon.hg38.vcf.gz -O SRR_somatic_mutect.vcf.gz; 13:24:08.400 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/GATK/4.1.8.1-GCCcore-9.3.0-Java-1.8/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Sep 25, 2020 1:24:08 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:24:08.556 INFO Mutect2 - ------------------------------------------------------------; 13:24:08.557 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.8.1; 13:24:08.557 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:24:08.557 INFO Mutect2 - Executing as xxx on Linux v3.10.0-1127.18.2.el7.x86_64 amd64; 13:24:08.557 INFO Mutect2 - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_192-b12; 13:24:08.557 INFO Mutect2 - Start Date/Time: September 25, 2020 1:24:08 PM BST; 13:24:08.557 INFO Mutect2 - ------------------------------------------------------------; 13:24:08.557 INFO Mutect2 - ------------------------------------------------------------; 13:24:08.557 INFO Mutect2 - HTSJDK Version: 2.23.0; 13:24:08.557 INFO Mutect2 - Picard Version: 2.22.8; 13:24:08.558 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 13:24:08.558 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_I",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6851:1302,detect,detect,1302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6851,1,['detect'],['detect']
Safety,"related to issue #211 and #233 - in CreateVariantIngestFiles, when writing missing positions to the pet tsv, we were looping through blocks of missing intervals twice, once to construct the pet rows, holding them all in memory, and again to write the pet rows. in this PR, we merge those steps into one: for each missing block, we write the pet lines to file as we loop through, avoiding the need to hold large blocks in memory. note that this does not resolve the memory issues that originally prompted issues #211 and #233, but it's nonetheless a minor improvement that helps immensely when there are large numbers of missing locations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7098:379,avoid,avoiding,379,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7098,1,['avoid'],['avoiding']
Safety,remove bad test that was causing tests to abort,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2262:42,abort,abort,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2262,1,['abort'],['abort']
Safety,remove code redundancy AS_ReadPosRankSumTest vs ReadPosRankSumTest,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1882:12,redund,redundancy,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1882,1,['redund'],['redundancy']
Safety,"removing redundant builds:; we will now have:; openJDK builds for cloud, integration, and unit tests; docker builds for integration and unit tests; an oracleJDK build for integration tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2770:9,redund,redundant,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2770,1,['redund'],['redundant']
Safety,removing the non-docker unit and integration test matrix entries because; they were redundant with the docker ones,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2804:84,redund,redundant,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2804,1,['redund'],['redundant']
Safety,removing the redundant command line parse exception,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/197:13,redund,redundant,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/197,1,['redund'],['redundant']
Safety,removing two redundant travis matrix entries to save vms,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2804:13,redund,redundant,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2804,1,['redund'],['redundant']
Safety,renceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4458:8351,abort,abortStage,8351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458,1,['abort'],['abortStage']
Safety,rg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.ap,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:10890,abort,abortStage,10890,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['abort'],['abortStage']
Safety,"rg.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002); at ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4515:3752,abort,abortStage,3752,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515,1,['abort'],['abortStage']
Safety,rging=ALL interval_padding=0 reference_sequence=human_g1k_v37.fasta nonDeterministicRandomSeed=false disableDithering=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 baq=OFF baqGapOpenPenalty=40.0 refactor_NDN_cigar_string=false fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false useOriginalQualities=false defaultBaseQualities=-1 performanceLog=null BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 globalQScorePrior=-1.0 validation_strictness=SILENT remove_program_records=false keep_program_records=false sample_rename_mapping_file=null unsafe=null disable_auto_index_creation_and_locking_when_reading_rods=false no_cmdline_in_header=false sites_only=false never_trim_vcf_format_field=false bcf=false bam_compression=null simplifyBAM=false disable_bam_indexing=false generate_md5=false num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false variant_index_type=DYNAMIC_SEEK variant_index_parameter=-1 logging_level=INFO log_to_file=null help=false version=false variant=(RodBinding name=variant source=dbsnp_138.b37.vcf) discordance=(RodBinding name= source=UNBOUND) concordance=(RodBinding name= source=UNBOUND) out=/home/unix/droazen/bundle_snippets/dbsnp_138.b37.20.21.vcf sample_name=[] sample_expressions=null sample_file=null exclude_sample_name=[] exclude_sample_file=[] exclude_sample_expressions=[] selectexpressions=[] invertselect=false excludeNonVariants=false excludeFiltered=false preserveAlleles=false removeUnusedAlternates=false restrictAllelesTo=ALL keepOriginalAC=false keepOriginalDP=false mendelianViolation=false invertMendelianViolation=false mendelianViolationQualThreshold=0.0 selec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2269:1385,unsafe,unsafe,1385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2269,1,['unsafe'],['unsafe']
Safety,"riantReview/inversion/chm) of a callset generated a long time ago (but still useful for studying filtering inversion breakpoints), and is designed to be integrated with the experimental code path. ### proposed algo. #### input:; * the ""INV55/INV33""-annotated `BND` records output by the upstream experimental code path; * BND's have related concepts of `MATE` and `PARTNER` (see figure below, left); * `MATE`: novel adjacency, i.e. contiguity on sample that is absent on reference (e.g. mobile element insertions, deletions); * `PARTNER`: novel disruption, i.e. contiguity on reference disrupted on sample (e.g. insertions, deletions). ![inversion_demo](https://user-images.githubusercontent.com/16310888/40271739-6d999b30-5b6f-11e8-86db-78fa11db4305.png). * complex variants detected by the upstream experimental code path; the reason is that sometimes inversion calls are incorporated as part of a larger, more complex event and the logic implemented in the upstream code, theoretically, allows for arbitrarily complex rearrangement; shown above on the right is a table of inversion calls (TP/FP 12/7 using PacBio calls on CHM-1 & 13 cell lines as truth) extracted from the `<CPX>` calls, they were extracted by the tool proposed in PR #4602. #### stages:. * Primitive filter on breakpoints ; * low MQ of assembly contigs' mappings that evidenced the BND records, ; * suspiciously large distance between mates (mate pairs whose distance are over $10^5$bp (~1/3 of input, see blelow) are more likely to be artifact or dispersed/segmental duplications). <p align=""center""><img src=""https://user-images.githubusercontent.com/16310888/40271740-6daa2b9e-5b6f-11e8-9dbb-89085450db6d.png"" width=""420"" height=""420"" ></p>; * if overlaps with CPX (supposedly they should be captured already, or is more complex than what can be comprehended by the logic proposed here). The mates are then converted to intervals bounded by the mates' locations. These ""normal sized"" variants are sent down for further analysi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4789:2057,detect,detected,2057,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789,1,['detect'],['detected']
Safety,"ribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 4G --num-executors 4 --executor-cores 6 --executor-memory 16G --conf spark.dynamicAllocation.enabled=false /opt/NfsDir/BioDir/GATK4/gatk/build/libs/gatk-package-4.beta.5-50-g8d666b6-SNAPSHOT-spark.jar BwaAndMarkDuplicatesPipelineSpark --bwamemIndexImage hdfs:///user/sun/ucsc.hg19.fasta.img -I hdfs:///user/sun/1982.unmapped.bam -R hdfs:///user/sun/ucsc.hg19.fasta -O hdfs:///user/sun/17F02897_17F02897M_WES_img.bwa.bam --sparkMaster yarn; WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2) overrides detected (/opt/cloudera/parcels/SPARK2/lib/spark2).; WARNING: Running spark-class from user-defined location.; 18:30:33.354 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:30:33.534 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/NfsDir/BioDir/GATK4/gatk/build/libs/gatk-package-4.beta.5-50-g8d666b6-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [January 9, 2018 6:30:33 PM CST] BwaAndMarkDuplicatesPipelineSpark --bwamemIndexImage hdfs:///user/sun/ucsc.hg19.fasta.img --output hdfs:///user/sun/17F02897_17F02897M_WES_img.bwa.bam --reference hdfs:///user/sun/ucsc.hg19.fasta --input hdfs:///user/sun/1982.unmapped.bam --sparkMaster yarn --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:2643,detect,detected,2643,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['detect'],['detected']
Safety,"rk web interface showed errors in `sortByKey` steps:; ![sparkjob](https://user-images.githubusercontent.com/812850/27811313-9000019c-6097-11e7-82ac-aac557be31db.PNG).; And the program failed eventually:; ```; 18:24:57.885 INFO BwaAndMarkDuplicatesPipelineSpark - Shutting down engine; [July 3, 2017 6:24:57 PM CST] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark done. Elapsed time: 269.29 minutes.; Runtime.totalMemory()=4172283904; org.apache.spark.SparkException: Job aborted due to stage failure: Task 607 in stage 3.0 failed 4 times, most recent failure: Lost task 607.13 in stage 3.0 (TID 14832, 12.9.68.0, executor 24): ExecutorLostFailure (executor 24 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 169939 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312758363:1274,abort,abortStage,1274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312758363,1,['abort'],['abortStage']
Safety,"rk.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). 18/07/24 21:02:27 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 0.0 failed 4 times; aborting job; 18/07/24 21:02:27 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 21:02:27.703 INFO PrintReadsSpark - Shutting down engine; [July 24, 2018 9:02:27 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.32 minutes.; Runtime.totalMemory()=2463629312; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 7, shuang-small-m.c.broad-dsde-methods.internal, executor 2): htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:11191,abort,aborting,11191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['abort'],['aborting']
Safety,rk.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **Driver stacktrace:** ; **at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)** ; **at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)** ; **at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)** ; **at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)** ; **at scala.Option.foreach(Option.scala:257)** ; **at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)** ; **at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)** ; **at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)** ; **at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)** ; **at org.apache.spark.util.EventLoop,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:46023,abort,abortStage,46023,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['abort'],['abortStage']
Safety,rocessing assembly region at chrM:2544-2841 isActive: true numReads: 5108; 11:35:48.094 DEBUG ReadThreadingGraph - Recovered 17 of 20 dangling tails; 11:35:48.198 DEBUG ReadThreadingGraph - Recovered 16 of 50 dangling heads; 11:35:48.511 DEBUG IntToDoubleFunctionCache - cache miss 2389 > 10 expanding to 2399; 11:35:48.874 DEBUG Mutect2Engine - Active Region chrM:2544-2841; 11:35:48.874 DEBUG Mutect2Engine - Extended Act Region chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Ref haplotype coords chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Haplotype count 128; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:08.907 INFO ProgressMeter - chrM:2544 0.4 10 22.3; 11:36:08.954 DEBUG Mutect2 - Processing assembly region at chrM:2842-2920 isActive: false numReads: 4726; 11:36:09.094 DEBUG Mutect2 - Processing assembly region at chrM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dan,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:10072,Recover,Recovered,10072,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Safety,"rogram.java:31); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:149); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:190); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); at org.broadinstitute.hellbender.testutils.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:107); at org.broadinstitute.hellbender.tools.HaplotypeCallerSparkIntegrationTest.testNonStrictVCFModeIsConsistentWithPastResults(HaplotypeCallerSparkIntegrationTest.java:109); Caused by:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; at java.util.ArrayList.sort(ArrayList.java:1464); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerArgumentCollection.createReadThreadingAssembler(AssemblyBasedCallerArgumentCollection.java:36); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.initialize(HaplotypeCallerEngine.java:231); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.<init>(HaplotypeCallerEng",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690:4361,abort,aborted,4361,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690,1,['abort'],['aborted']
Safety,"rsion v4.1.7.0, it runs without an error.... ```; gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; 14:40:45.497 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 10, 2021 2:40:45 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:40:45.786 INFO HaplotypeCaller - ------------------------------------------------------------; 14:40:45.787 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.7.0; 14:40:45.787 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:40:45.788 INFO HaplotypeCaller - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.6.1.el7.x86_64 amd64; 14:40:45.788 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 14:40:45.789 INFO HaplotypeCaller - Start Date/Time: February 10, 2021 2:40:45 PM EST; 14:40:45.789 INFO HaplotypeCaller - ------------------------------------------------------------; 14:40:45.789 INFO HaplotypeCaller - ------------------------------------------------------------; 14:40:45.791 INFO HaplotypeCaller - HTSJDK Version: 2.21.2; 14:40:45.791 INFO HaplotypeCaller - Picard Version: 2.21.9; 14:40:45.791 INFO Hapl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7076:6249,detect,detect,6249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076,1,['detect'],['detect']
Safety,"rt command used](https://github.com/Sydney-Informatics-Hub/Germline-ShortV/blob/master/gatk4_genomicsdbimport.sh) (university bioinformatics core facility's pipeline, not mine).; - 1 core and 4 GB RAM per task, but tasks seem to be using only about 1 GB RAM per task. 768 tasks (16 nodes) in total.; ```; %CPU WallTime Time Lim RSS mem memlim cpus; normal-exe = open&run; 105581211 R ds6924 hm82 genotype 4 00:18:25 02:00:00 1064GB 1064GB 3072GB 768; ```; - Jobs eventually finish if not running out of allocated time.; - Takes a long time to begin processing the first set of variants.; ```; 13:51:37.925 INFO GenotypeGVCFs - ------------------------------------------------------------; 13:51:39.736 INFO GenotypeGVCFs - Done initializing engine; 13:51:39.923 INFO ProgressMeter - Starting traversal; 13:51:39.923 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 14:23:57.323 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr17:18363145 the annotation AS_RAW_MQ=64800.000|50400.000|0.000 was not a numerical value and was ignored; 14:23:57.346 WARN ReferenceConfidenceVariantContextMerger - Reducible annotation 'AS_RAW_MQ' detected, add -G Standard -G AS_Standard to the command to annotate in the final VC with this annotation.; 14:23:58.180 INFO ProgressMeter - chr17:18363854 32.3 1000 31.0; 14:24:13.258 INFO ProgressMeter - chr17:18376854 32.6 14000 430.0; 14:24:58.358 INFO ProgressMeter - chr17:18382854 33.3 20000 600.5; 14:32:49.287 INFO ProgressMeter - chr17:18393855 41.2 31000 753.2; 14:33:39.240 INFO ProgressMeter - chr17:18405856 42.0 43000 1024.1; 14:33:49.493 INFO ProgressMeter - chr17:18411856 42.2 49000 1162.3; 14:34:17.285 INFO ProgressMeter - chr17:18425856 42.6 63000 1478.1; ```. CPU utilisation does not improve after the variants begin processing after half an hour preparing traversal. ```; %CPU WallTime Time Lim RSS mem memlim cpus; normal-exe = ope",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879551089:1073,Detect,Detected,1073,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879551089,1,['Detect'],['Detected']
Safety,"run `SelectVariants -select 'dbSNPBuildID=119'`. this blows up . ```; Invalid JEXL expression detected for select-0 with message ![0,18]: 'dbSNPBuildID = 119;' context is readonly; ```. which is a suboptimal message - it should point to a doc to JEXL. (btw the fix seems to be to use `==` not `=`)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1313:94,detect,detected,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1313,1,['detect'],['detected']
Safety,ryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 21/04/13 07:32:24 ERROR SparkHadoopWriter: Task attempt_20210413073224_0026_r_000000_0 aborted.; 21/04/13 07:32:24 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 105); org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:5457,abort,aborted,5457,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['abort'],['aborted']
Safety,"s the available number of bins to represent the variations in the CDF in the most accurate way. Here's what I had written earlier:; ```; def get_counts_summary(counts, lo_cutoff=0.01, hi_cutoff=0.99, num_divisions=50):; sorted_counts = np.sort(counts); num_points = len(counts); lo_index = int(np.floor(lo_cutoff * num_points)); hi_index = min(int(np.ceil(hi_cutoff * num_points)), num_points - 1); lo_count = sorted_counts[lo_index]; hi_count = sorted_counts[hi_index]; abscissa_indices = np.round(np.linspace(lo_index, hi_index, num=num_divisions + 1)).astype(int); abscissa = np.asarray([sorted_counts[idx] for idx in abscissa_indices]); abscissa_counts = abscissa_indices[1:] - abscissa_indices[0:-1]; collapsed_abscissa, collapsed_abscissa_counts = collapse_abscissa_triplets(abscissa, abscissa_counts); return collapsed_abscissa, collapsed_abscissa_counts. def collapse_abscissa_triplets(abscissa, abscissa_counts):; if len(abscissa) < 3:; return abscissa, abscissa_counts; else:; pos = 0; collapsed_abscissa = [abscissa[pos]]; collapsed_abscissa_counts = []; while pos < len(abscissa) - 1:; first = abscissa[pos]; last = abscissa[pos + 1]; count = abscissa_counts[pos]; if first != last:; collapsed_abscissa += [last]; collapsed_abscissa_counts += [count]; pos += 1; else:; j = 1; while pos + j + 1 < len(abscissa):; if abscissa[pos + j + 1] == last:; count += abscissa_counts[pos + j]; j += 1; else:; break; collapsed_abscissa += [last]; collapsed_abscissa_counts += [count]; pos += j; return np.asarray(collapsed_abscissa), np.asarray(collapsed_abscissa_counts); ```; Here, `get_counts_summary` returns a tuple of `(abscissa, occurrences)`. The summary is interpreted as follows: there are `occurrences[m]` bins with counts >= `abscissa[m]` and <= `abscissa[m+1]`, for `m = 0, ...`, up to `num_divisions` (but could be smaller if some of the redundant abscissa are collapsed). In the PyMC3 code, we could evaluate the `NegativeBinomial` the the midpoint of `abscissa[m]` and `abscissa[m+1]`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376291049:1996,redund,redundant,1996,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376291049,1,['redund'],['redundant']
Safety,"s were aligned; INFO 21:38:54,554 ProgressMeter - done 3.31246907E8 31.8 m 5.0 s 99.7% 31.8 m 5.0 s ; INFO 21:38:54,554 ProgressMeter - Total runtime 1905.29 secs, 31.75 min, 0.53 hours ; ------------------------------------------------------------------------------------------; Done. There were 4 WARN messages, the first 4 are repeated below.; WARN 17:39:57,688 IndexDictionaryUtils - Track variant doesn't have a sequence dictionary built in, skipping dictionary validation ; WARN 18:13:42,039 SimpleTimer - Clock drift of -1,503,348,737,016,211,299 - -1,503,346,772,578,127,937 = 1,964,438,083,362 nanoseconds detected, vs. max allowable drift of 5,000,000,000. Assuming checkpoint/restart event. ; WARN 20:14:18,043 SimpleTimer - Clock drift of -1,503,355,916,564,964,097 - -1,503,348,737,015,111,124 = 7,179,549,852,973 nanoseconds detected, vs. max allowable drift of 5,000,000,000. Assuming checkpoint/restart event. ; WARN 21:10:35,064 SimpleTimer - Clock drift of -1,503,359,203,412,549,926 - -1,503,355,916,564,817,209 = 3,286,847,732,717 nanoseconds detected, vs. max allowable drift of 5,000,000,000. Assuming checkpoint/restart event. ; ------------------------------------------------------------------------------------------; WMCF9-CB5:Mutect2 shlee$ ; ```. ### Notice the following line from above. > 0 variants were aligned. Also, it would be great if the tool, which appears to keep track of the lengths of reference alleles that are too long, could give me the **maximum length** reference allele so that I can go back and set the `--reference_window_stop` argument appropriately in a second round so that I can left-align _all_ of my variants. . ### MD5 and looking into the files, we see input and output are different and in fact the tool did change allele representations:; ```; WMCF9-CB5:Mutect2 shlee$ gzcat zeta_af-only-gnomad_Hg19toGRCh38.vcf.gz | grep -v '##' > zeta_headless.txt; WMCF9-CB5:Mutect2 shlee$ md5 zeta_headless.txt ; MD5 (zeta_headless.txt) = 6d93f1ea32c99a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487:8064,detect,detected,8064,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487,1,['detect'],['detected']
Safety,s(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:6193,abort,abortStage,6193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['abort'],['abortStage']
Safety,s(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:18992,abort,abortStage,18992,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['abort'],['abortStage']
Safety,s.spark.pathseq.PSKmerSet$Serializer.read(PSKmerSet.java:97)** ; **at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:709)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:134)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)** ; **20/03/05 09:28:58 INFO ShutdownHookManager: Shutdown hook called** ; **20/03/05 09:28:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-9e0e0327-45a3-46e8-872a-f5a63c3c7a98** ; **Using GATK jar /mnt/clinix1/Analysis/mongol/phenomata/Tools/Anaconda3/envs/gatk4/s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:50908,Unsafe,UnsafeShuffleWriter,50908,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['Unsafe'],['UnsafeShuffleWriter']
Safety,"s.spark.pathseq.PSKmerSet$Serializer.read(PSKmerSet.java:97)** ; **at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:709)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:134)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)** ; **20/03/05 09:28:58 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40, localhost, executor driver, partition 40, PROCESS\_LOCAL, 7972 bytes)** ; **20/03/05 09:28:58 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)** ; **20/03/05 09:28:58 WARN TaskSetManag",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:28476,Unsafe,UnsafeShuffleWriter,28476,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['Unsafe'],['UnsafeShuffleWriter']
Safety,s.spark.pathseq.PSKmerSet$Serializer.read(PSKmerSet.java:97)** ; **at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:709)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:134)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **20/03/05 09:28:58 ERROR TaskSetManager: Task 34 in stage 0.0 failed 1 times; aborting job** ; **20/03/05 09:28:58 INFO TaskSchedulerImpl: Cancelling stage 0** ; **20/03/05 09:28:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled** ; **20/03/,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:31924,Unsafe,UnsafeShuffleWriter,31924,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['Unsafe'],['UnsafeShuffleWriter']
Safety,"s.spark.pathseq.PSKmerSet$Serializer.read(PSKmerSet.java:97)** ; **at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:709)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:134)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **Driver stacktrace:** ; **20/03/05 09:28:58 INFO DAGScheduler: Job 0 failed: count at PathSeqPipelineSpark.java:245, took 63.806676 s** ; **20/03/05 09:28:58 INFO SparkUI: Stopped Spark web UI at http://cm132:4040** ; **20/03/05 09:28:58 INFO MapOutputTrackerMasterEndpoin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:40538,Unsafe,UnsafeShuffleWriter,40538,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['Unsafe'],['UnsafeShuffleWriter']
Safety,s.spark.pathseq.PSKmerSet$Serializer.read(PSKmerSet.java:97)** ; **at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:709)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:134)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **Driver stacktrace:** ; **at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)** ; **a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:44965,Unsafe,UnsafeShuffleWriter,44965,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['Unsafe'],['UnsafeShuffleWriter']
Safety,"s/wesep-229191-f.bam -O results/wesep-229191-f.vcf --alleles ../wesid-226998-m.haplotypecaller.final.vcf.gz -L 0005-scattered.inter ; ; val\_list -bamout results/wesep-229191-f.variants.bam -G StandardAnnotation -G StandardHCAnnotation --dragen-mode --dragstr-params-path /home/gvandeweyer/elprep\_streaming/results/wesep-229191-f.bam.params ; ; 22:06:39.332 WARN  GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default ; ; 22:06:39.337 WARN  GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default ; ; 22:06:39.383 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Mar 12, 2022 10:06:39 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 22:06:39.543 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.543 INFO  HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; 22:06:39.543 INFO  HaplotypeCaller - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 22:06:39.543 INFO  HaplotypeCaller - Executing as [gvandeweyer@ngsvm-pipelines.uza.be](mailto:gvandeweyer@ngsvm-pipelines.uza.be) on Linux v4.4.0-210-generic amd64 ; ; 22:06:39.543 INFO  HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_312-b07 ; ; 22:06:39.544 INFO  HaplotypeCaller - Start Date/Time: March 12, 2022 10:06:39 PM CET ; ; 22:06:39.544 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.544 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.544 INFO  Haploty",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7741:4742,detect,detect,4742,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741,1,['detect'],['detect']
Safety,s; 12:06:11.465 DEBUG Mutect2Engine - Active Region chrM:12730-13020; 12:06:11.470 DEBUG Mutect2Engine - Extended Act Region chrM:12630-13120; 12:06:11.474 DEBUG Mutect2Engine - Ref haplotype coords chrM:12630-13120; 12:06:11.478 DEBUG Mutect2Engine - Haplotype count 128; 12:06:11.481 DEBUG Mutect2Engine - Kmer sizes count 0; 12:06:11.485 DEBUG Mutect2Engine - Kmer sizes values []; 12:08:48.420 DEBUG Mutect2 - Processing assembly region at chrM:13021-13320 isActive: false numReads: 44155; 12:08:49.628 INFO ProgressMeter - chrM:13021 33.1 50 1.5; 12:09:01.241 DEBUG Mutect2 - Processing assembly region at chrM:13321-13620 isActive: false numReads: 55070; 12:09:01.757 DEBUG Mutect2 - Processing assembly region at chrM:13621-13636 isActive: false numReads: 55240; 12:09:02.341 DEBUG Mutect2 - Processing assembly region at chrM:13637-13936 isActive: true numReads: 110273; 12:09:09.957 DEBUG ReadThreadingGraph - Recovered 24 of 26 dangling tails; 12:09:10.041 DEBUG ReadThreadingGraph - Recovered 6 of 14 dangling heads; 12:09:10.602 DEBUG Mutect2Engine - Active Region chrM:13637-13936; 12:09:10.608 DEBUG Mutect2Engine - Extended Act Region chrM:13537-14036; 12:09:10.613 DEBUG Mutect2Engine - Ref haplotype coords chrM:13537-14036; 12:09:10.617 DEBUG Mutect2Engine - Haplotype count 128; 12:09:10.621 DEBUG Mutect2Engine - Kmer sizes count 0; 12:09:10.625 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:51.290 DEBUG Mutect2 - Processing assembly region at chrM:13937-13944 isActive: true numReads: 54773; 12:13:53.989 DEBUG ReadThreadingGraph - Recovered 29 of 59 dangling tails; 12:13:54.004 DEBUG ReadThreadingGraph - Recovered 0 of 35 dangling heads; 12:13:54.432 DEBUG Mutect2Engine - Active Region chrM:13937-13944; 12:13:54.440 DEBUG Mutect2Engine - Extended Act Region chrM:13837-14044; 12:13:54.447 DEBUG Mutect2Engine - Ref haplotype coords chrM:13837-14044; 12:13:54.452 DEBUG Mutect2Engine - Haplotype count 128; 12:13:54.456 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:54,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:20203,Recover,Recovered,20203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Safety,"sGermlineCNVCalls \\. \--model-shard-path ${gCNV\_model\_prefix}-model \\. \--calls-shard-path ${gCNV\_case\_prefix}-calls \\. \--allosomal-contig chrX --allosomal-contig chrY \\. \--contig-ploidy-calls ${ploidy\_case\_prefix}-calls \\. \--sample-index ${sample\_index} \\. \--output-denoised-copy-ratios ${cnv\_dir}/${sampleID}.sample\_${sample\_index}.denoised\_copy\_ration.tsv \\. \--output-genotyped-intervals ${cnv\_dir}/genotyped-intervals-case-${sampleID}-vs-${probe}cohort.vcf.gz \\. \--output-genotyped-segments ${cnv\_dir}/genotyped-segments-case-${sampleID}-vs-${probe}cohort.vcf.gz \\. \--sequence-dictionary ${ref\_gen}/ucsc.hg19.dict. c) Entire error log:. 11:04:20.841 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/yangyxt/software/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Aug 30, 2021 11:04:20 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:04:20.983 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------ ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Executing as yangyxt@paedyl02 on Linux v3.10.0-1160.11.1.el7.x86\_64 amd64 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_152-release-1056-b12 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Start Date/Time: August 30, 2021 11:04:20 AM HKT ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------ ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - -----------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444:1608,detect,detect,1608,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444,1,['detect'],['detect']
Safety,safety checks for restarts via preemptibles,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7590:0,safe,safety,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7590,1,['safe'],['safety']
Safety,"samjdk.compression\_level=2 -Xmx24G -jar /mnt/d/GenLab/WES/software/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar FilterVariantTranches -I D1394-recal.bam -V D1394-annotated.vcf -R /mnt/d/GenLab/WES/reference/hg19.fasta --create-output-variant-index true --resource /mnt/d/GenLab/WES/db/00-All.vcf.gz --resource /mnt/d/GenLab/WES/db/00-common\_all.vcf.gz --resource /mnt/d/GenLab/WES/reference/1000G\_phase1.indels.hg19.sites.vcf --resource /mnt/d/GenLab/WES/reference/Mills\_and\_1000G\_gold\_standard.indels.hg19.sites.vcf --snp-tranche 99.9 --snp-tranche 99.95 --indel-tranche 99.0 --indel-tranche 99.4 -O D1394-filtered.vcf --tmp-dir /mnt/d/GenLab/WES/output/tmp ; ; 14:50:12.699 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/mnt/d/GenLab/WES/software/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Jul 06, 2020 2:50:12 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:50:12.890 INFO FilterVariantTranches - ------------------------------------------------------------ ; ; 14:50:12.891 INFO FilterVariantTranches - The Genome Analysis Toolkit (GATK) v4.1.8.0 ; ; 14:50:12.891 INFO FilterVariantTranches - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 14:50:12.891 INFO FilterVariantTranches - Executing as root@Genlab-srv on Linux v4.4.0-18362-Microsoft amd64 ; ; 14:50:12.891 INFO FilterVariantTranches - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_252-8u252-b09-1~18.04-b09 ; ; 14:50:12.891 INFO FilterVariantTranches - Start Date/Time: July 6, 2020 2:50:12 PM MSK ; ; 14:50:12.891 INFO FilterVariantTranches - ------------------------------------------------------------ ; ; 14:50:12.891 INFO FilterVariantTranches - ------------------------------------------------------------ ; ; 14:50:12.892 INFO FilterVariant",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6701:2807,detect,detect,2807,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6701,1,['detect'],['detect']
Safety,sanity check failed as expected,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8390#issuecomment-1613622866:0,sanity check,sanity check,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8390#issuecomment-1613622866,1,['sanity check'],['sanity check']
Safety,scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:13039,abort,abortStage,13039,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['abort'],['abortStage']
Safety,se --disable-cap-base-qu; alities-to-map-quality false --enable-dynamic-read-disqualification-for-genotyping false --dynamic-read-disqualification-threshold 1.0 --native-pair-hmm-threads 4 --native-pair-hmm-use-dou; ble-precision false --flow-hmm-engine-min-indel-adjust 6 --flow-hmm-engine-flat-insertion-penatly 45 --flow-hmm-engine-flat-deletion-penatly 45 --pileup-detection false --pileup-detection-; enable-indel-pileup-calling false --num-artificial-haplotypes-to-add-per-allele 5 --artifical-haplotype-filtering-kmer-size 10 --pileup-detection-snp-alt-threshold 0.1 --pileup-detection-i; ndel-alt-threshold 0.5 --pileup-detection-absolute-alt-depth 0.0 --pileup-detection-snp-adjacent-to-assembled-indel-range 5 --pileup-detection-bad-read-tolerance 0.0 --pileup-detection-pro; per-pair-read-badness true --pileup-detection-edit-distance-read-badness-threshold 0.08 --pileup-detection-chimeric-read-badness true --pileup-detection-template-mean-badness-threshold 0.0; --pileup-detection-template-std-badness-threshold 0.0 --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --override-fragment-softclip-check false --min-base-quality-s; core 10 --smith-waterman JAVA --max-mnp-distance 0 --force-call-filtered-alleles false --reference-model-deletion-quality 30 --soft-clip-low-quality-ends false --allele-informative-reads-o; verlap-margin 2 --smith-waterman-dangling-end-match-value 25 --smith-waterman-dangling-end-mismatch-penalty -50 --smith-waterman-dangling-end-gap-open-penalty -110 --smith-waterman-danglin; g-end-gap-extend-penalty -6 --smith-waterman-haplotype-to-reference-match-value 200 --smith-waterman-haplotype-to-reference-mismatch-penalty -150 --smith-waterman-haplotype-to-reference-ga; p-open-penalty -260 --smith-waterman-haplotype-to-reference-gap-extend-penalty -11 --smith-waterman-read-to-haplotype-match-value 10 --smith-waterman-read-to-haplotype-mismatch-penalty -15; --smith-waterman-read-to-haplotype-gap-open-penalty -30 --smith-waterman-rea,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:7179,detect,detection-template-std-badness-threshold,7179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,1,['detect'],['detection-template-std-badness-threshold']
Safety,"seReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:08:45.223 INFO DenoiseReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:08:45.223 INFO DenoiseReadCounts - Deflater: IntelDeflater; 20:08:45.223 INFO DenoiseReadCounts - Inflater: IntelInflater; 20:08:45.223 INFO DenoiseReadCounts - GCS max retries/reopens: 20; 20:08:45.223 INFO DenoiseReadCounts - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:08:45.223 INFO DenoiseReadCounts - Initializing engine; 20:08:45.223 INFO DenoiseReadCounts - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 20:08:45.300 INFO DenoiseReadCounts - Reading read-counts file (BT1813.counts.hdf5)...; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fsuper.c line 277 in H5F_super_read(): file signature not found; major: File accessibilty; minor: Not an HDF5 file; 20:08:49.800 INFO DenoiseReadCounts - Shutting down engine; [May 18, 2021 8:08:49 PM EDT] org.broadinstitute.hellbender.tools.copynumber.DenoiseReadCounts done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=1789919232; org.broadinstitute.hdf5.HDF5LibException: exception when opening '/hpf/largeprojects/tabori/projects/bmmrd/CNA_project/gatk_cna/gatk/analysis/lgg/cnvponC2.pon.hdf5' with READ_ONLY ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7258:3928,detect,detected,3928,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258,1,['detect'],['detected']
Safety,"sensus_sequences_gatk//CCA0704.fasta.tmp -V pergene_gatk/CCA0704/CCA0704.vcf.gz; Using GATK jar /g/data/xe2/users/stephen-rodgers/latest-gatk/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /g/data/xe2/users/stephen-rodgers/latest-gatk/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar FastaAlternateReferenceMaker -R /g/data/xe2/references/eucalyptus/emel_scott/Emelliodora_CSIROg1_SISH00000000.1.fasta -O consensus_sequences_gatk//CCA0704.fasta.tmp -V pergene_gatk/CCA0704/CCA0704.vcf.gz; 15:43:14.276 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/g/data/xe2/users/stephen-rodgers/latest-gatk/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 03, 2020 3:43:15 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:43:15.230 INFO FastaAlternateReferenceMaker - ------------------------------------------------------------; 15:43:15.230 INFO FastaAlternateReferenceMaker - The Genome Analysis Toolkit (GATK) v4.1.4.1; 15:43:15.230 INFO FastaAlternateReferenceMaker - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:43:15.231 INFO FastaAlternateReferenceMaker - Executing as kdm801@gadi-login-01.gadi.nci.org.au on Linux v4.18.0-80.11.2.el8_0.x86_64 amd64; 15:43:15.240 INFO FastaAlternateReferenceMaker - Java runtime: OpenJDK 64-Bit Server VM v13+33; 15:43:15.240 INFO FastaAlternateReferenceMaker - Start Date/Time: 3 February 2020 at 3:43:14 pm AEDT; 15:43:15.240 INFO FastaAlternateReferenceMaker - ------------------------------------------------------------; 15:43:15.240 INFO FastaAlternateReferenceMaker - ------------------------------------------------------------; 15:43:15.241 INFO FastaAlternateReference",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6434:1832,detect,detect,1832,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6434,1,['detect'],['detect']
Safety,"sites annotated with PLs forced to true for reference-model confidence output; 22:42:22.734 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 22:42:22.734 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 22:42:22.734 INFO NativeLibraryLoader - Loading libgkl_pairhmm.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm.dylib; 22:42:22.748 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 22:42:22.748 WARN IntelPairHmm - Ignoring request for 4 threads; not using OpenMP implementation; 22:42:22.748 INFO PairHMM - Using the AVX-accelerated native PairHMM implementation; 22:42:22.751 WARN GATKVariantContextUtils - Can't determine output variant file format from output file extension ""bam"". Defaulting to VCF.; 22:42:22.776 INFO ProgressMeter - Starting traversal; 22:42:22.777 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010f47efd3, pid=96919, tid=0x0000000000002303; #; # JRE version: OpenJDK Runtime Environment (8.0_192-b01) (build 1.8.0_192-b01); # Java VM: OpenJDK 64-Bit Server VM (25.192-b01 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libgkl_smithwaterman4496658849792952100.dylib+0x1fd3] _Z22smithWatermanBackTrackP10dnaSeqPairiiiiPii+0x3c3; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /private/tmp/hs_err_pid96919.log; #; # If you would like to submit a bug report, please visit:; # http://www.azulsystems.com/support/; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737:4410,detect,detected,4410,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737,1,['detect'],['detected']
Safety,"somal-contig chrX --allosomal-contig chrY --contig-ploidy-calls /home/lmbs02/bio/work/gatk_cnv/genomed_genome_hg19//cohort_ploidy//cohort-calls/ --sample-index 16 --output-genotyped-intervals /home/lmbs02/bio/work/gatk_cnv/genomed_genome_hg19//vcfs//sample.intervals.vcf --output-genotyped-segments /home/lmbs02/bio/work/gatk_cnv/genomed_genome_hg19//vcfs//sample.segments.vcf --sequence-dictionary /home/lmbs02/bio/databases/referenses/hg19_37/ucsc/hg19.dict --output-denoised-copy-ratios /home/lmbs02/bio/work/gatk_cnv/genomed_genome_hg19//vcfs//sample.copy_ratios.tsv `. In the same time, if you use the first 4 or the third and 4 at the same time, an error pops up.; `12:49:08.552 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/lmbs02/bio/biosoft/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 29, 2020 12:49:08 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:49:08.687 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 12:49:08.687 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.1.8.1; 12:49:08.687 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:49:08.687 INFO PostprocessGermlineCNVCalls - Executing as lmbs02@Lmbs01 on Linux v5.4.0-48-generic amd64; 12:49:08.687 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_265-8u265-b01-0ubuntu2~18.04-b01; 12:49:08.687 INFO PostprocessGermlineCNVCalls - Start Date/Time: October 29, 2020 12:49:08 PM MSK; 12:49:08.687 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 12:49:08.687 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 12:49:08.688 INFO PostprocessGermlineCNVCalls - HTSJ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924:7256,detect,detect,7256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924,1,['detect'],['detect']
Safety,"some queries like `isPaired` or `isUnmapped` are called over and over on GATKReads and definitely have impact on performance, eg see #2032. It may make sense to cache those values - the risk is that reads are mutable and so caches need to be invalidated and that cached valued (even booleans) add overhead to memory footprint and shuffle footprint. . This ticket is to implement caching of those frequently called queries and measure performance impact.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2058:186,risk,risk,186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2058,1,['risk'],['risk']
Safety,spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:11135,abort,abortStage,11135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['abort'],['abortStage']
Safety,"spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); ... 24 more; 05:12:04.045 INFO HaplotypeCallerSpark - Shutting down engine; [May 18, 2017 5:12:04 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 131.63 minutes.; Runtime.totalMemory()=16201547776; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 1 times, most recent failure: Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayI; ndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:382); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:65); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:865); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:630); at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:297); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:313); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGSch",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3019:6344,abort,aborted,6344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019,1,['abort'],['aborted']
Safety,spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:14648,abort,abortStage,14648,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['abort'],['abortStage']
Safety,spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607); at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(D,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:31583,abort,abortStage,31583,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['abort'],['abortStage']
Safety,"st -L chr33.bed --genomicsdb-workspace-path chr33.db.The output log file is as follows，Using GATK jar /mnt/nvme1/opt/reseq_softwares/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx100g -Xms100g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /mnt/nvme1/opt/reseq_softwares/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar GenomicsDBImport -R /mnt/nvme1/reference/Gallus_gallus/Ensembl_g6a/Gallus_gallus.GRCg6a.dna.toplevel.fa --sample-name-map samplelist -L chr33.bed --genomicsdb-workspace-path chr33.db; 11:19:39.692 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/nvme1/opt/reseq_softwares/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 04, 2022 11:19:40 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:19:40.099 INFO GenomicsDBImport - ------------------------------------------------------------; 11:19:40.099 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.0; 11:19:40.100 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:19:40.100 INFO GenomicsDBImport - Executing as maosong@nygpu on Linux v3.10.0-1160.45.1.el7.x86_64 amd64; 11:19:40.100 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_312-b07; 11:19:40.100 INFO GenomicsDBImport - Start Date/Time: 2022年7月4日 上午11时19分39秒; 11:19:40.100 INFO GenomicsDBImport - ------------------------------------------------------------; 11:19:40.100 INFO GenomicsDBImport - ------------------------------------------------------------; 11:19:40.101 INFO GenomicsDBImport - HTSJDK Version: 2.22.0; 11:19:40.101 INFO GenomicsDBImport - Picard Version: 2.22.8; 11:19:40.101 INFO GenomicsDBImport - HTSJDK",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460:1292,detect,detect,1292,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460,1,['detect'],['detect']
Safety,"st/Data/73318\_CNNScore\_test.vcf.gz -tensor-type read\_tensor > /media/analyst/Data/CNNScoreVariants.log. Entire console output:. Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar CNNScoreVariants -I 73318\_WES\_hg19\_recalibrated.sorted.bam -V 73318\_80\_IDTv1.vcf.gz -R /media/analyst/Data/Reference\_data/hg19.fa -O /media/analyst/Data/73318\_CNNScore\_test.vcf.gz -tensor-type read\_tensor ; ; 11:17:58.509 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Apr 25, 2022 11:17:58 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:17:58.668 INFO  CNNScoreVariants - ------------------------------------------------------------ ; ; 11:17:58.668 INFO  CNNScoreVariants - The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; 11:17:58.669 INFO  CNNScoreVariants - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:17:58.669 INFO  CNNScoreVariants - Executing as analyst@WGS on Linux v5.13.0-40-generic amd64 ; ; 11:17:58.669 INFO  CNNScoreVariants - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13 ; ; 11:17:58.669 INFO  CNNScoreVariants - Start Date/Time: April 25, 2022 at 11:17:58 AM CEST ; ; 11:17:58.669 INFO  CNNScoreVariants - ------------------------------------------------------------ ; ; 11:17:58.669 INFO  CNNScoreVariants - ------------------------------------------------------------ ; ; 11:17:58.670 INFO  CNNScoreVariants - HTSJDK Version: 2.24.1 ; ; 11:17:58.670 INFO  CNN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7811:2755,detect,detect,2755,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811,1,['detect'],['detect']
Safety,st/resources/org/broadinstitute/hellbender/tools/exome/calculatetargetcoverage/exome-read-counts-NA12872.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/exome/calculatetargetcoverage/exome-read-counts-NA12878.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/exome/calculatetargetcoverage/exome-read-counts-NA12878.output; src/test/resources/org/broadinstitute/hellbender/tools/exome/calculatetargetcoverage/exome-read-counts-sample-NA12878.output; src/test/resources/org/broadinstitute/hellbender/tools/exome/calculatetargetcoverage/test_reference.dict; src/test/resources/org/broadinstitute/hellbender/tools/exome/calculatetargetcoverage/test_reference.fasta.fai; src/test/resources/org/broadinstitute/hellbender/tools/exome/conversion/allelicbalancecaller/cell_line_full-sim-final.seg; src/test/resources/org/broadinstitute/hellbender/tools/exome/create-pon-some-targets.bed; src/test/resources/org/broadinstitute/hellbender/tools/exome/detectcoveragedropout; src/test/resources/org/broadinstitute/hellbender/tools/exome/detectcoveragedropout/HCC1143T-100_27M_37M.seg; src/test/resources/org/broadinstitute/hellbender/tools/exome/detectcoveragedropout/test.tn.HCC1143T-100_27M_37M.tsv; src/test/resources/org/broadinstitute/hellbender/tools/exome/discover-germline-input-to-xhmm-zscores.pl; src/test/resources/org/broadinstitute/hellbender/tools/exome/discover-germline-xhmm-output-4-6-70-3-3.tab; src/test/resources/org/broadinstitute/hellbender/tools/exome/dummy_cov_profile.txt; src/test/resources/org/broadinstitute/hellbender/tools/exome/dupReadsMini.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/exome/eval/eval-calls.vcf; src/test/resources/org/broadinstitute/hellbender/tools/exome/eval/eval-calls.vcf.gz; src/test/resources/org/broadinstitute/hellbender/tools/exome/eval/eval-calls.vcf.gz.tbi; src/test/resources/org/broadinstitute/hellbender/tools/exome/eval/eval-targets.tsv; src/test/resources/org/broadinstitute/hellbender/tools/exome/ev,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905:30739,detect,detectcoveragedropout,30739,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905,2,['detect'],['detectcoveragedropout']
Safety,stitute.hellbender.tools.spark.pathseq.PSKmerSet$Serializer.read(PSKmerSet.java:97)** ; **at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:709)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:134)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)** ; **20/03/05 09:28:58 INFO ShutdownHookManager: Shutdown hook called** ; **20/03/05 09:28:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-9e0e0327-45a3-46e8-872a-f5a63c3c7a98** ; **Using GATK jar /mnt/clinix1/Analysis/mongol/phenomata/Tools,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:50882,Unsafe,UnsafeShuffleWriter,50882,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['Unsafe'],['UnsafeShuffleWriter']
Safety,"stitute.hellbender.tools.spark.pathseq.PSKmerSet$Serializer.read(PSKmerSet.java:97)** ; **at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:709)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:134)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)** ; **20/03/05 09:28:58 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40, localhost, executor driver, partition 40, PROCESS\_LOCAL, 7972 bytes)** ; **20/03/05 09:28:58 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)** ; **20/03/05 09:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:28450,Unsafe,UnsafeShuffleWriter,28450,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['Unsafe'],['UnsafeShuffleWriter']
Safety,stitute.hellbender.tools.spark.pathseq.PSKmerSet$Serializer.read(PSKmerSet.java:97)** ; **at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:709)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:134)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **20/03/05 09:28:58 ERROR TaskSetManager: Task 34 in stage 0.0 failed 1 times; aborting job** ; **20/03/05 09:28:58 INFO TaskSchedulerImpl: Cancelling stage 0** ; **20/03/05 09:28:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:31898,Unsafe,UnsafeShuffleWriter,31898,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['Unsafe'],['UnsafeShuffleWriter']
Safety,"stitute.hellbender.tools.spark.pathseq.PSKmerSet$Serializer.read(PSKmerSet.java:97)** ; **at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:709)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:134)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **Driver stacktrace:** ; **20/03/05 09:28:58 INFO DAGScheduler: Job 0 failed: count at PathSeqPipelineSpark.java:245, took 63.806676 s** ; **20/03/05 09:28:58 INFO SparkUI: Stopped Spark web UI at http://cm132:4040** ; **20/03/05 09:28:58 INFO MapOut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:40512,Unsafe,UnsafeShuffleWriter,40512,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['Unsafe'],['UnsafeShuffleWriter']
Safety,stitute.hellbender.tools.spark.pathseq.PSKmerSet$Serializer.read(PSKmerSet.java:97)** ; **at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:709)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:134)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35)** ; **at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **Driver stacktrace:** ; **at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGSchedu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:44939,Unsafe,UnsafeShuffleWriter,44939,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['Unsafe'],['UnsafeShuffleWriter']
Safety,strip thread safety from CachingIndexedFastaSequenceFile and move it to htsjdk,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/89:13,safe,safety,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/89,1,['safe'],['safety']
Safety,t 128; 11:39:08.001 DEBUG Mutect2Engine - Kmer sizes count 0; 11:39:08.002 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:12.623 DEBUG Mutect2 - Processing assembly region at chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8672-8829 isActive: false numReads: 148658; 11:39:47.277 DEBUG IntToDoubleFunctionCache - cache miss 92836 > 47638 expanding to 95278; 11:40:02.830 DEBUG Mutect2 - Processing assembly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:15528,Recover,Recovered,15528,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Safety,"t org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:236); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Based on the discussion around #4963 and the [test VCF](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/testGenotypeGivenAllelesMode_givenAlleles.vcf), I gather that this is intended to work without error. I was trying to figure out how these cases differed from the spanning deletion in the aforementioned test VCF. One thing I noticed was that these two problematic cases have the SNP at the very last base of the spanning deletion. I'm just speculating here, but maybe it is related to an off-by-one bug of some sort? . I am testing with v. 4.0.9.0.; I also tried with v. 4.0.5.1 which does not crash, but rather prints the warnings discussed in #4963:; `00:02:10.995 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16137302-16137302, only considering the first record`; `00:03:08.220 WARN HaplotypeCallerEngine - Multiple valid VCF records detected in the alleles input file at site 22:16464051-16464051, only considering the first record`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5336:5619,detect,detected,5619,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5336,2,['detect'],['detected']
Safety,"t sample1.mutect2\_out.bam --recover-all-dangling-branches true -min-pruning 1 --min-dangling-branch-length 2 --debug --max-reads-per-alignment-start 0 --genotype-pon-sites True --f1r2-tar-gz vcf\_files/f1r2.sample1.tar.gz -O vcf\_files/unfiltered.sample1.vcf  . In the debug mode, the following log messages are generated for this region. 08:01:26.086 INFO  Mutect2Engine - Assembling chr12:**2539**8242-**2539**8320 with 14298 reads:    (with overlap region = chr12:**2539**8142-**2539**8420). I have another call with similar VAF that is detected in the vcf output(chr12:25380275). **chr12** 25380275   .    T    G    .    .     AS\_SB\_TABLE=3911,5343|26,21;DP=9485;ECNT=1;MBQ=36,36;MFRL=0,0;MMQ=42,42;MPOS=18;POPAF=7.30;TLOD=53.53     GT:AD:AF:DP:F1R2:F2R1:SB   0/1:9254,47:4.970e-03:9301:5321,21:3867,26:3911,5343,26,21. The input and the output BAMs show this call with the variant. ![](https://gatk.broadinstitute.org/hc/user_images/FVlI3WhNIzYK7NB7PakCmw.png). In the logs, it shows the detection of an active region here:. 08:01:23.642 INFO  Mutect2Engine - Assembling chr12:**2538**0238-**2538**0327 with 19912 reads:    (with overlap region = chr12:**2538**0138-**2538**0427). 08:01:24.119 INFO  EventMap - >> Events = EventMap{chr12:**2538**0275-**2538**0275 \[T\*, G\],}. 08:01:24.154 INFO  AssemblyResultSet - Trimming active region AssemblyRegion chr12:**2538**0238-**2538**0327 active?=true nReads=19912 with 2 haplotypes. 08:01:24.154 INFO  AssemblyResultSet - Trimmed region to chr12:**2538**0255-**2538**0295 and reduced number of haplotypes from 2 to only 2. 08:01:25.383 INFO  EventMap - >> Events = EventMap{chr12:**2538**0275-**2538**0275 \[T\*, G\],}. I have tried troubleshooting with the steps stated in this \[blog\](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant). However, it does not change the output vcf. I used the force-calling mode by giving the above call in an input vcf and the call did appear in the vcf file. *",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7232:2577,detect,detection,2577,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232,1,['detect'],['detection']
Safety,"t scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.TaskSetManager: Task 284 in stage 25.0 failed 4 times; aborting job; 18/01/12 20:38:37 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@23007ed{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(50,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(52,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(34,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(60,WrappedArray()); 20:38:37.897 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [January 12, 2018 8:38:37 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:5053,abort,aborting,5053,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['abort'],['aborting']
Safety,"t$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:479); at java.util.stream.ReferencePipeline.max(ReferencePipeline.java:515); at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.handleFragments(MarkDuplicatesSparkUtils.java:396); at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$markDuplicateRecords$fa45b352$1(MarkDuplicatesSparkUtils.java:304); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409); at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); `. Can you give me some advice?; Thanks,; sun",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7001:2298,Unsafe,UnsafeShuffleWriter,2298,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7001,2,['Unsafe'],['UnsafeShuffleWriter']
Safety,"t.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; 21/04/13 07:32:25 INFO DAGScheduler: Job 2 failed: runJob at SparkHadoopWriter.scala:78, took 0.365288 s; 21/04/13 07:32:25 ERROR SparkHadoopWriter: Aborting job job_20210413073224_0026.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:13900,Abort,Aborting,13900,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['Abort'],['Aborting']
Safety,"tUtils.java:103); ... 16 more. 01:44 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:39 INFO TaskSetManager: Starting task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:35903 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:39 INFO TaskSetManager: Lost task 1.3 in stage 2.0 (TID 10) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 18/04/24 17:56:39 ERROR TaskSetManager: Task 1 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:56:39 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 45.219 s due to Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:34975,abort,aborted,34975,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['abort'],['aborted']
Safety,tect2 - Processing assembly region at chrM:13945-14244 isActive: false numReads: 54745; 12:13:56.962 DEBUG Mutect2 - Processing assembly region at chrM:14245-14544 isActive: false numReads: 0; 12:13:56.973 DEBUG Mutect2 - Processing assembly region at chrM:14545-14844 isActive: false numReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false numReads: 0; 12:13:57.009 DEBUG Mutect2 - Processing assembly region at chrM:15445-15744 isActive: false numReads: 0; 12:13:57.027 INFO ProgressMeter - chrM:15445 38.3 60 1.6; 12:13:57.035 DEBUG Mutect2 - Processing assembly region at chrM:15745-15960 isActive: false numReads: 14; 12:13:57.047 DEBUG Mutect2 - Processing assembly region at chrM:15961-16230 isActive: true numReads: 30; 12:13:57.055 DEBUG ReadThreadingGraph - Recovered 1 of 1 dangling tails; 12:13:57.063 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 12:13:57.096 DEBUG ReadThreadingGraph - Recovered 3 of 3 dangling tails; 12:13:57.106 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling heads; 12:13:57.464 DEBUG Mutect2Engine - Active Region chrM:15961-16230; 12:13:57.469 DEBUG Mutect2Engine - Extended Act Region chrM:15861-16299; 12:13:57.472 DEBUG Mutect2Engine - Ref haplotype coords chrM:15861-16299; 12:13:57.476 DEBUG Mutect2Engine - Haplotype count 111; 12:13:57.479 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:57.482 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:58.821 DEBUG Mutect2 - Processing assembly region at chrM:16231-16299 isActive: false numReads: 15; 12:13:58.938 INFO Mutect2 - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityNotZeroReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonChimericOriginalAli,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:22274,Recover,Recovered,22274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Safety,ter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:16658,abort,abortStage,16658,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['abort'],['abortStage']
Safety,"terator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 00:11:09.634 ERROR TaskSetManager:70 - Task 15 in stage 1.0 failed 1 times; aborting job; 00:11:09.810 WARN TaskSetManager:66 - Lost task 33.0 in stage 1.0 (TID 528, localhost): TaskKilled (killed intentionally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018:9257,abort,aborting,9257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018,1,['abort'],['aborting']
Safety,"tes** total and produced an **11GB PoN** (this file includes all of the input read counts---which take up 20GB as a combined TSV file and a whopping 63GB as individual TSV files---as well as the eigenvectors, filtering results, etc.). The run can be found in /dsde/working/slee/wgs-pon-test/tieout/no-gc. It completed successfully with **-Xmx32G** (in comparison, CreatePanelOfNormals crashed after 40 minutes with -Xmx128G). The runtime breakdown was as follows:. - ~45 minutes simply from reading of the 90 TSV read-count files in serial. Hopefully #3349 should greatly speed this up. (In comparison, CombineReadCounts reading 10 files in parallel at a time took ~100 minutes to create the aforementioned 20GB combined TSV file, creating 25+GB of temp files along the way.). - ~5 minutes from the preprocessing and filtering steps. We could probably further optimize some of this code in terms of speed and heap usage. (I had to throw in a call to System.gc() to avoid an OOM with -Xmx32G, which I encountered in my first attempt at the run...). - ~5 minutes from performing the SVD of the post-filtering 8643028 x 86 matrix, maintaining 30 eigensamples. I could write a quick implementation of randomized SVD, which I think could bring this down a bit (the scikit-learn implementation takes <2 minutes on a 10M x 100 matrix), but this can probably wait. Clearly making I/O faster and more space efficient is the highest priority. Luckily it's also low hanging fruit. The 8643028 x 30 matrix of eigenvectors takes <2 minutes to read from HDF5 when the WGS PoN is used in DenoiseReadCounts, which gives us a rough idea of how long it should take to read in the original ~11.5M x 90 counts from HDF5. So once #3349 is in, then I think that a **~15 minute single-core WGS PoN could easily be viable**. I believe that a PoN on the order of this size will be all that is required for WGS denoising, if it is not already overkill. To go bigger by more than an order of magnitude, we'll have to go out of ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503:1066,avoid,avoid,1066,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503,2,['avoid'],['avoid']
Safety,test run (was aborted after changed logic ran successfully): https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/ed86ae93-41e7-4487-b8b1-3fc184bc64c3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8824:14,abort,aborted,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8824,1,['abort'],['aborted']
Safety,thSafeFinally(Utils.scala:1337); 	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5545:9728,abort,abortStage,9728,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545,1,['abort'],['abortStage']
Safety,thanks for the review @kcibul. I made some changes accordingly. re: PrepareCallset file of sample names. That would be nice! It would make this workflow simpler and it also simplifies the access requirements for PrepareCallset. re: Dockstore. We actually ruled this out because Terra says that the definition of a method configuration can change automatically if its updated in dockstore. Which can be useful but it adds a security risk since a compromised Dockstore can change the definition of the production AoU extraction WDL which runs with highly elevated permissions. We already have a script that creates method configurations from github so I can probably add something a little hacky to resolve relative imports to the raw github file that it refers to.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7242#issuecomment-846494686:432,risk,risk,432,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7242#issuecomment-846494686,2,['risk'],['risk']
Safety,"the invalid reads strikes back - i got this when running the ReadsPipelineSpark on qurynamesorted file `hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam`:. ```; Job aborted due to stage failure: Task 47 in stage 2.0 failed 4 times, most recent failure: Lost task 47.3 in stage 2.0 (TID 680, dataflow05.broadinstitute.org): java.lang.IllegalArgumentException: ; Invalid interval. Contig:20 start:62720124 end:62720123; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:34); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:46); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.lambda$join$3d1c3858$1(BroadcastJoinReadsWithVariants.java:27); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:28); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1560:189,abort,aborted,189,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1560,1,['abort'],['aborted']
Safety,"the simplest example, for sanity checking of instalations",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/920:26,sanity check,sanity checking,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/920,1,['sanity check'],['sanity checking']
Safety,"ther bam files. Additionally, changing the number of threads still results in the same stacktrace. I've also tried running the BQSRPipelineSpark to see if that would suffer the same issue and it fails in the same manner. Additionally, I've run ValidateSamFile. There are some reads missing their mates, but this hasn't presented an issue in other tools (including vanilla BaseRecalibrator). Searching thru the forum, I found an old issue with a similar stacktrace, but that issue appears to occur in GATK 2.4: https://gatkforums.broadinstitute.org/gatk/discussion/3265/bqsrgatherer-exception. In the below stacktrace, I've bolded the error message that seems to occur in each of these samples. `Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5854:1531,abort,abortStage,1531,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854,1,['abort'],['abortStage']
Safety,there's some highly redundant code in those classes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1882:20,redund,redundant,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1882,1,['redund'],['redundant']
Safety,"this PR:; - changes CreateVariantIngestFiles to name the output files in a predictable way - i.e. rather than using a sample_id, it uses the name of the input gvcf. e.g. `pet_001_NA12878.tsv` becomes `pet_001_NA12878.haplotypeCalls.reblocked.vcf.gz.tsv`; - added a test in CreateVariantIngestFilesIntegrationTest to assert that the files are named as expected. - changes the GvsImportGenomes.wdl to:; - check whether, for the given input gvcf file and for each of pet, vet, and sample_info, the output TSV already exists somewhere in the output directory. it checks subdirectories.; - if the output TSV exists in a `set_X` subdirectory, we move that file back into the parent directory so that subsetting works as desired when we get to LoadTables; - if the output TSV exists in a `done` subdirectory, we exit with an error. notes:; - this does not check whether the sample is in the same table_id (e.g. pet_001 versus pet_002). this has been tested as follows:; - ran once with an `exit 1` before bq load, to simulate generating TSVs and putting them into set_X subdirectories and then exiting, simulating a permissions or other bq issue; - removed LOCKFILE, removed exit before bq load, then ran again - TSVs were not regenerated, the existing ones were moved into the parent directory and loaded properly into bq; - then ran again with the same samples - as expected, errored out because the TSVs already existed in a `done` folder",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7226:75,predict,predictable,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7226,1,['predict'],['predictable']
Safety,"this bam file has no reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.bam`. the cram file also has no reads ``src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.cram`. but the corresponding sam file has 52 reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.sam`. However `CollectInsertSizeMetricsTest` does not catch this and incorrectly reports that the test successfully passed. The issue comes from using only a for loop for asserts (the for loop executes 0 times on this file and so it thinks everything is great). The task here is to:; - fix the test to catch this error, then; - fix the bam file to not have this problem, then; - check and fix all other tests that use the same testing pattern (using the forloop). I suggest switching to using actual files for such tests (as done in `MeanQualityByCycleIntegrationTest`) to avoid such problems in the future",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1512:1058,avoid,avoid,1058,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1512,1,['avoid'],['avoid']
Safety,"this guy never dies. ```; ""SAMFileWriterThread-187"" #471 daemon prio=5 os_prio=31 tid=0x00007f818d4b3000 nid=0xd98b waiting on condition [0x0000000126530000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000007507b5178> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:116); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1741:220,Unsafe,Unsafe,220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1741,1,['Unsafe'],['Unsafe']
Safety,this should stop the timeout errors,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1322:21,timeout,timeout,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1322,1,['timeout'],['timeout']
Safety,this was made redundant by #3353,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2804#issuecomment-317848172:14,redund,redundant,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2804#issuecomment-317848172,1,['redund'],['redundant']
Safety,"ties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. While the marginalized model is free of such spurious local minima, it does not yield discrete posteriors in a tractable way; hence, the necessity of ultimately decoupling in the ""main"" stage. - Capped phred-scaled qualities to maximum values permitted by machine precision in order to avoid NaNs and overflows. - Took a first step toward tracking and logging parameters during inference, starting with the ELBO history. In the future, it may be desirable to allow tracking of arbitrary RVs and deterministics via command line args (for debugging and exploratory work). Notes:. - We still need to decide about `GermlineCNVCaller` default arguments. See issue #4719.; - The case denoising and calling is unlikely to benefit from the marginalized warmup strategy. Therefore, it is not included in this PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4720:2284,avoid,avoid,2284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720,1,['avoid'],['avoid']
Safety,"ting task 0.1 in stage 0.0 (TID 1, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 1) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 1]; 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 2, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 2]; 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 3) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 3]; 18/04/23 20:42:02 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job; 18/04/23 20:42:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool; 18/04/23 20:42:02 INFO TaskSchedulerImpl: Cancelling stage 0; 18/04/23 20:42:02 INFO DAGScheduler: ResultStage 0 (first at ReadsSparkSource.java:221) failed in 11.519 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:14935,abort,aborting,14935,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['abort'],['aborting']
Safety,"tion ; I think there's a problem with the StrandOddsRatio (SOR) annotation and the `--map-mnp-distance` flag. I'm looking at a small region of NA24143 (one of the GIAB samples). There's a pair of SNPs in very close proximity. When called without the MNP output I get a pair of variants as follows (some info removed for clarity), coordinates are HG19:. ```; chr4 5743509 . C T 5903.03 . FS=0.000;QD=25.36;SOR=9.825 GT:AD:DP:GQ:PL 1/1:0,135:135:99:5917,406,0; chr4 5743512 . T C 2766.60 . FS=0.000;QD=21.12;SOR=0.983 GT:AD:DP:GQ:PL 0/1:57,74:131:99:2774,0,2060; ```. I'm trying to get permission to share the BAM over this region, but the key information is that every single read that spans or is in proximity to these variants is on the R strand. There is zero F strand coverage. This seems reasonable. It's a bit odd to me that the first SNP which is hom-var has a SOR value of 9.825, but it's homozygous so it's more or less irrelevant. Looking at the code, I think the problem here is that the code avoids divide-by-zero errors by adding pseudo-counts of `1.0` to the table, which for homozygous variants with no coverage on one strand creates a weird situation. I think it would be better to just detect if _all_ coverage is on one strand and short-circuit the calculation, but I digress. The real problem comes when running with `--max-mnp-distance 5`. Then I get this single variant:. ```; chr4 5743509 . CTAT TTAC,TTAT 5506.10 . FS=0.000;QD=25.36;SOR=9.750 GT:AD:DP:GQ:PL 1/2:0,74,56:130:99:5523,2213,2060,3016,0,2774; ```. Now I have a het variant with an SOR of 9.75. This seems really wrong to me - note how FS is 0.0. Again all coverage of all alleles is on one strand. And the het SNP that forms part of this MNP had an SOR of 0.983 when called independently. Since the first SNP is hom-var and the second is het, I would have expected the SOR value for the MNP call to closely mirror that of the het SNP. My suspicion is that what's going on here is probably that the calculation is bein",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5698:1236,avoid,avoids,1236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5698,1,['avoid'],['avoids']
Safety,titute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: **BUG: couldn't set field value. For normalPileupPValueThreshold in org.broadinstitute.hellbender.tools.walkers.mutect.filtering.M2FiltersArgumentCollection@69d45cca with value 1.0E-4 This shouldn't happen since we setAccessible(true)**; 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser$ArgumentDefinition.setFieldValue(CommandLineArgumentParser.java:1248); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.setArgument(CommandLineArgumentParser.java:710); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:427); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:232); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:206); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.lang.IllegalAccessException: Can not set static final double field org.broadinstitute.hellbender.tools.walkers.mutect.filtering.M2FiltersArgumentCollection.normalPileupPValueThreshold to java.lang.Double; 	at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:76); 	at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:80); 	at sun.reflect.UnsafeQualifiedStaticDoubleFieldAccessorImpl.set(UnsafeQualifiedStaticDoubleFieldAccessorImpl.java:77); 	at java.lang.reflect.Field.set(Field.java:764); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser$ArgumentDefinition.setFieldValue(CommandLineArgumentParser.java:1245); 	... 7 more. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/24071/filtermutectcalls-bug/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5978:2640,Unsafe,UnsafeFieldAccessorImpl,2640,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5978,6,['Unsafe'],"['UnsafeFieldAccessorImpl', 'UnsafeQualifiedStaticDoubleFieldAccessorImpl']"
Safety,tive: false numReads: 0; 11:36:40.771 INFO ProgressMeter - chrM:5144 1.0 20 20.4; 11:36:40.774 DEBUG Mutect2 - Processing assembly region at chrM:5444-5743 isActive: false numReads: 0; 11:36:41.211 DEBUG IntToDoubleFunctionCache - cache miss 11898 > 5320 expanding to 11908; 11:36:41.213 DEBUG IntToDoubleFunctionCache - cache miss 17632 > 11908 expanding to 23818; 11:36:41.254 DEBUG IntToDoubleFunctionCache - cache miss 29537 > 23818 expanding to 47638; 11:36:42.578 DEBUG Mutect2 - Processing assembly region at chrM:5744-6043 isActive: false numReads: 0; 11:36:47.533 DEBUG Mutect2 - Processing assembly region at chrM:6044-6343 isActive: false numReads: 30078; 11:36:47.979 DEBUG Mutect2 - Processing assembly region at chrM:6344-6353 isActive: false numReads: 30081; 11:36:48.322 DEBUG Mutect2 - Processing assembly region at chrM:6354-6629 isActive: true numReads: 60135; 11:36:55.630 DEBUG ReadThreadingGraph - Recovered 8 of 11 dangling tails; 11:36:55.645 DEBUG ReadThreadingGraph - Recovered 7 of 16 dangling heads; 11:36:55.737 DEBUG IntToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Haplotype count 128; 11:36:56.119 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:56.120 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:06.762 DEBUG Mutect2 - Processing assembly region at chrM:6630-6929 isActive: false numReads: 30053; 11:39:07.547 DEBUG Mutect2 - Processing assembly region at chrM:6930-7229 isActive: false numReads: 0; 11:39:07.574 DEBUG Mutect2 - Processing assembly region at chrM:7230-7493 isActive: false numReads: 359; 11:39:07.584 DEBUG Mutect2 - Processing assembly region at chrM:7494-7771 isActive: true numReads: 7,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:13117,Recover,Recovered,13117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Safety,"tl;dr Added sanity checks, a smoke test, and fixed sample invocations in Docker build scripts. Last month I built a bad base image when I was unsuccessfully trying to combine GCP and Azure CLIs in one image. What's worse is I seem to have tagged this broken image with a tag previously used for a good version of the image. These changes harden the Docker image build script to refuse to write over an existing tag and execute a smoke test against the image before pushing to GCR. While I was in there I also fixed the sample invocations that we saw during mobbing did not work on all versions of `date`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8291:12,sanity check,sanity checks,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8291,1,['sanity check'],['sanity checks']
Safety,"toplevel.fa -V gendb://my\_database -O output.vcf.gz --new-qual --tmp-dir temp/. c) Entire error log:. Using GATK jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx12g -Xms12g -jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R Triticum\_dicoccoides.WEWSeq\_v.1.0.dna.toplevel.fa -V gendb://my\_database -O output.vcf.gz --new-qual --tmp-dir temp/ ; ; 14:28:22.448 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Jul 07, 2021 2:28:22 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:28:22.617 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 14:28:22.618 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 14:28:22.618 INFO GenotypeGVCFs - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 14:28:22.618 INFO GenotypeGVCFs - Executing as alonzi@khalil1 on Linux v4.19.0-17-amd64 amd64 ; ; 14:28:22.618 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_282-b08 ; ; 14:28:22.618 INFO GenotypeGVCFs - Start Date/Time: July 7, 2021 2:28:22 PM IDT ; ; 14:28:22.618 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 14:28:22.618 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 14:28:22.618 INFO GenotypeGVCFs - HTSJDK Version: 2.24.0 ; ; 14:28:22.618 INFO GenotypeGVCFs - Picard Version: 2.25.0 ; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7348:2471,detect,detect,2471,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7348,1,['detect'],['detect']
Safety,tor.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:14551,abort,abortStage,14551,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['abort'],['abortStage']
Safety,"tute.org/hc/en-us/articles/9570326304155-ScoreVariantAnnotations-BETA-. I used these tags to specify the variables corresponding to argument names (e.g., StandardArgumentDefinitions#INTERVALS_LONG_NAME instead of intervals , USE_ALLELE_SPECIFIC_ANNOTATIONS_LONG_NAME instead of use-allele-specific-annotations, etc.), and while they show up correctly when rendering the Javadoc within IntelliJ, it seems the same is not true on the GATK website. Is there an easy fix in the code for generating these docs, or should I just avoid using this tag?. My original response:. > I tested this using the new Java 17 doclets in the hope that it would just work, but the result is the same. However, the new Java language model classes make it easy to interpolate these, so I’ll fix this in the barclay Java 17 branch. However, in looking more closely, it's not as easy to fix as I first thought, and the problem is a little deeper than I first realized. Although it's easy to detect these using the new Java 17 apis, it's more difficult to retrieve the actual values. And even then, because the gatkdoc process only consumes a subset of the classes consumed by the javadoc process (it only sees `@DocumentedFeature`s), it's quite easy to reference something in the javadoc comment that can be resolved by javdoc, but not by gatkdoc. But it appears that even the javadoc process isn't rendering these tags correctly. Here is the raw javadoc comment:; ```; * Input VCF file. Site-level annotations will be extracted from the contained variants (or alleles,; * if the {@value USE_ALLELE_SPECIFIC_ANNOTATIONS_LONG_NAME} argument is specified).; ```; The rendering in javadoc (the argument name is missing entirely, but it should be interpolated):; <img width=""780"" alt=""Screen Shot 2023-01-05 at 12 17 43 PM"" src=""https://user-images.githubusercontent.com/10062863/210841121-15a4d357-dbfa-47e2-808b-08cdeb6d42be.png"">. The rendering in gatkdoc (the variable name appears in the text, but it should be interpolated)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8146:1159,detect,detect,1159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8146,1,['detect'],['detect']
Safety,"u.edu, executor 2, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:54 INFO TaskSetManager:54 - Lost task 4.2 in stage 0.0 (TID 9) on scc-q20.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 1, start 93925364, span 266689, expected MD5 54babf05a23e9e88a8738dfbb20a1683) [duplicate 2]; 2019-01-09 13:35:56 INFO TaskSetManager:54 - Starting task 4.3 in stage 0.0 (TID 12, scc-q20.scc.bu.edu, executor 2, partition 4, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:56 INFO TaskSetManager:54 - Lost task 1.3 in stage 0.0 (TID 11) on scc-q20.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 3]; 2019-01-09 13:35:56 ERROR TaskSetManager:70 - Task 1 in stage 0.0 failed 4 times; aborting job; 2019-01-09 13:35:56 INFO YarnScheduler:54 - Cancelling stage 0; 2019-01-09 13:35:56 INFO YarnScheduler:54 - Stage 0 was cancelled; 2019-01-09 13:35:56 INFO DAGScheduler:54 - ResultStage 0 (count at CountReadsSpark.java:80) failed in 12.543 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 11, scc-q20.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:32948,abort,aborting,32948,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['abort'],['aborting']
Safety,"u.edu, executor 2, partition 9, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:11 INFO TaskSetManager:54 - Lost task 2.1 in stage 0.0 (TID 7) on scc-q12.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 160972515, span 170618, expected MD5 0cc5e1f5ec5c1b06d5a4bd5fff11b77f) [duplicate 1]; 2019-01-07 11:34:12 INFO TaskSetManager:54 - Starting task 3.3 in stage 0.0 (TID 11, scc-q21.scc.bu.edu, executor 1, partition 3, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:12 INFO TaskSetManager:54 - Lost task 1.3 in stage 0.0 (TID 9) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 3]; 2019-01-07 11:34:12 ERROR TaskSetManager:70 - Task 1 in stage 0.0 failed 4 times; aborting job; 2019-01-07 11:34:12 INFO YarnScheduler:54 - Cancelling stage 0; 2019-01-07 11:34:12 INFO YarnScheduler:54 - Stage 0 was cancelled; 2019-01-07 11:34:12 INFO DAGScheduler:54 - ResultStage 0 (count at CountReadsSpark.java:80) failed in 9.293 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 9, scc-q21.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:4",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:33200,abort,aborting,33200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['abort'],['aborting']
Safety,"ually less expensive in typical (non-extreme) cases than reconstructing the full set of post-downsampling reads in an active region from multiple AlignmentContexts emitted by LIBS without any duplicates. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in LocusIteratorByState) but keep all undownsampled reads in memory anyway (defeating the main purpose of that first pass), then do a second downsampling pass per active region that does not respect dcov (uses the hardcoded per-region limit).; - If we find that we can't avoid storing all of the undownsampled reads in memory at once for some reason, then perhaps the right thing to do would be to completely disable the downsampling pass in LocusIteratorByState for active region traversals, and disallow the -dcov argument for active region walkers. Downsampling would then by controlled solely by the new argument to set the max # of reads per active region.; - Clarifying the meaning of the per-locus DP annotation for the HC given things like realignment of ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:5690,avoid,avoid,5690,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345,1,['avoid'],['avoid']
Safety,"ublic release version 4.1.8.1; java version ; ```; openjdk version ""1.8.0_152-release""; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12); OpenJDK 64-Bit Server VM (build 25.152-b12, mixed mode); ```. ### Description ; keep get the error message like below. ```; Using GATK jar /cache/home/xc278/p/GATK/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Dsamjdk.compression_level=5 -Xms10G -jar /cache/home/xc278/p/GATK/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCallerSpark -R GRCh38_full_analysis_set_plus_decoy_hla.fa -I SRR1573206.GatherBamFiles.bam -O SRR1573206.g.vcf.gz -G StandardAnnotation -G StandardHCAnnotation -G AS_StandardAnnotation -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -ERC GVCF; 09:38:05.617 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 09:38:05.617 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 09:38:05.655 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cache/home/xc278/p/GATK/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 15, 2020 9:38:05 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 09:38:05.911 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 09:38:05.912 INFO HaplotypeCallerSpark - The Genome Analysis Toolkit (GATK) v4.1.8.1; 09:38:05.912 INFO HaplotypeCallerSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:38:05.912 INFO HaplotypeCallerSpark - Executing as xc278@amarel2.amarel.rutgers.edu on Linux v3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6750:1338,Redund,Redundant,1338,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6750,1,['Redund'],['Redundant']
Safety,un-physical-phasing ; false --do-not-correct-overlapping-quality false --use-filtered-reads-for-annotations false --use-flow-aligner-for-stepwise-hc-filtering false --adaptive-pruning false --do-not-recover-dan; gling-branches false --recover-dangling-heads false --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 ; --min-dangling-branch-length 4 --recover-all-dangling-branches false --max-num-haplotypes-in-population 128 --min-pruning 2 --adaptive-pruning-initial-error-rate 0.001 --pruning-lod-thresh; old 2.302585092994046 --pruning-seeding-lod-threshold 9.210340371976184 --max-unpruned-variants 100 --linked-de-bruijn-graph false --disable-artificial-haplotype-recovery false --enable-le; gacy-graph-cycle-detection false --debug-assembly false --debug-graph-transformations false --capture-assembly-failure-bam false --num-matching-bases-in-dangling-end-to-recover -1 --error-; correction-log-odds -Infinity --error-correct-reads false --kmer-length-for-read-error-correction 25 --min-observations-for-kmer-to-be-solid 20 --likelihood-calculation-engine PairHMM --ba; se-quality-score-threshold 18 --dragstr-het-hom-ratio 2 --dont-use-dragstr-pair-hmm-scores false --pair-hmm-gap-continuation-penalty 10 --expected-mismatch-rate-for-read-disqualification 0; .02 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --disable-symmetric-hmm-normalizing false --disable-cap-base-qu; alities-to-map-quality false --enable-dynamic-read-disqualification-for-genotyping false --dynamic-read-disqualification-threshold 1.0 --native-pair-hmm-threads 4 --native-pair-hmm-use-dou; ble-precision false --flow-hmm-engine-min-indel-adjust 6 --flow-hmm-engine-flat-insertion-penatly 45 --flow-hmm-engine-flat-deletion-penatly 45 --pileup-detection false --pileup-detection-; enable-indel-pileup-calling false --num-artificial-haplotypes-to-add-per-allele,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:5432,recover,recovery,5432,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,3,"['detect', 'recover']","['detection', 'recover', 'recovery']"
Safety,"unning:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar Mutect2 --input unique.bam --output unique.mutect2.g.vcf --reference chrM.fa --use-jdk-inflater true --use-jdk-deflater true --mitochondria-mode --emit-ref-confidence GVCF --disable-read-filter ReadLengthReadFilter --disable-read-filter NotDuplicateReadFilter --disable-read-filter MappingQualityAvailableReadFilter --read-filter null --dont-use-soft-clipped-bases true --max-reads-per-alignment-start 0 --min-base-quality-score 0 --soft-clip-low-quality-ends false --verbosity DEBUG; Picked up _JAVA_OPTIONS: -Xmx128g; 11:35:39.890 WARN GATKReadFilterPluginDescriptor - Values were supplied for (ReadLengthReadFilter) that is also disabled; May 31, 2021 11:35:40 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:35:40.175 INFO Mutect2 - ------------------------------------------------------------; 11:35:40.176 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.2.0.0; 11:35:40.176 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:35:40.177 INFO Mutect2 - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_144-b01; 11:35:40.177 INFO Mutect2 - Start Date/Time: May 31, 2021 11:35:39 AM EDT; 11:35:40.177 INFO Mutect2 - ------------------------------------------------------------; 11:35:40.177 INFO Mutect2 - ------------------------------------------------------------; 11:35:40.178 INFO Mutect2 - HTSJDK Version: 2.24.0; 11:35:40.179 INFO Mutect2 - Picard Version: 2.25.0; 11:35:40.179 INFO Mutect2 - Built for Spark Version: 2.4.5; 11:35:40.187 INFO Mutect2 - HTSJDK Defaults.BUFFER_SIZE : 131072; 11:35:40.188 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:35:40.188 INFO Mutect",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:1354,detect,detect,1354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['detect'],['detect']
Safety,"up from the most recent GATK 4.1.7 on a WES sample like this:. gatk PileupSpark --spark-runner SPARK --spark-master local[{threads}] --conf ""spark.driver.memory=22g"" -I $DATA/NA12878.proper.wes.md.bam -R $DATA/Homo_sapiens_assembly18.fasta -O /tmp/gatk4s_{threads}.pileup'. mwiewior@Mareks-MacBook-Pro ~ % spark-submit --version; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 2.4.5; /_/; ; Using Scala version 2.11.12, OpenJDK 64-Bit Server VM, 1.8.0_252; Branch HEAD. No matter how high I set the max file descriptors (even to 1M); mwiewior@Mareks-MacBook-Pro ~ % ulimit -a; -t: cpu time (seconds) unlimited; -f: file size (blocks) unlimited; -d: data seg size (kbytes) unlimited; -s: stack size (kbytes) 8192; -c: core file size (blocks) 0; -v: address space (kbytes) unlimited; -l: locked-in-memory size (kbytes) unlimited; -u: processes 2048; -n: file descriptors 1000000. I'm keep on getting the following error:. 20/06/06 14:56:35 ERROR Utils: Aborting task; org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file file:///private/var/folders/5s/v5t08tmd42z_2m2c30vqf6kc0000gn/T/spark-556aa7a2-4d88-4bae-ad16-36d5af920fa9/userFiles-aeb68992-3215-4897-8f8a-040396296185/Homo_sapiens_assembly18.fasta. Error was: Fasta index file could not be opened: /private/var/folders/5s/v5t08tmd42z_2m2c30vqf6kc0000gn/T/spark-556aa7a2-4d88-4bae-ad16-36d5af920fa9/userFiles-aeb68992-3215-4897-8f8a-040396296185/Homo_sapiens_assembly18.fasta.fai; at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.<init>(CachingIndexedFastaSequenceFile.java:159); at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.<init>(CachingIndexedFastaSequenceFile.java:125); at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.<init>(CachingIndexedFastaSequenceFile.java:110); at org.broadinstitute.hellbender.engine.ReferenceFileSource.<init>(ReferenceFileSourc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6642:1042,Abort,Aborting,1042,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6642,1,['Abort'],['Aborting']
Safety,"updating our git lfs recommendation to a newer version to avoid the problems Geraldine had. fixes #952. <!-- Reviewable:start -->. [<img src=""https://reviewable.io/review_button.png"" height=40 alt=""Review on Reviewable""/>](https://reviewable.io/reviews/broadinstitute/gatk/1108). <!-- Reviewable:end -->",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1108:58,avoid,avoid,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1108,1,['avoid'],['avoid']
Safety,uplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 18:35:26.517 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 18:35:26.517 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:35:26.517 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 18:35:26.517 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 18:35:26.517 INFO MarkDuplicatesSpark - GCS max retries/reopens: 20; 18:35:26.517 INFO MarkDuplicatesSpark - Requester pays: disabled; 18:35:26.517 INFO MarkDuplicatesSpark - Initializing engine; 18:35:26.517 INFO MarkDuplicatesSpark - Done initializing engine; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/user/wup/miniconda3/envs/gatk/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar) to method java.nio.Bits.unaligned(); WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 20/10/08 18:35:27 INFO SparkContext: Running Spark version 2.4.5; 18:35:27.640 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 20/10/08 18:35:27 INFO SparkContext: Submitted application: MarkDuplicatesSpark; 20/10/08 18:35:27 INFO SecurityManager: Changing view acls to: wup; 20/10/08 18:35:27 INFO SecurityManager: Changing modify acls to: wup; 20/10/08 18:35:27 INFO SecurityManager: Changing view acls groups to: ; 20/10/08 18:35:27 INFO SecurityManager: Changing modify acls groups to: ; 20/10/08 18:35:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(wup); groups with view p,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6875:2277,unsafe,unsafe,2277,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6875,1,['unsafe'],['unsafe']
Safety,"ur help!. Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_s amtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_leve l=2 -jar /gatk/gatk-package-4.2.0.0-local.jar GetPileupSummaries -I /gatk/my\_dat a/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_processi ng\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -V /gat k/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common \_3.hg19.vcf.gz -O /gatk/my\_data/wgs\_BAM/step1\_3/getpileupsummaries\_LP6005115-DNA \_B07.table ; ; 01:03:32.752 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar: file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compressi on.so ; ; Sep 12, 2021 1:03:32 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCre dentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 01:03:32.953 INFO GetPileupSummaries - ---------------------------------------- -------------------- ; ; 01:03:32.954 INFO GetPileupSummaries - The Genome Analysis Toolkit (GATK) v4.2. 0.0 ; ; 01:03:32.954 INFO GetPileupSummaries - For support and documentation go to http s://software.broadinstitute.org/gatk/ ; ; 01:03:32.954 INFO GetPileupSummaries - Executing as root@a2e87404023d on Linux v5.8.0-1039-azure amd64 ; ; 01:03:32.954 INFO GetPileupSummaries - Java runtime: OpenJDK 64-Bit Server VM v 1.8.0\_242-8u242-b08-0ubuntu3~18.04-b08 ; ; 01:03:32.954 INFO GetPileupSummaries - Start Date/Time: September 12, 2021 1:03 :32 AM GMT ; ; 01:03:32.955 INFO GetPileupSummaries - ---------------------------------------- -------------------- ; ; 01:03:32.955 INFO GetPileupSummaries - ---------------------------------------- -------------------- ; ; 01:03:32.955 INFO GetPileupSummaries - HTSJDK Version: 2.24.0 ; ; 01:03:32.955 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7479:2598,detect,detect,2598,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7479,1,['detect'],['detect']
Safety,use-dragstr-pair-hmm-scores false --pair-hmm-gap-continuation-penalty 10 --expected-mismatch-rate-for-read-disqualification 0; .02 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --disable-symmetric-hmm-normalizing false --disable-cap-base-qu; alities-to-map-quality false --enable-dynamic-read-disqualification-for-genotyping false --dynamic-read-disqualification-threshold 1.0 --native-pair-hmm-threads 4 --native-pair-hmm-use-dou; ble-precision false --flow-hmm-engine-min-indel-adjust 6 --flow-hmm-engine-flat-insertion-penatly 45 --flow-hmm-engine-flat-deletion-penatly 45 --pileup-detection false --pileup-detection-; enable-indel-pileup-calling false --num-artificial-haplotypes-to-add-per-allele 5 --artifical-haplotype-filtering-kmer-size 10 --pileup-detection-snp-alt-threshold 0.1 --pileup-detection-i; ndel-alt-threshold 0.5 --pileup-detection-absolute-alt-depth 0.0 --pileup-detection-snp-adjacent-to-assembled-indel-range 5 --pileup-detection-bad-read-tolerance 0.0 --pileup-detection-pro; per-pair-read-badness true --pileup-detection-edit-distance-read-badness-threshold 0.08 --pileup-detection-chimeric-read-badness true --pileup-detection-template-mean-badness-threshold 0.0; --pileup-detection-template-std-badness-threshold 0.0 --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --override-fragment-softclip-check false --min-base-quality-s; core 10 --smith-waterman JAVA --max-mnp-distance 0 --force-call-filtered-alleles false --reference-model-deletion-quality 30 --soft-clip-low-quality-ends false --allele-informative-reads-o; verlap-margin 2 --smith-waterman-dangling-end-match-value 25 --smith-waterman-dangling-end-mismatch-penalty -50 --smith-waterman-dangling-end-gap-open-penalty -110 --smith-waterman-danglin; g-end-gap-extend-penalty -6 --smith-waterman-haplotype-to-reference-match-value 200 --smith-waterman-haplotype-to-reference-mismatch-penalty -150 --smith-waterman-ha,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:6864,detect,detection-snp-adjacent-to-assembled-indel-range,6864,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,2,['detect'],"['detection-bad-read-tolerance', 'detection-snp-adjacent-to-assembled-indel-range']"
Safety,"ut variants.funcotated.vcf --output-file-format VCF; Using GATK jar /home/deepak/software_library/gatk-4.1.7.0/gatk-package-4.1.7.0-local.jar. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/deepak/software_library/gatk-4.1.7.0/gatk-package-4.1.7.0-local.jar Funcotator --variant /home/deepak/software_library/gatk-4.1.7.0/SAMPL3_VARIANTFIL.vcf --reference /media/deepak/EXTRA/Genomedir/hg38/hg38.fasta --ref-version hg38 --data-sources-path /media/deepak/EXTRA/FUNCOTATOR_DATA/DATA_SOURCES --output variants.funcotated.vcf --output-file-format VCF; 16:01:36.165 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/deepak/software_library/gatk-4.1.7.0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 12, 2020 4:01:36 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:01:36.870 INFO Funcotator - ------------------------------------------------------------; 16:01:36.871 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.1.7.0; 16:01:36.871 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:01:36.871 INFO Funcotator - Executing as deepak@ngs on Linux v5.3.0-26-generic amd64; 16:01:36.871 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_241-b07; 16:01:36.871 INFO Funcotator - Start Date/Time: 12 May, 2020 4:01:35 PM IST; 16:01:36.871 INFO Funcotator - ------------------------------------------------------------; 16:01:36.871 INFO Funcotator - ------------------------------------------------------------; 16:01:36.872 INFO Funcotator - HTSJDK Version: 2.21.2; 16:01:36.872 INFO Funcotator - Picard Version: 2.21.9; 16:01:36.872 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:01:36.872 INFO Funcotator - H",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-664565036:1281,detect,detect,1281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-664565036,1,['detect'],['detect']
Safety,"ut.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] Old code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be for",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:1895,redund,redundant,1895,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,2,['redund'],['redundant']
Safety,"utCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4725:2529,abort,abortStage,2529,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725,1,['abort'],['abortStage']
Safety,"ute.org/hc/en-us/articles/4405451404699-Concordance#--summary](/hc/en-us/articles/4405451404699-Concordance#--summary)). Please also find the log file below. Is the summary file required as input file to run the below script? Please advice.  gatk Concordance \\ ; ;    -R /scicore/home/cichon/GROUP/memory\_optimization/data/reference/gch38.fa \\ ; ;    -eval /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/sample1\_affect.filtered.vcf \\ ; ;    --truth /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/NA12878.vcf.gz \\ ; ;    --summary /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/summary.tsv   ; 11:26:21.545 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/scicore/soft/apps/GATK/4.2.2.0-foss-2018b-Java-1.8/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Nov 11, 2021 11:26:21 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:26:21.681 INFO Concordance - ------------------------------------------------------------ ; ; 11:26:21.682 INFO Concordance - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 11:26:21.682 INFO Concordance - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:26:21.682 INFO Concordance - Executing as [thirun0000@shi85.cluster.bc2.ch](mailto:thirun0000@shi85.cluster.bc2.ch) on Linux v3.10.0-1062.18.1.el7.x86\_64 amd64 ; ; 11:26:21.682 INFO Concordance - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_212-b03 ; ; 11:26:21.682 INFO Concordance - Start Date/Time: November 11, 2021 11:26:21 AM CET ; ; 11:26:21.682 INFO Concordance - ------------------------------------------------------------ ; ; 11:26:21.682 INFO Concordance - ------------------------------------------------------------ ; ; 11:26:21.683 INFO Concordance - HTSJDK Version: 2.24.1 ; ;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7562:1912,detect,detect,1912,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7562,1,['detect'],['detect']
Safety,uter.access$200(ExecuteActionsTaskExecuter.java:93); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$TaskExecution.execute(ExecuteActionsTaskExecuter.java:237); at org.gradle.internal.execution.steps.ExecuteStep.lambda$execute$1(ExecuteStep.java:33); at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:33); at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:26); at org.gradle.internal.execution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:58); at org.gradle.internal.execution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:35); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.steps.StoreSnapshotsSte,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:8551,Timeout,TimeoutStep,8551,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Timeout'],['TimeoutStep']
Safety,"utor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:745); 2019-02-17 16:25:50 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-02-17 16:25:50 INFO MemoryStore:54 - MemoryStore cleared; 2019-02-17 16:25:50 INFO BlockManager:54 - BlockManager stopped; 2019-02-17 16:25:50 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-02-17 16:25:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-02-17 16:25:50 INFO SparkContext:54 - Successfully stopped SparkContext; 16:25:50.893 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 17, 2019 4:25:50 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 5.28 minutes.; Runtime.totalMemory()=5059379200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 181 in stage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:47127,abort,aborted,47127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['abort'],['aborted']
Safety,"utors.java:511); at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$han",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4515:2935,abort,aborted,2935,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515,1,['abort'],['aborted']
Safety,"vaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.contig-sam-file.sam\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.sv.vcf.gz \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode client \; --executor-memory 85G\; --driver-memory 30g\; --num-executors 40\; --executor-cores 4\; --conf spark.yarn.submit.waitAppCompletion=false\; --name ""$SAMPLE"" \; --files $REF.img,$KMER \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120; #### Expected behavior. Should complete and write output files. . #### Actual behavior; Job aborts after running 45 min and no output files are written. The error message refers to filename that is not actually passed as a parameter to the tool: hdfs://scc:-1/. Not sure where the -1 is coming from. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942:6065,timeout,timeout,6065,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942,2,"['abort', 'timeout']","['aborts', 'timeout']"
Safety,"valid-interval-in-FuncotateSegments](https://gatk.broadinstitute.org/hc/en-us/community/posts/4418364848795-java-lang-IllegalArgumentException-Invalid-interval-in-FuncotateSegments). \--. Hi,. I tried to annotated a called segment file after following the somatic CNV detection workflow of GATK:. gatk --java-options ""-Xmx10g -Djava.io.tmpdir=/lscratch/$SLURM\_JOBID"" FuncotateSegments \\ ; ; \--data-sources-path funcotator\_dataSources.v1.7.20200521s/ \\ ; ; \--ref-version hg19 \\ ; ; \--output-file-format SEG \\ ; ; \-R hs37d5.fa \\ ; ; \--segments sample.called.seg \\ ; ; \-O sample.seg.funcotated.tsv \\ ; ; \--transcript-list funcotator\_dataSources.v1.7.20200521s/transcriptList.exact\_uniprot\_matches.AKT1\_CRLF2\_FGFR1.txt. But I got the following error message:. 12:37:55.534 INFO  FuncotateSegments - The following datasources support funcotation on segments:  ; ; 12:37:55.535 INFO  FuncotateSegments -  Gencode 34 CANONICAL ; ; 12:37:55.542 INFO  FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode.  Performing conversion. ; ; 12:37:55.542 WARN  FuncotatorEngine - WARNING: You are using B37 as a reference.  Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases.  There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references. ; ; 12:37:55.679 INFO  ProgressMeter - Starting traversal ; ; 12:37:55.679 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Features Processed  Features/Minute ; ; 12:37:56.198 WARN  FuncotatorUtils - Reference allele is different than the reference coding sequence (strand: -, alt = G, ref G != T reference coding seq) @\[chr1:13839497\]!  Substituting given allele for sequence code (TTC->GTC) ; ; 12:37:56.213 INFO  FuncotateSegments - Shutting down engine ; ; \[February 9, 2022 12:37:56 PM EST\] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.24 mi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7676:1210,detect,detected,1210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676,1,['detect'],['detected']
Safety,variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:638); at picard.util.LiftoverUtils.liftVariant(LiftoverUtils.java:92); at picard.vcf.LiftoverVcf.doWork(LiftoverVcf.java:426); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292). ```. #### Steps to reproduce. Download vcf from here:. ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_SVs_Integration_v0.6/HG002_SVs_Tier1_v0.6.vcf.gz. gatk LiftoverVcf \; -I b37/HG002_SVs_Tier1_v0.6.vcf.gz \; -O b38/HG002_SVs_Tier1_v0.6.hg38.vcf.gz \; -CHAIN grch37_to_grch38.over.chain.gz \; --REJECT b38/HG002_SVs_Tier1_v0.6.rejected.vcf.gz \; -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa. #### Expected behavior; The original b37 vcf has a deletion here:. 1 532077 ACATTCATGCTCACTCATACACACCCAGATCATATATACACTCGTGCACACATTCACACTCATACACACCCAAATCATACTCACATTCATGCACACATGTT A; SVLEN=-100;;SVTYPE=DEL;END=532177;sizecat=100to299;. The liftover to hg38 should look like this:; chr1 596697 REF=ACATTCATGCTCACTCATACACACCCAGATCATATATACACTCGTGCACACATTCACACTCATACACACCCAAATCATACTCACATTCATGCACACATGTT; ALT=A; INFO Fields; SVLEN=-100; SVTYPE=DEL;END=596797;sizecat=100to299;. The error message suggests LiftoverVcf is not updating the INFO/END field from 532177 to 596797 and an error is being triggered since the END is before the start. An incorrect INFO/END will cause problems with tabix and other programs. #### Actual behavior; It generates an error when the INFO/END is before the start and aborts.. ----. ## Feature request; Liftover INFO/END . ### Description; ; The INFO/END position also needs to be updated-not just the site position.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6725:5228,abort,aborts,5228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6725,1,['abort'],['aborts']
Safety,"vcf \\ ; ; .... ; ; \--variant normal80.vcf \\ ; ; \--genomicsdb-workspace-path pon\_db \\ ; ; \--tmp-dir /tmp1 \\ ; ; \-L /gatk\_bundle/hglft\_genome\_3bc14\_d6f440.bed \\ ; ; \--sequence-dictionary /gatk\_bundle/hg19\_v0\_Homo\_sapiens\_assembly19.dict \\ ; ; \--reader-threads 15 \\ ; ; \--java-options '-DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true'; ```. Here For interval list, I have downloaded the hg38 target interval from GATK resource bundle and converted into hg19 format using UCSC liftover utility. GenomicsDBImport is not reporting any error related to command but also not reporting any results. Here are the details from GenomicsDBImport log file:. ```; 17:16:16.069 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/akansha/vivekruhela/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Jan 12, 2021 5:16:16 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 17:16:16.329 INFO GenomicsDBImport - ------------------------------------------------------------ ; ; 17:16:16.329 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.9.0 ; ; 17:16:16.329 INFO GenomicsDBImport - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 17:16:16.330 INFO GenomicsDBImport - Executing as akansha@sbilab on Linux v4.4.0-169-generic amd64 ; ; 17:16:16.330 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_265-8u265-b01-0ubuntu2~16.04-b01 ; ; 17:16:16.330 INFO GenomicsDBImport - Start Date/Time: January 12, 2021 5:16:16 PM IST ; ; 17:16:16.330 INFO GenomicsDBImport - ------------------------------------------------------------ ; ; 17:16:16.330 INFO GenomicsDBImport - ------------------------------------------------------------ ; ; 17:16:16.331 INFO GenomicsDBImport - HTSJDK Version: 2.23.0 ; ; 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7037:2523,detect,detect,2523,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7037,1,['detect'],['detect']
Safety,"vcf.gz to gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz; 11:25:55.531 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 30, 2021 11:25:55 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:25:55.708 INFO ReblockGVCF - ------------------------------------------------------------; 11:25:55.709 INFO ReblockGVCF - The Genome Analysis Toolkit (GATK) v4.2.0.0; 11:25:55.709 INFO ReblockGVCF - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:25:55.709 INFO ReblockGVCF - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.25.1.el7.x86_64 amd64; 11:25:55.709 INFO ReblockGVCF - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 11:25:55.709 INFO ReblockGVCF - Start Date/Time: June 30, 2021 11:25:55 AM EDT; 11:25:55.710 INFO ReblockGVCF - ------------------------------------------------------------; 11:25:55.710 INFO ReblockGVCF - ------------------------------------------------------------; 11:25:55.710 INFO ReblockGVCF - HTSJDK Version: 2.24.0; 11:25:55.710 INFO ReblockGVCF - Picard Version: 2.25.0; 11:25:55.710 INFO ReblockGVCF - Built for Spark Version: 2.4.5; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7334:1773,detect,detect,1773,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7334,1,['detect'],['detect']
Safety,"versed.selfRef.shifted.homoplasmies.vcf.bgz \\ ; ; \--annotation StrandBiasBySample \\ ; ; \--mitochondria-mode \\ ; ; \--max-reads-per-alignment-start 75 \\ ; ; \--max-mnp-distance 0 \\ ; ; \-L chrM:8023-9140 \\ ; ; \--genotype-filtered-alleles \\ ; ; \--debug-assembly-variants-out /rej.vcf \\ ; ; \--bam-output bamout.bam. In this instance the variant in question is listed in the rej.vcf file obtained via `--debug-assembly-variants-out`. I have examined `bamout.bam` as well as the input bam and there appears to be ample coverage at the site of interest (the T at position 8316 is the position of interest, highlighted):. ![](https://gatk.broadinstitute.org/hc/user_images/aGbHKebG7Tb8Lgu33gGzXw.png). I have tried running this with some of the additional parameters in \[[https://gatk.broadinstitute.org/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant\](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant)](https://gatk.broadinstitute.org/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant)) (namely `--linked-de-bruijn-graph` and `--recover-all-dangling-branches`) to no avail. Coverage is very deep at this position (>2000x). Notably if I edit the input to `--alleles` and change the allele of interest (8316:T>A) to anything else (8316:T>C or T>G) it appropriately shows up in the output VCF. What am I missing here? Let me know if you have any solutions or if you need any additional files. UPDATE: Adding `--disable-adaptive-pruning` now produces the variant of interest specified in --alleles, but also adds several other new calls, in case that is helpful in isolating where this force-call variant is being lost.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/270138'>Zendesk ticket #270138</a>)<br> gz#270138</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7672:2471,recover,recover-all-dangling-branches,2471,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7672,1,['recover'],['recover-all-dangling-branches']
Safety,we don't test on windows and I doubt gatk would work on windows so let's not generate the windows startup to avoid confusing people. @lbergelson can you look at it?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1591:109,avoid,avoid,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1591,1,['avoid'],['avoid']
Safety,we need tests like those in `AbstractMarkDuplicatesCommandLineProgramTest` to run on Spark (this is required because the duplicate-detection logic is reimplemented on Spark and based on uniqueness of generated keys). Also `MarkDuplicatesTest` and probably `OpticalDuplicateFinderTest`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/591:131,detect,detection,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/591,1,['detect'],['detection']
Safety,"weinkauf/notes/persistence1d.html and http://www2.iap.fr/users/sousbie/web/html/indexd3dd.html?post/Persistence-and-simplification). A straightforward watershed algorithm can sort all local minima by persistence in linear time after an initial sort of the data.; 5) These sets of local minima from all window sizes together provide the pool of candidate changepoints (some of which may overlap exactly or approximately). We perform backwards selection using the global segmentation cost. That is, we compute the global segmentation cost given all the candidate changepoints, calculate the cost change for removing each of the changepoints individually, remove the changepoint with the minimum cost change, and repeat. This gives the global cost as a function of the number of changepoints _C_.; 6) Add a penalty _a C + b C log(N / C)_ to the global cost and find the minimum to determine the number of changepoints. For the above simulated data, _a = 2_ and _b = 2_ works well, recovering all of the changepoints in the above example with no false positives:; ![6](https://user-images.githubusercontent.com/11076296/29582517-fbd604be-874a-11e7-8ef7-7bd727f65dcb.png). ![7](https://user-images.githubusercontent.com/11076296/29582518-fddf015c-874a-11e7-89e4-87250d2a52ab.png). In contrast, CBS produces two false positives (around the third and seventh of the true changepoints):. ![8](https://user-images.githubusercontent.com/11076296/29582545-18875126-874b-11e7-9166-9061bb120e43.png). We can change the penalty factor to smooth out less significant segments (which may be due to systematic noise, GC waves, etc.). Setting _a = 10, b = 10_ gives:; ![9](https://user-images.githubusercontent.com/11076296/29582598-515dffe0-874b-11e7-93c4-59422cd43b54.png); ![10](https://user-images.githubusercontent.com/11076296/29583130-0fe5316c-874d-11e7-8504-43618928cf68.png). (Note that the DNAcopy implementation of CBS does not allow for such simple control of the ""false-positive rate,"" as even setting the",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586:2866,recover,recovering,2866,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586,2,['recover'],['recovering']
Safety,"when trying to build GATK fully I get this error:; ```; > Task :gatkDoc FAILED; Execution optimizations have been disabled for task ':gatkDoc' to ensure correctness due to the following reasons:; - Gradle detected a problem with the following location: '/home/jeremie/GATK/build/classes/java/main'. Reason: Task ':gatkDoc' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/home/jeremie/GATK/build/resources/main'. Reason: Task ':gatkDoc' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/jeremie/GATK/build/tmp/gatkDoc/javadoc.options'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.gradle.org/7.3.2/userguide/command_line_interface.html#sec:command_line_warnings. Execution optimizations have been disabled for 1 invalid unit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7936#issuecomment-1202544500:205,detect,detected,205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7936#issuecomment-1202544500,2,['detect'],['detected']
Safety,"whether these supplementary alignments that get flagged secondary (with the `-M) also get MAPQ of 0 or have other nonzero MAPQs. We want our tools, including HaplotypeCaller, to differentiate supplementary alignments and secondary alignments and use supplementary alignments in variant discovery. . Secondary alignments are meant for multimappers (multiple valid mapping locations) and supplementary alignments are meant for chimeric reads (say two records for the same read where one half aligns to the left and the other half aligns to the right of a very large deletion against the reference). This means that we should run bwa mem without the `-M` option. . Ok, so I'm going to resume thinking HaplotypeCaller filters on MAPQ of 20. ---. @sooheelee commented on [Wed May 11 2016](https://github.com/broadinstitute/gsa-unstable/issues/1360#issuecomment-218558684). The implications of this is that (I think) for any workflow that cares about detecting indels in the size range that BWA-MEM would create supplementary alignment records for would then require that we run BWA-MEM without the `-M` option that we currently recommend. We want both types of mappings. ---. @vdauwera commented on [Wed May 11 2016](https://github.com/broadinstitute/gsa-unstable/issues/1360#issuecomment-218591369). For anyone confused about the difference between secondary and supplementary alignments: http://seqanswers.com/forums/showthread.php?t=40239. Currently we actually *don't* want our variant calling tools to distinguish them -- we prefer to consider them unusable. My understanding is that the size of events that lead to supplementary alignments fall into the scope of structural variation, and any reads that map across them are considered suspect. . So anyway for this user's case it sounds like the difference between versions is that 3.1 was making a call based on data that we (no longer?) consider as receivable evidence. So we should be satisfied with the call made by 3.5. @ldgauthier would you sa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2916:8495,detect,detecting,8495,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2916,1,['detect'],['detecting']
Safety,"wup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta -I /juffowup2/malaria/haplotypecaller_arg_testing/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam -O /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz --bam-output /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam -contamination 0 --sample-ploidy 2 --linked-de-bruijn-graph --pileup-detection true --pileup-detection-enable-indel-pileup-calling true --max-reads-per-alignment-start 20 --annotate-with-num-discovered-alleles -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -G StandardAnnotation -G StandardHCAnnotation -ERC GVCF --verbosity INFO; 14:14:15.323 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 14:14:15.328 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 14:14:15.388 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/juffowup/gatk/build/install/gatk/lib/gkl-0.8.11.jar!/com/intel/gkl/native/libgkl_compression.so; 14:14:15.435 INFO HaplotypeCaller - ------------------------------------------------------------; 14:14:15.439 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0-44-g1529aa1-SNAPSHOT; 14:14:15.439 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:14:15.439 INFO HaplotypeCaller - Executing as jonn@dsde-methods-jonn-juffowup on Linux v5.4.0-1104-gcp amd64; 14:14:15.439 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v17.0.7+7; 14:14:15.440 INFO HaplotypeCaller - Start Date/Time: July 26, 2023 at 2:14:15 PM UTC; ...; 22:15:34.977 INFO HaplotypeCaller - Shutting down engine; [July 26, 2023 at 10:15:34 PM UTC] org.broadinstitute.hellbender.tools.walkers.haplotypecal",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8440:1222,Redund,Redundant,1222,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440,1,['Redund'],['Redundant']
Safety,"y large pileup which is probably instigating this. Additionally, if I remove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136.1; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:56:43.998 INFO ProgressMeter - Pf3D7_13_v3:2730055 100.9 114840 1138.3; 15:56:59.911 INFO ProgressMeter - Pf3D7_13_v3:2798281 101.2 115210 1139.0; 15:59:27.062 INFO ProgressMeter - Pf3D7_13_v3:2861780 103.6 115460 1114.4; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:59:37.457 INFO ProgressMeter - Pf3D7_13_v3:2869697 103.8 115500 1112.9. real 671m24.770s; user 777m30.923s; sys 6m13.682s. $ echo $?; 247; ```. Here is my command-line invocation:; ```; ./gatk --java-options ""-Xmx100000m -Xms25000m"" \; HaplotypeCaller \; -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta \; -I ${WORKING_DIR}/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam \; -O ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz \; --bam-output ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam \; -contamination 0 \; --sample-ploidy 2 \; --linked-de-bruijn-graph \; --pileup-detection true \; --pileup-detection-enable-indel-pileup-calling true \; --max-reads-per-alignment-start 20 \; --annotate-with-num-discovered-alleles \; -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 \; -G StandardAnn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8440:5601,recover,recovery,5601,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440,2,['recover'],['recovery']
Safety,y(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:50268,abort,abortStage,50268,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['abort'],['abortStage']
Safety,y(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); at org,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:38855,abort,abortStage,38855,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,4,['abort'],['abortStage']
Safety,"y.; The annotation dumped here is transcribed and translated from the ; genome assembly and is not the original input sequence data that ; we used for alignment. Therefore, the sequences provided by Ensembl ; may differ from the original input sequence data where the genome ; assembly is different to the aligned sequence. . Additionally, we provide a GTF file containing the predicted gene set; as generated by Genscan and other abinitio prediction tools.; This file is identified by the abinitio extension. -----------; FILE NAMES; ------------; The files are consistently named following this pattern:; <species>.<assembly>.<version>.gtf.gz. <species>: The systematic name of the species.; <assembly>: The assembly build name.; <version>: The version of Ensembl from which the data was exported.; gtf : All files in these directories are in GTF format; gz : All files are compacted with GNU Zip for storage efficiency. e.g.; Homo_sapiens.GRCh38.81.gtf.gz. For the predicted gene set, an additional abinitio flag is added to the name file.; <species>.<assembly>.<version>.abinitio.gtf.gz. e.g.; Homo_sapiens.GRCh38.81.abinitio.gtf.gz. --------------------------------; Definition and supported options; --------------------------------. The GTF (General Transfer Format) is an extension of GFF version 2 ; and used to represent transcription models. GFF (General Feature Format) ; consists of one line per feature, each containing 9 columns of data. . Fields. Fields are tab-separated. Also, all but the final field in each ; feature line must contain a value; ""empty"" columns are denoted ; with a '.'. seqname - name of the chromosome or scaffold; chromosome names ; without a 'chr' ; source - name of the program that generated this feature, or ; the data source (database or project name); feature - feature type name. Current allowed features are; {gene, transcript, exon, CDS, Selenocysteine, start_codon,; stop_codon and UTR}; start - start position of the feature, with sequence numbering ; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6488:1909,predict,predicted,1909,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6488,1,['predict'],['predicted']
Safety,"ype?; - The above is especially thorny for haplotypes that exhibit multiple variants.; - The FRD prior is only defined for individual events, not haplotypes.; - The BQD and FRD models use reads that overlap a variant site, but it is not clear how to use reads that only partially intersect a haplotype.; - BQD and FRD likelihoods are only defined for homozygous haplotypes, but heterozygous combinations of _haplotypes_ contribute to homozygous genotypes all loci where the distinct haplotypes agree. Clearly, generalizing BQD and FRD to entire haplotypes is not straightforward. Nor does it suffice to produce ""raw"" genotype likelihoods using the joint detection approach and then apply BQD and FRD on variant loci afterwards. Some difficulties with this include:. - BQD and FRD require the read-allele likelihoods matrix. Where are these likelihoods supposed to come from? The pre-joint-detection unrigorous ""marginalization"" where to each allele we assign the maximum likelihood over all haplotypes supporting that allele? Some read-allele likelihoods matrix derived from the read-haplotype likelihoods matrix?; - The drawbacks of the faulty ""marginalization"" actually become more severe with joint detection since genotyping multiple alleles together in a single determined span produces more haplotypes, which in turn increases the risk of the read-allele likelihoods cherry-picking from too many different haplotypes for different reads.; - The BQD and FRD models produce likelihoods on an absolute scale that is only meaningful relative to genotyping likelihoods from the pre-joint-detection approach. They do not inherently ""play nicely"" with the posterior probabilities produced by joint detection.; - BQD and FRD as currently implemented in the GATK modify likelihoods _before_ applying a prior, whereas joint detection yields posterior probabilities. Are we supposed to somehow un-apply the prior to joint detection likelihoods, apply BQD and FRD, then re-apply the prior? It is not clear.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8616:1800,detect,detection,1800,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8616,7,"['detect', 'risk']","['detection', 'risk']"
Safety,"{vcf_dir}/genomicdbimport_chr${1} \; -R ${ref_gen}/ucsc.hg19.fasta \; --batch-size 0 \; --sample-name-map ${gvcf}/batch_cohort.sample_map \; --reader-threads 5; check_return_code. # For GenotypeGVCFs; time ${gatk} --java-options ""-Xmx8g -Xms2g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" GenotypeGVCFs \; -R ${ref_gen}/ucsc.hg19.fasta \; -V gendb://${vcf_dir}/genomicdbimport_chr${1} \; -G StandardAnnotation \; -G AS_StandardAnnotation \; -L chr${1} \; -O ${bgvcf}/all_${seq_type}_samples_plus_${sample_batch}.chr${1}.HC.vcf. # These are log records:; 02:07:51.286 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 02:07:51.321 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/yangyxt/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl; Nov 06, 2020 2:07:56 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 02:07:56.529 INFO GenotypeGVCFs - ------------------------------------------------------------; 02:07:56.529 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 02:07:56.530 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 02:08:01.543 INFO GenotypeGVCFs - Executing as yangyxt@paedyl01 on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 02:08:01.543 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 02:08:01.543 INFO GenotypeGVCFs - Start Date/Time: November 6, 2020 2:07:51 AM HKT; 02:08:01.543 INFO GenotypeGVCFs - ------------------------------------------------------------; 02:08:01.544 INFO GenotypeGVCFs - ------------------------------------------------------------; 02:08:01.544 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 02:08:01.545 INFO GenotypeGVCFs - Picard Version: 2.22.8; 02:08:01.545 INFO GenotypeGVCFs - HTSJDK Defaults.C",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-722764059:1527,detect,detect,1527,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-722764059,1,['detect'],['detect']
Safety,"…bly to be activated if a mininum number of pieces of evidence agree on the distal target. Also:. - Some refactoring of the SATagAlignment and builder classes to support better treatment of SA tags.; - Increased the spark network timeout values for the SV pipeline to prevent nodes from losing heartbeats and being orphaned with running tasks. Since I made this change I have not had the issue. On the performance of this change on our calls:. I compared this branch with master. Master's results on the CHM1/13 mix:. ```; 16:57:37.270 INFO StructuralVariationDiscoveryPipelineSpark - Metadata retrieved.; 16:58:20.436 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 25977 intervals.; 16:58:20.517 INFO StructuralVariationDiscoveryPipelineSpark - Killed 377 intervals that were near reference gaps.; 16:58:49.939 INFO StructuralVariationDiscoveryPipelineSpark - Killed 175 intervals that had >1000x coverage.; 16:59:33.036 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 8773016 mapped template names.; 17:00:07.058 INFO StructuralVariationDiscoveryPipelineSpark - Ignoring 19200460 genomically common kmers.; 17:05:25.896 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 34752266 kmers.; 17:10:46.253 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 31945322 unique template names for assembly.; 17:45:06.748 INFO StructuralVariationDiscoveryPipelineSpark - Wrote SAM file of aligned contigs.; 17:45:26.199 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 5716 variants.; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - INV: 231; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 3262; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 1065; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1158; 17:45:26.397 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [May 8, 2017 5:45:26 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDis",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2684:230,timeout,timeout,230,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2684,1,['timeout'],['timeout']
Safety,"…joint-genotyping from the resulting GVCFs. @ldgauthier & @davidbenjamin this PR is a follow up from our short conversation in #4650 a couple of months ago, where I was wanting to generate GVCFs with MNP support. My goal here is that I really want to be able to generate a single VCF that a) gives me reference confidence and b) gives me MNPs for close by variants. This is for a clinical pipeline where all calling is done one sample at a time, so the problem of joint-genotyping from different MNP representations doesn't come up. I did briefly look at using `--emit-ref-confidence BP_RESOLUTION` but that has two issues that make me prefer this route:. 1) The generated files are really very large because they have a row for every single BP; 2) More problematic, is that when there is a MNP of say `ACG/GCT` two things happen that are less than ideal from my perspective. The first is that rows are emitted into the VCF for all three positions (the variant at A's position, and two `<NON_REF>` lines at the positions for the C and T respectively). Secondly, when one or more bases is the same in both MNP alleles (the C in this case) that base is output with a very high hom-ref GQ, which feels wrong!. I'm more than happy to modify this PR to address any concerns you have (e.g. adding a `--force-mnps-with-gvcfs` parameter that has to be specified, or requiring `--unsafe` to enable this). I'm also open to other solutions, but this seemed expedient and reasonable for folks running single-sample pipelines like you see in clinical settings.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5182:1371,unsafe,unsafe,1371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5182,1,['unsafe'],['unsafe']
Safety,"👍 Looks good to me. Did you want to try to switch to the release version, or should we merge this as is? . I didn't know even know we had redundant `hidden` / `hiddenOption` tags... both unused.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2293#issuecomment-264959626:138,redund,redundant,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2293#issuecomment-264959626,1,['redund'],['redundant']
Security,"	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); 	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); 	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 17/10/18 17:35:58 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/10/18 17:35:58 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-1,5,main]; java.lang.OutOfMemoryError: Java heap space; 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:208); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFil",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749:5520,access,access,5520,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749,1,['access'],['access']
Security,	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at org.gradle.wrapper.Download.downloadInternal(Download.java:77); 	at org.gradle.wrapper.Download.download(Download.java:44); 	at org.gradle.wrapper.Install$1.call(Install.java:61); 	at org.gradle.wrapper.Install$1.call(Install.java:48); 	at org.gradle.wrapper.ExclusiveFileAccessManager.access(ExclusiveFileAccessManager.java:69); 	at org.gradle.wrapper.Install.createDist(Install.java:48); 	at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:107); 	at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:116); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3375); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3368); 	at org.gradle.wrapper.Download.downloadInternal(Download.java:62); 	... 7 more; Caused by: java.net.SocketException: Connection reset; 	at java.net.SocketInputStream.read(SocketInputStream.java:210); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401:1788,secur,security,1788,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401,1,['secur'],['security']
Security," 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789); 	at StudentAws$.delayedEndpoint$StudentAws$1(StudentAws.scala:36); 	at StudentAws$delayedInit$body.apply(StudentAws.scala:8); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at StudentAws$.main(StudentAws.scala:8); 	at StudentAws.main(StudentAws.scala); Exception in thread ""main"" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z; 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method); 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793); 	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249); 	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454); 	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377); 	at org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8587:7846,access,access,7846,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587,1,['access'],['access']
Security," (Février in french) August (Août) or December (Décembre) have ISO-8859-1 encoding instead of UTF-8 encoding. Indeed, the output files have this line:. `##GATKCommandLine=<ID=GenotypeGVCFs,CommandLine=""GenotypeGVCFs --output /volumes/vol002/COVID/GenomicDB/vcf/COVID.05022021.int00.vcf.gz --variant gendb://dbtot/int00 --reference /volumes/vol002/reference/human_g1k_v37.fasta --tmp-dir /volumes/vol002/COVID/GenomicDB/tmp/tmpint00 --include-non-variant-sites false --merge-input-intervals false --input-is-somatic false --tumor-lod-to-emit 3.5 --allele-fraction-error 0.001 --keep-combined-raw-annotations false --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 --standard-min-confidence-threshold-for-calling 30.0 --max-alternate-alleles 6 --max-genotype-count 1024 --sample-ploidy 2 --num-reference-samples-if-no-call 0 --genomicsdb-use-bcf-codec false --genomicsdb-shared-posixfs-optimizations false --only-output-calls-starting-in-intervals false --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false --disable-tool-default-annotations false --enable-all-annotations false --allow-old-rms-mapping-quality-annotation-data false"",Version=""4.1.9.0"",Date=""5 f<E9>vrier 2021 10:42:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7081:1427,validat,validation-stringency,1427,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7081,1,['validat'],['validation-stringency']
Security, - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: HaplotypeCallerSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 19:01:43.730 INFO HaplotypeCallerSpark - Initializing engine; 19:01:43.730 INFO HaplotypeCallerSpark - Done initializing engine; 19/04/08 19:01:43 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19/04/08 19:01:43 INFO SparkContext: Running Spark version 2.3.0; 19/04/08 19:01:43 INFO SparkContext: Submitted application: HaplotypeCallerSpark; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set(); 19/04/08 19:01:44 INFO Utils: Successfully started service 'sparkDriver' on port 34715.; 19/04/08 19:01:44 INFO SparkEnv: Registering MapOutputTracker; 19/04/08 19:01:44 INFO SparkEnv: Registering BlockManagerMaster; 19/04/08 19:01:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 19/04/08 19:01:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 19/04/08 19:01:44 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-a0da7406-ea5f-4bd9-b3e6-46161a5cbb1b; 19/04/08 19:01:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 19/04/08 19:01:44 INFO SparkEnv: Registering OutputCommitCoordinator; 19/04/08 19:01,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5869:4423,Secur,SecurityManager,4423,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869,7,"['Secur', 'authenticat']","['SecurityManager', 'authentication']"
Security, - HTSJDK Version: 2.23.0; 14:24:33.844 INFO Funcotator - Picard Version: 2.23.3; 14:24:33.844 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:24:33.844 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:24:33.844 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:24:33.844 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:24:33.844 INFO Funcotator - Deflater: IntelDeflater; 14:24:33.844 INFO Funcotator - Inflater: IntelInflater; 14:24:33.844 INFO Funcotator - GCS max retries/reopens: 20; 14:24:33.844 INFO Funcotator - Requester pays: disabled; 14:24:33.844 INFO Funcotator - Initializing engine; 14:24:34.321 INFO FeatureManager - Using codec VCFCodec to read file file:///datastore/nextgenout5/share/labs/bioinformatics/alanh/test/nf-germline-exome_sim4/work/3f/5c862c695472a59dfab47a87afe4f3/cohort.vcf.gz; 14:24:34.510 INFO Funcotator - Done initializing engine; 14:24:34.511 INFO Funcotator - Validating sequence dictionaries...; 14:24:34.550 INFO Funcotator - Processing user transcripts/defaults/overrides...; 14:24:34.551 INFO Funcotator - Initializing data sources...; 14:24:34.554 INFO DataSourceUtils - Initializing data sources from directory: funcotator_dataSources.v1.7.20200521g; 14:24:34.560 INFO DataSourceUtils - Data sources version: 1.7.2020521g; 14:24:34.560 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200521g.tar.gz; 14:24:34.561 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200521.tar.gz; 14:24:34.587 INFO DataSourceUtils - Resolved data source file path: file:///datastore/nextgenout5/share/labs/bioinformatics/alanh/test/nf-germline-exome_sim4/work/3f/5c862c695472a59dfab47a87afe4f3/gencode.v34.annotation.REORDERED.gtf -> file:///datastore/nextgenout5/share/labs/bioinformatics/alanh/test/nf-germline-e,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6926:3839,Validat,Validating,3839,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6926,1,['Validat'],['Validating']
Security," . Here is part of the stacktrace : ; ```; 20:59:48.744 INFO Mutect2 - Inflater: IntelInflater; 20:59:48.744 INFO Mutect2 - GCS max retries/reopens: 20; 20:59:48.744 INFO Mutect2 - Requester pays: enabled. Billed to: broad-firecloud-ccle; 20:59:48.744 INFO Mutect2 - Initializing engine; 20:59:54.630 INFO FeatureManager - Using codec VCFCodec to read file gs://depmapomicsdata/1000g_pon.hg38.vcf.gz; 20:59:55.629 INFO Mutect2 - Shutting down engine; [October 4, 2021 8:59:55 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=876609536; code: 403; message: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; reason: forbidden; location: null; retryable: false; com.google.cloud.storage.StorageException: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:229); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:406); at com.google.cloud.storage.StorageImpl$4.call(StorageImpl.java:217); ...; ```. This happens while it runs the command:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m\ ; -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\ ; -I gs://cclebams/hg38_wes/CDS-00rz9N.hg38.bam -tumor BC1_HAEMATOPOIETIC_AND_LYMPHOID_TISSUE --germline-resource gs://gcp-public-data--gnomad/release/3.0/vcf/genomes/gnomad.genomes.r3.0.sites.vcf.bgz\ ; -pon gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz\ ; -L gs://fc-secure-d2a2d895-a7af-4117-bdc7-652d7d268324/7a157f4a-7d93-4a3e-aaf4-c41833463f5a/Mutect2/3be8ce8e-1075-4063-bc43-6",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7492:1227,access,access,1227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492,1,['access'],['access']
Security," 02:53:01,20] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-10-01 02:53:01,31] [info] Running with database db.url = jdbc:hsqldb:mem:c4b3296a-4b73-4053-b6bf-d4eeb71c8956;shutdown=false;hsqldb.tx=mvcc; [2019-10-01 02:53:01,85] [info] Slf4jLogger started; [2019-10-01 02:53:02,22] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-876ccf5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-10-01 02:53:02,28] [info] Metadata summary refreshing every 1 second.; [2019-10-01 02:53:02,31] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-10-01 02:53:02,31] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-10-01 02:53:02,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-10-01 02:53:02,32] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-10-01 02:53:02,40] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-10-01 02:53:02,43] [info] SingleWorkflowRunnerActor: Version 46.1; [2019-10-01 02:53:02,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-10-01 02:53:02,49] [info] Unspecified type (Unspecified version) workflow c55a06f3-abc1-4db1-8e0f-ea0303caab2c submitted; [2019-10-01 02:53:02,51] [info] SingleWorkflowRunnerActor: Workflow submitted c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,51] [info] 1 new workflows fetched by cromid-876ccf5: c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,52] [info] WorkflowManagerActor Starting workflow c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,53] [info] WorkflowManagerActor Successfully started WorkflowActor-c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,53] [in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6189:1583,hash,hash-lookup,1583,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6189,1,['hash'],['hash-lookup']
Security," 08:27:11.333 INFO Mutect2 - Done initializing engin; 08:27:11.381 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.s; 08:27:11.383 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.s; 08:27:11.426 INFO **IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHM**; 08:27:11.427 INFO IntelPairHmm - Available threads: 4; 08:27:11.428 INFO IntelPairHmm - Requested threads: 4; 08:27:11.428 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementatio; 08:27:11.432 INFO Mutect2 - Shutting down engin; [April 23, 2019 8:27:11 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.09 minutes.; Runtime.totalMemory()=190840832; java.lang.IllegalArgumentException: samples cannot be empt; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.ReferenceConfidenceModel.<init>(ReferenceConfidenceModel.java:116); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticReferenceConfidenceModel.<init>(SomaticReferenceConfidenceModel.java:38); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.<init>(Mutect2Engine.java:149); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.onTraversalStart(Mutect2.java:286); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:982); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:3312,validat,validateArg,3312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136,1,['validat'],['validateArg']
Security," 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Defaults.BUFFER_SIZE : 131072; 17:43:53.162 INFO ValidateVariants - Defaults.COMPRESSION_LEVEL : 1; 17:43:53.162 INFO ValidateVariants - Defaults.CREATE_INDEX : false; 17:43:53.163 INFO ValidateVariants - Defaults.CREATE_MD5 : false; 17:43:53.163 INFO ValidateVariants - Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dic",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2509:2836,Validat,ValidateVariants,2836,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509,1,['Validat'],['ValidateVariants']
Security," 131072; 17:43:53.162 INFO ValidateVariants - Defaults.COMPRESSION_LEVEL : 1; 17:43:53.162 INFO ValidateVariants - Defaults.CREATE_INDEX : false; 17:43:53.163 INFO ValidateVariants - Defaults.CREATE_MD5 : false; 17:43:53.163 INFO ValidateVariants - Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done initializing engine; 17:43:53.294 INFO ProgressMeter - Starting traversal; 17:43:53.294 INFO ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2509:2999,Validat,ValidateVariants,2999,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509,1,['Validat'],['ValidateVariants']
Security," 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260617185). let me talk with production to see if we can post-facto change the exome; file... On Mon, Nov 14, 2016 at 8:27 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > So, would adding a toggle be acceptable? And more importantly, can we make; > stringent validation default, with the option to not blow up on silly exome; > files? Will production accept that?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260519118,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0tUTNAAyuk3m_2cJ8j_3KYroaqB1ks5q-QpsgaJpZM4JNjE-; > . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287821154). Any update on this, @yfarjoun ?. ---. @yfarjoun commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287826525). I think we will only fix the interval list when we move exomes to; hg38....so, no. On Mon, Mar 20, 2017 at 12:45 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > Any update on this, @yfarjoun <https://github.com/yfarjoun> ?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287821154>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0hMTukUGLtk1oTOse4Oj3awHf_exks5rnq1CgaJpZM4JNjE->; > .; >. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287828851). OK well this workaround should really be moved to a ""validation stringency"" level decision, not a hardcoded hack. . @ronlevine Do you know if this hack is also present in GATK4's equivalent code?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2520:4556,validat,validation,4556,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2520,1,['validat'],['validation']
Security, 18607 +1212 ; ===============================================; Files 1168 1225 +57 ; Lines 62907 68654 +5747 ; Branches 9800 10837 +1037 ; ===============================================; + Hits 50604 55189 +4585 ; - Misses 8376 9267 +891 ; - Partials 3927 4198 +271; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3331?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...titute/hellbender/tools/walkers/GenotypeGVCFs.java](https://codecov.io/gh/broadinstitute/gatk/pull/3331?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL0dlbm90eXBlR1ZDRnMuamF2YQ==) | `90% <ø> (ø)` | `47 <0> (ø)` | :arrow_down: |; | [...org/broadinstitute/hellbender/utils/GenomeLoc.java](https://codecov.io/gh/broadinstitute/gatk/pull/3331?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9HZW5vbWVMb2MuamF2YQ==) | `68.362% <ø> (ø)` | `85 <0> (ø)` | :arrow_down: |; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/3331?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYWxpZGF0ZVZhcmlhbnRzLmphdmE=) | `80% <81.25%> (-0.597%)` | `28 <17> (+10)` | |; | [...ools/spark/pathseq/PSFilterArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/3331?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BTRmlsdGVyQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `85% <0%> (-15%)` | `3% <0%> (+2%)` | |; | [...lbender/tools/spark/pathseq/PathSeqScoreSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3331?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BhdGhTZXFTY29yZVNwYXJrLmphdmE=) | `72.84% <0%> (-7.16%)` | `13% <0%> (+5%)` | |; | [.../hellbender/tools/walkers/vqsr/TrancheManager.java](https://codecov.io/gh/broadinstitute/gatk/pull/3331?src=pr&el=tre,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3331#issuecomment-317128040:1530,Validat,ValidateVariants,1530,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3331#issuecomment-317128040,1,['Validat'],['ValidateVariants']
Security," : 2; 14:13:40.284 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:13:40.285 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:13:40.285 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:13:40.285 INFO Mutect2 - Deflater: IntelDeflater; 14:13:40.285 INFO Mutect2 - Inflater: IntelInflater; 14:13:40.286 INFO Mutect2 - GCS max retries/reopens: 20; 14:13:40.286 INFO Mutect2 - Requester pays: enabled. Billed to: broad-firecloud-ccle; 14:13:40.286 INFO Mutect2 - Initializing engine; 14:13:46.660 INFO FeatureManager - Using codec VCFCodec to read file gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz; 14:13:48.823 INFO FeatureManager - Using codec VCFCodec to read file gs://gatk-best-practices/somatic-hg38/af-only-gnomad.hg38.vcf.gz; 14:13:54.570 INFO FeatureManager - Using codec IntervalListCodec to read file gs://fc-secure-76d1542e-1c49-4411-8268-e41e92f9f311/729d209c-0ef4-409f-b3af-2e84ff45ee36/omics_mutect2/16911ef5-efb2-4e12-86f2-f3d5a54b28c0/call-mutect2/Mutect2/4e4a27e2-6c57-40e9-8ddc-1024bdcc50c1/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list; 14:13:55.076 INFO IntervalArgumentCollection - Processing 308828640 bp from intervals; 14:13:55.233 INFO Mutect2 - Done initializing engine; 14:13:56.023 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:13:56.039 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:13:56.116 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:13:56.122 INFO IntelPairHmm - Available threads: 1; 14:13:56.123 INFO IntelPairHmm - Requested threads: 4; 14:13:56.123 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:13:56.127 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849:2789,secur,secure-,2789,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849,1,['secur'],['secure-']
Security, <0%> (-30%)` | `1% <0%> (-2%)` | |; | [...er/tools/spark/sv/discovery/AlignmentInterval.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvQWxpZ25tZW50SW50ZXJ2YWwuamF2YQ==) | `91.064% <0%> (ø)` | `65% <0%> (-1%)` | :arrow_down: |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `66.667% <0%> (ø)` | `4% <0%> (+2%)` | :arrow_up: |; | [...itute/hellbender/tools/walkers/SplitIntervals.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL1NwbGl0SW50ZXJ2YWxzLmphdmE=) | `88.889% <0%> (+0.654%)` | `10% <0%> (+4%)` | :arrow_up: |; | [...r/tools/walkers/validation/RemoveNearbyIndels.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vUmVtb3ZlTmVhcmJ5SW5kZWxzLmphdmE=) | `91.429% <0%> (+0.952%)` | `9% <0%> (+4%)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `80% <0%> (+1.29%)` | `39% <0%> (ø)` | :arrow_down: |; | [...adinstitute/hellbender/tools/IndexFeatureFile.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9JbmRleEZlYXR1cmVGaWxlLmphdmE=) | `91.667% <0%> (+1.344%)` | `17% <0%> (+5%)` | :arrow_up: |; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4066#issuecomment-355695318:2180,validat,validation,2180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4066#issuecomment-355695318,1,['validat'],['validation']
Security," @ronlevine commented on [Mon Nov 28 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-263280085). That's exactly what I did in https://github.com/samtools/htsjdk/pull/759. I can expand this to all INFO field annotations. ---. @ldgauthier commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265221057). Expanding to all INFO annotations would be wonderful, but that can be a separate issue. ---. @ronlevine commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265223581). That's not the only one, @magicDGS requested validating the `AF` values (which can be a separate issue). . ---. @vdauwera commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-265226356). I think this one requires some additional discussion, so let's hold off for now -- it's not essential for 3.7 and we can't wait any longer to release. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-287824654). @ldgauthier Would it be ok to kick this down the road to whenever ValidateVariants gets ported to GATK4?. ---. @ldgauthier commented on [Tue Mar 21 2017](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-288223822). Yeah, this isn't critical for any production pipelines - pass that buck. On Mar 20, 2017 12:56 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > @ldgauthier <https://github.com/ldgauthier> Would it be ok to kick this; > down the road to whenever ValidateVariants gets ported to GATK4?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-287824654>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLPwS6I5nu9TQiw4BFqRojmTiL0aks5rnq_OgaJpZM4FaLwX>; > .; >",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2507:7832,Validat,ValidateVariants,7832,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507,2,['Validat'],['ValidateVariants']
Security," ApplyBQSR - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:41:16.953 INFO ApplyBQSR - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:41:16.953 INFO ApplyBQSR - Deflater: IntelDeflater; 23:41:16.953 INFO ApplyBQSR - Inflater: IntelInflater; 23:41:16.953 INFO ApplyBQSR - GCS max retries/reopens: 20; 23:41:16.953 INFO ApplyBQSR - Requester pays: disabled; 23:41:16.953 INFO ApplyBQSR - Initializing engine; 23:41:17.460 INFO ApplyBQSR - Done initializing engine; 23:41:17.527 INFO ProgressMeter - Starting traversal; 23:41:17.527 INFO ProgressMeter - Current Locus Elapsed Minutes Reads Processed Reads/Minute; 23:41:17.849 INFO ApplyBQSR - Shutting down engine; [February 26, 2020 11:41:17 PM EST] org.broadinstitute.hellbender.tools.walkers.bqsr.ApplyBQSR done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2411724800; java.lang.IllegalStateException: The covariates table is missing ReadGroup S3_2 in RecalTable0; 	at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:752); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.ReadGroupCovariate.keyForReadGroup(ReadGroupCovariate.java:81); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.ReadGroupCovariate.recordValues(ReadGroupCovariate.java:53); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.StandardCovariateList.recordAllValuesInStorage(StandardCovariateList.java:133); 	at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.computeCovariates(RecalUtils.java:546); 	at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.computeCovariates(RecalUtils.java:527); 	at org.broadinstitute.hellbender.transformers.BQSRReadTransformer.apply(BQSRReadTransformer.java:145); 	at org.broadinstitute.hellbender.transformers.BQSRReadTransformer.apply(BQSRReadTransformer.java:27); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at jav",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6242#issuecomment-592005237:7645,validat,validate,7645,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6242#issuecomment-592005237,1,['validat'],['validate']
Security, Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...nder/tools/walkers/vqsr/FilterVariantTranches.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvRmlsdGVyVmFyaWFudFRyYW5jaGVzLmphdmE=) | `92.24% <ø> (ø)` | `42 <0> (ø)` | :arrow_down: |; | [...der/tools/walkers/vqsr/CNNVariantWriteTensors.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OVmFyaWFudFdyaXRlVGVuc29ycy5qYXZh) | `85.71% <100%> (+2.38%)` | `4 <0> (ø)` | :arrow_down: |; | [...hellbender/tools/walkers/vqsr/CNNVariantTrain.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OVmFyaWFudFRyYWluLmphdmE=) | `60% <46.66%> (-20.65%)` | `4 <0> (ø)` | |; | [...lkers/validation/EvaluateInfoFieldConcordance.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vRXZhbHVhdGVJbmZvRmllbGRDb25jb3JkYW5jZS5qYXZh) | `72.58% <72.58%> (ø)` | `14 <14> (?)` | |; | [...ellbender/tools/walkers/vqsr/CNNScoreVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OU2NvcmVWYXJpYW50cy5qYXZh) | `73.68% <77.14%> (-1.32%)` | `41 <17> (+1)` | |; | [...ools/walkers/validation/InfoConcordanceRecord.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vSW5mb0NvbmNvcmRhbmNlUmVjb3JkLmphdmE=) | `93.93% <93.93%> (ø)` | `8 <8> (?)` | |; | [...n/EvaluateInfoFieldConcordanceIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pul,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5175#issuecomment-423756354:2067,validat,validation,2067,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5175#issuecomment-423756354,2,['validat'],['validation']
Security," Description ; @tomwhite I was asked to tag you, please let me know if you think anyone else should look at this. This issue is created from a forum bug report (https://gatkforums.broadinstitute.org/gatk/discussion/24511/error-in-readspipelinespark-version-4-1-4/p1). More information can be requested if necessary. Stack trace copied below:; > A USER ERROR has occurred: Couldn't write file hdfs://cloudera08/gatk-test2/WES2019-022_S4_out.vcf because writing failed with exception concat: target file /gatk-test2/WES2019-022_S4_out.vcf.parts/output is empty; > at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.concatInternal(FSNamesystem.java:2303); > at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.concatInt(FSNamesystem.java:2257); > at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.concat(FSNamesystem.java:2219); > at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.concat(NameNodeRpcServer.java:829); > at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.concat(AuthorizationProviderProxyClientProtocol.java:285); > at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.concat(ClientNamenodeProtocolServerSideTranslatorPB.java:580); > at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); > at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617); > at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073); > at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2278); > at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2274); > at java.security.AccessController.doPrivileged(Native Method); > at javax.security.auth.Subject.doAs(Subject.java:422); > at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924); > at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2272); > ; > org.broadinstitute.hellbender.exceptions",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6218:1115,Authoriz,AuthorizationProviderProxyClientProtocol,1115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6218,1,['Authoriz'],['AuthorizationProviderProxyClientProtocol']
Security, Files 1781 1782 +1 ; Lines 132255 133311 +1056 ; Branches 14734 15003 +269 ; ===============================================; + Hits 114191 115180 +989 ; - Misses 12750 12781 +31 ; - Partials 5314 5350 +36; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5031?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ava/org/broadinstitute/hellbender/utils/Utils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5031/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9VdGlscy5qYXZh) | `84.469% <ø> (+3.993%)` | `298 <0> (+153)` | :arrow_up: |; | [...aplotypecaller/HaplotypeCallerIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5031/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXJJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `91.499% <100%> (ø)` | `85 <6> (ø)` | :arrow_down: |; | [...walkers/validation/ConcordanceIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5031/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQ29uY29yZGFuY2VJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `100% <100%> (ø)` | `6 <0> (ø)` | :arrow_down: |; | [...ct/CreateSomaticPanelOfNormalsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5031/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9DcmVhdGVTb21hdGljUGFuZWxPZk5vcm1hbHNJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `100% <100%> (ø)` | `3 <0> (ø)` | :arrow_down: |; | [...eVcfWithExpectedAlleleFractionIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5031/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQW5ub3RhdGVWY2ZXaXRoRXhwZWN0ZWRBbGxlbGVGcmFjdGlvbkludGVncmF0aW9uVGVzdC5qYXZh) | `100% <100%> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5031#issuecomment-406311079:1572,validat,validation,1572,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5031#issuecomment-406311079,1,['validat'],['validation']
Security," GCS hadoop connector will not be configured properly; 12:33:52.162 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:33:53.793 INFO CreateReadCountPanelOfNormals - ------------------------------------------------------------; 12:33:53.794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5686:1511,secur,security,1511,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686,1,['secur'],['security']
Security," HaplotypeCaller - GCS max retries/reopens: 20 01:13:16.078 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes 01:13:16.078 INFO HaplotypeCaller - Initializing engine 01:13:17.087 INFO HaplotypeCaller - Shutting down engine [January 18, 2020 1:13:17 AM IST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.02 minutes. Runtime.totalMemory()=2216689664 java.lang.NullPointerException at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getContigNames(SequenceDictionaryUtils.java:463) at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getCommonContigsByName(SequenceDictionaryUtils.java:457) at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.compareDictionaries(SequenceDictionaryUtils.java:234) at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:150) at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:98) at org.broadinstitute.hellbender.engine.GATKTool.validateSequenceDictionaries(GATKTool.java:621) at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:563) at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:160) at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134) at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179) at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198) at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152) at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195) at org.broadinstitute.hellbender.Main.main(Main.java:275); > ; > Please suggest any solution. Thank you. hello，have you solved this problem？",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5947#issuecomment-1605272955:2760,validat,validateDictionaries,2760,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5947#issuecomment-1605272955,2,['validat'],"['validateDictionaries', 'validateSequenceDictionaries']"
Security," Here's some of the output:. ```; [March 9, 2017 7:03:42 PM EST] org.broadinstitute.hellbender.tools.picard.sam.ValidateSamFile --input src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --use_jdk_deflater true --use_jdk_inflater true --MODE VERBOSE --MAX_OUTPUT 100 --IGNORE_WARNINGS false --VALIDATE_INDEX true --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 1 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --help false --version false --verbosity INFO --QUIET false; [March 9, 2017 7:03:42 PM EST] Executing as gspowley@dna on Linux 3.10.0-514.10.2.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_111-b14; Version: Version:4.alpha.2-170-g8d06823-SNAPSHOT; 19:03:42.998 INFO ValidateSamFile - Defaults.BUFFER_SIZE : 131072; 19:03:42.999 INFO ValidateSamFile - Defaults.COMPRESSION_LEVEL : 1; 19:03:42.999 INFO ValidateSamFile - Defaults.CREATE_INDEX : false; 19:03:42.999 INFO ValidateSamFile - Defaults.CREATE_MD5 : false; 19:03:42.999 INFO ValidateSamFile - Defaults.CUSTOM_READER_FACTORY : ; 19:03:42.999 INFO ValidateSamFile - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 19:03:42.999 INFO ValidateSamFile - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 19:03:42.999 INFO ValidateSamFile - Defaults.REFERENCE_FASTA : null; 19:03:43.000 INFO ValidateSamFile - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_CRAM_REF_DOWNLOAD : false; 19:03:43.000 INFO ValidateSamFile - Deflater JdkDeflater; 19:03:43.000 INFO ValidateSamFile - Inflater JdkInflater; 19:03:43.000 INFO ValidateSamFile - Initializing engine; 19:03:43.000 INFO ValidateSamFile - Done initial",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-285513571:1086,Validat,ValidateSamFile,1086,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-285513571,1,['Validat'],['ValidateSamFile']
Security," INFO GetPileupSummaries - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_T RIBBLE : false ; ; 01:03:32.956 INFO GetPileupSummaries - Deflater: IntelDeflater ; ; 01:03:32.956 INFO GetPileupSummaries - Inflater: IntelInflater ; ; 01:03:32.956 INFO GetPileupSummaries - GCS max retries/reopens: 20 ; ; 01:03:32.956 INFO GetPileupSummaries - Requester pays: disabled ; ; 01:03:32.956 INFO GetPileupSummaries - Initializing engine ; ; 01:03:33.330 INFO FeatureManager - Using codec VCFCodec to read file file:///ga tk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_commo n\_3.hg19.vcf.gz ; ; 01:03:33.395 INFO GetPileupSummaries - Shutting down engine ; ; \[September 12, 2021 1:03:33 AM GMT\] org.broadinstitute.hellbender.tools.walkers. contamination.GetPileupSummaries done. Elapsed time: 0.01 minutes. ; ; Runtime.totalMemory()=462946304 ; ; java.lang.IllegalArgumentException: Dictionary cannot have size zero ; ; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798) ; ; at org.broadinstitute.hellbender.utils.MRUCachingSAMSequenceDictionary.< init>(MRUCachingSAMSequenceDictionary.java:35) ; ; at org.broadinstitute.hellbender.utils.GenomeLocParser.<init>(GenomeLocP arser.java:78) ; ; at org.broadinstitute.hellbender.utils.GenomeLocParser.<init>(GenomeLocP arser.java:62) ; ; at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArg umentCollection.getTraversalParameters(IntervalArgumentCollection.java:180) ; ; at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArg umentCollection.getIntervals(IntervalArgumentCollection.java:111) ; ; at org.broadinstitute.hellbender.engine.GATKTool.initializeIntervals(GAT KTool.java:514) ; ; at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java :709) ; ; at org.broadinstitute.hellbender.engine.LocusWalker.onStartup(LocusWalke r.java:136) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(Comm andLineProgram.java:138) ; ; at org",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7479:5008,validat,validateArg,5008,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7479,1,['validat'],['validateArg']
Security, INFO MarkDuplicatesSpark - ------------------------------------------------------------; 18:35:26.516 INFO MarkDuplicatesSpark - HTSJDK Version: 2.23.0; 18:35:26.516 INFO MarkDuplicatesSpark - Picard Version: 2.22.8; 18:35:26.516 INFO MarkDuplicatesSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 18:35:26.517 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 18:35:26.517 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 18:35:26.517 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:35:26.517 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 18:35:26.517 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 18:35:26.517 INFO MarkDuplicatesSpark - GCS max retries/reopens: 20; 18:35:26.517 INFO MarkDuplicatesSpark - Requester pays: disabled; 18:35:26.517 INFO MarkDuplicatesSpark - Initializing engine; 18:35:26.517 INFO MarkDuplicatesSpark - Done initializing engine; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/user/wup/miniconda3/envs/gatk/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar) to method java.nio.Bits.unaligned(); WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 20/10/08 18:35:27 INFO SparkContext: Running Spark version 2.4.5; 18:35:27.640 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 20/10/08 18:35:27 INFO SparkContext: Submitted application: MarkDuplicatesSpark; 20/10/08 18:35:27 INFO SecurityManager: Changing view acls to: wup; 20/10/08 18:35:27 INFO SecurityManager: Changing modi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6875:1968,access,access,1968,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6875,2,['access'],['access']
Security, Latest public release version 4.2.2.0. ### Description . Running apt-get inside docker image fails. #### Steps to reproduce. (base) fleharty@wm3b9-dfa docker % docker run -it broadinstitute/gatk:4.2.2.0; Unable to find image 'broadinstitute/gatk:4.2.2.0' locally; 4.2.2.0: Pulling from broadinstitute/gatk; a7fe112a8303: Already exists ; Digest: sha256:32175c3c7c1fb9f5bd6650183c9c5cf26fb822dddb0cad0123d48c33124b6065; Status: Downloaded newer image for broadinstitute/gatk:4.2.2.0; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# apt-get update; Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB] ; Get:3 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB] ; Get:4 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [543 kB] ; Get:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archiv,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7447:1148,secur,security,1148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447,1,['secur'],['security']
Security," October 25, 2020 7:53:34 PM CDT; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - HTSJDK Version: 2.22.0; 19:53:34.607 INFO ValidateVariants - Picard Version: 2.22.8; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:53:34.608 INFO ValidateVariants - Deflater: IntelDeflater; 19:53:34.608 INFO ValidateVariants - Inflater: IntelInflater; 19:53:34.608 INFO ValidateVariants - GCS max retries/reopens: 20; 19:53:34.608 INFO ValidateVariants - Requester pays: disabled; 19:53:34.608 INFO ValidateVariants - Initializing engine; 19:53:35.169 INFO FeatureManager - Using codec VCFCodec to read file file://chr1-22.phased.rename.reheader.vcf.gz; 19:53:35.594 INFO ValidateVariants - Done initializing engine; 19:53:35.594 WARN ValidateVariants - IDS validation cannot be done because no DBSNP file was provided; 19:53:35.594 WARN ValidateVariants - Other possible validations will still be performed; 19:53:35.594 INFO ProgressMeter - Starting traversal; 19:53:35.595 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 19:53:35.660 INFO ValidateVariants - Shutting down engine; [October 25, 2020 7:53:35 PM CDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2114453504; java.lang.ArrayIndexOutOfBoundsException: -87; 	at org.broadinstitute.hellbender.utils.BaseUtils.convertIUPACtoN(BaseUtils.java:123); 	at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.getSubse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911:2656,Validat,ValidateVariants,2656,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911,1,['Validat'],['ValidateVariants']
Security, PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 14:19:10.290 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:19:10.290 INFO PrintReadsSpark - Deflater: IntelDeflater; 14:19:10.290 INFO PrintReadsSpark - Inflater: IntelInflater; 14:19:10.290 INFO PrintReadsSpark - GCS max retries/reopens: 20; 14:19:10.290 INFO PrintReadsSpark - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 14:19:10.290 INFO PrintReadsSpark - Initializing engine; 14:19:10.290 INFO PrintReadsSpark - Done initializing engine; 17/10/11 14:19:10 INFO spark.SparkContext: Running Spark version 1.6.0; 17/10/11 14:19:10 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/11 14:19:10 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/11 14:19:10 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); users with modify permissions: Set(hdfs); 17/10/11 14:19:10 INFO util.Utils: Successfully started service 'sparkDriver' on port 43567.; 17/10/11 14:19:11 INFO slf4j.Slf4jLogger: Slf4jLogger started; 17/10/11 14:19:11 INFO Remoting: Starting remoting; 17/10/11 14:19:11 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.131.101.159:45501]; 17/10/11 14:19:11 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.131.101.159:45501]; 17/10/11 14:19:11 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 45501.; 17/10/11 14:19:11 INFO spark.SparkEnv: Registering MapOutputTracker; 17/10/11 14:19:11 INFO spark.SparkEnv: Registering BlockManagerMaster; 17/10/11 14:19:11 INFO storage.DiskBlockManager: Created local directory at /tmp/hdfs/blockmgr-3fe99005-cdde-437f-9ca5-cdc7b1b9c057; 17/10/11 14:19:11 INFO storage.MemoryStore: MemoryStore started with capacity 530.0 MB; 1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:3455,Secur,SecurityManager,3455,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,3,"['Secur', 'authenticat']","['SecurityManager', 'authentication']"
Security," QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle 15 TB /table /day import limit (#7167); - #260 filter out AS_QD, SOR, FS from cohort extract VCF (#7173); - Full scientific validation via end to end comparison of filtered results between WARP and BQ (#7179); - Cherry pick of commits to fix GATK tests from master (#7183); - ExtractCohort supports -XL exclusion and follows intervals, other optimizations (#7181); - ExtractFeatures supports -XL exclusion and follows intervals, other optimizations (#7184); - change 0/0 GQ0 sites to nocalls (#7190); - updated (#7195); - Rename ""metadata"" table to ""sample_info"" table, fix vet schema (#7196); - Allow users to specify VQSLOD sensitivity and apply threshold in ExtractCohort (#7194); - Calculate and Store site-level QCs (#7197); - Filter Failing QC Sites from Extract (#7201); - WDLize GvsPrepareCallset (briefly known as CreateCohortTable) (#7200); - default drop_state to 60, but allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:12710,validat,validation,12710,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['validat'],['validation']
Security," ResourceUtils - No custom resources configured for spark.driver.; 10:33:06.428 INFO ResourceUtils - ==============================================================; 10:33:06.428 INFO SparkContext - Submitted application: SortSamSpark; 10:33:06.446 INFO ResourceProfile - Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 600, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0); 10:33:06.454 INFO ResourceProfile - Limiting resource is cpu; 10:33:06.455 INFO ResourceProfileManager - Added ResourceProfile id: 0; 10:33:06.500 INFO SecurityManager - Changing view acls to: root; 10:33:06.501 INFO SecurityManager - Changing modify acls to: root; 10:33:06.501 INFO SecurityManager - Changing view acls groups to:; 10:33:06.502 INFO SecurityManager - Changing modify acls groups to:; 10:33:06.502 INFO SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); groups with view permissions: Set(); users with modify permissions: Set(root); groups with modify permissions: Set(); 10:33:06.755 INFO Utils - Successfully started service 'sparkDriver' on port 34861.; 10:33:06.784 INFO SparkEnv - Registering MapOutputTracker; 10:33:06.815 INFO SparkEnv - Registering BlockManagerMaster; 10:33:06.827 INFO BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 10:33:06.828 INFO BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up; 10:33:06.831 INFO SparkEnv - Registering BlockManagerMasterHeartbeat; 10:33:06.846 INFO DiskBlockManager - Created local directory at /raid/tmp/d6/c66ba827e22dbc38625af1cbc85adc/tmp/blockmgr-8dc41ac8-6cf4-4424-9b15-7e2cbfc9e538; 10:33:06.872 INFO MemoryStore - MemoryStore started with capacity 1076.2 Gi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:41979,Secur,SecurityManager,41979,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['Secur'],['SecurityManager']
Security," SchedulerExtensionServices: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Stopped; 19/04/08 19:03:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 19/04/08 19:03:28 INFO MemoryStore: MemoryStore cleared; 19/04/08 19:03:28 INFO BlockManager: BlockManager stopped; 19/04/08 19:03:28 INFO BlockManagerMaster: BlockManagerMaster stopped; 19/04/08 19:03:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 19/04/08 19:03:28 INFO SparkContext: Successfully stopped SparkContext; 19:03:28.389 INFO HaplotypeCallerSpark - Shutting down engine; [April 8, 2019 7:03:28 PM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=941096960; Exception in thread ""main"" java.lang.StackOverflowError; 	at java.util.HashMap.putMapEntries(HashMap.java:501); 	at java.util.HashMap.<init>(HashMap.java:490); 	at com.esotericsoftware.kryo.Generics.<init>(Generics.java:47); 	at com.esotericsoftware.kryo.serializers.FieldSerializerGenericsUtil.buildGenericsScope(FieldSerializerGenericsUtil.java:116); 	at com.esotericsoftware.kryo.serializers.FieldSerializerGenericsUtil.newCachedFieldOfGenericType(FieldSerializerGenericsUtil.java:225); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.newCachedField(FieldSerializer.java:368); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.createCachedFields(FieldSerializer.java:331); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:261); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:182); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5869:18479,Hash,HashMap,18479,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869,1,['Hash'],['HashMap']
Security," Using codec VCFCodec to read file file:///run/media/riadh/My%20Book_From%20Eiklid/Analysis/gatk-4.2.4.1/ensembl-vep/PE69_chr3.vcf; 10:58:20.063 INFO VariantAnnotator - Done initializing engine; 10:58:20.091 WARN VariantAnnotatorEngine - The requested expression attribute ""gnomad.ALT"" is missing from the header in its resource file gnomad; 10:58:20.140 INFO ProgressMeter - Starting traversal; 10:58:20.140 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 10:58:42.160 INFO VariantAnnotator - Shutting down engine; [March 17, 2022 at 10:58:42 AM CET] org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotator done. Elapsed time: 0.37 minutes.; Runtime.totalMemory()=17158897664; java.lang.IllegalStateException: Allele in genotype C not in the variant context [C*, CT]; 	at htsjdk.variant.variantcontext.VariantContext$Validation.validateGenotypes(VariantContext.java:382); 	at htsjdk.variant.variantcontext.VariantContext$Validation.access$200(VariantContext.java:323); 	at htsjdk.variant.variantcontext.VariantContext$Validation$2.validate(VariantContext.java:331); 	at htsjdk.variant.variantcontext.VariantContext.lambda$validate$0(VariantContext.java:1384); 	at java.base/java.lang.Iterable.forEach(Iterable.java:75); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1384); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:489); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:647); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:638); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.trimAlleles(GATKVariantContextUtils.java:1464); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.trimAlleles(GATKVariantContextUtils.java:1420); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.getMinRepresentationBiallelics(VariantAnnotatorEn",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053:4076,access,access,4076,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053,1,['access'],['access']
Security, `35 <1> (ø)` | :arrow_down: |; | [...bender/utils/text/parsers/AbstractInputParser.java](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXh0L3BhcnNlcnMvQWJzdHJhY3RJbnB1dFBhcnNlci5qYXZh) | `88.679% <0%> (+1.642%)` | `31 <0> (ø)` | :arrow_down: |; | [...ollections/OptionalIntervalArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL2FyZ3VtZW50Y29sbGVjdGlvbnMvT3B0aW9uYWxJbnRlcnZhbEFyZ3VtZW50Q29sbGVjdGlvbi5qYXZh) | `83.333% <0%> (+11.905%)` | `3 <1> (ø)` | :arrow_down: |; | [...stitute/hellbender/utils/pileup/PileupElement.java](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9waWxldXAvUGlsZXVwRWxlbWVudC5qYXZh) | `96.04% <0%> (+1.865%)` | `76 <0> (ø)` | :arrow_down: |; | [...bender/tools/exome/HashedListTargetCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9IYXNoZWRMaXN0VGFyZ2V0Q29sbGVjdGlvbi5qYXZh) | `90.741% <0%> (+1.65%)` | `43 <0> (ø)` | :arrow_down: |; | [.../utils/read/markduplicates/DuplicationMetrics.java](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL0R1cGxpY2F0aW9uTWV0cmljcy5qYXZh) | `85.366% <0%> (+2.033%)` | `13 <0> (ø)` | :arrow_down: |; | [...oadinstitute/hellbender/utils/read/CigarUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL0NpZ2FyVXRpbHMuamF2YQ==) | `89.404% <0%> (+0.588%)` | `68 <0> (ø)` | :arrow_down: |; | [...der/utils/locusiterator/AlignmentStateMachine.java](https://codecov.io/gh/broadinstitute/gatk/pull/2543?src=pr&e,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2543#issuecomment-290171890:2742,Hash,HashedListTargetCollection,2742,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2543#issuecomment-290171890,1,['Hash'],['HashedListTargetCollection']
Security, `85.71% <100%> (+2.38%)` | `4 <0> (ø)` | :arrow_down: |; | [...hellbender/tools/walkers/vqsr/CNNVariantTrain.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OVmFyaWFudFRyYWluLmphdmE=) | `60% <46.66%> (-20.65%)` | `4 <0> (ø)` | |; | [...lkers/validation/EvaluateInfoFieldConcordance.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vRXZhbHVhdGVJbmZvRmllbGRDb25jb3JkYW5jZS5qYXZh) | `72.58% <72.58%> (ø)` | `14 <14> (?)` | |; | [...ellbender/tools/walkers/vqsr/CNNScoreVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OU2NvcmVWYXJpYW50cy5qYXZh) | `73.68% <77.14%> (-1.32%)` | `41 <17> (+1)` | |; | [...ools/walkers/validation/InfoConcordanceRecord.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vSW5mb0NvbmNvcmRhbmNlUmVjb3JkLmphdmE=) | `93.93% <93.93%> (ø)` | `8 <8> (?)` | |; | [...n/EvaluateInfoFieldConcordanceIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vRXZhbHVhdGVJbmZvRmllbGRDb25jb3JkYW5jZUludGVncmF0aW9uVGVzdC5qYXZh) | `96% <96%> (ø)` | `3 <3> (?)` | |; | [...utils/smithwaterman/SmithWatermanIntelAligner.java](https://codecov.io/gh/broadinstitute/gatk/pull/5175/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zbWl0aHdhdGVybWFuL1NtaXRoV2F0ZXJtYW5JbnRlbEFsaWduZXIuamF2YQ==) | `50% <0%> (-30%)` | `1% <0%> (-2%)` | |; | [...ithwaterman/SmithWatermanIntelAlignerUnitTest.java](https://codecov.io/gh/bro,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5175#issuecomment-423756354:2692,validat,validation,2692,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5175#issuecomment-423756354,2,['validat'],['validation']
Security," access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:178); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:110); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:28); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5686:2469,validat,validateSettings,2469,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686,1,['validat'],['validateSettings']
Security," already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); Mutect2/FilterMutectCalls. ### Affected version(s); Both 4.1.6 and 4.1.9 were affected. Other version may be affected as well, but I have not tested them. ### Description . Output from vcf-validator:. 7 .. INFO field at chr1:160882084 .. INFO tag [AS_SB_TABLE=719,346|0,47] expected different number of values (1),INFO tag [AS_FilterStatus=weak_evidence,base_qual,strand_bias] expected different number of values (expected 1, found 3); 7 .. INFO field at chr1:230995820 .. INFO tag [AS_SB_TABLE=444,391|4,6|5,6] expected different number of values (1),INFO tag [AS_FilterStatus=weak_evidence|weak_evidence] expected different number of values (expected 2, found 1); 6 .. INFO field at chr2:169905124 .. INFO tag [AS_SB_TABLE=387,312|2,2] expected different number of values (1),INFO tag [AS_FilterStatus=weak_evidence,base_qual] expected different number of values (expected 1, found 2); 6 .. INFO field at chr3:42210085 .. INFO tag [AS_SB_TABLE=15,24|3,2|206,188|174,140|3,1] expected different number of values (1),INFO tag [AS_FilterStatus=weak_evidence|SITE|SITE|weak_evidence] expected different number of values (expected 4, found 1); 5 .. INFO field at chr1:82186950 .. INFO tag ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6931:1407,validat,validator,1407,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6931,1,['validat'],['validator']
Security," also removes by debugging code and comments. I think it is ready for a review. To some other questions you had above:. 1) The HashMap<FeatureInput<VariantContext>, HashMap<String, Collection<VariantContext>>> can be wrapped in a class with just a couple of methods, so we don't have to manifest that long type all over the place. I realize that's non-optimal, but this isnt anything I introduced here. I would really like to keep this PR as limited as we can, and address some larger refactoring in a different PR, once we've migrated to MultiVariantWalkerGroupedOnStart. 2) I know this PR still in an interim state, but passing the VariantWalker in as an argument to the comp methods doesn't seem like a step forward to me. If we can't solve that problem completely in this PR (which is fine, I'm all for trying to contain this), are those changes necessary ? Perhaps that part should just wait for the next round. As noted above, I'd like to propose this as iterative, with a second PR coming soon. I did this b/c it moved us toward not needing to pass around the walker. It minimizes the code that has access to the walker (as opposed to setting it after creating the instance of the Evaluator, etc. Yes, it exposes it for two methods, but those classes no longer hang on to it. I would like to ultimately remove this entirely. 3) To re-iterate testEvalTrackWithoutGenotypesWithSampleFields: the input file, noGenotypes.vcf, has a header dictionary with the full set of contigs, and a single variant from chr 1. Prior to this PR, the test executed and supplied b37_reference_20_21 as the reference FASTA. The variant in that VCF is from chr 1, not 20/21. The old code should have failed. It didnt, probably since it was preferentially taking the sequence dictionary from the VCF header and basically ignoring the FASTA. It didnt make much practical difference, but I believe my change here is right. I added the test case testEvalTrackWithoutGenotypesWithSampleFieldsWrongRef to cover that error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-747619130:1619,access,access,1619,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-747619130,2,"['access', 'expose']","['access', 'exposes']"
Security," attached here the exact command, stderr,validate-bam-inputs.json and validate-bam.wdl. The stdout just says 230. ; I would be grateful for any help on how to solve the issue. ; Best ; zara. <img width=""1280"" alt=""Screen Shot 2020-07-16 at 12 01 12 AM"" src=""https://user-images.githubusercontent.com/61913000/87845900-e6491480-c8e0-11ea-8dc0-ec5b3fbc15a8.png"">; <img width=""1280"" alt=""Screen Shot 2020-07-16 at 12 01 19 AM"" src=""https://user-images.githubusercontent.com/61913000/87845901-e77a4180-c8e0-11ea-8c6a-fb5783949ba3.png"">; <img width=""353"" alt=""Screen Shot 2020-07-16 at 12 00 30 AM"" src=""https://user-images.githubusercontent.com/61913000/87845902-ea753200-c8e0-11ea-96bc-caecee2ccb39.png"">; <img width=""1249"" alt=""stderr1"" src=""https://user-images.githubusercontent.com/61913000/87845904-eea14f80-c8e0-11ea-90bd-235c9205f72f.png"">. (gatk) root@bc3c6aca6231:/gatk/my_data/tools# java -jar cromwell-51.jar run /gatk/my_data/seq-format-validation/validate-bam.wdl --inputs /gatk/my_data/seq-format-validation/validate-bam.inputs.json; [2020-07-14 05:09:22,78] [info] Running with database db.url = jdbc:hsqldb:mem:f10b64bd-d8ca-4428-917b-311fca24c372;shutdown=false;hsqldb.tx=mvcc; [2020-07-14 05:09:29,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-07-14 05:09:29,37] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-07-14 05:09:29,47] [info] Running with database db.url = jdbc:hsqldb:mem:e337a356-2f0c-4389-92c5-255465180f24;shutdown=false;hsqldb.tx=mvcc; [2020-07-14 05:09:29,89] [info] Slf4jLogger started; [2020-07-14 05:09:30,10] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-ca5c695"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2020-07-14 05:09:30,23] [info] Metadata summary refreshing every 1 second.; [2020-07-14 05:09:30,23] [warn] 'docker.hash-looku",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:1202,validat,validation,1202,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,2,['validat'],"['validate-bam', 'validation']"
Security, back to uploading libraries under SPARK_HOME.; 20/10/22 12:02:29 INFO yarn.Client: Uploading resource file:/tmp/spark-28ab5ef4-82d1-425e-879f-5056e9b51e43/__spark_libs__7655440475844189559.zip -> hdfs://192.168.0.104:9000/user/jacky/.sparkStaging/application_1603353714322_0004/__spark_libs__7655440475844189559.zip; 20/10/22 12:02:31 INFO yarn.Client: Uploading resource file:/home/jacky/Exec/gatk/build/libs/gatk-spark.jar -> hdfs://192.168.0.104:9000/user/jacky/.sparkStaging/application_1603353714322_0004/gatk-spark.jar; 20/10/22 12:02:33 INFO yarn.Client: Uploading resource file:/tmp/spark-28ab5ef4-82d1-425e-879f-5056e9b51e43/__spark_conf__3248804172036151699.zip -> hdfs://192.168.0.104:9000/user/jacky/.sparkStaging/application_1603353714322_0004/__spark_conf__.zip; 20/10/22 12:02:33 INFO spark.SecurityManager: Changing view acls to: jacky; 20/10/22 12:02:33 INFO spark.SecurityManager: Changing modify acls to: jacky; 20/10/22 12:02:33 INFO spark.SecurityManager: Changing view acls groups to: ; 20/10/22 12:02:33 INFO spark.SecurityManager: Changing modify acls groups to: ; 20/10/22 12:02:33 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jacky); groups with view permissions: Set(); users with modify permissions: Set(jacky); groups with modify permissions: Set(); 20/10/22 12:02:33 INFO yarn.Client: Submitting application application_1603353714322_0004 to ResourceManager; 20/10/22 12:02:33 INFO impl.YarnClientImpl: Submitted application application_1603353714322_0004; 20/10/22 12:02:34 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:34 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: default; 	 start time: 1603360953394; 	 final status: UNDEFINED; 	 tracking URL: http://jacky:8088/proxy/application_1603353714322_0004/; 	 user: jacky; 20/10/22 12:02:35 INF,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6906:3403,Secur,SecurityManager,3403,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6906,1,['Secur'],['SecurityManager']
Security," because of https://github.com/Rdatatable/data.table/issues/1139 and the fact that /dev/shm is limited to 64MB in a standard GATK Docker container, this can yield the following error when running within Docker:. ````; Stderr: grep: write error: No space left on device; Error in fread(sprintf(""grep -v ^@ %s"", tsv_file), sep = ""\t"", stringsAsFactors = FALSE, : ; Expected sep ('	') but new line, EOF (or other non printing character) ends field 2 when detecting types from point 10: 2	229515751	229516; Calls: source ... eval -> eval -> WriteModeledSegmentsPlot -> ReadTSV. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:80); 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:19); 	at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:131); 	at org.broadinstitute.hellbender.tools.copynumber.plotting.PlotModeledSegments.writeModeledSegmentsPlot(PlotModeledSegments.java:289); 	at org.broadinstitute.hellbender.tools.copynumber.plotting.PlotModeledSegments.doWork(PlotModeledSegments.java:206); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); 	at org.broadinstitute.hellbender.Main.main(Main.java:275); ````. Starting a Docker container with a sufficiently large `--shm-size` resolves this, but I am not sure if we can access this via standard runtime attributes in Cromwell. Not sure if we'd want to increase this in the Docker image itself either.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4140:2005,access,access,2005,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4140,1,['access'],['access']
Security, bindings.; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark2/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark_llap/spark-llap-assembly-1.0.0.2.6.3.40-13.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:2912,Access,AccessController,2912,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['Access'],['AccessController']
Security," blocks when running ValidateVariants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants \; -V /path/to/our/.g.vcf.gz \; -R /path/to/our/.fa \; -L /path/to/our/.interval_list \; -gvcf \; --validation-type-to-exclude ALLELES \; --dbsnp /path/to/our/.vcf.gz \; --no-overlaps; ```. #### Expected behavior. The error message should report the _first_ overlapping interval. #### Actual behavior; The error message is reporting the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8103:1399,Validat,ValidateVariants,1399,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103,1,['Validat'],['ValidateVariants']
Security," cHmm_E20 cHmm_E21 cHmm_E22 cHmm_E23 cHmm_E; 24 cHmm_E25 GerpRS GerpRSpval GerpN GerpS tOverlapMotifs motifDist EncodeH3K4me1-sum EncodeH3K4me1-max EncodeH3K4me; 2-sum EncodeH3K4me2-max EncodeH3K4me3-sum EncodeH3K4me3-max EncodeH3K9ac-sum EncodeH3K9ac-max EncodeH3K9me3-sum En; codeH3K9me3-max EncodeH3K27ac-sum EncodeH3K27ac-max EncodeH3K27me3-sum EncodeH3K27me3-max EncodeH3K36me3-sum EncodeH3K36me3-m; ax EncodeH3K79me2-sum EncodeH3K79me2-max EncodeH4K20me1-sum EncodeH4K20me1-max EncodeH2AFZ-sum EncodeH2AFZ-max EncodeDNase-sum Encode; DNase-max EncodetotalRNA-sum EncodetotalRNA-max Grantham Dist2Mutation Freq100bp Rare100bp Sngl100bp Freq1000bp Rare; 1000bp Sngl1000bp Freq10000bp Rare10000bp Sngl10000bp EnsembleRegulatoryFeature dbscSNV-ada_score dbscSNV-rf_score Re; mapOverlapTF RemapOverlapCL RawScore PHRED. 1 10001 T TC INS 1 RegulatoryFeature REGULATORY 4 regulatory 0.448933333333 0.00993288590604 NA; NA NA NA NA NA NA ENSR00000344265 NA NA NA NA NA NA NA NA NA NA NA NA NA 1869 3670 NA NA NA NA NA NA NA NA NA NA 994 NA NA NA NA 0.008 0.000 0.000 0.000 0.016 0.000 0.024 0.087 0.472 0.000 0.000 0.000 0.000 0.000 0.394 NA NA 0 0 NA NA; NA NA NA GM1 10.04 2.84 8.0 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 2773 NA NA NA NA NA NA 3 2 32 NA NA -0.083014 1.567; ```. The funcotator command:; ```; gatk Funcotator \; --variant TriLevelv2_bqsr-filtered.vcf \; --output test_cadd.vcf \; --reference hg19.fa \; --data-sources-path /funcotator_dataSources.v1.6.20190124s \; --ref-version hg19 \; --output-file-format VCF \; --java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true' \; --verbosity DEBUG \; --disable-sequence-dictionary-validation true \; --disable-bam-index-caching true; ```. I am not sure what I missed here, although I am not quite sure about how should to add new data sources. Sincerely appreciate your help!. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/24521/funcotator-user-defined-data-sources/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6223:8621,validat,validation,8621,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6223,1,['validat'],['validation']
Security," causes the problem as removing it from the following test record solves the problem:. ```; chr1	10027	.	A	C,G	.	PASS	.	GT:AD:AF:DP	1/2:0,5,5:0.500,0.500:10; ```. vs. ```; chr1	10027	.	A	C,G	.	PASS	.	GT:AD:DP	1/2:0,5,5:10; ```. The output for GATK 4.1.4.1 or when the AF field is removed looks like this:; ```; chr1	10027	.	A	C	.	PASS	.	GT:AD:DP	./.:0,5:10; chr1	10027	.	A	G	.	PASS	.	GT:AD:DP	./.:0,5:10; ```. #### Steps to reproduce; ```; $gatk/gatk LeftAlignAndTrimVariants -R $reference --split-multi-allelics -V test.input.vcf -O test.output.vcf; ```. #### Expected behavior; LeftAlignAndTrimVariants should be able to split multiallelic records in a VCF to two separate records as in GATK version 4.1.4.1. The AF field is removed from the 4.1.4.1 output, however. #### Actual behavior; GATK fails at a multiallelic record with the following error (GATK 4.2.0.0):; ```; java.lang.IllegalArgumentException: the range size cannot be negative; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); 	at org.broadinstitute.hellbender.utils.IndexRange.validate(IndexRange.java:107); 	at org.broadinstitute.hellbender.utils.IndexRange.<init>(IndexRange.java:67); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitASSBTable(GATKVariantContextUtils.java:1533); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.splitSomaticVariantContextToBiallelics(GATKVariantContextUtils.java:1501); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariants.apply(LeftAlignAndTrimVariants.java:225); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7211:1463,validat,validateArg,1463,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7211,1,['validat'],['validateArg']
Security," chr9_gl000200_random, chr9_gl000201_random, chr11_gl000202_random, chr17_ctg5_hap1, chr17_gl000203_random, chr17_gl000204_random, chr17_gl000205_random, chr17_gl000206_random, chr18_gl000207_random, chr19_gl000208_random, chr19_gl000209_random, chr21_gl000210_random, chrUn_gl000211, chrUn_gl000212, chrUn_gl000213, chrUn_gl000214, chrUn_gl000215, chrUn_gl000216, chrUn_gl000217, chrUn_gl000218, chrUn_gl000219, chrUn_gl000220, chrUn_gl000221, chrUn_gl000222, chrUn_gl000223, chrUn_gl000224, chrUn_gl000225, chrUn_gl000226, chrUn_gl000227, chrUn_gl000228, chrUn_gl000229, chrUn_gl000230, chrUn_gl000231, chrUn_gl000232, chrUn_gl000233, chrUn_gl000234, chrUn_gl000235, chrUn_gl000236, chrUn_gl000237, chrUn_gl000238, chrUn_gl000239, chrUn_gl000240, chrUn_gl000241, chrUn_gl000242, chrUn_gl000243, chrUn_gl000244, chrUn_gl000245, chrUn_gl000246, chrUn_gl000247, chrUn_gl000248, chrUn_gl000249]; reads contigs = []; 	at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:163); 	at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:98); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.validateToolInputs(GATKSparkTool.java:469); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:361); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:233); 	at sun.ref",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:34619,validat,validateDictionaries,34619,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['validat'],['validateDictionaries']
Security, chrUn_GL000214v1 (137718 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_KI270742v1 (186739 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_GL000216v2 (176608 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_GL000218v1 (161147 bp); 23:44:42.545 DEBUG GenomeLocParser - chrEBV (171823 bp); 23:44:42.632 INFO FeatureManager - Using codec IntervalListCodec to read file file:///gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list; 23:44:42.739 DEBUG FeatureDataSource - Cache statistics for FeatureInput /gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list:/gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list:; 23:44:42.740 DEBUG FeatureCache - Cache hit rate was 0.00% (0 out of 0 total queries); 23:44:42.743 INFO IntervalArgumentCollection - Processing 1022379 bp from intervals; 23:44:42.756 INFO GermlineCNVCaller - Reading and validating annotated intervals...; 23:44:43.119 INFO GermlineCNVCaller - GC-content annotations for intervals found; explicit GC-bias correction will be performed...; 23:44:43.160 INFO GermlineCNVCaller - Running the tool in COHORT mode...; 23:44:43.160 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 23:44:43.160 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 23:44:43.160 DEBUG GenomeLocParser - chr1 (248956422 bp); 23:44:43.161 DEBUG GenomeLocParser - chr2 (242193529 bp); 23:44:43.161 DEBUG GenomeLocParser - chr3 (198295559 bp); 23:44:43.161 DEBUG GenomeLocParser - chr4 (190214555 bp); 23:44:43.161 DEBUG GenomeLocParser - chr5 (181538259 bp); 23:44:43.161 DEBUG GenomeLocParser - chr6 (170805979 bp); 23:44:43.161 DEBUG GenomeLocParser - chr7 (159345973 bp); 23:44:43.161 DEBUG GenomeLocParser - chr8 (145138636 bp); 23:44:43.161 DEBUG GenomeLocParser - chr9 (138394717 bp); 23:44:43.161 DEBUG GenomeLocParser - chr10 (13,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:19965,validat,validating,19965,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['validat'],['validating']
Security," compare the result of MarkDuplicates and MarkDuplicatesSpark.; the same input SAM file and the default parameter, the MarkDuplicatesSpark have more data marked as duplicated.; Can you give me any suggest how to debug it, why the Spark version have more data marked?. READ_PAIR_DUPLICATES; **11933661 (MarkDuplicates); 11974162 (MarkDuplicatesSpark)**. Here is the metric file; ```. MarkDuplicatesSpark --output hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates.bam --metrics-file hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates-metrics.txt --input hdfs://wolfpass-aep:9000/user/test/spark_412.bowtie2.bam --spark-master yarn --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES --do-not-mark-unmapped-mates false --read-name-regex <optimized capture of last three ':' separated fields as numeric values> --optical-duplicate-pixel-distance 100 --read-validation-stringency SILENT --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false. METRICS CLASS	org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11974162	585768	0.222961	05870713. MarkDuplicates --INPUT /home/test/WGS_pipeline/TEST/output/orig_412.bowtie2.bam --OUTPUT /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates.bam --METRICS_FILE /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates-metrics.txt -",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905:877,validat,validation-stringency,877,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905,2,['validat'],"['validation', 'validation-stringency']"
Security, easy just to upgrade the library version as we could end up with run time errors. I am adding this here so that its handy when ever you look at this further. Thanks again. . packageName | version | severity | language | module_id; -- | -- | -- | -- | --; com.google.protobuf:protobuf-java | 3.7.1 | high | java | [SNYK-JAVA-COMGOOGLEPROTOBUF-2331703 ](https://security.snyk.io/vuln/SNYK-JAVA-COMGOOGLEPROTOBUF-2331703 ); com.google.protobuf:protobuf-java | 3.7.1 | high | java | [SNYK-JAVA-COMGOOGLEPROTOBUF-3167772](https://security.snyk.io/vuln/SNYK-JAVA-COMGOOGLEPROTOBUF-3167772); io.netty:netty-codec-http2 | 4.1.96.Final | high | java | [SNYK-JAVA-IONETTY-5953332](https://security.snyk.io/vuln/SNYK-JAVA-IONETTY-5953332); log4j:log4j | 1.2.17 | high | java | [SNYK-JAVA-LOG4J-2342645](https://security.snyk.io/vuln/SNYK-JAVA-LOG4J-2342645); log4j:log4j | 1.2.17 | high | java | [SNYK-JAVA-LOG4J-2342646](https://security.snyk.io/vuln/SNYK-JAVA-LOG4J-2342646); log4j:log4j | 1.2.17 | high | java | [SNYK-JAVA-LOG4J-2342647](https://security.snyk.io/vuln/SNYK-JAVA-LOG4J-2342647); log4j:log4j | 1.2.17 | critical | java | [SNYK-JAVA-LOG4J-572732](https://security.snyk.io/vuln/SNYK-JAVA-LOG4J-572732); net.minidev:json-smart | 1.3.2 | high | java | [SNYK-JAVA-NETMINIDEV-3369748](https://security.snyk.io/vuln/SNYK-JAVA-NETMINIDEV-3369748); org.apache.zookeeper:zookeeper | 3.6.3 | high | java | [SNYK-JAVA-ORGAPACHEZOOKEEPER-5961102](https://security.snyk.io/vuln/SNYK-JAVA-ORGAPACHEZOOKEEPER-5961102); org.codehaus.jettison:jettison | 1.1 | high | java | [SNYK-JAVA-ORGCODEHAUSJETTISON-3168085](https://security.snyk.io/vuln/SNYK-JAVA-ORGCODEHAUSJETTISON-3168085); org.codehaus.jettison:jettison | 1.1 | high | java | [SNYK-JAVA-ORGCODEHAUSJETTISON-3367610](https://security.snyk.io/vuln/SNYK-JAVA-ORGCODEHAUSJETTISON-3367610); org.eclipse.jetty:jetty-http | 9.4.52.v20230823 | high | java | [SNYK-JAVA-ORGECLIPSEJETTY-5958847](https://security.snyk.io/vuln/SNYK-JAVA-ORGECLIPSEJETTY-5958847),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1890593067:1429,secur,security,1429,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1890593067,7,['secur'],['security']
Security," engine; 10:58:20.091 WARN VariantAnnotatorEngine - The requested expression attribute ""gnomad.ALT"" is missing from the header in its resource file gnomad; 10:58:20.140 INFO ProgressMeter - Starting traversal; 10:58:20.140 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 10:58:42.160 INFO VariantAnnotator - Shutting down engine; [March 17, 2022 at 10:58:42 AM CET] org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotator done. Elapsed time: 0.37 minutes.; Runtime.totalMemory()=17158897664; java.lang.IllegalStateException: Allele in genotype C not in the variant context [C*, CT]; 	at htsjdk.variant.variantcontext.VariantContext$Validation.validateGenotypes(VariantContext.java:382); 	at htsjdk.variant.variantcontext.VariantContext$Validation.access$200(VariantContext.java:323); 	at htsjdk.variant.variantcontext.VariantContext$Validation$2.validate(VariantContext.java:331); 	at htsjdk.variant.variantcontext.VariantContext.lambda$validate$0(VariantContext.java:1384); 	at java.base/java.lang.Iterable.forEach(Iterable.java:75); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1384); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:489); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:647); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:638); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.trimAlleles(GATKVariantContextUtils.java:1464); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.trimAlleles(GATKVariantContextUtils.java:1420); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.getMinRepresentationBiallelics(VariantAnnotatorEngine.java:568); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateExpressions(VariantAnnotatorEngine.java:509); 	at org.broadinstitute.hellbender.to",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053:4266,validat,validate,4266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053,1,['validat'],['validate']
Security," gatktestjenkins@broad-gatk-test.iam.gserviceaccount.com --key-file /scratch/testservice.json --project broad-gatk-test; ./gatk-launch MarkDuplicatesSpark \; --shardedOutput true \; -O /scratch/tmp.md.bam \; --numReducers 0 \; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; ```. Fails with:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Logging; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:52); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.bdgenomics.adam.serialization.ADAMKryoRegistrator.registerClasses(ADAMKryoRegistrator.scala:85); at org.broadinstitute.hellbender.engine.spark.GATKRegistrator.registerClasses(GATKRegistrator.java:74); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializerInst",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2183:1132,Access,AccessController,1132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183,1,['Access'],['AccessController']
Security," in failed state and the main output does not appear. I have attached here the exact command, stderr,validate-bam-inputs.json and validate-bam.wdl. The stdout just says 230. ; I would be grateful for any help on how to solve the issue. ; Best ; zara. <img width=""1280"" alt=""Screen Shot 2020-07-16 at 12 01 12 AM"" src=""https://user-images.githubusercontent.com/61913000/87845900-e6491480-c8e0-11ea-8dc0-ec5b3fbc15a8.png"">; <img width=""1280"" alt=""Screen Shot 2020-07-16 at 12 01 19 AM"" src=""https://user-images.githubusercontent.com/61913000/87845901-e77a4180-c8e0-11ea-8c6a-fb5783949ba3.png"">; <img width=""353"" alt=""Screen Shot 2020-07-16 at 12 00 30 AM"" src=""https://user-images.githubusercontent.com/61913000/87845902-ea753200-c8e0-11ea-96bc-caecee2ccb39.png"">; <img width=""1249"" alt=""stderr1"" src=""https://user-images.githubusercontent.com/61913000/87845904-eea14f80-c8e0-11ea-90bd-235c9205f72f.png"">. (gatk) root@bc3c6aca6231:/gatk/my_data/tools# java -jar cromwell-51.jar run /gatk/my_data/seq-format-validation/validate-bam.wdl --inputs /gatk/my_data/seq-format-validation/validate-bam.inputs.json; [2020-07-14 05:09:22,78] [info] Running with database db.url = jdbc:hsqldb:mem:f10b64bd-d8ca-4428-917b-311fca24c372;shutdown=false;hsqldb.tx=mvcc; [2020-07-14 05:09:29,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-07-14 05:09:29,37] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-07-14 05:09:29,47] [info] Running with database db.url = jdbc:hsqldb:mem:e337a356-2f0c-4389-92c5-255465180f24;shutdown=false;hsqldb.tx=mvcc; [2020-07-14 05:09:29,89] [info] Slf4jLogger started; [2020-07-14 05:09:30,10] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-ca5c695"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2020-07-14 05:09:30,23] [info] Metadata summary refreshing every 1 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:1140,validat,validation,1140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,2,['validat'],"['validate-bam', 'validation']"
Security," in the sampling of denoised copy ratios, fixes a memory leak by updating theano, and also adds some theano flags that typically yield a factor of ~2 speedup (notably, the OpenMP elemwise flag, although we also get a slight boost from using numpy MKL). This allows us to run, e.g.: . 2 shards of 50 samples by 100000 intervals on n1-standard-8s (8 CPU, 30GB memory, $0.08 / hr) each taking ~5 hours = ~1.6 cents / sample; 4 shards of 50 samples by 50000 intervals on n1-highmem-4s (4 CPU, 26GB memory, $0.05 / hr) each taking ~3.25 hours = ~1.3 cents / sample; 45 shards of 50 samples by 5000 intervals on *n1-standard-1s* (1CPU, 3.75GB memory, $0.01 / hr) each taking ~0.5 hours = ~0.5 cents / sample. For these runs, we used a slightly larger interval list and 1/4 the number of samples than in the first example, but because everything scales linearly, it's probably fair to compare the per-sample-and-interval costs. So we get a factor of ~8 savings if we keep the shard size the same. The cost was already satisfactory, but fixing the leak allows us to more easily run scatters that are not so wide, which may be crucial for running the megaWDL. Adding the OpenMP flag also lets CPU scalability work as intended. We can do a more systematic optimization for cost if desired, and we should also revalidate to make sure performance doesn't vary too much with shard size (from spot checking, it looks like marginal and/or single-bin calls may flicker on and off). Note that we have still not optimized inference for WES, although I believe @vruano has done some optimizations for WGS. @mwalker174 @vruano for WGS with 2kb bins, I would expect the cost of the gCNV step to be ~10 cents in cohort mode before inference optimizations, assuming we address #5716 to minimize disk costs. @asmirnov239 can you review? And maybe you can address dCR output in PostprocessGermlineCNVCalls and expose the number of samples in a separate PR? We can make some further changes to the dCR format there if we need.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5781#issuecomment-471570697:2172,expose,expose,2172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5781#issuecomment-471570697,1,['expose'],['expose']
Security, org.broadinstitute.hellbender.testutils.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:111); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReadsIntegrationTest.testLargeFileThatForcesSnappyUsage(SplitNCigarReadsIntegrationTest.java:85); Caused by:; htsjdk.samtools.util.RuntimeIOException: Write error; BinaryCodec in writemode; streamed file (filename not available); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:222); at htsjdk.samtools.util.BlockCompressedOutputStream.writeGzipBlock(BlockCompressedOutputStream.java:444); at htsjdk.samtools.util.BlockCompressedOutputStream.deflateBlock(BlockCompressedOutputStream.java:408); at htsjdk.samtools.util.BlockCompressedOutputStream.write(BlockCompressedOutputStream.java:301); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:220); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:212); at htsjdk.samtools.BAMRecordCodec.encode(BAMRecordCodec.java:168); at htsjdk.samtools.BAMFileWriter.writeAlignment(BAMFileWriter.java:134); ... 15 more; Caused by:; java.io.IOException: No space left on device; at sun.nio.ch.FileDispatcherImpl.write0(Native Method); at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60); at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93); at sun.nio.ch.IOUtil.write(IOUtil.java:65); at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:[211](https://github.com/broadinstitute/gatk/actions/runs/5547450688/jobs/10131043668#step:12:211)); at java.nio.channels.Channels.writeFullyImpl(Channels.java:78); at java.nio.channels.Channels.writeFully(Channels.java:101); at java.nio.channels.Channels.access$000(Channels.java:61); at java.nio.channels.Channels$1.write(Channels.java:174); at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82); at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126); at htsjdk.samtools.util.BinaryCodec.writeBytes(BinaryCodec.java:220); ... 22 more; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8404#issuecomment-1635002002:3390,access,access,3390,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8404#issuecomment-1635002002,1,['access'],['access']
Security," pretty big drop in recall when optimizing LL. But we should also expect some discrepancy between LL and F1, according to one of the papers linked above. I would hope that with more variants or reliable training/truth (as in your data), things might stabilize or line up better. I'll try running with more malaria data, as well. The following trios x sites heatmap (top plot) for the validation set might better illustrate the arbitrariness in F1 (click to enlarge):. ![image](https://user-images.githubusercontent.com/11076296/158385585-1a0dfe8e-d4b7-4770-aed0-19ad81162c92.png). Here, yellow = het errors (since these are supposed to be clonal malaria samples), red = Mendelian errors, grey = no calls, green = Mendelian consistency, white = reference. The second plot shows the training/truth positives used to train the model and to calculate the LL score in the validation shard. The third plot shows the ""orthogonal truth"" positives/negatives used to calculate F1. So we can see that the difficulty in deriving F1 as a function of the score along the horizontal axis to give the third plot lies in collapsing the columns in the top plot into a single condition positive or condition negative status. Again, hard to do so without some arbitrariness; I simply came up with some rules to convert various amounts of red, yellow, green, etc. in each column to a red/white/green status. If you're using a single gold-standard sample, this should definitely be more straightforward. In any case, the optimal validation LL score at ~0.02 does appear to line up quite well visually with where one might manually set a threshold. It corresponds pretty well with the transition from the yellow/red/grey junk to the clean green/white sites in the top plot. Here's the same for the test set:. ![image](https://user-images.githubusercontent.com/11076296/158385662-6693a6c9-709c-482f-9a7e-5bb7030b3383.png). Happy to chat more about how you might implement this in your WDL---should be pretty straightforward!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431:2782,validat,validation,2782,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431,2,['validat'],['validation']
Security," redirect tqdm progress bar to python logger. commit 2e45bd30968b921fae225de3901fb97ece690b0c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 19:45:49 2017 -0500. more arg related fixes. commit bb89a3bb338d88199881e8aca65f656f2acd7c0a; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 19:41:20 2017 -0500. arg related bugfixes in WDL, python, and java CLIs. commit 23569787ee2c8cc6c9227a44170cbbd02fe4427f; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 17:21:05 2017 -0500. fixed issue with python boolean argparse (they use weird semantics). commit ae841c9ed4cd9b2ca1ac0e9082d175ff8ea98298; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 16:44:02 2017 -0500. shorter gCNV WDL tests. commit 5466b806e36df16cad2d045be074e7f9afec0957; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 16:38:15 2017 -0500. fixed arg issues in somatic WDL; exposed all missing args to java side; major update to germline WDLs; all optional python args exposed to WDLs as optional args. commit 50cb6fd08de15469a9080cbb27ff30c8b7ee7e21; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 13:50:45 2017 -0500. missing serialVersionUID. commit 5f0f31eab63b0e6f6105708ded7f86c96c830781; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 13:35:33 2017 -0500. annotated intervals kebab case; updated germline WDL workflows. commit 29cc6234dbfb8db12559217a650c6ceb170c5797; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 13:15:28 2017 -0500. cleanup test files. commit 08a35bb4e65eceb735adcd41a91132e9a34d2b66; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 02:50:19 2017 -0500. update WDL scripts. commit 12bcfa192ee6fa6da21239ebf5b513633efe974f; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 02:47:33 2017 -0500. significant updates to GermlineCNVCaller; integration tests for GermlineCNVCaller w",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:5916,expose,exposed,5916,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,4,['expose'],['exposed']
Security," requested more than the maximum memory capability of the cluster (164726 MB per container); 17/10/13 18:11:36 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead; 17/10/13 18:11:36 INFO yarn.Client: Setting up container launch context for our AM; 17/10/13 18:11:36 INFO yarn.Client: Setting up the launch environment for our AM container; 17/10/13 18:11:36 INFO yarn.Client: Preparing resources for our AM container; 17/10/13 18:11:37 INFO yarn.Client: Uploading resource file:/tmp/hdfs/spark-c7e5eece-205e-4bce-a69b-4168c9b79045/__spark_conf__2918234914787361986.zip -> hdfs://mg:8020/user/hdfs/.sparkStaging/application_1507856833944_0003/__spark_conf__.zip; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); groups with view permissions: Set(); users with modify permissions: Set(hdfs); groups with modify permissions: Set(); 17/10/13 18:11:37 INFO yarn.Client: Submitting application application_1507856833944_0003 to ResourceManager; 17/10/13 18:11:37 INFO impl.YarnClientImpl: Submitted application application_1507856833944_0003; 17/10/13 18:11:37 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1507856833944_0003 and attemptId None; 17/10/13 18:11:38 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:38 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: root.users.hdfs; 	 start time: 1507889497661; 	 final status: UNDEFINED; 	 tracking URL: http://mg:8088",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:10788,Secur,SecurityManager,10788,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['Secur'],['SecurityManager']
Security, spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 15g --executor-cores 2 --executor-memory 8g /gatk/build/libs/gatk-spark.jar BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 4000000 --input hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_fastqtosam.bam --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img --disable-sequence-dictionary-validation true --output hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_dedup_reads.bam --spark-master spark://680776067ebd:7077; 16:33:44.918 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 16:33:45.152 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 16:33:45.587 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 16:33:45.587 INFO BwaAndMarkDuplicatesPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.2.0-4-gb59d863-SNAPSHOT; 16:33:45.588 INFO BwaAndMarkDuplicatesPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:33:45.588 INFO BwaAndMarkDuplicatesPipelineSpark - Executing as root@680776067ebd on Linux v4.4.0-127-generic amd64; 16:33:45.588 INFO BwaAndMarkDuplicatesPipel,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:10735,validat,validation,10735,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,1,['validat'],['validation']
Security, sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:97); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); 	at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:507); 	... 12 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:116); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3375); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:169); 	at java.io.FilterInputStream.read(FilterInputStream.java:107); 	at shaded.cloud_nio.com.google.api.client.util.ByteStreams.copy(ByteStreams.java:51); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:94); 	...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:8048,secur,security,8048,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['secur'],['security']
Security, sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:97); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); 	at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:510); 	... 47 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:116); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3375); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:169); 	at java.io.FilterInputStream.read(FilterInputStream.java:107); 	at shaded.cloud_nio.com.google.api.client.util.ByteStreams.copy(ByteStreams.java:51); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:94); 	...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308541727:6342,secur,security,6342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308541727,1,['secur'],['security']
Security, sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:97); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); 	at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:510); 	... 55 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:116); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3375); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:169); 	at java.io.FilterInputStream.read(FilterInputStream.java:107); 	at shaded.cloud_nio.com.google.api.client.util.ByteStreams.copy(ByteStreams.java:51); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:94); 	...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549138:9830,secur,security,9830,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549138,1,['secur'],['security']
Security," traversal; 19:53:35.595 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 19:53:35.660 INFO ValidateVariants - Shutting down engine; [October 25, 2020 7:53:35 PM CDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2114453504; java.lang.ArrayIndexOutOfBoundsException: -87; 	at org.broadinstitute.hellbender.utils.BaseUtils.convertIUPACtoN(BaseUtils.java:123); 	at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.getSubsequenceAt(CachingIndexedFastaSequenceFile.java:340); 	at org.broadinstitute.hellbender.engine.ReferenceFileSource.queryAndPrefetch(ReferenceFileSource.java:78); 	at org.broadinstitute.hellbender.engine.ReferenceDataSource.queryAndPrefetch(ReferenceDataSource.java:64); 	at org.broadinstitute.hellbender.engine.ReferenceContext.getBases(ReferenceContext.java:197); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants.apply(ValidateVariants.java:236); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	at org.broadinstitute.hellbender.engine.VariantWalker$$Lambda$76/1710491273.accept(Unknown Source); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluate",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911:4100,Validat,ValidateVariants,4100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911,1,['Validat'],['ValidateVariants']
Security," v3.10.0-1127.8.2.el7.x86_64 amd64; 19:53:34.607 INFO ValidateVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14; 19:53:34.607 INFO ValidateVariants - Start Date/Time: October 25, 2020 7:53:34 PM CDT; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - HTSJDK Version: 2.22.0; 19:53:34.607 INFO ValidateVariants - Picard Version: 2.22.8; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:53:34.608 INFO ValidateVariants - Deflater: IntelDeflater; 19:53:34.608 INFO ValidateVariants - Inflater: IntelInflater; 19:53:34.608 INFO ValidateVariants - GCS max retries/reopens: 20; 19:53:34.608 INFO ValidateVariants - Requester pays: disabled; 19:53:34.608 INFO ValidateVariants - Initializing engine; 19:53:35.169 INFO FeatureManager - Using codec VCFCodec to read file file://chr1-22.phased.rename.reheader.vcf.gz; 19:53:35.594 INFO ValidateVariants - Done initializing engine; 19:53:35.594 WARN ValidateVariants - IDS validation cannot be done because no DBSNP file was provided; 19:53:35.594 WARN ValidateVariants - Other possible validations will still be performed; 19:53:35.594 INFO ProgressMeter - Starting traversal; 19:53:35.595 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 19:53:35.660 INFO ValidateVariants - Shutting down engine; [October 25, 2020 7:53:35 PM CDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2114453504; java.lang.ArrayIndexOutOfBounds",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911:2465,Validat,ValidateVariants,2465,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911,1,['Validat'],['ValidateVariants']
Security, version 55; 	at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166); 	at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148); 	at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136); 	at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237); 	at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49); 	at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517); 	at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500); 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); 	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134); 	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134); 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); 	at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500); 	at org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175); 	at org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238); 	at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631); 	at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:355); 	at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:307); 	at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:306); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:306); 	at org.apach,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7035:7030,Hash,HashMap,7030,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7035,1,['Hash'],['HashMap']
Security," warning; :compileJava FAILED; :compileJava (Thread[Daemon worker Thread 2,5,main]) completed. Took 4.116 secs. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileJava'.; > Compilation failed; see the compiler error output for details. * Try:; Run with --debug option to get more log output. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':compileJava'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:69); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:35); at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:64); at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:52); at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:52); at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:53); at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43); at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:233); at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:215); at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:74); at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:55); at org.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4248:6041,Validat,ValidatingTaskExecuter,6041,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4248,1,['Validat'],['ValidatingTaskExecuter']
Security," with 25.4 GB RAM, BlockManagerId(7, scc-q14.scc.bu.edu, 36726, None); 18/03/07 20:31:49 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (192.168.18.195:47862) with ID 6; 18/03/07 20:31:49 INFO storage.BlockManagerMasterEndpoint: Registering block manager scc-q11.scc.bu.edu:46002 with 25.4 GB RAM, BlockManagerId(6, scc-q11.scc.bu.edu, 46002, None); 18/03/07 20:31:49 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 246.6 KB, free 8.4 GB); 18/03/07 20:31:50 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 25.3 KB, free 8.4 GB); 18/03/07 20:31:50 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.48.225.55:41567 (size: 25.3 KB, free: 8.4 GB); 18/03/07 20:31:50 INFO spark.SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:112; 18/03/07 20:31:50 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 7175 for farrell on ha-hdfs:scc; 18/03/07 20:31:50 INFO security.TokenCache: Got dt for hdfs://scc; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:scc, Ident: (HDFS_DELEGATION_TOKEN token 7175 for farrell); 18/03/07 20:31:50 INFO input.FileInputFormat: Total input paths to process : 1; 18/03/07 20:31:51 INFO spark.SparkContext: Starting job: aggregate at FlagStatSpark.java:73; 18/03/07 20:31:51 INFO scheduler.DAGScheduler: Got job 0 (aggregate at FlagStatSpark.java:73) with 629 output partitions; 18/03/07 20:31:51 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (aggregate at FlagStatSpark.java:73); 18/03/07 20:31:51 INFO scheduler.DAGScheduler: Parents of final stage: List(); 18/03/07 20:31:51 INFO scheduler.DAGScheduler: Missing parents: List(); 18/03/07 20:31:51 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at GATKSparkTool.java:220), which has no missing parents; 18/03/07 20:31:51 INFO memory.MemoryStore: Block broadcast_1 stored as values in mem",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371353888:5073,secur,security,5073,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371353888,2,['secur'],['security']
Security,!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CountReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 11:33:26.277 INFO CountReadsSpark - Initializing engine; 11:33:26.277 INFO CountReadsSpark - Done initializing engine; 2019-01-07 11:33:26 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-07 11:33:26 INFO SparkContext:54 - Running Spark version 2.3.0; 2019-01-07 11:33:26 INFO SparkContext:54 - Submitted application: CountReadsSpark; 2019-01-07 11:33:26 INFO SecurityManager:54 - Changing view acls to: farrell; 2019-01-07 11:33:26 INFO SecurityManager:54 - Changing modify acls to: farrell; 2019-01-07 11:33:26 INFO SecurityManager:54 - Changing view acls groups to:; 2019-01-07 11:33:26 INFO SecurityManager:54 - Changing modify acls groups to:; 2019-01-07 11:33:26 INFO SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-07 11:33:27 INFO Utils:54 - Successfully started service 'sparkDriver' on port 46828.; 2019-01-07 11:33:27 INFO SparkEnv:54 - Registering MapOutputTracker; 2019-01-07 11:33:27 INFO SparkEnv:54 - Registering BlockManagerMaster; 2019-01-07 11:33:27 INFO BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-07 11:33:27 INFO BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up; 2019-01-07 11:33:27 INFO DiskBlockManager:54 - Created local directory at /tmp/blockmgr-08460386-3abb-4431-ba8d-5b7d41a2a05c; 2019-01-07 11:33:27 INFO MemoryStore:54 - MemoryStore started with capacity 408.6 MB; 2019-01-07 11:33:27 INFO SparkEnv,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:5347,authenticat,authentication,5347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,8,"['Secur', 'authenticat']","['SecurityManager', 'authentication']"
Security,!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CountReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 13:35:11.512 INFO CountReadsSpark - Initializing engine; 13:35:11.512 INFO CountReadsSpark - Done initializing engine; 2019-01-09 13:35:11 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-09 13:35:11 INFO SparkContext:54 - Running Spark version 2.3.0; 2019-01-09 13:35:11 INFO SparkContext:54 - Submitted application: CountReadsSpark; 2019-01-09 13:35:11 INFO SecurityManager:54 - Changing view acls to: farrell; 2019-01-09 13:35:11 INFO SecurityManager:54 - Changing modify acls to: farrell; 2019-01-09 13:35:11 INFO SecurityManager:54 - Changing view acls groups to:; 2019-01-09 13:35:11 INFO SecurityManager:54 - Changing modify acls groups to:; 2019-01-09 13:35:11 INFO SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-09 13:35:12 INFO Utils:54 - Successfully started service 'sparkDriver' on port 42689.; 2019-01-09 13:35:12 INFO SparkEnv:54 - Registering MapOutputTracker; 2019-01-09 13:35:12 INFO SparkEnv:54 - Registering BlockManagerMaster; 2019-01-09 13:35:12 INFO BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-09 13:35:12 INFO BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up; 2019-01-09 13:35:12 INFO DiskBlockManager:54 - Created local directory at /tmp/blockmgr-dd94d6fb-7e3d-4def-a895-6e60f05d7a05; 2019-01-09 13:35:12 INFO MemoryStore:54 - MemoryStore started with capacity 372.6 MB; 2019-01-09 13:35:12 INFO SparkEnv,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:5086,authenticat,authentication,5086,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,8,"['Secur', 'authenticat']","['SecurityManager', 'authentication']"
Security,""" ; 7: Setting LC_MEASUREMENT failed, using ""C"" ; Error in readRDS(pfile) : ; cannot read workspace version 3 written by R 3.6.0; need R 3.5.0 or newer; Calls: source ... library -> find.package -> lapply -> FUN -> readRDS; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:80); 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:19); 	at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:126); 	at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.generatePlots(RecalUtils.java:360); 	at org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates.generatePlots(AnalyzeCovariates.java:329); 	at org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates.doWork(AnalyzeCovariates.java:341); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); 	at org.broadinstitute.hellbender.Main.main(Main.java:292); ```. when opening the docker container one can see installed version of R is 3.2.5. ```; Singularity broadinstitute_gatk_sha256_cec850f20311f0686fcf88510bc44e529590d78bec7076a603132115943c09e6.sif:~> R --version. R version 3.2.5 (2016-04-14) -- ""Very, Very Secure Dishes""; Copyright (C) 2016 The R Foundation for Statistical Computing; Platform: x86_64-pc-linux-gnu (64-bit); ```. #### Steps to reproduce; Run any AnalyzeCovariates. #### Expected behavior; create plots. #### Actual behavior; Docker faceplants",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6393:5742,Secur,Secure,5742,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6393,1,['Secur'],['Secure']
Security,"""D:\Program Files\Java\jdk1.8.0_121\bin\java.exe"" -agentlib:jdwp=transport=dt_socket,address=127.0.0.1:62530,suspend=y,server=n -XX:TieredStopAtLevel=1 -noverify -Dspring.output.ansi.enabled=always -Dcom.sun.management.jmxremote -Dspring.jmx.enabled=true -Dspring.liveBeansView.mbeanDomain -Dspring.application.admin.enabled=true -javaagent:C:\Users\Sweet\AppData\Local\JetBrains\IntelliJIdea2020.1\captureAgent\debugger-agent.jar -Dfile.encoding=UTF-8 -classpath ""D:\Program Files\Java\jdk1.8.0_121\jre\lib\charsets.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\deploy.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\access-bridge-64.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\cldrdata.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\dnsns.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\jaccess.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\jfxrt.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\localedata.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\nashorn.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\sunec.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\sunjce_provider.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\sunmscapi.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\sunpkcs11.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\zipfs.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\javaws.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\jce.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\jfr.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\jfxswt.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\jsse.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\management-agent.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\plugin.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\resources.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\rt.jar;C:\project\push\target\classes;E:\repository\org\springframework\boot\spring-boot-starter-jdbc\2.3.0.RELEASE\spring-boot-starter-jdbc-2.3.0.RELEASE.jar;E:\repository\org\springframework\boot\spring-boot-starter\2.3",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:622,access,access-bridge-,622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['access'],['access-bridge-']
Security,"# . ----------------------------------------------------------------------------------------------------------------------------------. the variants.funcotated.maf:. #version 2.4; ##; ## fileformat=VCFv4.2; ## FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ## FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ## FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read Depth"">; ## source=Funcotator; ## GATKCommandLine=<ID=Funcotator,CommandLine=""Funcotator --output ./my_data/variants.funcotated.maf --ref-version hg19 --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s --output-file-format MAF --variant ./my_data/test_b37.vcf --reference ./my_data/human_g1k_v37.fasta --disable-sequence-dictionary-validation true --remove-filtered-variants false --five-prime-flank-size 5000 --three-prime-flank-size 0 --force-b37-to-hg19-reference-contig-conversion false --transcript-selection-mode CANONICAL --lookahead-cache-bp 100000 --min-num-bases-for-segment-funcotation 150 --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false"",Version=""4.2.0.0"",Date=""March 24, 2021 12:11:32 PM GMT"">; ## Funcotator 4.2.0.0 | Date 20211124T121132 | Gencode 34 CANONICAL | Achilles 110303 | CGC full_2012_03-15 | ClinVar 12.03.20 | C",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7158:19014,validat,validation,19014,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158,2,['validat'],"['validation', 'validation-stringency']"
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2649?src=pr&el=h1) Report; > Merging [#2649](https://codecov.io/gh/broadinstitute/gatk/pull/2649?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/32ea767879741f62889ee9118ac4f94608d563c3?src=pr&el=desc) will **decrease** coverage by `0.002%`.; > The diff coverage is `66.667%`. ```diff; @@ Coverage Diff @@; ## master #2649 +/- ##; ===============================================; - Coverage 76.126% 76.124% -0.002% ; - Complexity 11151 11153 +2 ; ===============================================; Files 769 769 ; Lines 40752 40753 +1 ; Branches 7110 7112 +2 ; ===============================================; Hits 31023 31023 ; Misses 7062 7062 ; - Partials 2667 2668 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2649?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/2649?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYWxpZGF0ZVZhcmlhbnRzLmphdmE=) | `80.597% <66.667%> (-1.221%)` | `18 <0> (+2)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2649#issuecomment-298987781:940,Validat,ValidateVariants,940,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2649#issuecomment-298987781,1,['Validat'],['ValidateVariants']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2759?src=pr&el=h1) Report; > Merging [#2759](https://codecov.io/gh/broadinstitute/gatk/pull/2759?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/5fd6c965741e590ed7c8d7ee820f0b72fb528187?src=pr&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2759 +/- ##; ===========================================; Coverage 80.131% 80.131% ; Complexity 16992 16992 ; ===========================================; Files 1144 1144 ; Lines 61630 61630 ; Branches 9605 9605 ; ===========================================; Hits 49385 49385 ; Misses 8422 8422 ; Partials 3823 3823; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2759?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/2759?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYWxpZGF0ZVZhcmlhbnRzLmphdmE=) | `80.597% <ø> (ø)` | `18 <0> (ø)` | :arrow_down: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2759#issuecomment-304135170:890,Validat,ValidateVariants,890,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2759#issuecomment-304135170,1,['Validat'],['ValidateVariants']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3045?src=pr&el=h1) Report; > Merging [#3045](https://codecov.io/gh/broadinstitute/gatk/pull/3045?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/b01b71e2e84ceea6df6268594f0ee7aa4e7fe38f?src=pr&el=desc) will **increase** coverage by `0.017%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #3045 +/- ##; ===============================================; + Coverage 80.131% 80.149% +0.017% ; - Complexity 16990 17003 +13 ; ===============================================; Files 1144 1144 ; Lines 61630 61673 +43 ; Branches 9605 9621 +16 ; ===============================================; + Hits 49385 49430 +45 ; + Misses 8422 8419 -3 ; - Partials 3823 3824 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3045?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tools/walkers/haplotypecaller/HaplotypeCaller.java](https://codecov.io/gh/broadinstitute/gatk/pull/3045?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXIuamF2YQ==) | `94.118% <ø> (ø)` | `18 <0> (ø)` | :arrow_down: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3045?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `73.649% <0%> (+2.027%)` | `34% <0%> (ø)` | :arrow_down: |; | [...llbender/tools/walkers/validation/Concordance.java](https://codecov.io/gh/broadinstitute/gatk/pull/3045?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQ29uY29yZGFuY2UuamF2YQ==) | `91.367% <0%> (+2.825%)` | `39% <0%> (+11%)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3045#issuecomment-309612946:1549,validat,validation,1549,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3045#issuecomment-309612946,1,['validat'],['validation']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3108?src=pr&el=h1) Report; > Merging [#3108](https://codecov.io/gh/broadinstitute/gatk/pull/3108?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/994b25194cb1a697903e8db749412cba9b422481?src=pr&el=desc) will **increase** coverage by `0.005%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #3108 +/- ##; ===============================================; + Coverage 80.131% 80.136% +0.005% ; Complexity 16992 16992 ; ===============================================; Files 1144 1144 ; Lines 61630 61630 ; Branches 9605 9605 ; ===============================================; + Hits 49385 49388 +3 ; + Misses 8422 8419 -3 ; Partials 3823 3823; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3108?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...llbender/tools/walkers/validation/Concordance.java](https://codecov.io/gh/broadinstitute/gatk/pull/3108?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQ29uY29yZGFuY2UuamF2YQ==) | `88.542% <ø> (ø)` | `28 <0> (ø)` | :arrow_down: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3108?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `73.649% <0%> (+2.027%)` | `34% <0%> (ø)` | :arrow_down: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3108#issuecomment-308209516:926,validat,validation,926,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3108#issuecomment-308209516,1,['validat'],['validation']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3369?src=pr&el=h1) Report; > Merging [#3369](https://codecov.io/gh/broadinstitute/gatk/pull/3369?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/28c3b7d48a5b3c219a1057e96b1f728ab2f06b37?src=pr&el=desc) will **increase** coverage by `0.017%`.; > The diff coverage is `75%`. ```diff; @@ Coverage Diff @@; ## master #3369 +/- ##; ==============================================; + Coverage 79.923% 79.94% +0.017% ; - Complexity 17884 17897 +13 ; ==============================================; Files 1198 1198 ; Lines 64966 64980 +14 ; Branches 10114 10120 +6 ; ==============================================; + Hits 51923 51945 +22 ; + Misses 9010 9002 -8 ; Partials 4033 4033; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3369?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tools/spark/validation/CompareDuplicatesSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3369?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay92YWxpZGF0aW9uL0NvbXBhcmVEdXBsaWNhdGVzU3BhcmsuamF2YQ==) | `82.927% <50%> (-1.883%)` | `24 <0> (ø)` | |; | [...der/engine/spark/datasources/ReadsSparkSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/3369?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NvdXJjZS5qYXZh) | `80.198% <76.923%> (+10.724%)` | `38 <10> (+10)` | :arrow_up: |; | [...stitute/hellbender/engine/spark/GATKSparkTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/3369?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvR0FUS1NwYXJrVG9vbC5qYXZh) | `85% <85.714%> (+0.789%)` | `55 <5> (+2)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3369?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGU,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3369#issuecomment-325704642:927,validat,validation,927,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3369#issuecomment-325704642,1,['validat'],['validation']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3445?src=pr&el=h1) Report; > Merging [#3445](https://codecov.io/gh/broadinstitute/gatk/pull/3445?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/69fd3791cf5dd975ce1cc820226fc4a4c8904b65?src=pr&el=desc) will **increase** coverage by `0.023%`.; > The diff coverage is `91.667%`. ```diff; @@ Coverage Diff @@; ## master #3445 +/- ##; ==============================================; + Coverage 80.287% 80.31% +0.023% ; - Complexity 17633 17647 +14 ; ==============================================; Files 1178 1178 ; Lines 63847 63854 +7 ; Branches 9928 9930 +2 ; ==============================================; + Hits 51261 51281 +20 ; + Misses 8631 8628 -3 ; + Partials 3955 3945 -10; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3445?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/3445?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYWxpZGF0ZVZhcmlhbnRzLmphdmE=) | `82.353% <91.667%> (+2.353%)` | `32 <0> (+4)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3445?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `74.342% <0%> (-1.974%)` | `38% <0%> (ø)` | |; | [...bender/tools/spark/sv/evidence/ReadClassifier.java](https://codecov.io/gh/broadinstitute/gatk/pull/3445?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9SZWFkQ2xhc3NpZmllci5qYXZh) | `86.667% <0%> (+1.333%)` | `33% <0%> (+1%)` | :arrow_up: |; | [...er/tools/spark/sv/evidence/BreakpointEvidence.java](https://codecov.io/gh/broadinstitute/gatk/pull/3445?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVs,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3445#issuecomment-322886799:951,Validat,ValidateVariants,951,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3445#issuecomment-322886799,1,['Validat'],['ValidateVariants']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3461?src=pr&el=h1) Report; > Merging [#3461](https://codecov.io/gh/broadinstitute/gatk/pull/3461?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/a1292eecb05bc4c61abc33d7b01c81b29383a150?src=pr&el=desc) will **increase** coverage by `0.001%`.; > The diff coverage is `75%`. ```diff; @@ Coverage Diff @@; ## master #3461 +/- ##; ===============================================; + Coverage 80.307% 80.307% +0.001% ; - Complexity 17645 17666 +21 ; ===============================================; Files 1178 1178 ; Lines 63854 63907 +53 ; Branches 9930 9947 +17 ; ===============================================; + Hits 51279 51322 +43 ; - Misses 8627 8632 +5 ; - Partials 3948 3953 +5; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3461?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ute/hellbender/utils/bwa/BwaMemAlignmentUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3461?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9id2EvQndhTWVtQWxpZ25tZW50VXRpbHMuamF2YQ==) | `88.889% <75%> (-4.532%)` | `36 <26> (+5)` | |; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/3461?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYWxpZGF0ZVZhcmlhbnRzLmphdmE=) | `83.688% <0%> (+1.335%)` | `48% <0%> (+16%)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3461#issuecomment-323443004:1242,Validat,ValidateVariants,1242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3461#issuecomment-323443004,1,['Validat'],['ValidateVariants']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3530?src=pr&el=h1) Report; > Merging [#3530](https://codecov.io/gh/broadinstitute/gatk/pull/3530?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/bfa9af462f484c77597a9fdbdc46f66393afaff1?src=pr&el=desc) will **increase** coverage by `0.03%`.; > The diff coverage is `100%`. ```diff; @@ Coverage Diff @@; ## master #3530 +/- ##; ==============================================; + Coverage 80.079% 80.109% +0.03% ; - Complexity 17760 17791 +31 ; ==============================================; Files 1188 1188 ; Lines 64410 64543 +133 ; Branches 10004 10022 +18 ; ==============================================; + Hits 51579 51705 +126 ; Misses 8845 8845 ; - Partials 3986 3993 +7; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3530?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/3530?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYWxpZGF0ZVZhcmlhbnRzLmphdmE=) | `82.857% <100%> (+0.504%)` | `34 <0> (+2)` | :arrow_up: |; | [...er/tools/spark/sv/discovery/AlignmentInterval.java](https://codecov.io/gh/broadinstitute/gatk/pull/3530?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvQWxpZ25tZW50SW50ZXJ2YWwuamF2YQ==) | `89.831% <0%> (-0.847%)` | `23% <0%> (-1%)` | |; | [...nder/tools/spark/pipelines/ReadsPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3530?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUmVhZHNQaXBlbGluZVNwYXJrLmphdmE=) | `92.857% <0%> (-0.246%)` | `16% <0%> (+8%)` | |; | [...lotypecaller/readthreading/ReadThreadingGraph.java](https://codecov.io/gh/broadinstitute/gatk/pull/3530?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRp,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3530#issuecomment-325497752:947,Validat,ValidateVariants,947,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3530#issuecomment-325497752,1,['Validat'],['ValidateVariants']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3880?src=pr&el=h1) Report; > Merging [#3880](https://codecov.io/gh/broadinstitute/gatk/pull/3880?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/bf18c26bde607497ab340e550ef94540ccd3bb1d?src=pr&el=desc) will **decrease** coverage by `0.024%`.; > The diff coverage is `0%`. ```diff; @@ Coverage Diff @@; ## master #3880 +/- ##; ===============================================; - Coverage 79.347% 79.322% -0.024% ; - Complexity 17919 18703 +784 ; ===============================================; Files 1172 1173 +1 ; Lines 64706 66787 +2081 ; Branches 9880 10565 +685 ; ===============================================; + Hits 51342 52977 +1635 ; - Misses 9446 9815 +369 ; - Partials 3918 3995 +77; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3880?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...lbender/tools/validation/CompareBaseQualities.java](https://codecov.io/gh/broadinstitute/gatk/pull/3880?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy92YWxpZGF0aW9uL0NvbXBhcmVCYXNlUXVhbGl0aWVzLmphdmE=) | `75.676% <ø> (ø)` | `5 <0> (ø)` | :arrow_down: |; | [...titute/hellbender/tools/walkers/CountVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/3880?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL0NvdW50VmFyaWFudHMuamF2YQ==) | `100% <ø> (ø)` | `3 <0> (ø)` | :arrow_down: |; | [...ark/pipelines/metrics/MeanQualityByCycleSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3880?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvbWV0cmljcy9NZWFuUXVhbGl0eUJ5Q3ljbGVTcGFyay5qYXZh) | `90.816% <ø> (ø)` | `11 <0> (ø)` | :arrow_down: |; | [...bender/tools/copynumber/CallCopyRatioSegments.java](https://codecov.io/gh/broadinstitute/gatk/pull/3880?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3880#issuecomment-347373155:949,validat,validation,949,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3880#issuecomment-347373155,1,['validat'],['validation']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4894?src=pr&el=h1) Report; > Merging [#4894](https://codecov.io/gh/broadinstitute/gatk/pull/4894?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/41788f5de7711bf46222bb19c139d84946b40d70?src=pr&el=desc) will **increase** coverage by `0.191%`.; > The diff coverage is `94.231%`. ```diff; @@ Coverage Diff @@; ## master #4894 +/- ##; ===============================================; + Coverage 80.453% 80.644% +0.191% ; - Complexity 17837 18463 +626 ; ===============================================; Files 1092 1092 ; Lines 64231 65676 +1445 ; Branches 10348 10735 +387 ; ===============================================; + Hits 51676 52964 +1288 ; - Misses 8504 8601 +97 ; - Partials 4051 4111 +60; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4894?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tools/spark/validation/CompareDuplicatesSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4894/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay92YWxpZGF0aW9uL0NvbXBhcmVEdXBsaWNhdGVzU3BhcmsuamF2YQ==) | `89.63% <94.231%> (+4.683%)` | `40 <26> (+16)` | :arrow_up: |; | [...e/hellbender/tools/funcotator/FuncotatorUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4894/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3JVdGlscy5qYXZh) | `80.176% <0%> (-0.154%)` | `328% <0%> (+160%)` | |; | [...roadinstitute/hellbender/utils/read/ReadUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4894/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL1JlYWRVdGlscy5qYXZh) | `80% <0%> (ø)` | `200% <0%> (-2%)` | :arrow_down: |; | [...dataSources/gencode/GencodeFuncotationBuilder.java](https://codecov.io/gh/broadinstitute/gatk/pull/4894/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397746010:949,validat,validation,949,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397746010,1,['validat'],['validation']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4982?src=pr&el=h1) Report; > Merging [#4982](https://codecov.io/gh/broadinstitute/gatk/pull/4982?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/ddf042a04ad02f2e3207ff71bdace1bd7cec9a49?src=pr&el=desc) will **increase** coverage by `0.003%`.; > The diff coverage is `88.889%`. ```diff; @@ Coverage Diff @@; ## master #4982 +/- ##; ===============================================; + Coverage 80.784% 80.787% +0.003% ; - Complexity 17960 17966 +6 ; ===============================================; Files 1095 1095 ; Lines 64592 64619 +27 ; Branches 10392 10400 +8 ; ===============================================; + Hits 52180 52204 +24 ; - Misses 8388 8389 +1 ; - Partials 4024 4026 +2; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4982?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tmutpileup/ValidateBasicSomaticShortMutations.java](https://codecov.io/gh/broadinstitute/gatk/pull/4982/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9WYWxpZGF0ZUJhc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zLmphdmE=) | `86.905% <88.889%> (+0.94%)` | `13 <0> (+6)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4982#issuecomment-402356444:938,Validat,ValidateBasicSomaticShortMutations,938,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4982#issuecomment-402356444,1,['Validat'],['ValidateBasicSomaticShortMutations']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5007?src=pr&el=h1) Report; > Merging [#5007](https://codecov.io/gh/broadinstitute/gatk/pull/5007?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/1977c537b209d25fea504d2f601af7a9731debcf?src=pr&el=desc) will **increase** coverage by `0.015%`.; > The diff coverage is `80%`. ```diff; @@ Coverage Diff @@; ## master #5007 +/- ##; ===============================================; + Coverage 60.162% 60.177% +0.015% ; - Complexity 12772 12785 +13 ; ===============================================; Files 1095 1096 +1 ; Lines 64616 64666 +50 ; Branches 10394 10397 +3 ; ===============================================; + Hits 38874 38914 +40 ; - Misses 21504 21510 +6 ; - Partials 4238 4242 +4; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5007?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...e/hellbender/engine/AbstractConcordanceWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/5007/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvQWJzdHJhY3RDb25jb3JkYW5jZVdhbGtlci5qYXZh) | `84.706% <50%> (-0.836%)` | `13 <0> (ø)` | |; | [...s/walkers/validation/MergeMutect2CallsWithMC3.java](https://codecov.io/gh/broadinstitute/gatk/pull/5007/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vTWVyZ2VNdXRlY3QyQ2FsbHNXaXRoTUMzLmphdmE=) | `81.25% <81.25%> (ø)` | `13 <13> (?)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5007#issuecomment-404375834:1232,validat,validation,1232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5007#issuecomment-404375834,1,['validat'],['validation']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5322?src=pr&el=h1) Report; > Merging [#5322](https://codecov.io/gh/broadinstitute/gatk/pull/5322?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/63d864067e74463eb72f58fe60a46603224cf8d2?src=pr&el=desc) will **decrease** coverage by `0.001%`.; > The diff coverage is `25%`. ```diff; @@ Coverage Diff @@; ## master #5322 +/- ##; ===============================================; - Coverage 86.793% 86.792% -0.001% ; Complexity 30107 30107 ; ===============================================; Files 1842 1842 ; Lines 139393 139395 +2 ; Branches 15369 15370 +1 ; ===============================================; Hits 120984 120984 ; - Misses 12823 12824 +1 ; - Partials 5586 5587 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5322?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tmutpileup/ValidateBasicSomaticShortMutations.java](https://codecov.io/gh/broadinstitute/gatk/pull/5322/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9WYWxpZGF0ZUJhc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zLmphdmE=) | `80.172% <25%> (-0.529%)` | `19 <0> (ø)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5322/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `80.488% <0%> (-0.61%)` | `42% <0%> (ø)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5322#issuecomment-431054487:928,Validat,ValidateBasicSomaticShortMutations,928,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5322#issuecomment-431054487,1,['Validat'],['ValidateBasicSomaticShortMutations']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5718?src=pr&el=h1) Report; > Merging [#5718](https://codecov.io/gh/broadinstitute/gatk/pull/5718?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/226f6d70a9c09318d45506c726f6744e2379d60c?src=pr&el=desc) will **decrease** coverage by `0.002%`.; > The diff coverage is `86.667%`. ```diff; @@ Coverage Diff @@; ## master #5718 +/- ##; ===============================================; - Coverage 87.069% 87.067% -0.003% ; - Complexity 31875 31880 +5 ; ===============================================; Files 1940 1940 ; Lines 146738 146756 +18 ; Branches 16226 16229 +3 ; ===============================================; + Hits 127764 127776 +12 ; - Misses 13061 13065 +4 ; - Partials 5913 5915 +2; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5718?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...walkers/validation/ConcordanceIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5718/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQ29uY29yZGFuY2VJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `98.601% <100%> (-1.399%)` | `8 <6> (+2)` | |; | [...llbender/tools/walkers/validation/Concordance.java](https://codecov.io/gh/broadinstitute/gatk/pull/5718/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQ29uY29yZGFuY2UuamF2YQ==) | `87.179% <50%> (-1.417%)` | `41 <0> (+2)` | |; | [...oadinstitute/hellbender/utils/pairhmm/PairHMM.java](https://codecov.io/gh/broadinstitute/gatk/pull/5718/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9wYWlyaG1tL1BhaXJITU0uamF2YQ==) | `74.82% <0%> (-3.597%)` | `24% <0%> (ø)` | |; | [...hellbender/utils/pairhmm/VectorLoglessPairHMM.java](https://codecov.io/gh/broadinstitute/gatk/pull/5718/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbn,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5718#issuecomment-467164762:941,validat,validation,941,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5718#issuecomment-467164762,1,['validat'],['validation']
Security,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5984?src=pr&el=h1) Report; > Merging [#5984](https://codecov.io/gh/broadinstitute/gatk/pull/5984?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/730164d3c5ddaead665ce0750e3e4558cef074e7?src=pr&el=desc) will **increase** coverage by `6.783%`.; > The diff coverage is `80%`. ```diff; @@ Coverage Diff @@; ## master #5984 +/- ##; ===============================================; + Coverage 80.142% 86.924% +6.783% ; - Complexity 31040 32741 +1701 ; ===============================================; Files 2014 2014 ; Lines 151333 151367 +34 ; Branches 16612 16617 +5 ; ===============================================; + Hits 121281 131575 +10294 ; + Misses 24197 13728 -10469 ; - Partials 5855 6064 +209; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5984?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tools/walkers/ValidateVariantsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5984/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL1ZhbGlkYXRlVmFyaWFudHNJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `98.701% <100%> (+97.252%)` | `44 <4> (+42)` | :arrow_up: |; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/5984/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYWxpZGF0ZVZhcmlhbnRzLmphdmE=) | `80.62% <66.667%> (-2.263%)` | `42 <7> (+6)` | |; | [...nder/utils/runtime/StreamingProcessController.java](https://codecov.io/gh/broadinstitute/gatk/pull/5984/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9ydW50aW1lL1N0cmVhbWluZ1Byb2Nlc3NDb250cm9sbGVyLmphdmE=) | `67.299% <0%> (-0.474%)` | `33% <0%> (ø)` | |; | [.../walkers/vqsr/CNNScoreVariantsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5984/diff?src=,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5984#issuecomment-498803022:955,Validat,ValidateVariantsIntegrationTest,955,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5984#issuecomment-498803022,1,['Validat'],['ValidateVariantsIntegrationTest']
Security,## Bug Report. ### Affected tool(s) or class(es). HC java.lang.IllegalStateException: Padded span must contain active span. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description. ```; Runtime.totalMemory()=2494038016; java.lang.IllegalStateException: Padded span must contain active span.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:104); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:80); at org.broadinstitute.hellbender.utils.activityprofile.ActivityProfile.popNextReadyAssemblyRegion(ActivityProfile.java:332); at org.broadinstitute.hellbender.utils.activityprofile.ActivityProfile.popReadyAssemblyRegions(ActivityProfile.java:277); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:159); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:112); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:35); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:192); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7289:420,validat,validate,420,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7289,1,['validat'],['validate']
Security,"## Bug Report. ### Affected tool(s) or class(es). ValidateVariants: `--fail-gvcf-on-overlap` / `-no-overlaps`. ### Affected version(s); - [x] Latest public release version: 4.2.6.1; - [ ] ~Latest master branch as of~ [did not test, but affected file hasn't changed since August 2021]. ### Description . If there are overlapping reference blocks when running ValidateVariants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8103:50,Validat,ValidateVariants,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103,2,['Validat'],['ValidateVariants']
Security,"## Bug Report. ### Affected tool(s) or class(es); - gatk/scripts/cnv_wdl/germline/cnv_germline_case_workflow.wdl; - gatk/scripts/cnv_wdl/germline/cnv_getmline_cohort_workflow.wdl. ### Affected version(s); - **WDL** file from GATK latest release (4.2.5.0); - **GATK Docker** - latest (4.2.5.0). ### Description ; Accoridng to [GATK Germline CNV WDL instructions](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/README.md), I ran cnv_getmline_cohort_workflow.wdl and got data to run cnv_germline_case_workflow.wdl. (contig_ploidy_model_tar file and 40 gcnv_model_tars files). Then I tried to run cnv_germline_case_workflow.wdl with one sample and got an error: ; ```; java.lang.IllegalArgumentException: The number of input call shards must match the number of input model shards.; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); 	at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.validateArgum; ```. PostprocessGermlineCNVCalls only completes correctly if I use only one of the gcnv_model_tars files, but it only produces results for the iterval_list file that is included in the used gcnv_model_tars. #### Case mode files; [case.log](https://github.com/broadinstitute/gatk/files/8186658/case.log); [case-inputs.json.txt](https://github.com/broadinstitute/gatk/files/8186662/case-inputs.json.txt). #### Cohort mode files; [cohort.log](https://github.com/broadinstitute/gatk/files/8186665/cohort.log); [cohort-inputs.json.txt](https://github.com/broadinstitute/gatk/files/8186667/cohort-inputs.json.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7706:854,validat,validateArg,854,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7706,2,['validat'],"['validateArg', 'validateArgum']"
Security,"## Bug Report. ### Affected tool(s) or class(es); AnalyzeCovariates . ### Affected version(s); - [x] Latest public release version [v4.1.4.0] [hash:cec850f20311f0686fcf88510bc44e529590d78bec7076a603132115943c09e6]. ### Description ; AnalyzeCovariates fails with ; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.4.0-local.jar AnalyzeCovariates -bqsr /researchers/sebastian.hollizeck/lowcWGS/IN-PM01004/Bam/IN-PM01004_rmd.recal.bam.recalTable -plots /researchers/sebastian.hollizeck/lowcWGS/IN-PM01004/Bam/AnalyzeCovariates.pdf; 23:15:29.581 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 19, 2020 11:15:30 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 23:15:30.435 INFO AnalyzeCovariates - ------------------------------------------------------------; 23:15:30.437 INFO AnalyzeCovariates - The Genome Analysis Toolkit (GATK) v4.1.4.0; 23:15:30.437 INFO AnalyzeCovariates - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:15:30.438 INFO AnalyzeCovariates - Executing as shollizeck@papr-res-compute204.unix.petermac.org.au on Linux v3.10.0-1062.4.3.el7.x86_64 amd64; 23:15:30.438 INFO AnalyzeCovariates - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_212-8u212-b03-0ubuntu1.16.04.1-b03; 23:15:30.438 INFO AnalyzeCovariates - Start Date/Time: January 19, 2020 11:15:29 PM UTC; 23:15:30.439 INFO AnalyzeCovariates - ------------------------------------------------------------; 23:15:30.439 INFO AnalyzeCovariates - ------------------------------------------------------------; 23:15:30.439 INFO AnalyzeCovariates - HTSJDK Version: 2.20.3; 23:15:30.439 INFO AnalyzeCovariates - Picard Ve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6393:143,hash,hash,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6393,1,['hash'],['hash']
Security,"## Bug Report. ### Affected tool(s) or class(es); ApplyBQSRSpark. ### Affected version(s); gatk-4.1.9.0. ### Description ; After roughly 1h of running ApplyBQSRSpark on my WGS BAM, it throws the error `java.io.IOException: Bad file descriptor`.; I then used `samtools quickcheck` to validate the integrity of my BAM file and everything seems fine. The BAM file is coordinate sorted and the duplicates are marked. The size of the BAM file is 142GB. #### Steps to reproduce; This is the command I used:. ```bash; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar gatk-package-4.1.9.0-local.jar ApplyBQSRSpark -R ref-genome.fa -I buffy_coat.sorted.markdup.bam --spark-master local[45] --tmp-dirtmp --bqsr-recal-file buffy_coat_recal_bqsr.table -O buffy_coat.recal.bam; ```. Any help is much appreciated!. Cheers",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7139:283,validat,validate,283,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7139,2,"['integrity', 'validat']","['integrity', 'validate']"
Security,"## Bug Report. ### Affected tool(s) or class(es); BaseRecalibratorSpark; BQSRPipelineSpark. ### Affected version(s); 4.1.0.0. ### Description ; We are running into a problem using BaseRecalibratorSpark. The tool fails soon after starting. The same error appears with the same bam file on different machines. Additionally, vanilla BaseRecalibrator works just fine on these bams (so I don't think the issue is with the bam). They are all suffering from the same/similar stacktrace. We've had BaseRecalibratorSpark work fine on other bam files. Additionally, changing the number of threads still results in the same stacktrace. I've also tried running the BQSRPipelineSpark to see if that would suffer the same issue and it fails in the same manner. Additionally, I've run ValidateSamFile. There are some reads missing their mates, but this hasn't presented an issue in other tools (including vanilla BaseRecalibrator). Searching thru the forum, I found an old issue with a similar stacktrace, but that issue appears to occur in GATK 2.4: https://gatkforums.broadinstitute.org/gatk/discussion/3265/bqsrgatherer-exception. In the below stacktrace, I've bolded the error message that seems to occur in each of these samples. `Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(D",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5854:770,Validat,ValidateSamFile,770,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854,1,['Validat'],['ValidateSamFile']
Security,"## Bug Report. ### Affected tool(s) or class(es); CombineGVCFs. ### Affected version(s); - [X] Latest public release version [4.2.5.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; The auto-generated wdl for CombineGVCFs on dockstore won't work because it doesn't have any inputs for the indices of the input vcfs. This means GATK cannot access the vcf indices because they never get localized, so the workflow fails. . #### Steps to reproduce; Take any vcfs and run them through the workflow to get an error about missing indices. . #### Expected behavior; Including the indices in the task inputs will allow them to get localized along with the vcfs so GATK can operate normally. . #### Actual behavior; You get an error saying it requires index files to proceed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7681:361,access,access,361,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7681,1,['access'],['access']
Security,"## Bug Report. ### Affected tool(s) or class(es); Funcotator. ### Affected version(s); - [ ] v.4.1.3.0 using us.gcr.io/broad-gatk/gatk:4.1.3.0; - [ ] 11/22 failed in Terra. ### Description ; There is a bug in the logic with how Funcotator is handling this variant. It is a variant after ; chr14:24655355 ; **Stacktrace**; <img width=""1248"" alt=""Screen Shot 2019-11-27 at 4 37 31 PM"" src=""https://user-images.githubusercontent.com/13475639/69761316-026f2a00-1135-11ea-9b50-491f5b22971c.png"">. Jonn has the input files, log file, and WDL. #### Steps to reproduce; To reproduce this issue, all the inputs and full pipeline are listed in this[ Zendesk ticket 3847](https://broadinstitute.zendesk.com/agent/tickets/3847). Contact Tiffany for access. #### Expected behavior; The tool should handle this situation more gracefully?. #### Actual behavior; It fails with a java.lang.StringIndexOutOfBoundsException: String index out of range: 776. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6289:737,access,access,737,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6289,1,['access'],['access']
Security,"## Bug Report. ### Affected tool(s) or class(es); GATKProtectedVariantContextUtils.chooseAlleleForRead(...). ### Affected version(s); Latest master branch as of [July 27, 2018]. ### Description ; This method has no facility for recognizing equivalent alleles in a read. For example:. Let's say we have a pileup with a single base insertion (of 'T') after an 'A' in a poly-T.; In IGV, the reads will all show the insertion right after the A.; However, if the caller said ref alt was AT --> ATT, then `chooseAlleleForRead` will miss the supporting alts in the validation normal, since it needed A --> AT. However, AT-->ATT and A --> AT are equivalent. See ValidateBasicSomaticShortMutationsIntegrationTest line ~151 for the corresponding TODO and a test case. #### Steps to reproduce; Leverage the existing test (`ValidateBasicSomaticShortMutationsIntegrationTest`) and change the gtNumAltReadsInValidationNormal to 1, instead of zero. #### Expected behavior; The test should pass. #### Actual behavior; The test will fail expecting zero.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5061:558,validat,validation,558,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5061,3,"['Validat', 'validat']","['ValidateBasicSomaticShortMutationsIntegrationTest', 'validation']"
Security,"## Bug Report. ### Affected tool(s) or class(es); GenomicsDBImport / GenotypeGVCFs. ### Affected version(s); 4.3.0.0. ### Description ; When creating a GenomicsDB datastore, the created folder has permissions set to 700 (recursivelly).; As such, when trying to jointly calling genotypes using the GenotypeGVCFs, one encounters error:; ERROR: Couldn't create GenomicsDBFeatureReader. #### Steps to reproduce; - Create a datastore using GenomicsDBImport, e.g. ; gatk ... --genomicsdb-workspace-path IWANNAKILLYOU. - Recursively change access permission to the thus created genomicsdb; chmod 700 -R ./IWANNAKILLYOU. - Run the GenotypeGVCFs; gatk ... --variant gendb://IWANNAKILLYOU. #### Expected behavior; GenotypeGVCFs should initialize the engine normally and start processing the intervals as expected. #### Actual behavior; GenotypeGVCFs intializes the engine and throws out and error; ERROR: Couldn't create GenomicsDBFeatureReader. #### Proposed solution; Mention anywhere in the docs the genomicsdb datastore should be made readable to other users, i.e., change permissions to at least 744 if not do a 766.; Or just make sure the ./IWANNAKILLYOU has proper permissions from the get go. Much obliged",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8233:533,access,access,533,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233,1,['access'],['access']
Security,"## Bug Report. ### Affected tool(s) or class(es); GenomicsDBImport. ### Affected version(s); - [ ] Latest public release version [4.2.5.0]. ### Description ; My gVCF files are block compressed and indexed, but the files have the file extension "".gvcf.gz"" rather than "".vcf.gz"". When I run `GenomicsDBImport` with `--bypass-feature-reader`, the "".gvcf.gz"" file cannot be recognized as a block compressed vcf file. The code of `GenomicsDBImport` validates if input is block compressed by checking if the file extension is "".vcf.gz"". ```; private static void assertVariantFileIsCompressedAndIndexed(final Path path) {; if (!path.toString().toLowerCase().endsWith(FileExtensions.COMPRESSED_VCF)) {; throw new UserException(""Input variant files must be block compressed vcfs when using "" +; BYPASS_FEATURE_READER + "", but "" + path.toString() + "" does not appear to be"");; }; Path indexPath = path.resolveSibling(path.getFileName() + FileExtensions.COMPRESSED_VCF_INDEX);; IOUtils.assertFileIsReadable(indexPath);; }; ```. I understand that this is an issue on my side because I did not name my gVCF files with the standard extension "".vcf.gz"". Is it possible to make this check less stringent in a future release? Maybe make any "".gz""/"".bgz"" file acceptable, or check the "".tbi"" index file to identify block compression (existing index typically means the file is block compressed and indexed). . Thank you. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7691:444,validat,validates,444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7691,1,['validat'],['validates']
Security,"## Bug Report. ### Affected tool(s) or class(es); GermlineCNVCaller . ### Affected version(s); v4.0.4.0 and v4.0.11.0 tested with same result. ### Description ; ![screenshot 2018-11-02 14 50 17](https://user-images.githubusercontent.com/11543866/47934764-a8a71c80-deae-11e8-9f8f-c8a8b563d77a.png). ```; java.lang.IllegalArgumentException: Intervals for read-count file /home/shlee/gcnv/cvg/HG00096_chr20XY.hdf5 do not contain all specified intervals.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.writeIntervalSubsetReadCountFiles(GermlineCNVCaller.java:390); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:285); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Command runs fine sans `-XL` parameter. The contents of `-XL` are simply:. ![screenshot 2018-11-02 14 51 58](https://user-images.githubusercontent.com/11543866/47934827-e0ae5f80-deae-11e8-891e-473ec8420433.png). #### Expected behavior; It would be great to be able to iterate GermlineCNVCaller on coverage data while excluding various regions, e.g. centromeric regions, to test the impact of such regions on the denoising. Currently, the hypothetical workaround would be to collect coverage while excluding regions or to manually remove such intervals from the coverage data. Having to collect coverage once over all of the data is preferable to collecting coverage again and again over slightly variable regions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5388:497,validat,validateArg,497,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5388,1,['validat'],['validateArg']
Security,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller/ActiveProfile. ### Affected version(s); - [ ] Latest public release version [4.3.0.0]. ### Description ; In function findEndOfRegion (line 355 in src/main/java/org/broadinstitute/hellbender/utils/activityprofile/ActivityProfile.java), it tries to determine the end of an active region. . The problem happens here, (at line 356); ![activeregion](https://user-images.githubusercontent.com/34263164/205565469-84900a73-1180-48e1-ba9f-f96c23d91e11.PNG); There could be an edge case where stateList.size() = maxRegionSize + getMaxProbPropagationDistance(), the function processes forward for further calculation. Hence the end of active region is determined immediately. However, the end of region is determined earlier than we expected. If by coincidence location at maxRegionSize is determined as minimum, region end is determined here. IBut wait a sec... If location at maxRegionSize+50 (which is NOT involved in current code in the ""if"" judgement at line 356) has an active score larger than 0, it rises the probability value at location maxRegionSize. . Now you should understand what I said. The state of location at maxRegionSize+50 is not updated when you accessed it. Let's assume ; maxRegionSize = 300 and point at location 350 has active value > 0. We trasverse the region to find the minimum point where we could cut the region and we found location at 300 in current logic. However, location 350 can acturally increase the probability at point 300 but this is not considered (or not updated) when making region end decision. #### Expected behavior; Simply use less or equal to at line 356 in the above image would fix this problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8118:1224,access,accessed,1224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8118,1,['access'],['accessed']
Security,"## Bug Report. ### Affected tool(s) or class(es); JointGermlineCNVSegmentation. ### Affected version(s); - [x] Latest public release version [v4.3.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; I get the following exception when running JointGermlineCNVSegmentation on an exome trio dataset:. ```; [January 19, 2023 at 6:59:29 AM CET] org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation done. Elapsed time: 0.82 minutes.; Runtime.totalMemory()=300941312; java.lang.IllegalStateException: Encountered genotype with ploidy 0 but 1 alleles.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.correctGenotypePloidy(JointGermlineCNVSegmentation.java:701); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.prepareGenotype(JointGermlineCNVSegmentation.java:682); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.lambda$createDepthOnlyFromGCNVWithOriginalGenotypes$4(JointGermlineCNVSegmentation.java:666); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at java.base/java.util.ArrayList$Itr.forEachRemaining(ArrayList.java:1033); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.createDepthOnlyFromGCNVWithOriginalGenotypes(JointGermlineCNVSegmentation.java:667); at org.broadinstitute.hellbender",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8164:635,validat,validate,635,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8164,1,['validat'],['validate']
Security,## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of 7/18/18. ### Description ; When running Mutect yesterday on Mitochondrial data I got the following error:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:chrM start:-4 end:65. 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:728); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:86); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:48); 	at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); 	at org.broadinstitute.hellbender.transformers.ReadTransformer$$Lambda$107/1786040872.apply(Unknown Source); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.fillDownsampledReadsCache(ReadsDownsamplingIterator.java:69); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.advanceToNextRead(ReadsDownsamplingIterator.java:55); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.<init>(ReadsDownsamplingIterator.java:34); 	at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:149); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:109); 	at org.broadinstitute.hellbend,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5036:406,validat,validateArg,406,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036,2,['validat'],"['validateArg', 'validatePositions']"
Security,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); GATK version 4.2.5.0, run from the `us.gcr.io/broad-gatk/gatk:4.2.5.0` docker image. ### Description ; Rarely (~0.1%) within exomes that were sequenced at Broad (by GP), we encounter the error message whose stack trace is shown below. This occurs during batch processing, but it is specific to the .CRAM files: running Mutect2 on the same file produces the same error, and running Mutect2 on other files with the same arguments works fine. The files that trigger this error have contents that match the Broad GP-produced .md5 checksum, and they also pass `samtools quickcheck`. #### Steps to reproduce; (The variables are filled in as one might reasonably expect.); ```sh; /gatk/gatk --java-options ""-Xmx${RAM}G"" \; Mutect2 \; --input ${cram} \; --reference ${REFERENCE_FASTA} \; --panel-of-normals ${PON} \; --germline-resource ${GNOMAD} \; --intervals ${INTERVALS} \; --output ${unfiltered}; ```. #### Expected behavior; In all other cases, somatic variant calling proceeds successfully. #### Actual behavior; ```; 00:17:31.944 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 00:17:32.225 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.226 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.2.5.0; 00:17:32.226 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:17:32.227 INFO Mutect2 - Executing as root@8d398eecd56e on Linux v5.10.90+ amd64; 00:17:32.227 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 00:17:32.228 INFO Mutect2 - Start Date/Time: April 5, 2022 12:17:31 AM GMT; 00:17:32.228 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.228 INFO Mutect2 - ------------------------------------------------------------; 00:17:32.229 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7755:610,checksum,checksum,610,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7755,1,['checksum'],['checksum']
Security,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); gatk:4.1.4.1; ### Description ; Hello, I have an issue when i run Mutect2. Indeed it's seems to be a RAM issue (you will find 2 different log behind). To be aware of the context, I am using Mutect2 in production and in the new version which I use, approximately 1% of my Mutect2 failed. I first try to understand the log and it seems to me that the issue is due to memory management. Then I realized several test, by increasing both Java machine memory and docker job memory (10G) wich seems enough for mutect2 and my job was still failling. I also try to change CPU parameters but nothing change.; We did more test and try to run the same command with 2 others version of gatk (4.1.9.0 and 4.1.0.0). The job failed in 4.1.9.0 with the same log than 4.1.4.0 but the version 4.1.0.0 ran successfully. #### Steps to reproduce; you will find the command below. I'am not aware of the confidentiality about my input. If i can i will send it to you if needed.; `java -Xmx4000m -Xms4000m -XX:ParallelGCThreads=1 -XX:+AggressiveHeap -jar /usr/share/java/gatk-package-4.1.4.1-local.jar Mutect2 --smith-waterman FASTEST_AVAILABLE -I WES-T_S7_chr_1_bqsr.bam -I WGS-C_S12_chr_1_bqsr.bam -normal WGS-C -L 1 -O OUTPUT -R GRCh38.92.fa`; #### Expected behavior; _Tell us what should happen_. ### Description. > 10:29:22.302 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/share/java/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2021 10:29:22 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:29:22.407 INFO Mutect2 - ------------------------------------------------------------; 10:29:22.408 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.4.1; 10:29:22.408 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7032:964,confidential,confidentiality,964,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7032,1,['confidential'],['confidentiality']
Security,## Bug Report. ### Affected tool(s) or class(es); ReadsPipelineSpark. ### Affected version(s); - [x] Latest public release version 4.1.0.0; - [ ] Latest master branch as of [date of test?]. ### Description . ```; java.lang.IllegalArgumentException: Interval NC_007605:1-171823 not within the bounds of a contig in the provided dictionary; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:87); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:66); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.lambda$runTool$0(ReadsPipelineSpark.java:221); 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:222); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinsti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5644:385,validat,validateArg,385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5644,1,['validat'],['validateArg']
Security,"## Bug Report. ### Affected tool(s) or class(es); ReblockGVCF. ### Affected version(s); - [x] Latest public release version [4.2.6.1]; - [x] Latest master branch (probably). ### Description ; First position on a contig can be missing if that position is low quality in the input GVCF. #### Steps to reproduce; Run Reblock GVCF with the following parameters: --floor-blocks true --gvcf-gq-bands 20 --gvcf-gq-bands 30 --gvcf-gq-bands 40 --do-qual-score-approximation true --variant $inputVC -R $hg38. where inputVC contains; chr13	18173860	.	A	C,<NON_REF>	0	.	AS_RAW_BaseQRankSum=|-4.9,1|NaN;AS_RAW_MQ=51256.00|4709.00|0.00;AS_RAW_MQRankSum=|0.5,1|NaN;AS_RAW_ReadPosRankSum=|1.2,1|NaN;AS_SB_TABLE=23,2|1,1|0,0;BaseQRankSum=-4.896;DP=28;ExcessHet=0.0000;MLEAC=0,0;MLEAF=0.00,0.00;MQRankSum=0.564;RAW_MQandDP=59565,28;ReadPosRankSum=1.252	GT:AD:DP:GQ:PL:SB	0/0:25,2,0:27:61:0,61,946,75,951,965:23,2,1,1; appears to be dropped in output. Full input GVCF at gs://broad-dsde-methods-gauthier/reblocking-bug/. #### Expected behavior; QUAL 0 VC should be replaced with a GQ0 reference block. #### Actual behavior; Output GVCF is missing position chr13:18173860 and fails ValidateVCF task",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7884:1162,Validat,ValidateVCF,1162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7884,1,['Validat'],['ValidateVCF']
Security,"## Bug Report. ### Affected tool(s) or class(es); SAMSequenceDictionary function in IndexUtils.java: https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/IndexUtils.java. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; The SAMSequenceDictionary function always logs a warning when a file is passed in. Needs an if statement that validates whether or not the file is actually a sequence dictionary before logging the warning. #### Steps to reproduce; Run GATK's HaplotypeCaller with a --dbsnp option set, or just pass a sequence dictionary into the SAMSequenceDictionary function directly. #### Expected behavior; Should use the --dbsnp file that I pass in if valid, rather than log a warning and creating a separate sequence dictionary. #### Actual behavior; Logs a warning and instead tries to create a new sequence dictionary based on the index file it finds",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5692:470,validat,validates,470,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5692,1,['validat'],['validates']
Security,"## Bug Report. ### Affected tool(s) or class(es); VariantAnnotator. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description . Throws an exception on a legal variant. java.lang.IllegalStateException: Allele in genotype G not in the variant context [G*, G, GT]; 	at htsjdk.variant.variantcontext.VariantContext$Validation.validateGenotypes(VariantContext.java:382); 	at htsjdk.variant.variantcontext.VariantContext$Validation.access$200(VariantContext.java:323); 	at htsjdk.variant.variantcontext.VariantContext$Validation$2.validate(VariantContext.java:331); 	at htsjdk.variant.variantcontext.VariantContext.lambda$validate$0(VariantContext.java:1384); 	at java.lang.Iterable.forEach(Iterable.java:75); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1384); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:489); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:647); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:638); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.trimAlleles(GATKVariantContextUtils.java:1329); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.trimAlleles(GATKVariantContextUtils.java:1285); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.getMinRepresentationBiallelics(VariantAnnotatorEngine.java:499); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateExpressions(VariantAnnotatorEngine.java:440); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:285); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotator.apply(VariantAnnotator.java:230); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6689:392,Validat,Validation,392,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6689,8,"['Validat', 'access', 'validat']","['Validation', 'access', 'validate', 'validateGenotypes']"
Security,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [x] Latest public release version 4.1.7.0. ### Description . The following error message is output:. A USER ERROR has occurred: Bad input: Is the input a file of segment variant contexts? Variant context does not represent a copy number segment: [VC null @ 6:4130448-4130544 Q. of type=SYMBOLIC alleles=[C*, <DEL>] attr={END=4130544, Num_Probes=1, Segment_Call=-, Segment_Mean=-30.018694} GT=[] filters=. The local info in the segment file is:; 5 176563624 180687750 618 -0.053122 0; 6 203183 4128317 205 0.046724 0; 6 4130448 4130544 1 -30.018694 -; 6 4130545 6168103 42 -0.085445 0; 6 6174562 17463556 490 0.022415 0; 6 17493361 25510885 347 0.080520 0. This is a bad error message. The minimum size for a segment to be processed is 150 bases and that variant is only 96 bases, so it's failing that validation. #### Expected behavior; Should process variant without producing error. Hat tip: @jonn-smith for figuring out the problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6575:921,validat,validation,921,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6575,1,['validat'],['validation']
Security,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [x] Latest public release version 4.2.2.0. ### Description . Running apt-get inside docker image fails. #### Steps to reproduce. (base) fleharty@wm3b9-dfa docker % docker run -it broadinstitute/gatk:4.2.2.0; Unable to find image 'broadinstitute/gatk:4.2.2.0' locally; 4.2.2.0: Pulling from broadinstitute/gatk; a7fe112a8303: Already exists ; Digest: sha256:32175c3c7c1fb9f5bd6650183c9c5cf26fb822dddb0cad0123d48c33124b6065; Status: Downloaded newer image for broadinstitute/gatk:4.2.2.0; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# apt-get update; Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB] ; Get:3 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB] ; Get:4 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [543 kB] ; Get:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7447:736,secur,security,736,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447,5,['secur'],['security']
Security,"## Bug Report. ### Affected tool(s); CombineGVCFs. ### Affected version(s); 4.0.2.0. ### Description ; CombineGVCFs fails with an IllegalArgumentException when trying to combine GVCFs. However, when trying to combine a different subset of the GVCFs, it fails with a ClassCastException. The GVCFs all validate with ValidateVariants and were produced with GATK4 HaplotypeCaller. #### Steps to reproduce; Details in https://github.com/broadinstitute/dsde-docs/issues/2990. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11503/combinegvcfs-java-lang-illegalargumentexception-unexpected-base-in-allele-bases-aacc/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4525:300,validat,validate,300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4525,2,"['Validat', 'validat']","['ValidateVariants', 'validate']"
Security,"## Bug Report. ### Tool(s) or class(es) involved; `ValidateBasicSomaticShortMutations`. ### Description; The first fix should simply count the number of supporting alt reads in the normal. For now, if the variant has more than 2 reads supporting the alt in the normal, we will flag as not validating. We may change the strategy later to account for possible tumor-in-normal contamination.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5059:51,Validat,ValidateBasicSomaticShortMutations,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5059,2,"['Validat', 'validat']","['ValidateBasicSomaticShortMutations', 'validating']"
Security,"## Bug Report; when I run the MarkDuplicatesSpark, it throws me an error: basically it shows the spark engine stopped when run this function. ; the part of the error log is here:; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/rnaseq_pipeline_app/Apps/GATK/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar) to method java.nio.Bits.unaligned(); WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 21/01/12 15:50:31 INFO SparkContext: Running Spark version 2.4.5; 21/01/12 15:50:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 21/01/12 15:50:31 INFO SparkContext: Submitted application: MarkDuplicatesSpark; 21/01/12 15:50:31 INFO SecurityManager: Changing view acls to: root; 21/01/12 15:50:31 INFO SecurityManager: Changing modify acls to: root; 21/01/12 15:50:31 INFO SecurityManager: Changing view acls groups to: ; 21/01/12 15:50:31 INFO SecurityManager: Changing modify acls groups to: ; 21/01/12 15:50:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); groups with view permissions: Set(); users with modify permissions: Set(root); groups with modify permissions: Set(); 21/01/12 15:50:31 INFO Utils: Successfully started service 'sparkDriver' on port 36657.; 21/01/12 15:50:31 INFO SparkEnv: Registering MapOutputTracker; 21/01/12 15:50:31 INFO SparkEnv: Registering BlockManagerMaster; 21/01/12 15:50:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 21/01/12 15:50:31 INFO B",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7035:211,access,access,211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7035,5,['access'],['access']
Security,"## Documentation request. ### Tool(s) or class(es) involved; Readme for M2 in https://github.com/broadinstitute/gatk/tree/master/scripts/mutect2_wdl. ### Description ; This is the text currently in the readme, it needs to be updated to feature Funcotator instead of Oncotator:. > Functional annotation (Oncotator); > ; > The M2 WDL can optionally run oncotator for functional annotation and produce a TCGA MAF from the M2 VCF. Oncotator is not a GATK4 tool and is provided in the M2 WDL as a convenience. There are several notes and caveats; > ; > Several parameters should be passed in to populate the TCGA MAF metadata fields. Default values are provided, though we recommend that you specify the values. These parameters are ignored if you do not run oncotator.; > ; > Several fields in a TCGA MAF cannot be generated by M2 and oncotator, such as all fields relating to validation alleles. These will need to be populated by a downstream process created by the user.; > ; > Oncotator does not enforce the TCGA MAF controlled vocabulary, since it is often too restrictive for general use. This is up to the user to specify correctly. Therefore, we cannot guarantee that a TCGA MAF generated here will pass the TCGA Validator. If you are unsure about the ramifications of this statement, then it probably does not concern you.; > ; > More information about Oncotator can be found at: http://archive.broadinstitute.org/cancer/cga/oncotator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5889:873,validat,validation,873,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5889,2,"['Validat', 'validat']","['Validator', 'validation']"
Security,"## Documentation system request. Currently, the tooldoc generation system does not separate arguments that relate to deployment decisions like compute platform (eg `--gcs-project-for-requester-pays`) from the ones that modify the analytical or processing behavior of the tools. This adds to the cognitive burden involved in sorting through all the options available for a given tool. We'd like to have a separate category for these arguments so that they would be isolated from the rest. . In addition, there are a bunch of convenience arguments in the common args section that have more to do with how we're running the tool than its analysis behavior, and could also be consolidated into this separate category (or their own category but that might be too granular). Examples below are from the popular tool [SelectVariants](https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_walkers_variantutils_SelectVariants.php):. #### Arguments that would be stratified as platform args. `--cloud-index-prefetch-buffer`; `--cloud-prefetch-buffer`; `--disable-bam-index-caching`; `--gcs-max-retries`; `--gcs-project-for-requester-pays`. #### Arguments that would be stratified as convenience args. `--arguments_file` ; `--help` ; `--version` ; `--create-output-bam-index` ; `--create-output-bam-md5`; `--create-output-variant-index`; `--create-output-variant-md5`; `--gatk-config-file`; `--QUIET`; `--seconds-between-progress-updates`; `--tmp-dir`; `--use-jdk-deflater`; `--use-jdk-inflater`; `--verbosity`; `--showHidden` -> I thought we had got rid of hidden args??. These could also be stratified as convenience but one could argue they affect tool behavior qualitatively:. `--disable-sequence-dictionary-validation`; `--lenient`; `--read-validation-stringency`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5234:1755,validat,validation,1755,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5234,2,['validat'],"['validation', 'validation-stringency']"
Security,"## Feature request / documentation request. ### Tool(s) or class(es) involved; Reading files from non-public GCS paths. ### Description; I did not have Application Default Credentials set up when I tried to read from a private bucket. This failed, as expected. Could we add a comment explaining that running `gcloud auth application-default login` is the necessary step to making this work? I didn't see anything on the forum about how to solve this. The solution was in the comments to #2394. ### Observed; GATK errored out with a stack trace:; ```; code: 401; message: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; reason: required; location: Authorization; retryable: false; com.google.cloud.storage.StorageException: Anonymous caller does not have storage.objects.get access to joel-cram/SAM24339124.cram.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:220); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:415); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:198); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:195); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); at com.google.cloud.RetryHelper.run(RetryHelper.java:74); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:51); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:195); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:673); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:429); at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:206); ```. ```; Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 401 Unauthorized; {; ""code"" : 401,; ""errors"" : [ {; ""domain"" : ""global"",; ""location"" : ""Authoriza",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5468:622,access,access,622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5468,3,"['Authoriz', 'access']","['Authorization', 'access']"
Security,"## Feature request. ### Tool involved - GATK cnv_somatic_pair_workflow version 4.2.0.0; Link to main wdl - https://dockstore.org/workflows/github.com/broadinstitute/gatk/cnv_somatic_pair_workflow:4.2.0.0?tab=files. ### Description; Requesting for addition of a new feature called the `IndexFeatureFile` to be added as initial step in the `Funcotate_Segment` task of the `cnv_somatic_pair_workflow` wdl . **Detailed description:** ; An error was encountered while running the CNVSomaticPairWorkflow: `“A USER ERROR has occurred: Input /cromwell_root/fc-a21facc8-da03-4987-bb5b-dfadbfda2747/a923baec-ddd9-429a-b046-1f03c5ebda64/CNVSomaticPairWorkflow/42ba0311-ba93-4fdd-9b5e-bd7348c0ad42/call-CallCopyRatioSegmentsTumor/AMP-18-003-TIS.called.seg must support random access to enable traversal by intervals. If it's a file, please index it using the bundled tool IndexFeatureFile”`. - The FuncotateSegments task is asking an index file for the seg file, which is unusual to create an index for seg; - The same command from the cnv wdl was tested on-prem (Broad server using ish) by transferring all required files on-prem, this was done to replicate the error and find a solution without wasting compute money or resources on Terra; - To fix the issue, we initially ran Indexfilefeature tool (on-prem) for creating index file for the seg file, using the following command. ; `gatk IndexFeatureFile -I seg file`; - And then ran the main command `./gatk --java-options -Xmx2000m FuncotateSegments --data-sources-path data_sources_directory --ref-version hg19 --output-file-format SEG -R fasta_file_path --segments seg_file_path -O output_file_path/{basename}.seg.funcotated.tsv -L interval_list_path --transcript-selection-mode CANONICAL`. After this was complete, annotated file was generated correctly. In conclusion - the error identified is that `Indexfilefeature` tool had to be run as the first step. Requesting this change to be incorporated into `cnv_somatic_funcotate_seg_workflow.wdl`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7294:764,access,access,764,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7294,1,['access'],['access']
Security,"## Feature request. ### Tool(s) or class(es) involved; All WDL tests (Mutect2, CNV, Mitochondria pipeline, etc). ### Description; I'd like to be able to include tasks in GATK WDLs that use NIO (tasks with `String input_file` rather than `File input_file`). When I tried this with local git lfs files I got an error saying that the file could not be found (even though the file was being downloaded correctly). I then tried putting the file in `gs://hellbender/test/resources/large`, but when the tool tried to run on travis I got a permission error (see below). It would be great if the WDL tests all ran in the cloud since that's the main way we expect to run these WDLs. It would also be great it the local tests could account for NIO tasks (especially as we want to make more tasks use NIO in the future). ```; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified. It is possible to skip checking for Compute Engine metadata by specifying the environment variable NO_GCE_CHECK=true.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:438); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:239); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:236); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); at com.google.cloud.RetryHelper.run(RetryHelper.java:76); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:235); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:687); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.asse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5855:886,secur,security,886,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5855,2,"['access', 'secur']","['access', 'security']"
Security,"## Feature request. ### Tool(s) or class(es) involved; GATK PrintReads. ### Description; - Currently, this tool appears to consider reads independently of their mate, therefore if one partner is filtered and the other is not the SAM flags for the remaining read will be incorrect (indeed resulting BAMs from this tool fail GATK ValidateSamReads with error MATE_NOT_FOUND). ; - Here is a flagstat of one of these BAMs produced from this tool (note that there are no singleton reads listed, but they actually present -- you can even see this in the read1 and read2 counts; these counts should be equal if there are no supplementary, secondary, and/or singleton reads):. ```; 179466279 + 0 in total (QC-passed reads + QC-failed reads); 0 + 0 secondary; 0 + 0 supplementary; 0 + 0 duplicates; 179466279 + 0 mapped (100.00% : N/A); 179466279 + 0 paired in sequencing; 89740338 + 0 read1; 89725941 + 0 read2; 179466279 + 0 properly paired (100.00% : N/A); 179466279 + 0 with itself and mate mapped; 0 + 0 singletons (0.00% : N/A); 0 + 0 with mate mapped to a different chr; 0 + 0 with mate mapped to a different chr (mapQ>=5); ```. - I have two suggestions:; - Add a `--remove-mates` option that would ensure that if one read in a pair does not pass the read filters, the read pair will be filtered.; - Alternatively, add an `--update-flags` option that would update the filtered-in mate's SAM flags to be technically correct (i.e. if the read's partner was filtered, remove the 0x1, 0x2, 0x8, 0x20, 0x40, and 0x80 flags if they were present)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6839:328,Validat,ValidateSamReads,328,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6839,1,['Validat'],['ValidateSamReads']
Security,"## Feature request. ### Tool(s) or class(es) involved; ValidateBasicSomaticShortMutations . ### Description; It turns out that this tool is doing a subset of the CGA tool, MutationValidator. Originally, the understanding (by both DSP and CGA) was the the GATK tool was doing a different algorithm, but this turned out to be incorrect. We should rename the GATK tool perhaps to SomaticShortMutationValidator and cite MutationValidator. Any relevant WDL should be updated to prevent unnecessary workflow failures. @davidbenjamin . (citation does not exist as per last offline meeting with CGA)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5871:55,Validat,ValidateBasicSomaticShortMutations,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5871,1,['Validat'],['ValidateBasicSomaticShortMutations']
Security,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_, _MafOutputRenderer_, _VcfOutputRenderer_. ### Description; For VCFs, we render each funcotation separately, then concatenate strings. This approach has the drawback of being less flexible in terms of ordering the fields (and setting up aliases), but that has not mattered yet. Regardless, it means that all string operations (e.g. excluding fields and sanitizing values) must be in the same method (in this case renderSanitizedFuncotationForVcf) and that method must work on a funcotation. For MAFs, we flatten out the funcotations and put the fields into a giant map. Then we do the changes to field names and values on that map. But by the time I want to exclude fields and sanitize, the map is already made, so we do not render individual funcotations. Therefore no need for a renderSantiziedFuncotationForMaf. We should investigate how easy it would be to generalize an output renderer to use the `map` convention like in `MafOutputRenderer` so we can bubble up that functionality. Since there are only 2 output types now, it might be best to do it before we get more of them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5240:421,sanitiz,sanitizing,421,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5240,2,['sanitiz'],"['sanitize', 'sanitizing']"
Security,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_. ### Description; Currently the data sources for the clinical pipeline work contain ExAC. This must be updated to use gnomAD. The change will require a new release of the data sources which must be connected to the data source downloader tool. Additionally, these new data sources must be validated in four ways:; - By visually inspecting the gnomAD source file for correctness.; - By verifying that the source file for gnomAD does not contain special characters.; - By validating that the source file for gnomAD is a valid VCF (assuming it is used VCF format).; - By running a large file and spot checking at least 10 variants for correctness. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5259:357,validat,validated,357,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5259,2,['validat'],"['validated', 'validating']"
Security,"## Feature request. ### Tool(s) or class(es) involved; src/main/java/org/broadinstitute/hellbender/tools/walkers/rnaseq/SplitNCigarReads.java. ### Description. Modification requested. When splitting a read based on the spliced RNA cigar (N cigar symbol), the new split read fragment that gets incorporated into the bam file has a new 'HC' identifier for the read name, and the original read name is lost. There are cases where we really need to access the original read name. Would it be possible to incorporate the original read name somewhere into the read alignment record, perhaps as a custom SAM tag or attribute, or as a prefix or suffix to the HC-identifier in the read name?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8703:445,access,access,445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8703,1,['access'],['access']
Security,"## Feature request. Make joint genotyping functionality available as a publicly accessible function to another class/program without passing through the GATK command line interface. ### Tool(s) or class(es) involved; VariantContext; GenotypeGVCFs; Underlying engine classes. ### Description; For various use cases where our pipelines produce in-memory VariantContext objects it would be faster and easier to pass these directly to a joint genotyping function and extract the results back into memory rather than writing to VCF, running the GenotypeGVCFs pipeline via the command line interface and then re-ingesting the resultant VCFs. From discussions during the GATK Working Group meetings it appears this request is similar in principle to existing functionality for the HaplotypeCaller that was implemented by ""extracting the engine"" from the HaplotypeCaller walkers so that it can be instantiated outside the command line utility. Ideally, this implementation should make it possible to instantiate any necessary engine classes pass VariantContext objects directly to the GenotypeGVCFs.apply or GenotypeGVCFs.regenotypeVC and receive the re-genotyped VariantContext objects back for further processing from Java code. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5910:80,access,accessible,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5910,1,['access'],['accessible']
Security,"## Feature request. Related to #6239. I'm interested in performing joint genotyping on a set of given alleles in order to avoid deflating rare variants (previously genotype given alleles, now force call filtered alleles). In particular, I'd like to regenotype on all of the alleles in the input gVCF output by CombineGVCFs, which I'll define in Proposal 1. To be more in line with HaplotypeCaller/Mutect, I've also provided Proposal 2. If this is something you'd be open to having in the GATK, I'd be happy to submit a PR. ### Tool(s) or class(es) involved. #### Proposal 1. If we move the `force-call-filtered-alleles` argument from `AssemblyBasedCallerArgumentCollection` to `GenotypeCalculationArgumentCollection`, this will expose it from `GenotypeGVCFs`. If this argument is true, we use the input alleles for regenotyping in `GenotypeGVCFsEngine` in `genotypingEngine. calculateGenotypes`. #### Proposal 2. If we move the `force-call-filtered-alleles` and `alleles` arguments from `AssemblyBasedCallerArgumentCollection` to `GenotypeCalculationArgumentCollection`, this will expose them from `GenotypeGVCFs`. If provided, the features can then be used for regenotyping in `GenotypeGVCFsEngine` in `genotypingEngine. calculateGenotypes`. ### Description. When performing joint genotyping, the user could tell GenotypeGVCFs to regenotype on a given set of alleles, similar to how they would for HaplotypeCaller. #### Proposal 1. ```; GenotypeGVCFs; --force-call-filtered-alleles true; --input combined.g.vcf; ```. #### Proposal 2. ```; GenotypeGVCFs; --force-call-filtered-alleles true; --alleles rare.combined.g.vcf; --input combined.g.vcf; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6550:728,expose,expose,728,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6550,2,['expose'],['expose']
Security,"### Affected tool(s) or class(es); Funcotator. ### Affected version(s); - [ ] Latest master branch as of June 7, 2018. ### Description ; Currently, the metadata for `GencodeFuncotation` will always be default, unknown for all fields. However, we know what each field is, so we could populate . This cannot be seen anywhere by a user, yet. However, future requirements will probably cause the metadata to be rendered into a VCF header or a MAF comment, so this should be populated. #### Expected behavior; The Gencode funcotations should not have unknown types and descriptions. This can be validated with an automated test. #### Actual behavior; The Gencode funcotations have unknown types and descriptions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4857:590,validat,validated,590,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4857,1,['validat'],['validated']
Security,"### Affected tool(s) or class(es); GenotypeGVCFs is reporting:; ```. [TileDB::ArrayIterator] Error: Cannot advance iterator; Buffer overflow.; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : VariantArrayCellIterator increment failed; TileDB error message : [TileDB::ArrayIterator] Error: Cannot advance iterator; Buffer overflow; ```. ### Affected version(s); ```; Using GATK jar /home/xuql/miniconda3/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/xuql/miniconda3/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar --version; The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Version: 2.27.1. ```. ### Description ; Hi, I developed AnchorWave to call long indels(could be a couple of Mb). We are trying to connect the AnchorWave variant calling result with GATK to generate VCF files. We generated whole genome alignments for 26 maize accession via AnchorWave. And we wrote out own code to generate GVCF files from the outputs of AnchorWave. Those GVCF files works well with GATK GenomicsDBImport. While, the `GenotypeGVCFs` function is reporting `Buffer overflow` errors and could generate the complete VCF files. . Here is the command we used:. ```; gatk --java-options ""-Xmx100g"" GenotypeGVCFs -R Zm-B73-REFERENCE-NAM-5.0.fa -stand-call-conf 0 -ploidy 1 -V gendb:///home/xuql/NAM_anchorwave_song/NAM_out_gatk9 -O gatk9.vcf.gz --cloud-prefetch-buffer 10000 --cloud-index-prefetch-buffer 10000 --genomicsdb-max-alternate-alleles 110 --max-alternate-alleles 100 --tmp-dir /home/xuql/NAM_anchorwave_song/temp9 --gcs-max-retries 1000; ```; ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7976:1136,access,accession,1136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7976,1,['access'],['accession']
Security,"### Summary; This user was able to access the GenomicsDB workspace but is having performance issues with SelectVariants. They tried the same command locally and it took less than a minute. Are there any changes with how the user is running SelectVariants to improve the performance?. ### GATK Info; GATK 4.1.9.0; . This request was created from a contribution made by Lucas Taniguti on February 01, 2021 22:41 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360076845511-How-do-I-SelectVariants-from-GenomicsDB-stored-in-GCS-#community\_comment\_360014183291](https://gatk.broadinstitute.org/hc/en-us/community/posts/360076845511-How-do-I-SelectVariants-from-GenomicsDB-stored-in-GCS-#community_comment_360014183291). \--. Thank you, it has started to work with gendb.gs://. But now I think it does not run. I have only one sample stored into the database and I'm selecting only chr20:1-1000000 and it is running for more than 30 minutes. Is it expected?. I'm using a VM from GCE, in the same region as the GCS bucket. Using GATK jar /home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar ; ; ```; Running: ; ;    java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx10g -Xms5g - ; ; jar /home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar SelectVariants -R Homo\_sapiens\_assembly38.fasta -V gendb.gs://mybucket/genomicsdb -L chr20:1-1000000 -O teste. ; ; vcf.gz ; ; 23:01:23.595 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl\_compres ; ; sion.so ; ; 23:01:23.914 INFO  SelectVariants - ------------------------------------------------------------ ; ; 23:01:23.915 INFO  SelectVariants - The Genome Analysis Toolkit (GATK) v4.1.9.0 ; ; 23:01:23.915 INFO  SelectVariants - For support and documentation go to [https://software.bro",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7070:35,access,access,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7070,1,['access'],['access']
Security,"############################################ | 100%; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Requirement 'build/gatkPythonPackageArchive.zip' looks like a filename, but the file does not exist; Processing ./build/gatkPythonPackageArchive.zip; Exception:; Traceback (most recent call last):; File ""/BioinfSoftware/Anaconda/envs/gatk/lib/python3.6/site-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/BioinfSoftware/Anaconda/envs/gatk/lib/python3.6/site-packages/pip/commands/install.py"", line 335, in run; wb.build(autobuilding=True); File ""/BioinfSoftware/Anaconda/envs/gatk/lib/python3.6/site-packages/pip/wheel.py"", line 749, in build; self.requirement_set.prepare_files(self.finder); File ""/BioinfSoftware/Anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/BioinfSoftware/Anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/BioinfSoftware/Anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 809, in unpack_url; unpack_file_url(link, location, download_dir, hashes=hashes); File ""/BioinfSoftware/Anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 715, in unpack_file_url; unpack_file(from_path, location, content_type, link); File ""/BioinfSoftware/Anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 599, in unpack_file; flatten=not filename.endswith('.whl'); File ""/BioinfSoftware/Anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 482, in unzip_file; zipfp = open(filename, 'rb'); FileNotFoundError: [Errno 2] Datei oder Verzeichnis nicht gefunden: '/BioinfSoftware/build/gatkPythonPackageArchive.zip'. CondaValueError: pip returned an error; ```. Thanks also to all the other ones for their good hints for using germlineCNV caller.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357188460:2652,hash,hashes,2652,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357188460,4,['hash'],['hashes']
Security,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:16415,validat,validation,16415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,10,['validat'],"['validate', 'validation']"
Security,"'Passing' workflow at: https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/AoU_DRC_WGS_12-6-21_beta_ingest/job_history/e51afc46-ef55-4c59-b4dd-6ab7d3de0ca8 . Note that this workflow fails because one of the actual validations fails, so the report that is generated indicates this",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7937#issuecomment-1182284459:216,validat,validations,216,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7937#issuecomment-1182284459,1,['validat'],['validations']
Security,"()` method below, and it implies that either the `perAlleleValues` for one of the alleles is itself null, or the `perAlleleValues` for one of the alleles contains a null `Integer`. In `computeSBAnnotation()` we have `perAlleleValues.values().removeIf(Objects::isNull)`, which seems to rule out the former option (perAlleleValues for a particular allele itself being null), and implies that instead one of the individual Integers in the `List<Integer>` perAlleleValues for a particular allele is null. Any ideas on how that could happen?. ```; public static String encode(List<Integer> alleleValues) {; return String.join("","", alleleValues.stream().map(i -> i.toString()).collect(Collectors.toList()));; }. protected static String makeRawAnnotationString(final List<Allele> vcAlleles, final Map<Allele, List<Integer>> perAlleleValues) {; final List<String> alleleStrings = vcAlleles.stream(); // does not replace a null value with zero list - only if the key is not in the map; .map(a -> perAlleleValues.getOrDefault(a, ZERO_LIST)); .map(StrandBiasUtils::encode); .collect(Collectors.toList());; return String.join(AnnotationUtils.ALLELE_SPECIFIC_RAW_DELIM, alleleStrings);. }. public static Map<String, Object> computeSBAnnotation(VariantContext vc, AlleleLikelihoods<GATKRead, Allele> likelihoods, String key) {; // calculate the annotation from the likelihoods; // likelihoods can come from HaplotypeCaller or Mutect2 call to VariantAnnotatorEngine; final Map<String, Object> annotations = new HashMap<>();; final ReducibleAnnotationData<List<Integer>> myData = new AlleleSpecificAnnotationData<>(vc.getAlleles(),null);; getStrandCountsFromLikelihoodMap(vc, likelihoods, myData, MIN_COUNT);; Map<Allele, List<Integer>> perAlleleValues = new LinkedHashMap<>(myData.getAttributeMap());; perAlleleValues.values().removeIf(Objects::isNull);; final String annotationString = makeRawAnnotationString(vc.getAlleles(), perAlleleValues);; annotations.put(key, annotationString);; return annotations;; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-697902360:1674,Hash,HashMap,1674,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-697902360,1,['Hash'],['HashMap']
Security,(+2)` | :arrow_up: |; | [...r/tools/walkers/mutect/Mutect2IntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5317/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QySW50ZWdyYXRpb25UZXN0LmphdmE=) | `94.369% <100%> (+0.588%)` | `71 <3> (+3)` | :arrow_up: |; | [...te/hellbender/tools/walkers/annotator/CountNs.java](https://codecov.io/gh/broadinstitute/gatk/pull/5317/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9Db3VudE5zLmphdmE=) | `75% <75%> (ø)` | `8 <8> (?)` | |; | [...r/tools/walkers/mutect/Mutect2FilteringEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/5317/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QyRmlsdGVyaW5nRW5naW5lLmphdmE=) | `85.232% <76%> (-1.089%)` | `63 <4> (+4)` | |; | [...nstitute/hellbender/utils/gcs/BucketUtilsTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5317/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHNUZXN0LmphdmE=) | `56.303% <0%> (-3.103%)` | `13% <0%> (+1%)` | |; | [...ls/genomicsdb/GenomicsDBImportIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5317/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWljc2RiL0dlbm9taWNzREJJbXBvcnRJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `86.682% <0%> (-2.388%)` | `77% <0%> (ø)` | |; | [...tmutpileup/ValidateBasicSomaticShortMutations.java](https://codecov.io/gh/broadinstitute/gatk/pull/5317/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9WYWxpZGF0ZUJhc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zLmphdmE=) | `80.172% <0%> (-0.529%)` | `19% <0%> (ø)` | |; | ... and [9 more](https://codecov.io/gh/broadinstitute/gatk/pull/5317/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5317#issuecomment-430675873:3714,Validat,ValidateBasicSomaticShortMutations,3714,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5317#issuecomment-430675873,1,['Validat'],['ValidateBasicSomaticShortMutations']
Security,(You can also permanently turn the validation off for your tool by overriding the `getSequenceDictionaryValidationArgumentCollection()` method),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6589#issuecomment-625367361:35,validat,validation,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6589#issuecomment-625367361,1,['validat'],['validation']
Security,"(assumes that #2457 is already complete); Currently, the docker image creation will grab all the local files, which is prone to error. The github hash/tag parameters are simply to determine how to tag the image in docker hub. In gatk-protected, we used to actually grab the source code from github in the Dockerfile. I'm not suggesting we go back to this model. What we might want to consider:; The build_docker.sh script could download the github hash/tag to a temp dir and then build the docker image from the temp dir. This solution would be more robust and would decrease errors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2700:146,hash,hash,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2700,2,['hash'],['hash']
Security,"(executor driver) (2/2); 23/11/16 12:09:10 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool ; 23/11/16 12:09:10 INFO DAGScheduler: ResultStage 2 (parquet at StudentAws.scala:36) finished in 10.361 s; 23/11/16 12:09:10 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job; 23/11/16 12:09:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished; 23/11/16 12:09:10 INFO DAGScheduler: Job 2 finished: parquet at StudentAws.scala:36, took 10.369237 s; 23/11/16 12:09:10 INFO FileFormatWriter: Start to commit write Job b17a4b92-9ee1-46cc-858a-08ed0b22fb8b.; 23/11/16 12:09:10 ERROR FileFormatWriter: Aborting job b17a4b92-9ee1-46cc-858a-08ed0b22fb8b.; java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z; 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method); 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793); 	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249); 	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454); 	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377); 	at org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8587:2203,access,access,2203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587,1,['access'],['access']
Security,"(https://github.com/samtools/htsjdk/issues/1115); I am getting an error using gatk's VariantRecalibrator:. `htsjdk.tribble.TribbleException: Line 104: there aren't enough columns for line entrainScore=0.7203;HW=4.306476E-6 (we expected 9 tokens, and saw 1 ), for input source: file:///share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/2ce78a09-433b-48eb-83d0-1fa1771d1181/call-SNPsVariantRecalibratorCreateModel/inputs/share/ClusterShare/biodata/contrib/evaben/hs37d5x/vcf/1000G_omni2.5.b37.vcf`. I am having trouble getting to the bottom of the issue. I cannot find the string 'entrainScore=0.7203;HW=4.306476E-6' in the vcf file, and Line 104 is part of the header (I think Line 104 refers to tribble source though). Gatks ValidateVariants does not have any issues with the vcf file, and looking visually with bcftools I cannot see an issue either. . Can anyone suggest further diagnosis steps? Should I take this to GATK issue tracker?. The VCF file is here: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/1000G_omni2.5.b37.vcf. Here is the gatk command line: ; ```; gatk --java-options ""-Xmx100g -Xms100g"" \; VariantRecalibrator \; -V /share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/2ce78a09-433b-48eb-83d0-1fa1771d1181/call-SNPsVariantRecalibratorCreateModel/inputs/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/2ce78a09-433b-48eb-83d0-1fa1771d1181/call-SitesOnlyGatherVcf/execution/NA12878.sites_only.vcf.gz \; -O NA12878.snps.recal \; --tranches-file NA12878.snps.tranches \; -trust-all-polymorphic \; -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 \; -an QD -an MQRankSum -an ReadPosRankSum -an FS -an MQ -an SOR -an DP \; -mode SNP \; -sample-every 10 \; --output-model NA12878.snps.model.report \; --max-gaussians 6 \; -resource hapmap,known=false,training=true,truth=true,prior=",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4761:748,Validat,ValidateVariants,748,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4761,1,['Validat'],['ValidateVariants']
Security,"(this is mainly relevant for Picard tools, which often have lots of log.info() calls). CommandLineProgram's VERBOSITY is set to INFO by default, which is reasonable when you're actually interacting with a tool, but quickly gets spammy when running unit tests. I propose injecting VERBOSITY=ERROR (the strictest setting) into CommandLineProgramTest to avoid this. This would fix https://github.com/broadinstitute/hellbender/issues/134",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/147:270,inject,injecting,270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/147,1,['inject'],['injecting']
Security,) | `88.889% <0%> (+0.654%)` | `10% <0%> (+4%)` | :arrow_up: |; | [...r/tools/walkers/validation/RemoveNearbyIndels.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vUmVtb3ZlTmVhcmJ5SW5kZWxzLmphdmE=) | `91.429% <0%> (+0.952%)` | `9% <0%> (+4%)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `80% <0%> (+1.29%)` | `39% <0%> (ø)` | :arrow_down: |; | [...adinstitute/hellbender/tools/IndexFeatureFile.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9JbmRleEZlYXR1cmVGaWxlLmphdmE=) | `91.667% <0%> (+1.344%)` | `17% <0%> (+5%)` | :arrow_up: |; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYWxpZGF0ZVZhcmlhbnRzLmphdmE=) | `85.465% <0%> (+2.608%)` | `58% <0%> (+24%)` | :arrow_up: |; | [...kers/variantutils/UpdateVCFSequenceDictionary.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9VcGRhdGVWQ0ZTZXF1ZW5jZURpY3Rpb25hcnkuamF2YQ==) | `89.873% <0%> (+2.917%)` | `23% <0%> (+9%)` | :arrow_up: |; | [...oadinstitute/hellbender/tools/GatherVcfsCloud.java](https://codecov.io/gh/broadinstitute/gatk/pull/4066/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9HYXRoZXJWY2ZzQ2xvdWQuamF2YQ==) | `77.656% <0%> (+6.845%)` | `54% <0%> (+14%)` | :arrow_up: |; | ... and [1 more](https://codecov.io/gh/broadinstitute/gatk/pull/4066,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4066#issuecomment-355695318:3100,Validat,ValidateVariants,3100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4066#issuecomment-355695318,1,['Validat'],['ValidateVariants']
Security,); 4.1.4.1; ### Description ; when trying to compile with .gradlew bundle get the error above. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/usr/bin/gatk/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --info or --debug option to get more log output. Run with --scan to get full insights. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':gatkDoc'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$3.accept(ExecuteActionsTaskExecuter.java:166); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$3.accept(ExecuteActionsTaskExecuter.java:163); at org.gradle.internal.Try$Failure.ifSuccessfulOrElse(Try.java:191); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:156); at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:62); at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:108); at org.gradle.api.internal.tasks.execution.ResolveBeforeExecutionOutputsTaskExecuter.execute(ResolveBeforeExecutionOutputsTaskExecuter.java:67); at org.gradle.api.internal.tasks.execution.ResolveAfterPreviousExecutionStateTaskExecuter.execute(ResolveAfterPreviousExecutionStateTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute(CleanupStaleOutputsExecuter.java:94); at org.gradle.api.internal.tasks.execution.FinalizePropertiesTaskExecuter.execute(FinalizePropertiesTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.ResolveTaskExecutionModeExecuter.execute(ResolveTaskExecutionModeExecuter.java:95); at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:57); at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskEx,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466:1124,Validat,ValidatingTaskExecuter,1124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466,1,['Validat'],['ValidatingTaskExecuter']
Security,); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:300); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:347); ... 17 more; Caused by:; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1316); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1291); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250); at shaded.cloud-nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.jav,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2514:3730,secur,security,3730,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514,1,['secur'],['security']
Security,* Adding a beta version of http-nio which allows streaming http files and seeking within them.; * This allows using https urls including signed urls to access remote files.; * Bams/crams can be read by specifying the index manually. Automatic index resolution does not work correctly at the moment.; * known caveats; * some methods are not implement in the nio filesystem library yet; * failures are not retryied. I'm currently fighting with sonatype to get a real release pushed out... it seems close...,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6526:152,access,access,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6526,1,['access'],['access']
Security,* Adding the dataproc-cluster-ui script to the release bundle so users can access it.; * Fixes #5400,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5401:75,access,access,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5401,1,['access'],['access']
Security,* Change extract so that when we filter at the genotype level (with FT) the VCF header has the FT filter definition in the comment/unspecified field.; * Also minor renaming of ExtractCohort argument.; * Point to updated truth.; ; [Here's](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/94129da8-6faf-419b-ab75-a46c228b1bbe) an integration test run. Passing everything except ValidateVDS because `reference_data` not being written due to issues beyond my control.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8850:400,Validat,ValidateVDS,400,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8850,1,['Validat'],['ValidateVDS']
Security,* Have GvsCreateVATfromVDS.wdl take sites-only-vcf as an optional input.; * Added logic to allow/disallow CopyFile to overwrite. [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/bb8906d4-7111-4fd1-a723-b5616b354c23) is a passing run using an existing sites-only VCF.; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/9c8be4d5-f707-4c54-bde5-18d9d23cde66) is a run where it tried to generate the sites-only VCF. Failing because of Echo issues with creating VDS.; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/8f8cc493-b0ff-4d8c-8813-6c463dbf17c0) is an integration test. It's failing in ValidateVDS on two paths (the ones that create VDSes) since this is based off of EchoCallset branch - this is expected,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8866:705,Validat,ValidateVDS,705,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8866,1,['Validat'],['ValidateVDS']
Security,* It turned out we weren't logging in to dockerhub on the wdl test cases in travis because I misunderstood how the DOCKER_TEST flag was used.; * Now we unconditionally authenticate to dockerhub in all test shards instead of trying to pick only the relevant ones.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7256:168,authenticat,authenticate,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7256,1,['authenticat'],['authenticate']
Security,* Moving the artifactory key and username out of the .travis.yml and into the travis settings directly.; * This will make it easier to rotate passwords in the future without requiring a new commit and rebases.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7208:142,password,passwords,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7208,1,['password'],['passwords']
Security,* add a new deploy key for travis to use to authenticate to github,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7524:44,authenticat,authenticate,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7524,1,['authenticat'],['authenticate']
Security,"* changing the key hash we use to download R package keys on travis from an insecure 32 bit hash that has been compromised to a more secure longer hash; * we will no longer be installing the ""Totally Legit Signing Key""; * see https://evil32.com/ for a summary of the problem",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5214:19,hash,hash,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5214,4,"['hash', 'secur']","['hash', 'secure']"
Security,* the old version didn't include certain keys in the json that are; necessary for GenomicsDB to authenticate; * closes #5305,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5308:96,authenticat,authenticate,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5308,1,['authenticat'],['authenticate']
Security,"*before*: GATK crashes with a stack trace. The stack trace contains useful; info, but:; * it's hard to read; * it doesn't include the name of the file we cannot access. *now*, a better message that addresses both issues:; > A USER ERROR has occurred: Couldn't read file gs://(...). Error was:; > 401: Anonymous users does not have storage.objects.get access to object (...). Additional information (including a stack trace) is displayed if the; user specifies `--verbosity=DEBUG`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2417:161,access,access,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2417,2,['access'],['access']
Security,- Added code to populate `Match_Norm_Seq_Allele1` and 2.; - Added two samples to `regressionTestVariantSetHG38.vcf` file.; - Regenerated and validated expected outputs for large tests. Fixes #7408,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7422:141,validat,validated,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7422,1,['validat'],['validated']
Security,- Added max version check for data sources. This will automatically be; used when validating a data sources version for running. Fixes #6712,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6807:82,validat,validating,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6807,1,['validat'],['validating']
Security,- Closes #5114 ; - Updates the WDL as well to expose the new files.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5115:46,expose,expose,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5115,1,['expose'],['expose']
Security,"- Created FuncotationFactory as new base class for DataSourceFuncotationFactory; - Created ComputedFuncotationFactory class, inheriting from FuncotationFactory and acting as a base class for GCContentFuncotationFactory and ReferenceContextFuncotationFactory; - Extracted GC content calculation and reference context annotations from previous classes; - Created two new arguments for reference window size and gc content window size; - Created unit tests for GCContent- and ReferenceContextFuncotationFactories; - Regenerated validation files",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6033:525,validat,validation,525,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6033,1,['validat'],['validation']
Security,"- Created `FuncotationMetadata` class. An instance of this will appear as an attribute on all Funcotations. This describes the fields in the Funcotation. This is also useful for when you need to know all possible field names in formats where these may not be specified in every variant (e.g. VCF); - Due to previous bullet point, Funcotator will now throw an exception if an attribute in a variant is not in the header.; - For now, `FuncotationMetadata` only supports INFO fields in a VCF.; - `tumor_f` was being mapped to an incorrect field. This has been removed, but not fixed. See #4634 and #4871 ; - Sanitizing spaces in funcotation field values.; - Fixes #3895 ; - Fix where other transcript field was being populated incorrectly.; - (Dev) Creating a Funcotation for the input VCF INFO attributes.; - (Dev) Made the `VcfOutputRenderer` ignore VCF input funcotations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4872:605,Sanitiz,Sanitizing,605,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4872,1,['Sanitiz'],['Sanitizing']
Security,- Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:25:07.557 INFO PrintReadsSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:25:07.557 INFO PrintReadsSpark - Deflater IntelDeflater; 23:25:07.557 INFO PrintReadsSpark - Initializing engine; 23:25:07.557 INFO PrintReadsSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 16/11/16 23:25:07 INFO SparkContext: Running Spark version 1.6.2; 16/11/16 23:25:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 16/11/16 23:25:07 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).; 16/11/16 23:25:07 INFO SecurityManager: Changing view acls to: root; 16/11/16 23:25:07 INFO SecurityManager: Changing modify acls to: root; 16/11/16 23:25:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root); 16/11/16 23:25:08 INFO Utils: Successfully started service 'sparkDriver' on port 53746.; 16/11/16 23:25:08 INFO Slf4jLogger: Slf4jLogger started; 16/11/16 23:25:08 INFO Remoting: Starting remoting; 16/11/16 23:25:08 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.32.65.22:33197]; 16/11/16 23:25:08 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 33197.; 16/11/16 23:25:08 INFO SparkEnv: Registering MapOutputTracker; 16/11/16 23:25:08 INFO SparkEnv: Registering BlockManagerMaster; 16/11/16 23:25:08 INFO DiskBlockManager: Created local directory at /gpfs/ngsdata/sparkcache/blockmgr-1cb5e4dd-5f50-43a8-b1ca-3d5df9aa6f1c; 16/11/16 23:25:08 INFO MemoryStore: MemoryStore started with capacity 1247.3 MB; 16/11/16 23:25:08 INFO SparkEnv: Registering OutputCommitCoordinator; 16/11/16 23:25:08 INFO Utils: Succ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:3916,Secur,SecurityManager,3916,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,5,"['Secur', 'authenticat']","['SecurityManager', 'authentication']"
Security,- Extracted the order validation for GVCF files into a separate method and included; a check to reset the counter when a new contig is found. Contigs have to; occur in continuous blocks; validation for files in which contigs occur; alternatingly is not supported.; - Added a set of integration tests for GVCF files with two and three contigs. Fixes #6023,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6028:22,validat,validation,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6028,2,['validat'],['validation']
Security,- Fixed issues that were caused by tostring methods and dot format.; - Added method in BaseGraph that will sanitize names for the DOT format.; - Added methods in SeqGraph that will serialize to GFA1 and GFA2.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6274:107,sanitiz,sanitize,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6274,1,['sanitiz'],['sanitize']
Security,"- Funcotator will populate the MAF DB SNP Validation status field with proper values (e.g. ""by1000genomes"") instead of boolean value (e.g. ""TRUE"") Closes #4985 ; - Funcotator now handles multiple records in a VCF funcotation factory that have the same pos, ref, and alt combination, even if equivalent and not exact matches. Closes #4972",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5046:42,Validat,Validation,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5046,1,['Validat'],['Validation']
Security,"- HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:11:33.871 INFO PrintReadsSpark - Deflater: IntelDeflater; 18:11:33.871 INFO PrintReadsSpark - Inflater: IntelInflater; 18:11:33.871 INFO PrintReadsSpark - GCS max retries/reopens: 20; 18:11:33.871 INFO PrintReadsSpark - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 18:11:33.871 INFO PrintReadsSpark - Initializing engine; 18:11:33.871 INFO PrintReadsSpark - Done initializing engine; 17/10/13 18:11:33 INFO spark.SparkContext: Running Spark version 2.2.0.cloudera1; 17/10/13 18:11:34 WARN spark.SparkConf: spark.master yarn-client is deprecated in Spark 2.0+, please instead use ""yarn"" with specified deploy mode.; 17/10/13 18:11:34 INFO spark.SparkContext: Submitted application: PrintReadsSpark; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing view acls groups to: ; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing modify acls groups to: ; 17/10/13 18:11:34 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); groups with view permissions: Set(); users with modify permissions: Set(hdfs); groups with modify permissions: Set(); 17/10/13 18:11:34 INFO util.Utils: Successfully started service 'sparkDriver' on port 45754.; 17/10/13 18:11:34 INFO spark.SparkEnv: Registering MapOutputTracker; 17/10/13 18:11:34 INFO spark.SparkEnv: Registering BlockManagerMaster; 17/10/13 18:11:34 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 17/10/13 18:11:34 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 17/10/13 18:11:34 INFO storage.DiskBlockManager: Created local directory at /tmp/hdfs/blockmgr-ea0e",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:4031,Secur,SecurityManager,4031,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['Secur'],['SecurityManager']
Security,- Include custom validation in try-catch block to handle custom `UserException.CommandLineException` (solves #2225); - Unchanged behaviour for the rest of the tools.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2226:17,validat,validation,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2226,1,['validat'],['validation']
Security,"- Initializing engine; 08:37:21.698 INFO GermlineCNVCaller - Done initializing engine; 08:37:22.015 INFO GermlineCNVCaller - Retrieving intervals from read-count file (results/200219_X008378.counts.tsv)...; 08:37:22.119 INFO GermlineCNVCaller - No annotated intervals were provided...; 08:37:22.120 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 08:37:22.194 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 08:37:22.195 INFO GermlineCNVCaller - Shutting down engine; [February 26, 2019 8:37:22 AM GMT] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 0.29 minutes.; Runtime.totalMemory()=330301440; java.lang.IllegalArgumentException: Output directory results/190226.181217_K00178.CNVCaller does not exist.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.validateArguments(GermlineCNVCaller.java:361); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:281); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /mnt/storage/apps/software/gatk/4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /mnt/storage/apps/software/gatk/4.1.0.0/gatk-package-4.1.0.0-loc`. I'm running these comm",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:15954,validat,validateArguments,15954,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398,1,['validat'],['validateArguments']
Security,- Successful [VAT creation](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9503ad5e-676f-4e48-90ac-44022bc4d608); - Reasonably successful [VAT validation](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9b4dc893-cad5-4204-b40d-3d3f0a541db3)? Could use a check on this,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8670:168,validat,validation,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8670,1,['validat'],['validation']
Security,"- When querying VCFs, the VcfFuncotationFactory will consider the allele (i.e. Number=""R"" and ""A"") in the output. This allows single-allele queries hitting datasource multiallelic variant contexts to be rendered properly. Closes #4957 ; - Added very simple caching to VCF FuncotationFactory; - VCF Funcotation factory can recognize when alleles are not exactly the same, but equivalent. ; - Fixed small speed bottleneck where Set.equals(...) could be used instead of more complex method. This was happening in validation of funcotation metadata.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4977:510,validat,validation,510,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4977,1,['validat'],['validation']
Security,- `ValidateBasicSomaticShortMutations` will now provide a count of supporting alt alleles in the validation normals. This value does not affect the validation status. Closes #5059 ; - Note that there are indels that will not be caught by this functionality. See #5061,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5062:3,Validat,ValidateBasicSomaticShortMutations,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5062,3,"['Validat', 'validat']","['ValidateBasicSomaticShortMutations', 'validation']"
Security,"- reviewed and restricted the access modifiers of all ICG-related classes; - got rid of the functionality to hold on to old caches: whenever a cache goes out of date, the reference is immediately made null in the new ICG; - completely rewrote ComputableGraphStructure in a functional style; - got rid of unused and unnecessary methods; - Created an ImmutableComputableGraphUtils and factored out the builder and other common methods; - math equality asserts for NDArray; - unit tests for ComputableGraphStructure; - unit tests for ImmutableComputableGraph; - unit tests for ImmutableComputableGraphUtils; - CacheNode keys and tags have their own classes now for code clarity",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3034:30,access,access,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3034,1,['access'],['access']
Security,"- reviewed and restricted the access modifiers of all ICG-related classes; - got rid of the functionality to hold on to old caches: whenever a cache goes out of date, the reference is immediately made null in the new ICG; - completely rewrote ComputableGraphStructure in a functional style; - got rid of unused and unnecessary methods; - Created an ImmutableComputableGraphUtils and factored out the builder and other common methods; - math equality asserts for NDArray; - unit tests for ComputableGraphStructure; - unit tests for ImmutableComputableGraph; - unit tests for ImmutableComputableGraphUtils; - keys and tags have their own classes now",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035:30,access,access,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035,1,['access'],['access']
Security,- run to create the VDS: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/6af5e497-7ee2-48de-9c35-79a586d7d0eb; - run to create the VAT: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/0c11d926-a0cc-40f7-83fd-ee37c69b67f5; - run to validate the VAT: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/926767be-fd29-4696-b341-bf9592d9e6e1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8678:313,validat,validate,313,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8678,1,['validat'],['validate']
Security,- use aou service account to access gvcf and to load to BQ; - do manual localization with aou service account in create ingest tsv task to minimize vm spin up,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7133:29,access,access,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7133,1,['access'],['access']
Security,"--------------------------------------------------------------------------------------------------------. funcotator output:. (gatk) root@75181703d894:/gatk# ./gatk Funcotator \; > --variant ./my_data/test_b37.vcf \; > --reference ./my_data/human_g1k_v37.fasta \; > --ref-version hg19 \; > --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s \; > --output ./my_data/variants.funcotated.maf \; > --output-file-format MAF \; > --disable-sequence-dictionary-validation; Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Funcotator --variant ./my_data/test_b37.vcf --reference ./my_data/human_g1k_v37.fasta --ref-version hg19 --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s --output ./my_data/variants.funcotated.maf --output-file-format MAF --disable-sequence-dictionary-validation; 12:11:19.732 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 24, 2021 12:11:19 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:11:19.904 INFO Funcotator - ------------------------------------------------------------; 12:11:19.904 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.2.0.0; 12:11:19.904 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:11:19.905 INFO Funcotator - Executing as root@75181703d894 on Linux v4.15.0-132-generic amd64; 12:11:19.905 INFO Funcotator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 12:11:19.905 INFO Funcotator - Start Date/Time: March 24, 2021 12:11:19 PM GMT; 12:11:19.905 INFO Funcotator - -----------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7158:2590,validat,validation,2590,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158,1,['validat'],['validation']
Security,"----------------------------------------------------------; 19:53:34.606 INFO ValidateVariants - The Genome Analysis Toolkit (GATK) v4.1.8.0; 19:53:34.606 INFO ValidateVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:53:34.607 INFO ValidateVariants - Executing as zepengmu@midway2-login1.rcc.local on Linux v3.10.0-1127.8.2.el7.x86_64 amd64; 19:53:34.607 INFO ValidateVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14; 19:53:34.607 INFO ValidateVariants - Start Date/Time: October 25, 2020 7:53:34 PM CDT; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - HTSJDK Version: 2.22.0; 19:53:34.607 INFO ValidateVariants - Picard Version: 2.22.8; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:53:34.608 INFO ValidateVariants - Deflater: IntelDeflater; 19:53:34.608 INFO ValidateVariants - Inflater: IntelInflater; 19:53:34.608 INFO ValidateVariants - GCS max retries/reopens: 20; 19:53:34.608 INFO ValidateVariants - Requester pays: disabled; 19:53:34.608 INFO ValidateVariants - Initializing engine; 19:53:35.169 INFO FeatureManager - Using codec VCFCodec to read file file://chr1-22.phased.rename.reheader.vcf.gz; 19:53:35.594 INFO ValidateVariants - Done initializing engine; 19:53:35.594 WARN ValidateVariants - IDS validation cannot be done because no DBSNP file was provided; 19:53:35.594 WARN ValidateVariants - Other possible validations will still be performed; 19:53:35.594 INFO ProgressMeter - Starting traversal; 19:53:35.595 INFO",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911:2124,Validat,ValidateVariants,2124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911,1,['Validat'],['ValidateVariants']
Security,"-------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - HTSJDK Version: 2.22.0; 19:53:34.607 INFO ValidateVariants - Picard Version: 2.22.8; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:53:34.608 INFO ValidateVariants - Deflater: IntelDeflater; 19:53:34.608 INFO ValidateVariants - Inflater: IntelInflater; 19:53:34.608 INFO ValidateVariants - GCS max retries/reopens: 20; 19:53:34.608 INFO ValidateVariants - Requester pays: disabled; 19:53:34.608 INFO ValidateVariants - Initializing engine; 19:53:35.169 INFO FeatureManager - Using codec VCFCodec to read file file://chr1-22.phased.rename.reheader.vcf.gz; 19:53:35.594 INFO ValidateVariants - Done initializing engine; 19:53:35.594 WARN ValidateVariants - IDS validation cannot be done because no DBSNP file was provided; 19:53:35.594 WARN ValidateVariants - Other possible validations will still be performed; 19:53:35.594 INFO ProgressMeter - Starting traversal; 19:53:35.595 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 19:53:35.660 INFO ValidateVariants - Shutting down engine; [October 25, 2020 7:53:35 PM CDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2114453504; java.lang.ArrayIndexOutOfBoundsException: -87; 	at org.broadinstitute.hellbender.utils.BaseUtils.convertIUPACtoN(BaseUtils.java:123); 	at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.getSubsequenceAt(CachingIndexedFastaSequenceFile.java:340); 	at org.broadinstitute.hellbender.engine.ReferenceFileSource.queryAndPrefetch(ReferenceFileSource.java:78); 	at org.broadins",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911:2829,Validat,ValidateVariants,2829,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911,1,['Validat'],['ValidateVariants']
Security,-----------------------------------------; 16:01:36.872 INFO Funcotator - HTSJDK Version: 2.21.2; 16:01:36.872 INFO Funcotator - Picard Version: 2.21.9; 16:01:36.872 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:01:36.872 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:01:36.872 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:01:36.872 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:01:36.872 INFO Funcotator - Deflater: IntelDeflater; 16:01:36.872 INFO Funcotator - Inflater: IntelInflater; 16:01:36.872 INFO Funcotator - GCS max retries/reopens: 20; 16:01:36.872 INFO Funcotator - Requester pays: disabled; 16:01:36.872 INFO Funcotator - Initializing engine; 16:01:37.316 INFO FeatureManager - Using codec VCFCodec to read file file:///home/deepak/software_library/gatk-4.1.7.0/SAMPL3_VARIANTFIL.vcf; 16:01:37.360 INFO Funcotator - Done initializing engine; 16:01:37.360 INFO Funcotator - Validating Sequence Dictionaries...; 16:01:37.366 INFO Funcotator - Processing user transcripts/defaults/overrides...; 16:01:37.366 INFO Funcotator - Initializing data sources...; 16:01:37.368 INFO DataSourceUtils - Initializing data sources from directory: /media/deepak/EXTRA/FUNCOTATOR_DATA/DATA_SOURCES; 16:01:37.369 WARN DataSourceUtils - Could not read MANIFEST.txt: unable to log data sources version information.; 16:01:37.375 INFO DataSourceUtils - Resolved data source file path: file:///home/deepak/software_library/gatk-4.1.7.0/simple_uniprot_Dec012014.tsv -> file:///media/deepak/EXTRA/FUNCOTATOR_DATA/DATA_SOURCES/data_source_14/hg38/simple_uniprot_Dec012014.tsv; 16:01:37.391 INFO DataSourceUtils - Resolved data source file path: file:///home/deepak/software_library/gatk-4.1.7.0/hgnc_download_Nov302017.tsv -> file:///media/deepak/EXTRA/FUNCOTATOR_DATA/DATA_SOURCES/data_source_13/hg38/hgnc_download_Nov302017.tsv; 16:01:37.393 INFO DataSourceUtils - Resolved data source file path: file:/,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-664565036:3016,Validat,Validating,3016,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-664565036,1,['Validat'],['Validating']
Security,"---------------------------------; 18:57:41.594 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 18:57:41.594 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:57:41.739 INFO PathSeqPipelineSpark - Initializing engine; 18:57:41.739 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/03/05 18:57:41 INFO SparkContext: Running Spark version 2.2.0; 18:57:41.968 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18:57:42.155 INFO PathSeqPipelineSpark - Shutting down engine; [5 March, 2019 6:57:42 PM IST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=645922816; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:178); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:110); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:28); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5802:2767,validat,validateSettings,2767,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802,1,['validat'],['validateSettings']
Security,---------------------------; > 12:28:16.542 INFO Funcotator - HTSJDK Version: 2.22.0; > 12:28:16.543 INFO Funcotator - Picard Version: 2.22.8; > 12:28:16.543 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; > 12:28:16.543 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 12:28:16.543 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 12:28:16.543 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 12:28:16.543 INFO Funcotator - Deflater: IntelDeflater; > 12:28:16.543 INFO Funcotator - Inflater: IntelInflater; > 12:28:16.543 INFO Funcotator - GCS max retries/reopens: 20; > 12:28:16.543 INFO Funcotator - Requester pays: disabled; > 12:28:16.543 INFO Funcotator - Initializing engine; > 12:28:17.254 INFO FeatureManager - Using codec VCFCodec to read file file:///home/pkus/mutect_test/filtered_variants/P1.vcf.gz; > 12:28:17.687 INFO Funcotator - Done initializing engine; > 12:28:17.688 INFO Funcotator - Validating Sequence Dictionaries...; > 12:28:17.755 INFO Funcotator - Processing user transcripts/defaults/overrides...; > 12:28:17.756 INFO Funcotator - Initializing data sources...; > 12:28:17.759 INFO DataSourceUtils - Initializing data sources from directory: /home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s; > 12:28:17.775 INFO DataSourceUtils - Data sources version: 1.6.2019124s; > 12:28:17.776 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.6.20190124s.tar.gz; > 12:28:17.776 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.6.20190124s.tar.gz; > 12:28:17.795 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.805 INFO DataSourceUtils,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:3145,Validat,Validating,3145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['Validat'],['Validating']
Security,---------------------------; > 15:16:39.788 INFO Funcotator - HTSJDK Version: 2.22.0; > 15:16:39.788 INFO Funcotator - Picard Version: 2.22.8; > 15:16:39.788 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; > 15:16:39.788 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 15:16:39.788 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 15:16:39.788 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 15:16:39.789 INFO Funcotator - Deflater: IntelDeflater; > 15:16:39.789 INFO Funcotator - Inflater: IntelInflater; > 15:16:39.789 INFO Funcotator - GCS max retries/reopens: 20; > 15:16:39.789 INFO Funcotator - Requester pays: disabled; > 15:16:39.789 INFO Funcotator - Initializing engine; > 15:16:40.573 INFO FeatureManager - Using codec VCFCodec to read file file:///home/pkus/mutect_test/filtered_variants/P1.vcf.gz; > 15:16:40.902 INFO Funcotator - Done initializing engine; > 15:16:40.903 INFO Funcotator - Validating Sequence Dictionaries...; > 15:16:40.971 INFO Funcotator - Processing user transcripts/defaults/overrides...; > 15:16:40.972 INFO Funcotator - Initializing data sources...; > 15:16:40.975 INFO DataSourceUtils - Initializing data sources from directory: /home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s; > 15:16:40.978 INFO DataSourceUtils - Data sources version: 1.7.2020429s; > 15:16:40.978 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; > 15:16:40.978 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; > 15:16:40.996 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/CancerGeneCensus_Table_1_full_2012-03-15.txt -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/cancer_gene_census/hg38/CancerGeneC,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708:3940,Validat,Validating,3940,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708,1,['Validat'],['Validating']
Security,----------; 12:11:19.906 INFO Funcotator - HTSJDK Version: 2.24.0; 12:11:19.906 INFO Funcotator - Picard Version: 2.25.0; 12:11:19.906 INFO Funcotator - Built for Spark Version: 2.4.5; 12:11:19.906 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:11:19.906 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:11:19.906 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:11:19.907 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:11:19.907 INFO Funcotator - Deflater: IntelDeflater; 12:11:19.907 INFO Funcotator - Inflater: IntelInflater; 12:11:19.907 INFO Funcotator - GCS max retries/reopens: 20; 12:11:19.907 INFO Funcotator - Requester pays: disabled; 12:11:19.907 INFO Funcotator - Initializing engine; 12:11:20.348 INFO FeatureManager - Using codec VCFCodec to read file file:///gatk/./my_data/test_b37.vcf; 12:11:20.368 INFO Funcotator - Done initializing engine; 12:11:20.368 INFO Funcotator - Skipping sequence dictionary validation.; 12:11:20.369 INFO Funcotator - Processing user transcripts/defaults/overrides...; 12:11:20.370 INFO Funcotator - Initializing data sources...; 12:11:20.375 INFO DataSourceUtils - Initializing data sources from directory: ./my_data/funcotator_dataSources.v1.7.20200521s; 12:11:20.376 INFO DataSourceUtils - Data sources version: 1.7.2020429s; 12:11:20.376 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 12:11:20.377 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 12:11:20.388 INFO DataSourceUtils - Resolved data source file path: file:///gatk/gencode.v34lift37.annotation.REORDERED.gtf -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/gencode/hg19/gencode.v34lift37.annotation.REORDERED.gtf; 12:11:20.389 INFO DataSourceUtils - Resolved data source fi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7158:4717,validat,validation,4717,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158,1,['validat'],['validation']
Security,--; 10:24:50.561 INFO Funcotator - HTSJDK Version: 2.18.2; 10:24:50.561 INFO Funcotator - Picard Version: 2.18.25; 10:24:50.561 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:24:50.561 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:24:50.562 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:24:50.562 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:24:50.562 INFO Funcotator - Deflater: IntelDeflater; 10:24:50.562 INFO Funcotator - Inflater: IntelInflater; 10:24:50.562 INFO Funcotator - GCS max retries/reopens: 20; 10:24:50.562 INFO Funcotator - Requester pays: disabled; 10:24:50.562 INFO Funcotator - Initializing engine; 10:24:51.150 INFO FeatureManager - Using codec VCFCodec to read file file:///sdb/research_th/Exon_Seq/2_classes/joint_out/relapse_joint/t_func/../relapse.filtered.snps.indels.vcf; 10:24:51.387 INFO Funcotator - Done initializing engine; 10:24:51.387 INFO Funcotator - Validating Sequence Dictionaries...; 10:24:51.422 INFO Funcotator - Processing user transcripts/defaults/overrides...; 10:24:51.423 INFO Funcotator - Initializing data sources...; 10:24:51.425 INFO DataSourceUtils - Initializing data sources from directory: /share/share/data/NGS/ref_index/GATK_bundle/funcotator_dataSources.v1.6.20190124g; 10:24:51.427 INFO DataSourceUtils - Data sources version: 1.6.2019124g; 10:24:51.427 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.6.20190124g.tar.gz; 10:24:51.427 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.6.20190124g.tar.gz; 10:24:51.436 INFO DataSourceUtils - Resolved data source file path: file:///sdb/research_th/Exon_Seq/2_classes/joint_out/relapse_joint/t_func/clinvar_20180429_hg38.vcf -> file:///share/share/data/NGS/ref_index/GATK_bundle/funcotator_dataSources.v1.6.20190124g/clinvar/hg38,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5903:3188,Validat,Validating,3188,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5903,1,['Validat'],['Validating']
Security,"-Adds start/end coordinate validation to `SVCallRecord`, checking contigs and positions against the sequence dictionary and their ordering.; -Adds some checks for invalid coordinates in places where `SimpleInterval.expandWithinContig()` can potentially return `null`.; -Addresses an issue where inter-chromosomal records' end positions were incorrectly disallowed from preceding the start position during record collapsing (despite being on a different contig).; -Deletes unused `SVCallRecordWithEvidence`. Includes regression tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7714:27,validat,validation,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7714,1,['validat'],['validation']
Security,-Cleaned up intermediate files in gCNV WDL. Closes #5382.; -Added output of MAD values as floats in somatic CNV WDL. Closes #5591.; -Exposed boot disk space for Oncotator in somatic CNV WDL. Closes #3566.; -Added check to skip outlier truncation if number of matrix elements exceeds Integer.MAX_VALUE in CreateReadCountPanelOfNormals. Closes #4734.; -Fixed some issues concerning intervals in DetermineGermlineContigPloidy documentation.; -Miscellaneous boy scout activities.; -Fixed non-kebab-case argument in CollectAllelicCountsSpark and other minor issues. See #5478.; -Improved consistency of style and input/output validation across CNV tools. Closes #4825. Closes #5744.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5699:133,Expose,Exposed,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5699,2,"['Expose', 'validat']","['Exposed', 'validation']"
Security,"-CompareSAMs not ported because ReadWalker traversal is not suited; for it. -SplitNCigarReads not ported because of the way it uses the reference; (could be ported to ReadWalker with some refactoring, however). There were a few engine changes as well to accomodate the new ReadWalker tools:. -Method to allow walkers to access the SAM header from the reads data source. -No longer require an index for BAM/SAM files when no intervals are; provided and no queries are performed. -onTraversalDone() now allows tools to return a value, which is printed; out by the engine. Resolves #113",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/122:320,access,access,320,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/122,1,['access'],['access']
Security,"-Created a new base class for Spark tools, GATKSparkTool, that centrally manages; and validates standard tool inputs (reads, reference, and intervals). This allows; us to enforce consistency across tools, delete duplicated boilerplate code from tools; to load inputs, and perform standard kinds of validation (eg., sequence dictionary; validation) in one place. -Tools that don't fit into the pattern established by GATKSparkTool can still extend; SparkCommandLineProgram directly. -This is just a first step -- there is still much work to be done to unify our data source; classes and transparently handle inputs from different sources (GCS, hdfs, files), but; having inputs centrally managed should make the remaining tasks much easier.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/955:86,validat,validates,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/955,3,['validat'],"['validates', 'validation']"
Security,"-Created a new class of tool, IntervalWalker, that processes a single interval at a time,; with the ability to query optional overlapping sources of reads, reference data, and/or; features/variants. Current implementation is simple/naive with no special caching;; performance issues will be addressed once we port this traversal type to dataflow. -Added the ability for VariantWalkers to access contextual reads/reference/feature data. -To enable the above changes, migrated most of the engine to use SimpleIntervals rather; than GenomeLocs. This allows for the creation of Context objects in traversals where there; is not necessarily a sequence dictionary available (eg., VariantWalker). -Moved shared arguments/code from Walker classes up into GATKTool. Still some issues; related to marking engine-wide arguments as optional/required on a per-traversal or; per-tool basis, but tickets have been created for these. -Since there isn't yet an htsjdk release that contains SimpleInterval, temporarily; checked a copy of it into our repo, which we can remove the next time we; rev htsjdk. TODOs:. -We currently still require a sequence dictionary to actually parse intervals in; IntervalArgumentCollection. This is due entirely to our support of intervals without; specific stop positions (eg., ""chr1"" and ""chr1:1+"") -- for these intervals we must; look up the stop position in a sequence dictionary. This means that IntervalWalkers; currently require at least one input that contains a sequence dictionary (although; VariantWalkers do not). We should look into ways of relaxing this restriction. Resolves #109",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/297:388,access,access,388,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/297,1,['access'],['access']
Security,"-Hooked up a new PositionalDownsampler to AssemblyRegionWalker controlled; via an argument --maxReadsPerAlignmentStart (tool sets the default value). -As a side effect, exposed the ""assigned"" position of a read in the; GATKRead interface. This lets us query the nominal positions of unmapped; reads that have been assigned a position for sorting purposes. -As a second side effect, modified ReadCoordinateComparator to sort; unmapped reads with positions in the correct bam/sam file order. Resolves #1642; Resolves #1911",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1972:169,expose,exposed,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1972,1,['expose'],['exposed']
Security,"-IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 1 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --help false --version false --verbosity INFO --QUIET false; [March 9, 2017 7:03:42 PM EST] Executing as gspowley@dna on Linux 3.10.0-514.10.2.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_111-b14; Version: Version:4.alpha.2-170-g8d06823-SNAPSHOT; 19:03:42.998 INFO ValidateSamFile - Defaults.BUFFER_SIZE : 131072; 19:03:42.999 INFO ValidateSamFile - Defaults.COMPRESSION_LEVEL : 1; 19:03:42.999 INFO ValidateSamFile - Defaults.CREATE_INDEX : false; 19:03:42.999 INFO ValidateSamFile - Defaults.CREATE_MD5 : false; 19:03:42.999 INFO ValidateSamFile - Defaults.CUSTOM_READER_FACTORY : ; 19:03:42.999 INFO ValidateSamFile - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 19:03:42.999 INFO ValidateSamFile - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 19:03:42.999 INFO ValidateSamFile - Defaults.REFERENCE_FASTA : null; 19:03:43.000 INFO ValidateSamFile - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_CRAM_REF_DOWNLOAD : false; 19:03:43.000 INFO ValidateSamFile - Deflater JdkDeflater; 19:03:43.000 INFO ValidateSamFile - Inflater JdkInflater; 19:03:43.000 INFO ValidateSamFile - Initializing engine; 19:03:43.000 INFO ValidateSamFile - Done initializing engine; ERROR: Record 9762, Read name 20GAVAAXX100126:7:2:8126:115177, bin field of BAM record does not equal value computed based on alignment start and end, and length of sequence to which read is aligned; ERROR: Record 24466, Read name 20FUKAAXX100202:7:46:13035:77621, bin field of BAM record does not equal value comp",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-285513571:1414,Validat,ValidateSamFile,1414,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-285513571,1,['Validat'],['ValidateSamFile']
Security,"-Within a tool, Feature headers can now be obtained from a FeatureContext within apply(),; or from the inherited method getHeaderForFeatures() outside of apply() (eg., in onTraversalStart()). -VariantWalkers have the additional inherited convenience method getHeaderForVariants(); that returns the header for the driving source of variants typed as a VCFHeader. -Engine-facing classes FeatureManager and FeatureDataSource now also expose headers. Requested by Adam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/308:431,expose,expose,431,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/308,1,['expose'],['expose']
Security,"-hadoop2; 17/11/27 20:39:45 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at droazen-test-cluster-m/10.240.0.10:8032; 17/11/27 20:39:47 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1511814592376_0002; 17/11/27 20:39:52 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@7fbe38a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 20:39:52.363 INFO CountReadsSpark - Shutting down engine; [November 27, 2017 8:39:52 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.16 minutes.; Runtime.totalMemory()=630718464; code: 0; message: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; reason: null; location: null; retryable: false; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:340); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:197); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:194); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); 	at com.google.cloud.RetryHelper.run(RetryHelper.java:74); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:51); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:194); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:614); 	at java.nio.file.Files.exists(Files.java:2385); 	at htsjdk.samtools.util.IOUtil.assertFileIsRea",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994:6007,secur,security,6007,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994,2,"['access', 'secur']","['access', 'security']"
Security,-haplotype-to-reference-mismatch-penalty -150 --smith-waterman-haplotype-to-reference-ga; p-open-penalty -260 --smith-waterman-haplotype-to-reference-gap-extend-penalty -11 --smith-waterman-read-to-haplotype-match-value 10 --smith-waterman-read-to-haplotype-mismatch-penalty -15; --smith-waterman-read-to-haplotype-gap-open-penalty -30 --smith-waterman-read-to-haplotype-gap-extend-penalty -5 --flow-assembly-collapse-hmer-size 0 --flow-assembly-collapse-partial-mode; false --flow-filter-alleles false --flow-filter-alleles-qual-threshold 30.0 --flow-filter-alleles-sor-threshold 3.0 --flow-filter-lone-alleles false --flow-filter-alleles-debug-graphs fal; se --min-assembly-region-size 50 --max-assembly-region-size 300 --active-probability-threshold 0.002 --max-prob-propagation-distance 50 --force-active false --assembly-region-padding 100 -; -padding-around-indels 75 --padding-around-snps 20 --padding-around-strs 75 --max-extension-into-assembly-region-padding-legacy 25 --max-reads-per-alignment-start 50 --enable-legacy-assemb; ly-region-trimming false --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-pro; gress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md; 5 false --max-variants-per-shard 0 --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --dis; able-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false ; --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false --minimum-mapping-quality 20 --disable-tool-default-annotations false --enable-all-annotati; ons false --a,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:9015,validat,validation-stringency,9015,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,1,['validat'],['validation-stringency']
Security,"-jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \; ASEReadCounter \; -L scattered.interval_list \; -R Homo_sapiens_assembly19.fasta \; -V 1000G_phase1.snps.high_confidence.b37.vcf.gz \; -I downsample_10k.bam \; -O output.txt --verbosity INFO . c) Entire error log:; 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/broad/software/free/Linux/redhat_7_x86_64/pkgs/gatk_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 14, 2021 7:13:26 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 19:13:26.217 INFO ASEReadCounter - ------------------------------------------------------------; 19:13:26.218 INFO ASEReadCounter - The Genome Analysis Toolkit (GATK) v4.1.8.1; 19:13:26.218 INFO ASEReadCounter - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:13:26.219 INFO ASEReadCounter - Executing as cbao@uger-c009.broadinstitute.org on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 19:13:26.219 INFO ASEReadCounter - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_181-b13; 19:13:26.219 INFO ASEReadCounter - Start Date/Time: June 14, 2021 7:13:25 PM UTC; 19:13:26.219 INFO ASEReadCounter - ------------------------------------------------------------; 19:13:26.219 INFO ASEReadCounter - ------------------------------------------------------------; 19:13:26.220 INFO ASEReadCounter - HTSJDK Version: 2.23.0; 19:13:26.220 INFO ASEReadCounter - Picard Version: 2.22.8; 19:13:26.220 INFO ASEReadCounter - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7314:1389,authenticat,authentication,1389,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7314,1,['authenticat'],['authentication']
Security,"-length read without FZ, CS or CQ tag; ERROR: Record 966616, Read name UMI-ACG-TGG-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966618, Read name UMI-ACT-GGG-11, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966620, Read name UMI-ACT-GGG-12, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966627, Read name UMI-GGC-TGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966674, Read name UMI-CCT-GTC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966699, Read name UMI-CCG-TGA-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966722, Read name UMI-AGG-TGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966742, Read name UMI-CCG-TCA-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966752, Read name UMI-GAA-GAT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966784, Read name UMI-CCT-TAT-12, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966875, Read name UMI-AGG-GGG-10, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966887, Read name UMI-AGG-CCG-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966916, Read name UMI-GCT-TCG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966939, Read name UMI-CAA-TGT-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966989, Read name UMI-GAA-TCA-7, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966991, Read name UMI-TAG-TGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 967245, Read name UMI-AAG-ATT-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 975151, Read name UMI-ACT-CCC-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 1064783, Read name UMI-GGA-GGT-6, Zero-length read without FZ, CS or CQ tag; Maximum output of [100] errors reached.; [Tue Jul 14 11:25:59 EDT 2020] picard.sam.ValidateSamFile done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=1450180608; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:11065,Validat,ValidateSamFile,11065,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['Validat'],['ValidateSamFile']
Security,"-length; ```; WMCF9-CB5:shlee$ ./gatk LeftAlignAndTrimVariants -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V ~/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --max-indel-length 250 -O zeta_snippet_leftalign_250_96branch.vcf.gz; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --max-indel-length 250 -O zeta_snippet_leftalign_250_96branch.vcf.gz; 14:03:44.243 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 06, 2018 2:03:44 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 14:03:44.358 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 14:03:44.358 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-25-g0c6f06f-SNAPSHOT; 14:03:44.359 INFO LeftAlignAndTrimVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:03:44.359 INFO LeftAlignAndTrimVariants - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 14:03:44.359 INFO LeftAlignAndTrimVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_111-b14; 14:03:44.359 INFO LeftAlignAndTrimVariants - Start Date/Time",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326:7097,authenticat,authenticated,7097,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326,1,['authenticat'],['authenticated']
Security,"-logs/staging/703469fc-52fe-441d-b6e0-8092a114fe2c//chr20$17960187$17981445/genomicsdb_meta_dir; hdfsBuilderConnect(forceNewInstance=0, nn=gs://hellbender-test-logs, port=0, kerbTicketCachePath=(NULL), userName=(NULL)) error:; java.io.IOException: Must supply a value for configuration setting: fs.gs.project.id; 	at com.google.cloud.hadoop.util.ConfigurationUtil.getMandatoryConfig(ConfigurationUtil.java:39); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createOptionsBuilderFromConfig(GoogleHadoopFileSystemBase.java:2185); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1832); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1013); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:976); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2812); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:100); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2849); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2831); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389); 	at org.apache.hadoop.fs.FileSystem$1.run(FileSystem.java:171); 	at org.apache.hadoop.fs.FileSystem$1.run(FileSystem.java:168); 	at java.base/java.security.AccessController.doPrivileged(Native Method); 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:168); 	at org.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Native Method); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:209); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:182); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:91); 	at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6522:1165,access,access,1165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6522,1,['access'],['access']
Security,".	GAAGA	G,GCA	.	.	.	GT:AD:DP	1/2:0,12,8:20	1/1:0,30,2:32; 5	112174757	.	GAAGA	G,GGA	.	.	.	GT:AD:DP	0/1:12,8,0:20	0/2:30,0,2:32; 5	112174757	.	GAAGA	G,GGA	.	.	.	GT:AD:DP	0/2:12,0,8:20	0/1:30,2,0:32; 6	41903782	.	AG	CA	.	.	.	GT:AD:DP	0/1:28,22:50	0/0:48,0:48; 7	116412043	.	G	C	.	.	.	GT:AD:DP	0/1:25,22:47	0/0:98,1:99; 13	28608242	.	A	AACTCCCATTTGAGATCATATTCATATTCTCTGAAATCAACGTAGAAGTACTCATTACCCCCTCGGGGGG	.	.	.	GT:AD:DP	0/1:10,10:20	0/0:11,0:11; 17	7579312	.	C	A	.	.	.	GT:AD:DP	0/1:20,22:42	0/0:18,1:1. ----------------------------------------------------------------------------------------------------------------------------------------------. funcotator output:. (gatk) root@75181703d894:/gatk# ./gatk Funcotator \; > --variant ./my_data/test_b37.vcf \; > --reference ./my_data/human_g1k_v37.fasta \; > --ref-version hg19 \; > --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s \; > --output ./my_data/variants.funcotated.maf \; > --output-file-format MAF \; > --disable-sequence-dictionary-validation; Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Funcotator --variant ./my_data/test_b37.vcf --reference ./my_data/human_g1k_v37.fasta --ref-version hg19 --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s --output ./my_data/variants.funcotated.maf --output-file-format MAF --disable-sequence-dictionary-validation; 12:11:19.732 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 24, 2021 12:11:19 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:11:19.904 INFO Funcotator - ----------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7158:2042,validat,validation,2042,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158,1,['validat'],['validation']
Security,. Here's an example test case: https://github.com/broadinstitute/gatk/commit/8b217f82352ceb55d21d7a5236e879818910d9c9. and the stacktrace:. ```; java.util.ServiceConfigurationError: java.nio.file.spi.FileSystemProvider: Provider com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider could not be instantiated; at java.util.ServiceLoader.fail(ServiceLoader.java:232); at java.util.ServiceLoader.access$100(ServiceLoader.java:185); at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384); at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); at java.util.ServiceLoader$1.next(ServiceLoader.java:480); at java.nio.file.spi.FileSystemProvider.loadInstalledProviders(FileSystemProvider.java:119); at java.nio.file.spi.FileSystemProvider.access$000(FileSystemProvider.java:77); at java.nio.file.spi.FileSystemProvider$1.run(FileSystemProvider.java:169); at java.nio.file.spi.FileSystemProvider$1.run(FileSystemProvider.java:166); at java.security.AccessController.doPrivileged(Native Method); at java.nio.file.spi.FileSystemProvider.installedProviders(FileSystemProvider.java:166); at java.nio.file.Paths.get(Paths.java:141); at org.broadinstitute.hellbender.engine.spark.datasources.NioProviderExceptionUnitTest.test(NioProviderExceptionUnitTest.java:12). Caused by:; java.lang.IllegalArgumentException: A project ID is required for this service but could not be determined from the builder or the environment. Please set a project ID using the builder.; at shaded.cloud-nio.com.google.common.base.Preconditions.checkArgument(Preconditions.java:122); at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:208); at com.google.cloud.HttpServiceOptions.<init>(HttpServiceOptions.java:153); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:69); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:27); at com.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:64); at com.google.cloud.storage.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2110:1173,Access,AccessController,1173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2110,1,['Access'],['AccessController']
Security,".....................; Exception in thread ""main"" javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.SSLSocketImpl.checkEOF(SSLSocketImpl.java:1541); 	at sun.security.ssl.AppInputStream.available(AppInputStream.java:60); 	at java.io.BufferedInputStream.available(BufferedInputStream.java:410); 	at sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at org.gradle.wrapper.Download.downloadInternal(Download.java:77); 	at org.gradle.wrapper.Download.download(Download.java:44); 	at org.gradle.wrapper.Install$1.call(Install.java:61); 	at org.gradle.wrapper.Install$1.call(Install.java:48); 	at org.gradle.wrapper.ExclusiveFileAccessManager.access(ExclusiveFileAccessManager.java:69); 	at org.gradle.wrapper.Install.createDist(Install.java:48); 	at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:107); 	at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:116); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401:1316,access,access,1316,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401,1,['access'],['access']
Security,".078 INFO HaplotypeCaller - GCS max retries/reopens: 20; 01:13:16.078 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 01:13:16.078 INFO HaplotypeCaller - Initializing engine; 01:13:17.087 INFO HaplotypeCaller - Shutting down engine; [January 18, 2020 1:13:17 AM IST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2216689664; java.lang.NullPointerException; at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getContigNames(SequenceDictionaryUtils.java:463); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getCommonContigsByName(SequenceDictionaryUtils.java:457); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.compareDictionaries(SequenceDictionaryUtils.java:234); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:150); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:98); at org.broadinstitute.hellbender.engine.GATKTool.validateSequenceDictionaries(GATKTool.java:621); at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:563); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:160); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:275). I have tried with different versions of Java, still i",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6384:2669,validat,validateDictionaries,2669,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6384,1,['validat'],['validateDictionaries']
Security,".089 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:41.089 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:41.089 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:45.460 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:45.460 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:11:09.609 WARN TaskMemoryManager:381 - leak 166.6 MB memory from org.apache.spark.util.collection.ExternalAppendOnlyMap@60a3c432; 00:11:09.611 ERROR Executor:91 - Exception in task 15.0 in stage 1.0 (TID 519); java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinsti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018:1440,Hash,HashMap,1440,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018,1,['Hash'],['HashMap']
Security,".1; 10:20:01.719 INFO GermlineCNVCaller - Picard Version: 2.27.5; 10:20:01.719 INFO GermlineCNVCaller - Built for Spark Version: 2.4.5; 10:20:01.719 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:20:01.719 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:20:01.719 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:20:01.719 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:20:01.719 INFO GermlineCNVCaller - Deflater: IntelDeflater; 10:20:01.719 INFO GermlineCNVCaller - Inflater: IntelInflater; 10:20:01.719 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 10:20:01.719 INFO GermlineCNVCaller - Requester pays: disabled; 10:20:01.720 INFO GermlineCNVCaller - Initializing engine; 10:20:07.111 INFO GermlineCNVCaller - Done initializing engine; 10:20:07.207 INFO GermlineCNVCaller - Running the tool in CASE mode...; 10:20:07.207 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 10:20:07.231 INFO GermlineCNVCaller - Aggregating read-count file /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_noProbe.hdf5 (1 / 1); log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 10:20:25.874 INFO GermlineCNVCaller - Shutting down engine; [March 14, 2024 at 10:20:25 AM CET] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 0.40 minutes.; Runtime.totalMemory()=2147483648; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1; Command Line: python /media/Data/tmp/case_denoising_calling.3564509013495540802.py --ploidy_calls_path=/media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_DGCP_noProbe-calls --output_calls_path=/media/Ergebnisse/0115-24_Masterpanel_NB501",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8740:3445,Validat,Validating,3445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740,1,['Validat'],['Validating']
Security,".2.4.1/ensembl-vep/PE69_chr3.vcf; 10:58:20.063 INFO VariantAnnotator - Done initializing engine; 10:58:20.091 WARN VariantAnnotatorEngine - The requested expression attribute ""gnomad.ALT"" is missing from the header in its resource file gnomad; 10:58:20.140 INFO ProgressMeter - Starting traversal; 10:58:20.140 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 10:58:42.160 INFO VariantAnnotator - Shutting down engine; [March 17, 2022 at 10:58:42 AM CET] org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotator done. Elapsed time: 0.37 minutes.; Runtime.totalMemory()=17158897664; java.lang.IllegalStateException: Allele in genotype C not in the variant context [C*, CT]; 	at htsjdk.variant.variantcontext.VariantContext$Validation.validateGenotypes(VariantContext.java:382); 	at htsjdk.variant.variantcontext.VariantContext$Validation.access$200(VariantContext.java:323); 	at htsjdk.variant.variantcontext.VariantContext$Validation$2.validate(VariantContext.java:331); 	at htsjdk.variant.variantcontext.VariantContext.lambda$validate$0(VariantContext.java:1384); 	at java.base/java.lang.Iterable.forEach(Iterable.java:75); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1384); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:489); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:647); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:638); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.trimAlleles(GATKVariantContextUtils.java:1464); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.trimAlleles(GATKVariantContextUtils.java:1420); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.getMinRepresentationBiallelics(VariantAnnotatorEngine.java:568); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053:4175,validat,validate,4175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053,1,['validat'],['validate']
Security,.24.0; 10:25:49.666 INFO Funcotator - Picard Version: 2.25.0; 10:25:49.666 INFO Funcotator - Built for Spark Version: 2.4.5; 10:25:49.666 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:25:49.667 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:25:49.667 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:25:49.667 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:25:49.667 INFO Funcotator - Deflater: IntelDeflater; 10:25:49.667 INFO Funcotator - Inflater: IntelInflater; 10:25:49.667 INFO Funcotator - GCS max retries/reopens: 20; 10:25:49.667 INFO Funcotator - Requester pays: disabled; 10:25:49.667 INFO Funcotator - Initializing engine; 10:25:49.761 INFO FeatureManager - Using codec VCFCodec to read file file:///technology/research_development/WES/vcf/P01.mutect2.somatic.filterMutectCalls.indels.vcf.gz; 10:25:49.781 INFO Funcotator - Done initializing engine; 10:25:49.781 INFO Funcotator - Validating sequence dictionaries...; 10:25:49.782 INFO Funcotator - Processing user transcripts/defaults/overrides...; 10:25:49.783 INFO Funcotator - Initializing data sources...; 10:25:49.784 INFO DataSourceUtils - Initializing data sources from directory: /technology/dependent_resource/variation/hg19/funcotator_dataSources.v1.7.20200521s; 10:25:49.785 INFO DataSourceUtils - Data sources version: 1.7.2020429s; 10:25:49.785 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 10:25:49.785 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 10:25:49.788 INFO DataSourceUtils - Resolved data source file path: file:///technology/research_development/WES/vcf/gencode_xrefseq_v75_37.tsv -> file:///technology/dependent_resource/variation/hg19/funcotator_dataSources.v1.7.20200521s/gencode_xrefseq/hg19/gencode_xref,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7135:3524,Validat,Validating,3524,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7135,1,['Validat'],['Validating']
Security,".32-642.6.2.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_92-b14; Version: Version:null; WARNING 2017-01-20 09:07:57 SamFiles The index file /seq/fargo_picard_aggregation/C802/HSCX1550T/v3/HSCX1550T.bai was found by resolving the canonical path of a symlink: /dsde/working/lichtens/test_dl_oxoq/code/test_dl_oxoq/cromwell-executions/full_dl_ob_training/2c9b89c2-27ef-448a-b5af-1d090e76ada1/call-dl_ob_training/shard-25/dl_ob_training/00210f87-b7f5-4057-bb02-d3be31029996/call-CollectSequencingArtifactMetrics/inputs/seq/picard_aggregation/C802/HSCX1550T/v3/HSCX1550T.bam -> /seq/fargo_picard_aggregation/C802/HSCX1550T/v3/HSCX1550T.bam; [January 20, 2017 9:23:02 AM EST] org.broadinstitute.hellbender.tools.picard.analysis.artifacts.CollectSequencingArtifactMetrics done. Elapsed time: 15.09 minutes.; Runtime.totalMemory()=4831313920; java.lang.NullPointerException; at org.broadinstitute.hellbender.tools.picard.analysis.artifacts.ContextAccumulator$AlignmentAccumulator.access$100(ContextAccumulator.java:113); at org.broadinstitute.hellbender.tools.picard.analysis.artifacts.ContextAccumulator.countRecord(ContextAccumulator.java:39); at org.broadinstitute.hellbender.tools.picard.analysis.artifacts.ArtifactCounter.countRecord(ArtifactCounter.java:82); at org.broadinstitute.hellbender.tools.picard.analysis.artifacts.CollectSequencingArtifactMetrics.acceptRead(CollectSequencingArtifactMetrics.java:228); at org.broadinstitute.hellbender.tools.picard.analysis.SinglePassSamProgram.makeItSo(SinglePassSamProgram.java:114); at org.broadinstitute.hellbender.tools.picard.analysis.SinglePassSamProgram.doWork(SinglePassSamProgram.java:53); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:62); at org.broadinstit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2351:2343,access,access,2343,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2351,1,['access'],['access']
Security,".595 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 19:53:35.660 INFO ValidateVariants - Shutting down engine; [October 25, 2020 7:53:35 PM CDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2114453504; java.lang.ArrayIndexOutOfBoundsException: -87; 	at org.broadinstitute.hellbender.utils.BaseUtils.convertIUPACtoN(BaseUtils.java:123); 	at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.getSubsequenceAt(CachingIndexedFastaSequenceFile.java:340); 	at org.broadinstitute.hellbender.engine.ReferenceFileSource.queryAndPrefetch(ReferenceFileSource.java:78); 	at org.broadinstitute.hellbender.engine.ReferenceDataSource.queryAndPrefetch(ReferenceDataSource.java:64); 	at org.broadinstitute.hellbender.engine.ReferenceContext.getBases(ReferenceContext.java:197); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants.apply(ValidateVariants.java:236); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	at org.broadinstitute.hellbender.engine.VariantWalker$$Lambda$76/1710491273.accept(Unknown Source); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911:4123,Validat,ValidateVariants,4123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911,1,['Validat'],['ValidateVariants']
Security,.782 INFO GermlineCNVCaller - Initializing engine; 17:28:34.716 INFO GermlineCNVCaller - Done initializing engine; 17:28:34.723 INFO GermlineCNVCaller - Intervals specified...; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 17:28:35.689 INFO FeatureManager - Using codec IntervalListCodec to read file file:///media/Data/AnnotationDBs/CNV/Genom/hdf5/../Genom.filtered.interval_list; 17:28:42.892 INFO IntervalArgumentCollection - Processing 2741406000 bp from intervals; 17:28:43.237 INFO GermlineCNVCaller - Reading and validating annotated intervals...; 17:28:51.740 INFO GermlineCNVCaller - GC-content annotations for intervals found; explicit GC-bias correction will be performed...; 17:28:57.410 INFO GermlineCNVCaller - Running the tool in COHORT mode...; 17:28:57.410 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 17:28:57.940 INFO GermlineCNVCaller - Aggregating read-count file 0028-21.hdf5 (1 / 44); 17:29:00.837 INFO GermlineCNVCaller - Aggregating read-count file 0045-21.hdf5 (2 / 44); 17:29:03.690 INFO GermlineCNVCaller - Aggregating read-count file 0098-18.hdf5 (3 / 44); 17:29:06.658 INFO GermlineCNVCaller - Aggregating read-count file 0156-21.hdf5 (4 / 44); 17:29:09.435 INFO GermlineCNVCaller - Aggregating read-count file 0429-20.hdf5 (5 / 44); 17:29:12.235 INFO GermlineCNVCaller - Aggregating read-count file 0779-18.hdf5 (6 / 44); 17:29:14.939 INFO GermlineCNVCaller - Aggregating read-count file 1030-20.hdf5 (7 / 44); 17:29:17.822 INFO GermlineCNVCaller - Aggregating read-count file 1098-13.hdf5 (8 / 44); 17:29:20.668 INFO GermlineCNVCaller - Aggregating read-count file 1450-20.hdf5 (9 / 44); 17:29:23.485 INFO GermlineCNVCaller - Aggregating read-count file 1495-17.hdf5 (10 / 44); 17:29:26.245 INFO GermlineCNVCaller - Aggregating read-count ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7234:5591,Validat,Validating,5591,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234,1,['Validat'],['Validating']
Security,".794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:178); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:110",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5686:1820,access,access,1820,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686,3,['access'],['access']
Security,".971 INFO FeatureManager - Using codec VCFCodec to read file file:///run/media/riadh/My%20Book_From%20Eiklid/Analysis/gatk-4.2.4.1/ensembl-vep/PE69_chr3.vcf; 10:58:20.063 INFO VariantAnnotator - Done initializing engine; 10:58:20.091 WARN VariantAnnotatorEngine - The requested expression attribute ""gnomad.ALT"" is missing from the header in its resource file gnomad; 10:58:20.140 INFO ProgressMeter - Starting traversal; 10:58:20.140 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 10:58:42.160 INFO VariantAnnotator - Shutting down engine; [March 17, 2022 at 10:58:42 AM CET] org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotator done. Elapsed time: 0.37 minutes.; Runtime.totalMemory()=17158897664; java.lang.IllegalStateException: Allele in genotype C not in the variant context [C*, CT]; 	at htsjdk.variant.variantcontext.VariantContext$Validation.validateGenotypes(VariantContext.java:382); 	at htsjdk.variant.variantcontext.VariantContext$Validation.access$200(VariantContext.java:323); 	at htsjdk.variant.variantcontext.VariantContext$Validation$2.validate(VariantContext.java:331); 	at htsjdk.variant.variantcontext.VariantContext.lambda$validate$0(VariantContext.java:1384); 	at java.base/java.lang.Iterable.forEach(Iterable.java:75); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1384); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:489); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:647); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:638); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.trimAlleles(GATKVariantContextUtils.java:1464); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.trimAlleles(GATKVariantContextUtils.java:1420); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.getMinRepresentationBia",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053:4065,Validat,Validation,4065,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053,1,['Validat'],['Validation']
Security,".ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Sun Jul 26 10:20:35 EDT 2020] Executing as farrell@scc-hadoop.bu.edu on Linux 3.10.0-1062.12.1.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.7.0; INFO 2020-07-26 10:20:35 LiftoverVcf Loading up the target reference genome.; INFO 2020-07-26 10:20:56 LiftoverVcf Lifting variants over and sorting (not yet writing the output file.); [Sun Jul 26 10:20:56 EDT 2020] picard.vcf.LiftoverVcf done. Elapsed time: 0.36 minutes.; Runtime.totalMemory()=5861015552; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp; htsjdk.tribble.TribbleException: Badly formed variant context at location chr1:596697; getEnd() was 596797 but this VariantContext contains an END key with value 532177; at htsjdk.variant.variantcontext.VariantContext.validateStop(VariantContext.java:1401); at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1383); at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:489); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:647); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:638); at picard.util.LiftoverUtils.liftVariant(LiftoverUtils.java:92); at picard.vcf.LiftoverVcf.doWork(LiftoverVcf.java:426); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292). ```. #### Steps to reproduce. Download vcf from here:. ftp://ftp-trace.ncbi.nlm.nih.gov/giab/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6725:3060,validat,validateStop,3060,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6725,1,['validat'],['validateStop']
Security,".USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 18:11:33.871 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:11:33.871 INFO PrintReadsSpark - Deflater: IntelDeflater; 18:11:33.871 INFO PrintReadsSpark - Inflater: IntelInflater; 18:11:33.871 INFO PrintReadsSpark - GCS max retries/reopens: 20; 18:11:33.871 INFO PrintReadsSpark - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 18:11:33.871 INFO PrintReadsSpark - Initializing engine; 18:11:33.871 INFO PrintReadsSpark - Done initializing engine; 17/10/13 18:11:33 INFO spark.SparkContext: Running Spark version 2.2.0.cloudera1; 17/10/13 18:11:34 WARN spark.SparkConf: spark.master yarn-client is deprecated in Spark 2.0+, please instead use ""yarn"" with specified deploy mode.; 17/10/13 18:11:34 INFO spark.SparkContext: Submitted application: PrintReadsSpark; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing view acls groups to: ; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing modify acls groups to: ; 17/10/13 18:11:34 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); groups with view permissions: Set(); users with modify permissions: Set(hdfs); groups with modify permissions: Set(); 17/10/13 18:11:34 INFO util.Utils: Successfully started service 'sparkDriver' on port 45754.; 17/10/13 18:11:34 INFO spark.SparkEnv: Registering MapOutputTracker; 17/10/13 18:11:34 INFO spark.SparkEnv: Registering BlockManagerMaster; 17/10/13 18:11:34 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 17/10/13 18:11:34 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 17/10/13 18:11:34 INFO ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:3956,Secur,SecurityManager,3956,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['Secur'],['SecurityManager']
Security,.apache.hadoop.hdfs.server.namenode.FSNamesystem.concat(FSNamesystem.java:2219); > at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.concat(NameNodeRpcServer.java:829); > at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.concat(AuthorizationProviderProxyClientProtocol.java:285); > at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.concat(ClientNamenodeProtocolServerSideTranslatorPB.java:580); > at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); > at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617); > at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073); > at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2278); > at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2274); > at java.security.AccessController.doPrivileged(Native Method); > at javax.security.auth.Subject.doAs(Subject.java:422); > at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924); > at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2272); > ; > org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file hdfs://cloudera08/gatk-test2/WES2019-022_S4_out.vcf because writing failed with exception concat: target file /gatk-test2/WES2019-022_S4_out.vcf.parts/output is empty; > at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.concatInternal(FSNamesystem.java:2303); > at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.concatInt(FSNamesystem.java:2257). #### Steps to reproduce; The user's command line was. > nohup /opt/gatk/gatk-4.1.4.0/gatk ReadsPipelineSpark --spark-runner SPARK --spark-master yarn --spark-submit-command spark2-submit -I hdfs://cloudera08/gatk-test2/WES2019-022_S4.bam -O hdfs://cloudera08/gatk-test2/WES2019-022_S4_out.vcf -R hdfs://cloudera08/gatk-test1/ucsc.hg19.fast,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6218:1888,secur,security,1888,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6218,1,['secur'],['security']
Security,.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /root/gatk-4.0.7.0/gatk-package-4.0.7.0-spark.jar PrintReadsSpark -I ../6484_snippet.bam -O ../output.bam --spark-master spark://10.0.0.21:7077; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark2/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.40-13/spark_llap/spark-llap-assembly-1.0.0.2.6.3.40-13.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:2620,Secur,SecureClassLoader,2620,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['Secur'],['SecureClassLoader']
Security,.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 20g --executor-cores 4 --executor-memory 8g /gatk/gatk-package-4.0.4.0-spark.jar BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 --input hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_fastqtosam.bam --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img --disable-sequence-dictionary-validation true --output hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_dedup_reads.bam --spark-master spark://926a0516ccf6:7077; 11:01:48.445 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 11:01:48.743 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.0.4.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 11:01:49.333 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 11:01:49.334 INFO BwaAndMarkDuplicatesPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0; 11:01:49.334 INFO BwaAndMarkDuplicatesPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:01:49.334 INFO BwaAndMarkDuplicatesPipelineSpark - Executing as root@926a0516ccf6 on Linux v4.4.0-127-generic amd64; 11:01:49.335 INFO BwaAndMarkDuplicatesPipelineSpark - Java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:14475,validat,validation,14475,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,1,['validat'],['validation']
Security,.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at org.apache.hadoop.ipc.Client.call(Client.java:1475); 	at org.apache.hadoop.ipc.Client.call(Client.java:1412); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229); 	at com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:255); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191); 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHa,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:9819,secur,security,9819,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294,1,['secur'],['security']
Security,.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:237); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:488); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:468); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:458); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:3661,secur,security,3661,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294,1,['secur'],['security']
Security,.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106); 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73); 	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1228); 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1213); 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1201); 	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:306); 	at ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:6578,secur,security,6578,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294,1,['secur'],['security']
Security,.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslato,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:1998,secur,security,1998,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294,1,['secur'],['security']
Security,.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:515); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:93); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:49); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	... 41 more; Caused by: javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.SSLSocketImpl.checkEOF(SSLSocketImpl.java:1541); 	at sun.security.ssl.AppInputStream.available(AppInputStream.java:60); 	at java.io.BufferedInputStream.available(BufferedInputStream.java:410); 	at sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:97); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); 	at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:510); 	... 47 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308541727:5137,secur,security,5137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308541727,1,['secur'],['security']
Security,.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:515); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:93); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:49); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	... 49 more; Caused by: javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.SSLSocketImpl.checkEOF(SSLSocketImpl.java:1541); 	at sun.security.ssl.AppInputStream.available(AppInputStream.java:60); 	at java.io.BufferedInputStream.available(BufferedInputStream.java:410); 	at sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:97); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); 	at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:510); 	... 55 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549138:8625,secur,security,8625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549138,1,['secur'],['security']
Security,.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:97); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); 	at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:507); 	... 12 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:116); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3375); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:169); 	at java.io.FilterInputStream.read(FilterInputStream.java:107); 	at shaded.cloud_nio.com.google.api.client.util.ByteStreams.copy(ByteStreams.java:51); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:94); 	... 15 more; Caused by: java.net.SocketException: Connection reset; 	at java.net,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:8125,secur,security,8125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['secur'],['security']
Security,.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:97); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); 	at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:510); 	... 47 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:116); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3375); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:169); 	at java.io.FilterInputStream.read(FilterInputStream.java:107); 	at shaded.cloud_nio.com.google.api.client.util.ByteStreams.copy(ByteStreams.java:51); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:94); 	... 50 more; Caused by: java.net.SocketException: Connection reset; 	at java.net,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308541727:6419,secur,security,6419,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308541727,1,['secur'],['security']
Security,.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:97); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); 	at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:510); 	... 55 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:116); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3375); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:169); 	at java.io.FilterInputStream.read(FilterInputStream.java:107); 	at shaded.cloud_nio.com.google.api.client.util.ByteStreams.copy(ByteStreams.java:51); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:94); 	... 58 more; Caused by: java.net.SocketException: Connection reset; 	at java.net,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549138:9907,secur,security,9907,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549138,1,['secur'],['security']
Security,.java:3448); 	at org.gradle.wrapper.Download.downloadInternal(Download.java:77); 	at org.gradle.wrapper.Download.download(Download.java:44); 	at org.gradle.wrapper.Install$1.call(Install.java:61); 	at org.gradle.wrapper.Install$1.call(Install.java:48); 	at org.gradle.wrapper.ExclusiveFileAccessManager.access(ExclusiveFileAccessManager.java:69); 	at org.gradle.wrapper.Install.createDist(Install.java:48); 	at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:107); 	at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:116); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3375); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3368); 	at org.gradle.wrapper.Download.downloadInternal(Download.java:62); 	... 7 more; Caused by: java.net.SocketException: Connection reset; 	at java.net.SocketInputStream.read(SocketInputStream.java:210); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	at sun.security.ssl.InputRecord.readV3Record(InputRecord.java:593); 	at sun.security.ssl.InputRecord.read(InputRecord.java:532); 	at sun.security.ssl.SSLS,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401:2009,secur,security,2009,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401,1,['secur'],['security']
Security,.namenode.FSNamesystem.concatInt(FSNamesystem.java:2257); > at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.concat(FSNamesystem.java:2219); > at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.concat(NameNodeRpcServer.java:829); > at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.concat(AuthorizationProviderProxyClientProtocol.java:285); > at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.concat(ClientNamenodeProtocolServerSideTranslatorPB.java:580); > at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); > at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617); > at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073); > at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2278); > at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2274); > at java.security.AccessController.doPrivileged(Native Method); > at javax.security.auth.Subject.doAs(Subject.java:422); > at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924); > at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2272); > ; > org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file hdfs://cloudera08/gatk-test2/WES2019-022_S4_out.vcf because writing failed with exception concat: target file /gatk-test2/WES2019-022_S4_out.vcf.parts/output is empty; > at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.concatInternal(FSNamesystem.java:2303); > at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.concatInt(FSNamesystem.java:2257). #### Steps to reproduce; The user's command line was. > nohup /opt/gatk/gatk-4.1.4.0/gatk ReadsPipelineSpark --spark-runner SPARK --spark-master yarn --spark-submit-command spark2-submit -I hdfs://cloudera08/gatk-test2/WES2019-022_S4.bam -O hdfs://cloudera08/gatk-test2/WES,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6218:1822,secur,security,1822,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6218,1,['secur'],['security']
Security,.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.SocketTimeoutException: Read timed out; 	at java.net.SocketInputStream.socketRead0(Native Method); 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); 	at java.net.SocketInputStream.read(SocketInputStream.java:171); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	at sun.security.ssl.InputRecord.read(InputRecord.java:503); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); 	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1569); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1474); 	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); 	at shaded.cloud_nio.com.google.a,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:6585,secur,security,6585,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['secur'],['security']
Security,".rename.reheader.vcf.gz -R ../../../../index/hg19.fa.gz; Using GATK jar ~/tools/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/zepengmu/tools/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar ValidateVariants -V ../../data/geno/phased/chr1-22.phased.rename.reheader.vcf.gz -R ../../../../index/hg19.fa.gz; 19:53:34.379 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/zepengmu/tools/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 25, 2020 7:53:34 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:53:34.606 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.606 INFO ValidateVariants - The Genome Analysis Toolkit (GATK) v4.1.8.0; 19:53:34.606 INFO ValidateVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:53:34.607 INFO ValidateVariants - Executing as zepengmu@midway2-login1.rcc.local on Linux v3.10.0-1127.8.2.el7.x86_64 amd64; 19:53:34.607 INFO ValidateVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14; 19:53:34.607 INFO ValidateVariants - Start Date/Time: October 25, 2020 7:53:34 PM CDT; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - HTSJDK Version: 2.22.0; 19:53:34.607 INFO ValidateVariants - Picard Version: 2.22.8; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:53:34.608 INFO ValidateVariants - HTS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911:1215,Validat,ValidateVariants,1215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911,1,['Validat'],['ValidateVariants']
Security,".samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); 	at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:124); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.samtools.SAMException: Exception creating BAM index for record i 1/2 76b aligned read.; 	at htsjdk.samtools.BAMIndexer.processAlignment(BAMIndexer.java:119); 	at htsjdk.samtools.BAMFileWriter.writeAlignment(BAMFileWriter.java:139); 	... 5 more; Caused by: htsjdk.samtools.SAMException: IOException in BinaryBAMIndexWriter reference 0; 	at htsjdk.samtools.BinaryBAMIndexWriter.writeReference(BinaryBAMIndexWriter.java:151); 	at htsjdk.samtools.BAMIndexer.advanceToReference(BAMIndexer.java:138); 	at htsjdk.samtools.BAMIndexer.processAlignment(BAMIndexer.java:115); 	... 6 more; Caused by: java.nio.channels.ClosedByInterruptException; 	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202); 	at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:216); 	at java.nio.channels.Channels.writeFullyImpl(Channels.java:78); 	at java.nio.channels.Channels.writeFully(Channels.java:101); 	at java.nio.channels.Channels.access$000(Channels.java:61); 	at java.nio.channels.Channels$1.write(Channels.java:174); 	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82); 	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140); 	at htsjdk.samtools.BinaryBAMIndexWriter.writeReference(BinaryBAMIndexWriter.java:149); 	... 8 more; ```. One of the logs is here: https://storage.googleapis.com/hellbender-test-logs/build_reports/12617.7/tests/test/index.html. We saw a whole host of those cigar validation errors you're seeing when we updated htsdjk the last time, it's a new validation that wasn't previously checked in htsjdk, so a lot of the test files had errors in them that no one had ever noticed/bothered fixing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-329967977:1648,access,access,1648,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-329967977,3,"['access', 'validat']","['access', 'validation']"
Security,".scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 00:11:09.634 ERROR TaskSetManager:70 - Task 15 in stage 1.0 failed 1 times; aborting job; 00:11:09.810 WARN TaskSetManager:66 - Lost task 33.0 in stage 1.0 (TID 528, localhost): TaskKilled (killed intentionally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018:9957,Hash,HashMap,9957,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018,1,['Hash'],['HashMap']
Security,".so; 12:33:53.793 INFO CreateReadCountPanelOfNormals - ------------------------------------------------------------; 12:33:53.794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFacto",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5686:1754,secur,security,1754,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686,1,['secur'],['security']
Security,"/360061452132-GATK4-RNAseq-short-variant-discovery-SNPs-Indels-), but then for Haplotypecaller, and you have opened a bugreport to add a feature to ValidateVariants: https://github.com/broadinstitute/gatk/issues/6553. However, it would be nice if you could actually investigate the formatting error. Unfortunately my formatting error isn't the same as reported in the other post. I have 105 error in which the 1st alternative allele is a spanning deletion and the 2nd (and 3rd) is either an indel or snp. It's true that the 2nd and 3rd allele is actually not found in my samples. I even have 7 occurances in which the 1st allele (spanning deletion) has allele frequency 1.00. my code is the following for GenotypeGVCFs:. java -Xms32G -Xmx32G -jar ${gatk4} GenotypeGVCFs -R ${ref} -V ${pipeline}/${name}\_v4.1.6.0.g.vcf.gz -O ${vcf}/${name}\_v4.1.6.0.vcf.gz -L ${pipeline}/${name}\_intervals.list 2> ${log}/${name}\_v4.1.6.0\_genotype.log. for ValidateVariants:. java -Xms10G -Xmx10G -jar ${gatk4} ValidateVariants -R ${ref} -V ${name}\_v4.1.6.0.vcf.gz -L ${pipeline}/${name}\_intervals.list --warn-on-errors 2> ${log}/${name}\_v4.1.6.0\_genotype\_valivar.log. the warning in ValidateVariants and the site look like this:. 14:12:15.126 WARN ValidateVariants - \*\*\*\*\* Input 1st\_v4.1.6.0.vcf.gz fails strict validation of type ALL: one or more of the ALT allele(s) for the record at position chr\_1:1088200 are not observed at all in the sample genotypes \*\*\*\*\* ; ; chr\_1 1088200 . T \*,TAAAAAAAAAAAA 64.39 . AC=8,0;AF=0.667,0.00;AN=12;DP=118;ExcessHet=3.0103;FS=0.000;InbreedingCoeff=0.4286;MLEAC=7,7;MLEAF=0.583,0.583;MQ=58.73;QD=32.19;SOR=2.303 GT:AD:DP:GQ:PL ./.:9,0,0:9:.:0,0,0,0,0,0 0/0:9,0,0:9:0:0,0,113,0,113,113 ./.:10,0,0:10:.:0,0,0,0,0,0 ./.:5,0,0:5:.:0,0,0,0,0,0 1/1:0,0,1:1:0:225,15,0,15,0,0 ./.:0,0,0:0:.:0,0,0,0,0,0 ./.:12,0,0:12:.:0,0,0,0,0,0 ./.:8,0,0:8:.:0,0,0,0,0,0 0/0:3,0,0:3:0:0,0,43,0,43,43 ./.:7,0,0:7:.:0,0,0,0,0,0 ./.:1,0,0:1:.:0,0,0,0,0,0 ./.:0,0,0:0:.:0,0,0,0,0,0 .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6630:1662,Validat,ValidateVariants,1662,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6630,1,['Validat'],['ValidateVariants']
Security,"/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646119>:; >; > > + createBins();; > + }; > +; > + /**; > + * Generates binning coverage in the intervals given by the user.; > + * The width of bins, the intervals and the output file's path are given by the user.; > + */; > + public void createBins() {; > + // check if the output directory exists; > + if (!outputFile.exists() && !outputFile.mkdir()) {; > + throw new RuntimeException(""Unable to create file: "" + outputFile.getAbsolutePath());; > + }; > +; > + // check if the bin widths are set appropriately; > + if(widthOfBins <= 0) {; > + throw new IllegalArgumentException(""Width of bins "" + Integer.toString(widthOfBins) + "" should be >= 0."");; >; > @asmirnov <https://github.com/asmirnov> and @samuelklee; > <https://github.com/samuelklee> are both correct, but for the future in; > cases where you *would* want an IllegalArgumentException you should use; > Utils.validateArg to render this sort of thing a one-liner.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646132>:; >; > > + doc = ""width of the bins"",; > + fullName = WIDTH_OF_BINS_LONG_NAME,; > + shortName = WIDTH_OF_BINS_SHORT_NAME,; > + optional = true,; > + minValue = 1; > + ); > + private int widthOfBins = 1;; > +; > + @Argument(; > + doc = ""width of the padding regions"",; > + fullName = PADDING_LONG_NAME,; > + shortName = PADDING_SHORT_NAME,; > + optional = true,; > + minValue = 0; > + ); > + private int padding = 0;; >; > . . . and if this padding is different from the inherited padding then; > this demands a comment to avoid confusion.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646146>:; >; > > +",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211:4257,validat,validateArg,4257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211,1,['validat'],['validateArg']
Security,"/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/resources/large/sv_evidence_classifier.bin; - Data used for validation of performance in unit tests; src/test/resources/sv_classifier_test_data.json; src/test/resources/sv_features_test_data.json",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:3079,validat,validation,3079,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477,2,['validat'],['validation']
Security,"/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Defaults.BUFFER_SIZE : 131072; 17:43:53.162 INFO ValidateVariants - Defaults.COMPRESSION_LEVEL : 1; 17:43:53.162 INFO ValidateVariants - Defaults.CREATE_INDEX : false; 17:43:53.163 INFO ValidateVariants - Defaults.CREATE_MD5 : false; 17:43:53.163 INFO ValidateVariants - Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2509:2047,Validat,ValidateVariants,2047,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509,1,['Validat'],['ValidateVariants']
Security,"/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --maxIndelSize 250 -O zeta_snippet_leftalign_maxindelsize250.vcf.gz; 17:24:16.345 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 05, 2018 5:24:16 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 17:24:16.502 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 17:24:16.502 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-24-gb43bc27-SNAPSHOT; 17:24:16.502 INFO LeftAlignAndTrimVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:24:16.502 INFO LeftAlignAndTrimVariants - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 17:24:16.502 INFO LeftAlignAndTrimVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_111-b14; 17:24:16.503 INFO LeftAlignAndTrimVariants - Start Date/Time: September 5, 2018 5:24:16 PM EDT; 17:24:16.503 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 17:24:16.503 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 17:24:16.503 INFO LeftAlignAndTrimVariants - HTSJDK Version: 2.16.0; 17:24:16.503 INFO LeftAlignAndTrimVariants - Picard V",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418887543:1394,authenticat,authentication,1394,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418887543,1,['authenticat'],['authentication']
Security,"/com/intel/gkl/native/libgkl_compression.so; Oct 25, 2020 7:53:34 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:53:34.606 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.606 INFO ValidateVariants - The Genome Analysis Toolkit (GATK) v4.1.8.0; 19:53:34.606 INFO ValidateVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:53:34.607 INFO ValidateVariants - Executing as zepengmu@midway2-login1.rcc.local on Linux v3.10.0-1127.8.2.el7.x86_64 amd64; 19:53:34.607 INFO ValidateVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14; 19:53:34.607 INFO ValidateVariants - Start Date/Time: October 25, 2020 7:53:34 PM CDT; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - HTSJDK Version: 2.22.0; 19:53:34.607 INFO ValidateVariants - Picard Version: 2.22.8; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:53:34.608 INFO ValidateVariants - Deflater: IntelDeflater; 19:53:34.608 INFO ValidateVariants - Inflater: IntelInflater; 19:53:34.608 INFO ValidateVariants - GCS max retries/reopens: 20; 19:53:34.608 INFO ValidateVariants - Requester pays: disabled; 19:53:34.608 INFO ValidateVariants - Initializing engine; 19:53:35.169 INFO FeatureManager - Using codec VCFCodec to read file file://chr1-22.phased.rename.reheader.vcf.gz; 19:53:35.594 INFO ValidateVariants - Done initializing en",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911:1827,Validat,ValidateVariants,1827,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911,1,['Validat'],['ValidateVariants']
Security,"/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done initializing engine; 17:43:53.294 INFO ProgressMeter - Starting traversal; 17:43:53.294 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 17:43:53.302 INFO ValidateVariants - Shutting down engine; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=194510848; java.lang.IllegalArgumentException: Illegal base [] seen in the allele; 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:231); 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:374); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants.apply(ValidateVariants.java:181); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2509:4249,Validat,ValidateVariants,4249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509,1,['Validat'],['ValidateVariants']
Security,/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/complexEvents.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/gvcf.basepairResolution.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/NA12891.AS.chr20snippet_BAD_INCOMPLETE_REGION.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/NA12891.AS.chr20snippet.BAD_MISSING_NON_REF.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/NA12891.AS.chr20snippet.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/NA12891.AS.chr20snippet.missingrefblock.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleBad3.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleBad.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleRSIDonPositionNotInDBSNP.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationUnusedAllelesBugFix.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/validation/basicshortmutpileup/IS3.snv.indel.sv-vs-G15512.prenormal.sorted.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/validation/basicshortmutpileup/synthetic.challenge.set1.tumor-vs-synthetic.challenge.set1.normal-filtered.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/CalculateGenotypePosteriors/CEUtriMixedPloidyTest.vcf; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/CalculateGenotypePosteriors/CEUtrioPopPriorsTest_chr1.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/CalculateGenotype,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905:61376,Validat,ValidateVariants,61376,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905,2,"['Validat', 'validat']","['ValidateVariants', 'validationExampleGood']"
Security,/test/resources/org/broadinstitute/hellbender/tools/walkers/rnaseq/ASEReadCounter/NA12878.chr20_2444518_2637800.RNAseq.warnings.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/UnmarkDuplicates/allDuplicates.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/complexEvents_incompatibleDict.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/complexEvents_lexDict.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/complexEvents.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/gvcf.basepairResolution.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/NA12891.AS.chr20snippet_BAD_INCOMPLETE_REGION.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/NA12891.AS.chr20snippet.BAD_MISSING_NON_REF.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/NA12891.AS.chr20snippet.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/NA12891.AS.chr20snippet.missingrefblock.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleBad3.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleBad.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleRSIDonPositionNotInDBSNP.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationUnusedAllelesBugFix.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/validation/basicshortmutpileup/IS3.snv.indel.sv-vs-G15512.prenormal.sorted.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/vali,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905:60909,Validat,ValidateVariants,60909,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905,1,['Validat'],['ValidateVariants']
Security,"0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0	0|0; ```; Running `$HOME/gatk-4.0.11.0/gatk --java-options ""-Xmx4g"" HaplotypeCaller -R $HOME/GRCh37files/hs37d5.fa -I /mnt/fast/test.bam -O test.out.vcf.gz -L 22 --genotyping-mode GENOTYPE_GIVEN_ALLELES --alleles test.vcf.gz`, the resulting error is:; ```; java.lang.IllegalStateException: Allele in genotype GGTTTGTTT not in the variant context [GGTTTGTTT*, GGTTTGTTTGTTT, GGTTTGTTTGTTTGTTT, G]; at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:864); at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.simpleMerge(GATKVariantContextUtils.java:646); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.makeMergedVariantContext(AssemblyBasedCallerUtils.java:228); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:157); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:599); at org.broadinstitute.hellbende",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5355:41279,validat,validate,41279,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5355,1,['validat'],['validate']
Security,"0-01-07 16:51:53 SinglePassSamProgram Processed 2,000,000 records. Elapsed time: 00:01:01s. Time for last 1,000,000: 28s. Last read position: chr11:121,228,669 ; ; \[Tue Jan 07 16:52:25 PST 2020\] picard.analysis.CollectGcBiasMetrics done. Elapsed time: 4.10 minutes. ; ; Runtime.totalMemory()=4236247040 ; ; To get help, see [http://broadinstitute.github.io/picard/index.html#GettingHelp](http://broadinstitute.github.io/picard/index.html#GettingHelp) ; ; Exception in thread ""main"" htsjdk.samtools.SAMException: Exception counting mismatches for read XXXXXXXX0434501/1 32b aligned to chrX:51305151-51305182. ; ; at htsjdk.samtools.util.SequenceUtil.countMismatches(SequenceUtil.java:490) ; ; at htsjdk.samtools.util.SequenceUtil.countMismatches(SequenceUtil.java:466) ; ; at htsjdk.samtools.util.SequenceUtil.countMismatches(SequenceUtil.java:504) ; ; at picard.analysis.GcBiasMetricsCollector.addRead(GcBiasMetricsCollector.java:389) ; ; at picard.analysis.GcBiasMetricsCollector.access$600(GcBiasMetricsCollector.java:48) ; ; at picard.analysis.GcBiasMetricsCollector$PerUnitGcBiasMetricsCollector.addReadToGcData(GcBiasMetricsCollector.java:221) ; ; at picard.analysis.GcBiasMetricsCollector$PerUnitGcBiasMetricsCollector.acceptRecord(GcBiasMetricsCollector.java:155) ; ; at picard.analysis.GcBiasMetricsCollector$PerUnitGcBiasMetricsCollector.acceptRecord(GcBiasMetricsCollector.java:100) ; ; at picard.metrics.MultiLevelCollector$AllReadsDistributor.acceptRecord(MultiLevelCollector.java:192) ; ; at picard.metrics.MultiLevelCollector.acceptRecord(MultiLevelCollector.java:315) ; ; at picard.analysis.CollectGcBiasMetrics.acceptRead(CollectGcBiasMetrics.java:172) ; ; at picard.analysis.SinglePassSamProgram.makeItSo(SinglePassSamProgram.java:158) ; ; at picard.analysis.SinglePassSamProgram.doWork(SinglePassSamProgram.java:94) ; ; at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305) ; ; at picard.cmdline.PicardCommandLine.instanceMain(PicardCommandLine.java:103) ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6372:3158,access,access,3158,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6372,1,['access'],['access']
Security,"0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu Mar 07 16:08:17 UTC 2019] ValidateSamFile --INPUT CQ-NEQAS-2018.ILLUMINA.library.000000000-BCFDC.1.1.sorted.bam --MODE SUMMARY --MAX_OUTPUT 100 --IGNORE_WARNINGS false --VALIDATE_INDEX true --INDEX_VALIDATION_STRINGENCY EXHAUSTIVE --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --SKIP_MATE_VALIDATION false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. Last read position: chr9:32,633,613; INFO 2019-03-07 16:12:22 SamFileValidator Validated Read 20,000,000 records. Elapsed time: 00:03:58s. Time for last 10,000,000: 117s. Last read position: chrM:11,340; No errors found; [Thu Mar 07 16:13:05 UTC 2019] picard.sam.ValidateSamFile done. Elapsed time: 4.79 minutes.; Runtime.totalMemory()=2602041344; Tool returned:; 0; ```. But when run BaseRecalibrator got the _fromIndex toIndex_ error:; `gatk BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz`; ```; ERROR: return code 3; STDERR:; 15:46",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5807:1751,Validat,ValidateSamFile,1751,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807,2,"['Validat', 'validat']","['ValidateSamFile', 'validation']"
Security,"0/gatk-package-4.1.8.0-local.jar ValidateVariants -V ../../data/geno/phased/chr1-22.phased.rename.reheader.vcf.gz -R ../../../../index/hg19.fa.gz; 19:53:34.379 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/zepengmu/tools/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 25, 2020 7:53:34 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:53:34.606 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.606 INFO ValidateVariants - The Genome Analysis Toolkit (GATK) v4.1.8.0; 19:53:34.606 INFO ValidateVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:53:34.607 INFO ValidateVariants - Executing as zepengmu@midway2-login1.rcc.local on Linux v3.10.0-1127.8.2.el7.x86_64 amd64; 19:53:34.607 INFO ValidateVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14; 19:53:34.607 INFO ValidateVariants - Start Date/Time: October 25, 2020 7:53:34 PM CDT; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - HTSJDK Version: 2.22.0; 19:53:34.607 INFO ValidateVariants - Picard Version: 2.22.8; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:53:34.608 INFO ValidateVariants - Deflater: IntelDeflater; 19:53:34.608 INFO ValidateVariants - Inflater: IntelInflater; 19:53:34.608 INFO ValidateVariants - GCS max retries/reopens: 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911:1541,Validat,ValidateVariants,1541,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911,1,['Validat'],['ValidateVariants']
Security,000195.1 (182896 bp); 23:38:14.600 DEBUG GenomeLocParser - GL000212.1 (186858 bp); 23:38:14.600 DEBUG GenomeLocParser - GL000222.1 (186861 bp); 23:38:14.601 DEBUG GenomeLocParser - GL000200.1 (187035 bp); 23:38:14.601 DEBUG GenomeLocParser - GL000193.1 (189789 bp); 23:38:14.601 DEBUG GenomeLocParser - GL000194.1 (191469 bp); 23:38:14.601 DEBUG GenomeLocParser - GL000225.1 (211173 bp); 23:38:14.601 DEBUG GenomeLocParser - GL000192.1 (547496 bp); 23:38:14.601 DEBUG GenomeLocParser - NC_007605 (171823 bp); 23:38:14.601 DEBUG GenomeLocParser - hs37d5 (35477943 bp); 23:38:15.218 INFO IntervalArgumentCollection - Processing 13461 bp from intervals; 23:38:15.222 INFO GermlineCNVCaller - No annotated intervals were provided...; 23:38:15.222 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 23:38:15.235 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 23:38:15.236 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 23:38:15.237 INFO GermlineCNVCaller - Aggregating read-count file /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/371827342/P0000335.b37.counts.hdf5 (1 / 5); 23:38:15.653 INFO GermlineCNVCaller - Aggregating read-count file /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/-1425124017/P0000480.b37.counts.hdf5 (2 / 5); 23:38:15.905 INFO GermlineCNVCaller - Aggregating read-count file /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/1072891920/P0000481.b37.counts.hdf5 (3 / 5); 23:38:16.174 INFO GermlineCNVCaller - Aggregating read-count file /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:11805,Validat,Validating,11805,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Validat'],['Validating']
Security,"04_random, chr17_gl000205_random, chr17_gl000206_random, chr18_gl000207_random, chr19_gl000208_random, chr19_gl000209_random, chr21_gl000210_random, chrUn_gl000211, chrUn_gl000212, chrUn_gl000213, chrUn_gl000214, chrUn_gl000215, chrUn_gl000216, chrUn_gl000217, chrUn_gl000218, chrUn_gl000219, chrUn_gl000220, chrUn_gl000221, chrUn_gl000222, chrUn_gl000223, chrUn_gl000224, chrUn_gl000225, chrUn_gl000226, chrUn_gl000227, chrUn_gl000228, chrUn_gl000229, chrUn_gl000230, chrUn_gl000231, chrUn_gl000232, chrUn_gl000233, chrUn_gl000234, chrUn_gl000235, chrUn_gl000236, chrUn_gl000237, chrUn_gl000238, chrUn_gl000239, chrUn_gl000240, chrUn_gl000241, chrUn_gl000242, chrUn_gl000243, chrUn_gl000244, chrUn_gl000245, chrUn_gl000246, chrUn_gl000247, chrUn_gl000248, chrUn_gl000249]; reads contigs = []; 	at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:163); 	at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:98); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.validateToolInputs(GATKSparkTool.java:469); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:361); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:233); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccess",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:34739,validat,validateDictionaries,34739,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['validat'],['validateDictionaries']
Security,05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:33); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:112); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:106); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:5150,access,access,5150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['access'],['access']
Security,"0:00.685 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:00:00.685 INFO PostprocessGermlineCNVCalls - Deflater: IntelDeflater; 17:00:00.685 INFO PostprocessGermlineCNVCalls - Inflater: IntelInflater; 17:00:00.685 INFO PostprocessGermlineCNVCalls - GCS max retries/reopens: 20; 17:00:00.685 INFO PostprocessGermlineCNVCalls - Requester pays: disabled; 17:00:00.685 INFO PostprocessGermlineCNVCalls - Initializing engine; 17:00:04.480 INFO PostprocessGermlineCNVCalls - Done initializing engine; 17:00:07.582 INFO PostprocessGermlineCNVCalls - Shutting down engine; [October 29, 2020 5:00:07 PM MSK] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=2468347904; java.lang.IllegalArgumentException: Records were not strictly sorted in dictionary order.; 	at org.broadinstitute.hellbender.tools.copynumber.arguments.CopyNumberArgumentValidationUtils.validateIntervals(CopyNumberArgumentValidationUtils.java:60); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractLocatableCollection.getShardedCollectionSortOrder(AbstractLocatableCollection.java:142); 	at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.onTraversalStart(PostprocessGermlineCNVCalls.java:297); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1047); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /home/lmbs02/bio/biosoft/gatk/gatk-4.1.9.0/gat",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-718787427:2939,validat,validateIntervals,2939,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-718787427,1,['validat'],['validateIntervals']
Security,"0:05:57.036 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 00:07:26.967 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr16:10185 the annotation AS_VarDP=59|115|0 was not a numerical value and was ignored; 00:07:26.967 WARN ReferenceConfidenceVariantContextMerger - Reducible annotation 'AS_VarDP' detected, add -G StandardAnnotation -G AS_StandardAnnotation to the command to annotate in the final VC with this annotation.; 00:07:26.991 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.02938786500000001,Cpu time(s),0.029037034000000003; [August 25, 2021 12:07:27 AM EDT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 1.55 minutes.; Runtime.totalMemory()=1807745024; java.lang.NullPointerException; at java.util.HashMap.putMapEntries(HashMap.java:500); at java.util.HashMap.putAll(HashMap.java:784); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.combineAnnotations(VariantAnnotatorEngine.java:211); at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.mergeAttributes(ReferenceConfidenceVariantContextMerger.java:318); at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:142); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFsEngine.callRegion(GenotypeGVCFsEngine.java:130); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.apply(GenotypeGVCFs.java:281); at org.broadinstitute.hellbender.engine.VariantLocusWalker.lambda$traverse$0(VariantLocusWalker.java:135); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.uti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7437:8102,Hash,HashMap,8102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437,1,['Hash'],['HashMap']
Security,"0; 01:13:16.078 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 01:13:16.078 INFO HaplotypeCaller - Initializing engine; 01:13:17.087 INFO HaplotypeCaller - Shutting down engine; [January 18, 2020 1:13:17 AM IST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2216689664; java.lang.NullPointerException; at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getContigNames(SequenceDictionaryUtils.java:463); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getCommonContigsByName(SequenceDictionaryUtils.java:457); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.compareDictionaries(SequenceDictionaryUtils.java:234); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:150); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:98); at org.broadinstitute.hellbender.engine.GATKTool.validateSequenceDictionaries(GATKTool.java:621); at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:563); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:160); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:275). I have tried with different versions of Java, still it persists.; Please suggest any solution.; Thank you.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6384:2788,validat,validateDictionaries,2788,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6384,2,['validat'],"['validateDictionaries', 'validateSequenceDictionaries']"
Security,0> (ø)` | :arrow_down: |; | [...tools/funcotator/dataSources/TableFuncotation.java](https://codecov.io/gh/broadinstitute/gatk/pull/4276/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL1RhYmxlRnVuY290YXRpb24uamF2YQ==) | `60% <100%> (ø)` | `20 <0> (ø)` | :arrow_down: |; | [.../tools/copynumber/utils/MergeAnnotatedRegions.java](https://codecov.io/gh/broadinstitute/gatk/pull/4276/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL01lcmdlQW5ub3RhdGVkUmVnaW9ucy5qYXZh) | `100% <100%> (ø)` | `3 <3> (?)` | |; | [...ils/annotatedinterval/AnnotatedIntervalHeader.java](https://codecov.io/gh/broadinstitute/gatk/pull/4276/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL2Fubm90YXRlZGludGVydmFsL0Fubm90YXRlZEludGVydmFsSGVhZGVyLmphdmE=) | `100% <100%> (ø)` | `6 <6> (?)` | |; | [...tmutpileup/ValidateBasicSomaticShortMutations.java](https://codecov.io/gh/broadinstitute/gatk/pull/4276/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9WYWxpZGF0ZUJhc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zLmphdmE=) | `85.965% <100%> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...nder/tools/copynumber/utils/TagGermlineEvents.java](https://codecov.io/gh/broadinstitute/gatk/pull/4276/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL1RhZ0dlcm1saW5lRXZlbnRzLmphdmE=) | `100% <100%> (ø)` | `3 <3> (?)` | |; | [...ataSources/xsv/LocatableXsvFuncotationFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4276/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL3hzdi9Mb2NhdGFibGVYc3ZGdW5jb3RhdGlvbkZhY3RvcnkuamF2YQ==) | `85.185% <61.538%> (+0.491%)` | `24 <0> (-4)` | :arrow_down: |; | [...g/broadi,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4276#issuecomment-361355746:2779,Validat,ValidateBasicSomaticShortMutations,2779,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4276#issuecomment-361355746,1,['Validat'],['ValidateBasicSomaticShortMutations']
Security,"100 KB.; 20:38:27.207 INFO StructuralVariationDiscoveryPipelineSpark - Processing 501267 raw alignments from 426041 contigs.; 18/01/12 20:38:27 WARN org.apache.spark.scheduler.TaskSetManager: Stage 20 contains a task of very large size (2518 KB). The maximum recommended task size is 100 KB.; 20:38:35.835 INFO StructuralVariationDiscoveryPipelineSpark - Primitive filtering based purely on MQ left 339065 contigs.; 20:38:37.378 INFO StructuralVariationDiscoveryPipelineSpark - 17574 contigs with chimeric alignments potentially giving SV signals.; 18/01/12 20:38:37 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 284.0 in stage 25.0 (TID 43189, cw-test-w-6.c.broad-dsde-methods.internal, executor 7): java.lang.IllegalArgumentException: two input alignments' overlap on read consumes completely one of them.	1_1097_chrUn_JTFH01000492v1_decoy:501-1597_+_1097M6H_60_1_1092_O	483_612_chr17:26962677-26962806_-_482S130M491S_60_-1_281_S; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.ContigAlignmentsModifier.removeOverlap(ContigAlignmentsModifier.java:36); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.AssemblyContigAlignmentSignatureClassifier.lambda$processContigsWithTwoAlignments$e28aa838$1(AssemblyContigAlignmentSignatureClassifier.java:114); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.collection.ExternalSor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:3305,validat,validateArg,3305,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['validat'],['validateArg']
Security,"11:26:21.683 INFO Concordance - Deflater: IntelDeflater ; ; 11:26:21.684 INFO Concordance - Inflater: IntelInflater ; ; 11:26:21.684 INFO Concordance - GCS max retries/reopens: 20 ; ; 11:26:21.684 INFO Concordance - Requester pays: disabled ; ; 11:26:21.684 INFO Concordance - Initializing engine ; ; 11:26:22.217 INFO FeatureManager - Using codec VCFCodec to read file file:///scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/NA12878.vcf.gz ; ; 11:26:22.497 INFO FeatureManager - Using codec VCFCodec to read file file:///scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/sample1\_affect.filtered.vcf ; ; 11:26:22.663 INFO Concordance - Done initializing engine ; ; 11:26:22.672 INFO ProgressMeter - Starting traversal ; ; 11:26:22.672 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute ; ; 11:26:22.682 INFO Concordance - Shutting down engine ; ; \[November 11, 2021 11:26:22 AM CET\] org.broadinstitute.hellbender.tools.walkers.validation.Concordance done. Elapsed time: 0.02 minutes. ; ; Runtime.totalMemory()=559939584 ; ; java.lang.NullPointerException ; ; at htsjdk.variant.variantcontext.VariantContextComparator.compare(VariantContextComparator.java:87) ; ; at org.broadinstitute.hellbender.engine.AbstractConcordanceWalker$ConcordanceIterator.next(AbstractConcordanceWalker.java:192) ; ; at org.broadinstitute.hellbender.engine.AbstractConcordanceWalker$ConcordanceIterator.next(AbstractConcordanceWalker.java:174) ; ; at java.util.Iterator.forEachRemaining(Iterator.java:116) ; ; at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ; ; at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580) ; ; at org.broadinstitute.hellbender.engine.AbstractConcordanceWalker.traverse(AbstractConcordanceWalker.java:132) ; ; at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7562:4391,validat,validation,4391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7562,1,['validat'],['validation']
Security,"12 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (164726 MB per container); 17/10/11 14:19:12 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead; 17/10/11 14:19:12 INFO yarn.Client: Setting up container launch context for our AM; 17/10/11 14:19:12 INFO yarn.Client: Setting up the launch environment for our AM container; 17/10/11 14:19:12 INFO yarn.Client: Preparing resources for our AM container; 17/10/11 14:19:12 INFO gcs.GoogleHadoopFileSystemBase: GHFS version: 1.6.1-hadoop2; 17/10/11 14:19:12 INFO yarn.Client: Uploading resource file:/tmp/hdfs/spark-8c88439f-dcb0-48b2-86f3-fc82cef4c438/__spark_conf__8945422067005652415.zip -> hdfs://mg:8020/user/hdfs/.sparkStaging/application_1507683879816_0006/__spark_conf__8945422067005652415.zip; 17/10/11 14:19:13 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/11 14:19:13 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/11 14:19:13 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); users with modify permissions: Set(hdfs); 17/10/11 14:19:13 INFO yarn.Client: Submitting application 6 to ResourceManager; 17/10/11 14:19:13 INFO impl.YarnClientImpl: Submitted application application_1507683879816_0006; 17/10/11 14:19:14 INFO yarn.Client: Application report for application_1507683879816_0006 (state: ACCEPTED); 17/10/11 14:19:14 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: root.users.hdfs; 	 start time: 1507702753100; 	 final status: UNDEFINED; 	 tracking URL: http://mg:8088/proxy/application_1507683879816_0006/; 	 user: hdfs; 17/10/11 14:19:15 INFO yarn.Client: Application report for application_1507683879816_0006 (state: ACCEPTED); 17/10/11 14:19:15 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster reg",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:6228,Secur,SecurityManager,6228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['Secur'],['SecurityManager']
Security,12892.readnamesort.bam -R hdfs://sn1:8020/user/$USER/gatk/human_g1k_v37.fasta -O hdfs://sn1:8020/user/$USER/gatk/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -- --sparkRunner SPARK --sparkMaster spark://sn1:7077 --driver-memory 8G --num-executors 4 --executor-cores 9 --executor-memory 27g; ```. ```; java.lang.NullPointerException; at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getContigNames(SequenceDictionaryUtils.java:464); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getCommonContigsByName(SequenceDictionaryUtils.java:458); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.compareDictionaries(SequenceDictionaryUtils.java:234); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:150); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:98); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.validateToolInputs(GATKSparkTool.java:402); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:312); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSub,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2020:1127,validat,validateToolInputs,1127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2020,1,['validat'],['validateToolInputs']
Security,"12:11:34.899 INFO PlotACNVResults - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 12:11:34.899 INFO PlotACNVResults - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:11:34.899 INFO PlotACNVResults - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 12:11:34.899 INFO PlotACNVResults - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:11:34.899 INFO PlotACNVResults - Defaults.USE_CRAM_REF_DOWNLOAD : false; 12:11:34.900 INFO PlotACNVResults - Deflater IntelDeflater; 12:11:34.900 INFO PlotACNVResults - Initializing engine; 12:11:34.900 INFO PlotACNVResults - Done initializing engine; 12:11:35.009 INFO PlotACNVResults - Shutting down engine; [February 15, 2017 12:11:35 PM EST] org.broadinstitute.hellbender.tools.exome.plotting.PlotACNVResults done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=1502085120; java.lang.IllegalArgumentException: There must be at least one contig above the threshold length in the sequence dictionary.; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:673); 	at org.broadinstitute.hellbender.tools.exome.plotting.PlotACNVResults.doWork(PlotACNVResults.java:120); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); ```. ---. @samuelklee commented on [Wed Feb 22 2017](https://github.com/broadinstitute/gatk-protected/issues/902#issuecomment-281814944). Hmm, perhaps I should not be using `ReferenceUtils.loadFastaDictionary` to load the dictionary, or this method needs to check that the file it is loading is ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2941:2953,validat,validateArg,2953,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2941,1,['validat'],['validateArg']
Security,"14:50:59.205 INFO FilterMutectCalls - Deflater: IntelDeflater; 14:50:59.205 INFO FilterMutectCalls - Inflater: IntelInflater; 14:50:59.205 INFO FilterMutectCalls - GCS max retries/reopens: 20; 14:50:59.205 INFO FilterMutectCalls - Requester pays: disabled; 14:50:59.205 INFO FilterMutectCalls - Initializing engine; 14:51:00.692 INFO FeatureManager - Using codec VCFCodec to read file file:///workdir/mparment/data/process/A2683/PTC2_unfiltered.vcf.gz; 14:51:01.406 INFO FilterMutectCalls - Done initializing engine; 14:51:02.360 INFO FilterMutectCalls - Shutting down engine; [December 12, 2020 2:51:02 PM CET] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2385510400; java.lang.IllegalStateException: Duplicate key 7.395307178412063E-4; at java.util.stream.Collectors.lambda$throwingMerger$138(Collectors.java:133); at java.util.stream.Collectors$$Lambda$67/403388441.apply(Unknown Source); at java.util.HashMap.merge(HashMap.java:1245); at java.util.stream.Collectors.lambda$toMap$196(Collectors.java:1320); at java.util.stream.Collectors$$Lambda$69/854719230.accept(Unknown Source); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.<init>(ContaminationFilter.java:26); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6996:3674,Hash,HashMap,3674,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6996,1,['Hash'],['HashMap']
Security,"15:18.986 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 03:15:18.987 INFO PostprocessGermlineCNVCalls - Deflater: IntelDeflater; 03:15:18.988 INFO PostprocessGermlineCNVCalls - Inflater: IntelInflater; 03:15:18.988 INFO PostprocessGermlineCNVCalls - GCS max retries/reopens: 20; 03:15:18.989 INFO PostprocessGermlineCNVCalls - Requester pays: disabled; 03:15:18.990 INFO PostprocessGermlineCNVCalls - Initializing engine; 03:15:43.480 INFO PostprocessGermlineCNVCalls - Done initializing engine; 03:15:47.833 INFO PostprocessGermlineCNVCalls - Shutting down engine; [April 15, 2024, 3:15:47 AM CST] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 0.49 minutes.; Runtime.totalMemory()=1207959552; java.lang.IllegalArgumentException: Records were not strictly sorted in dictionary order.; 	at org.broadinstitute.hellbender.tools.copynumber.arguments.CopyNumberArgumentValidationUtils.validateIntervals(CopyNumberArgumentValidationUtils.java:74); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractLocatableCollection.getShardedCollectionSortOrder(AbstractLocatableCollection.java:142); 	at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.onTraversalStart(PostprocessGermlineCNVCalls.java:388); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1096); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /data/xiangxd/project/software/callers/gatk_4.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8776:21332,validat,validateIntervals,21332,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776,1,['validat'],['validateIntervals']
Security,166); 	at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148); 	at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136); 	at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237); 	at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49); 	at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517); 	at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500); 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); 	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134); 	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134); 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); 	at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500); 	at org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175); 	at org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238); 	at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631); 	at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:355); 	at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:307); 	at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:306); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:306); 	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162); 	at org.apac,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7035:7102,Hash,HashMap,7102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7035,1,['Hash'],['HashMap']
Security,"19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:53:34.608 INFO ValidateVariants - Deflater: IntelDeflater; 19:53:34.608 INFO ValidateVariants - Inflater: IntelInflater; 19:53:34.608 INFO ValidateVariants - GCS max retries/reopens: 20; 19:53:34.608 INFO ValidateVariants - Requester pays: disabled; 19:53:34.608 INFO ValidateVariants - Initializing engine; 19:53:35.169 INFO FeatureManager - Using codec VCFCodec to read file file://chr1-22.phased.rename.reheader.vcf.gz; 19:53:35.594 INFO ValidateVariants - Done initializing engine; 19:53:35.594 WARN ValidateVariants - IDS validation cannot be done because no DBSNP file was provided; 19:53:35.594 WARN ValidateVariants - Other possible validations will still be performed; 19:53:35.594 INFO ProgressMeter - Starting traversal; 19:53:35.595 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 19:53:35.660 INFO ValidateVariants - Shutting down engine; [October 25, 2020 7:53:35 PM CDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2114453504; java.lang.ArrayIndexOutOfBoundsException: -87; 	at org.broadinstitute.hellbender.utils.BaseUtils.convertIUPACtoN(BaseUtils.java:123); 	at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.getSubsequenceAt(CachingIndexedFastaSequenceFile.java:340); 	at org.broadinstitute.hellbender.engine.ReferenceFileSource.queryAndPrefetch(ReferenceFileSource.java:78); 	at org.broadinstitute.hellbender.engine.ReferenceDataSource.queryAndPrefetch(ReferenceDataSource.java:64); 	at org.broadinstitute.hellbender.engine.ReferenceContext.getBases(ReferenceContext.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911:2995,Validat,ValidateVariants,2995,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911,2,"['Validat', 'validat']","['ValidateVariants', 'validations']"
Security,2); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at org.apache.hadoop.ipc.Client.call(Client.java:1475); 	at org.apache.hadoop.ipc.Client.call(Client.java:1412); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229); 	at com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:255); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191); 	at org.ap,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:9751,secur,security,9751,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294,1,['secur'],['security']
Security,2); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:237); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:488); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:468); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:458); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLinePro,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:3593,secur,security,3593,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294,1,['secur'],['security']
Security,2); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106); 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73); 	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1228); 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1213); 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1201); 	at org.apache.hadoop.hdfs.DFSInputStream.fe,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:6510,secur,security,6510,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294,1,['secur'],['security']
Security,2); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apa,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:1930,secur,security,1930,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294,1,['secur'],['security']
Security,2.936 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 15:47:53.565 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:19555 0.2 90 508.0; 15:48:05.962 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:136820 0.4 600 1563.5; 15:48:16.023 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:360783 0.6 1560 2828.9; 15:48:19.342 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.010346494000000001; 15:48:19.342 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 6.453042841; 15:48:19.347 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 10.39 sec; 15:48:19.348 INFO Mutect2 - Shutting down engine; [28 novembre 2019 15:48:19 CET] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.72 minutes.; Runtime.totalMemory()=3822583808; java.lang.IllegalArgumentException: Cannot construct fragment from more than two reads; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:725); 	at org.broadinstitute.hellbender.utils.read.Fragment.create(Fragment.java:36); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods.groupEvidence(AlleleLikelihoods.java:595); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:93); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:251); 	at or,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558:3831,validat,validateArg,3831,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558,1,['validat'],['validateArg']
Security,"2833200,787;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. #### Steps to reproduce; * init; ```; hg19=pipeline/hg19/hg19_chM_male_mask.fa; ```; * reproduce of 4.0.8.1; ```; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.gvcf -ERC GVCF && tail target.4.0.8.1.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.vcf && tail target.4.0.8.1.vcf; ```; * reproduce of 4.0.9.0; ```; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.gvcf -ERC GVCF && tail target.4.0.9.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.vcf && tail target.4.0.9.0.vcf; ```; * reproduce of 4.1.2.0; ```; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.gvcf -ERC GVCF && tail target.4.1.2.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.vcf && tail target.4.1.2.0.vcf; ```. #### Expected behavior; 1. expose the rule that the second variant (T>TAAAA) filtered (especially for version 4.0.9.0).; 2. give the right QUAL of the second variant; 3. then this type of variant can be retain in VCF as default operation or with some addition parameters.; 4. can GATK have ability to detect the `real` variant such as TTT>AAAA. #### Actual behavior; ~~_Tell us what happens instead_~~; unknown",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5975:7132,expose,expose,7132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975,1,['expose'],['expose']
Security,2:40.673 INFO Funcotator - HTSJDK Version: 2.20.1; 06:42:40.673 INFO Funcotator - Picard Version: 2.20.5; 06:42:40.673 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 06:42:40.673 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 06:42:40.673 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 06:42:40.673 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 06:42:40.673 INFO Funcotator - Deflater: IntelDeflater; 06:42:40.674 INFO Funcotator - Inflater: IntelInflater; 06:42:40.674 INFO Funcotator - GCS max retries/reopens: 20; 06:42:40.674 INFO Funcotator - Requester pays: disabled; 06:42:40.674 INFO Funcotator - Initializing engine; 06:42:41.406 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/GenomicsDBImport/200923_A00268_0517_AHKL37DSXY/Set20-5_L2_159A59.somatic.filterMutectCalls.vcf.gz; 06:42:41.561 INFO Funcotator - Done initializing engine; 06:42:41.561 INFO Funcotator - Validating Sequence Dictionaries...; 06:42:41.589 INFO Funcotator - Processing user transcripts/defaults/overrides...; 06:42:41.590 INFO Funcotator - Initializing data sources...; 06:42:41.594 INFO DataSourceUtils - Initializing data sources from directory: /data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g; 06:42:41.596 INFO DataSourceUtils - Data sources version: 1.7.2020521g; 06:42:41.597 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200521g.tar.gz; 06:42:41.597 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200521.tar.gz; 06:42:41.609 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7090:2643,Validat,Validating,2643,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090,1,['Validat'],['Validating']
Security,2@cb2-VirtualBox:~/gatk$ ./gradlew bundle --stacktrace; > Task :gatkDoc FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/cb2/gatk/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --info or --debug option to get more log output. Run with --scan to get full insights. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':gatkDoc'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$3.accept(ExecuteActionsTaskExecuter.java:166); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$3.accept(ExecuteActionsTaskExecuter.java:163); at org.gradle.internal.Try$Failure.ifSuccessfulOrElse(Try.java:191); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:156); at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:62); at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:108); at org.gradle.api.internal.tasks.execution.ResolveBeforeExecutionOutputsTaskExecuter.execute(ResolveBeforeExecutionOutputsTaskExecuter.java:67); at org.gradle.api.internal.tasks.execution.ResolveAfterPreviousExecutionStateTaskExecuter.execute(ResolveAfterPreviousExecutionStateTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute(CleanupStaleOutputsExecuter.java:94); at org.gradle.api.internal.tasks.execution.FinalizePropertiesTaskExecuter.execute(FinalizePropertiesTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.ResolveTaskExecutionModeExecuter.execute(ResolveTaskExecutionModeExecuter.java:95); at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:57); at org.gradle.api.internal.tasks,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716:2077,Validat,ValidatingTaskExecuter,2077,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716,1,['Validat'],['ValidatingTaskExecuter']
Security,2xvdWQuamF2YQ==) | `70.811% <0%> (ø)` | `40 <0> (ø)` | :arrow_down: |; | [...rg/broadinstitute/hellbender/utils/IndexUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9JbmRleFV0aWxzLmphdmE=) | `80.702% <100%> (ø)` | `16 <2> (ø)` | :arrow_down: |; | [...kers/variantutils/UpdateVCFSequenceDictionary.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9VcGRhdGVWQ0ZTZXF1ZW5jZURpY3Rpb25hcnkuamF2YQ==) | `86.957% <100%> (ø)` | `14 <0> (ø)` | :arrow_down: |; | [...itute/hellbender/tools/walkers/SplitIntervals.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL1NwbGl0SW50ZXJ2YWxzLmphdmE=) | `88.235% <100%> (ø)` | `6 <2> (ø)` | :arrow_down: |; | [...der/tools/walkers/variantutils/SelectVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9TZWxlY3RWYXJpYW50cy5qYXZh) | `80.663% <100%> (ø)` | `125 <0> (ø)` | :arrow_down: |; | [...broadinstitute/hellbender/engine/FeatureInput.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZUlucHV0LmphdmE=) | `94.203% <100%> (ø)` | `16 <0> (ø)` | :arrow_down: |; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYWxpZGF0ZVZhcmlhbnRzLmphdmE=) | `82.857% <66.667%> (ø)` | `34 <0> (ø)` | :arrow_down: |; | ... and [3 more](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3989#issuecomment-352845785:3650,Validat,ValidateVariants,3650,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3989#issuecomment-352845785,1,['Validat'],['ValidateVariants']
Security,"3.793 INFO CreateReadCountPanelOfNormals - ------------------------------------------------------------; 12:33:53.794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:178)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5686:1763,authenticat,authentication,1763,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686,1,['authenticat'],['authentication']
Security,30:54.424 INFO BwaAndMarkDuplicatesPipelineSpark - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 18:30:54.424 INFO BwaAndMarkDuplicatesPipelineSpark - Initializing engine; 18:30:54.424 INFO BwaAndMarkDuplicatesPipelineSpark - Done initializing engine; 18/01/09 18:30:54 INFO spark.SparkContext: Running Spark version 2.2.0.cloudera1; 18/01/09 18:30:54 INFO spark.SparkContext: Submitted application: BwaAndMarkDuplicatesPipelineSpark; 18/01/09 18:30:54 INFO spark.SecurityManager: Changing view acls to: sun; 18/01/09 18:30:54 INFO spark.SecurityManager: Changing modify acls to: sun; 18/01/09 18:30:54 INFO spark.SecurityManager: Changing view acls groups to: ; 18/01/09 18:30:54 INFO spark.SecurityManager: Changing modify acls groups to: ; 18/01/09 18:30:54 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(sun); groups with view permissions: Set(); users with modify permissions: Set(sun); groups with modify permissions: Set(); 18/01/09 18:30:55 INFO util.Utils: Successfully started service 'sparkDriver' on port 38793.; 18/01/09 18:30:55 INFO spark.SparkEnv: Registering MapOutputTracker; 18/01/09 18:30:55 INFO spark.SparkEnv: Registering BlockManagerMaster; 18/01/09 18:30:55 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/01/09 18:30:55 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/01/09 18:30:55 INFO storage.DiskBlockManager: Created local directory at /tmp/sun/blockmgr-b03058dc-763a-449c-bd05-18f3304c01ea; 18/01/09 18:30:55 INFO memory.MemoryStore: MemoryStore started with capacity 2004.6 MB; 18/01/09 18:30:55 INFO spark.SparkEnv: Registering OutputCommitCoordinator; 18/01/09 18:30:55 INFO util.log: Logging initialized @25356ms; 18/01/09 18:30:55 INFO server.Server: jetty-9.3.z-SNAPSHOT; 18/01/09 ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:5677,Secur,SecurityManager,5677,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,3,"['Secur', 'authenticat']","['SecurityManager', 'authentication']"
Security,32 INFO FeatureManager - Using codec IntervalListCodec to read file file:///gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list; 23:44:42.739 DEBUG FeatureDataSource - Cache statistics for FeatureInput /gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list:/gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list:; 23:44:42.740 DEBUG FeatureCache - Cache hit rate was 0.00% (0 out of 0 total queries); 23:44:42.743 INFO IntervalArgumentCollection - Processing 1022379 bp from intervals; 23:44:42.756 INFO GermlineCNVCaller - Reading and validating annotated intervals...; 23:44:43.119 INFO GermlineCNVCaller - GC-content annotations for intervals found; explicit GC-bias correction will be performed...; 23:44:43.160 INFO GermlineCNVCaller - Running the tool in COHORT mode...; 23:44:43.160 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 23:44:43.160 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 23:44:43.160 DEBUG GenomeLocParser - chr1 (248956422 bp); 23:44:43.161 DEBUG GenomeLocParser - chr2 (242193529 bp); 23:44:43.161 DEBUG GenomeLocParser - chr3 (198295559 bp); 23:44:43.161 DEBUG GenomeLocParser - chr4 (190214555 bp); 23:44:43.161 DEBUG GenomeLocParser - chr5 (181538259 bp); 23:44:43.161 DEBUG GenomeLocParser - chr6 (170805979 bp); 23:44:43.161 DEBUG GenomeLocParser - chr7 (159345973 bp); 23:44:43.161 DEBUG GenomeLocParser - chr8 (145138636 bp); 23:44:43.161 DEBUG GenomeLocParser - chr9 (138394717 bp); 23:44:43.161 DEBUG GenomeLocParser - chr10 (133797422 bp); 23:44:43.161 DEBUG GenomeLocParser - chr11 (135086622 bp); 23:44:43.161 DEBUG GenomeLocParser - chr12 (133275309 bp); 23:44:43.161 DEBUG GenomeLocParser - chr13 (114364328 bp); 23:44:43.161 DEBUG GenomeLocParser - chr14 (107043718 bp); 23:44:43.161 DEBUG GenomeLocParser - chr15 (1019911,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:20244,Validat,Validating,20244,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Validat'],['Validating']
Security,321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:54:55 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:54:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:54:56 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls to: userx; 18/04/24 17:54:56 INFO SecurityManager: Changing modify acls to: userx; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:54:56 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:54:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(userx); groups with view permissions: Set(); users with modify permissions: Set(userx); groups with modify permissions: Set(); 18/04/24 17:54:57 INFO Utils: Successfully started service 'sparkDriver' on port 59501.; 18/04/24 17:54:57 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:54:57 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:54:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/24 17:54:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/24 17:54:57 INFO DiskBlockManager: Created local directory at /tmp/userx/blockmgr-213553f6-dd2d-455d-85ef-3ed03ae12f7f; 18/04/24 17:54:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 18/04/24 17:54:57 INFO SparkEnv: Registering OutputCommitCoordin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:7915,Secur,SecurityManager,7915,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,7,"['Secur', 'authenticat']","['SecurityManager', 'authentication']"
Security,"334:50070; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/output/:/output/; - /data/ngs/:/ngs/; datanode:; image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - datanode:/hadoop/dfs/data; environment:; SERVICE_PRECONDITION: ""namenode:50070""; # depends_on:; # - namenode; env_file:; - ./hadoop.env; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50075. volumes:; datanode:; namenode:. networks:; workbench:; external: true; ```; the datanodes and namenode and spark master and workers are all working.; My hardware resources are:; 16 core and 1Tb memory ssd and 56Gb ram for 3 machines. I have this problem when I launch the version(GATK) v4.0.4.0 but not with this version v4.0.2.0-4-gb59d863-SNAPSHOT:. >java.lang.IllegalStateException: Duplicate key -1; 	at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$mark$2142e97f$1(MarkDuplicatesSpark.java:82); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(J",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:3705,Hash,HashMap,3705,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,1,['Hash'],['HashMap']
Security,35% <ø> (ø)` | `6 <0> (ø)` | :arrow_down: |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4070/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `22.807% <ø> (ø)` | `2 <0> (ø)` | :arrow_down: |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4070/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `66.667% <ø> (ø)` | `2 <0> (ø)` | :arrow_down: |; | [...k/pipelines/BwaAndMarkDuplicatesPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4070/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvQndhQW5kTWFya0R1cGxpY2F0ZXNQaXBlbGluZVNwYXJrLmphdmE=) | `78.947% <ø> (ø)` | `4 <0> (ø)` | :arrow_down: |; | [...er/tools/walkers/variantutils/VariantsToTable.java](https://codecov.io/gh/broadinstitute/gatk/pull/4070/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYXJpYW50c1RvVGFibGUuamF2YQ==) | `93.182% <ø> (ø)` | `75 <0> (ø)` | :arrow_down: |; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4070/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYWxpZGF0ZVZhcmlhbnRzLmphdmE=) | `82.857% <ø> (ø)` | `34 <0> (ø)` | :arrow_down: |; | [...broadinstitute/hellbender/tools/GetSampleName.java](https://codecov.io/gh/broadinstitute/gatk/pull/4070/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9HZXRTYW1wbGVOYW1lLmphdmE=) | `62.5% <ø> (ø)` | `6 <0> (ø)` | :arrow_down: |; | ... and [36 more](https://codecov.io/gh/broadinstitute/gatk/pull/4070/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4070#issuecomment-355848388:3461,Validat,ValidateVariants,3461,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4070#issuecomment-355848388,1,['Validat'],['ValidateVariants']
Security,"3; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:15:05.204 INFO FuncotatorDataSourceDownloader - Deflater: IntelDeflater; 13:15:05.204 INFO FuncotatorDataSourceDownloader - Inflater: IntelInflater; 13:15:05.204 INFO FuncotatorDataSourceDownloader - GCS max retries/reopens: 20; 13:15:05.204 INFO FuncotatorDataSourceDownloader - Requester pays: disabled; 13:15:05.204 INFO FuncotatorDataSourceDownloader - Initializing engine; 13:15:05.205 INFO FuncotatorDataSourceDownloader - Done initializing engine; 13:15:05.205 INFO FuncotatorDataSourceDownloader - Germline data sources selected.; 13:15:05.207 INFO FuncotatorDataSourceDownloader - Collecting expected checksum...; 13:19:33.264 INFO FuncotatorDataSourceDownloader - Shutting down engine; [November 18, 2023 1:19:33 PM CST] org.broadinstitute.hellbender.tools.funcotator.FuncotatorDataSourceDownloader done. Elapsed time: 4.48 minutes.; Runtime.totalMemory()=1967128576; code: 0; message: All 3 retries failed. Waited a total of 14000 ms between attempts; reason: null; location: null; retryable: false; com.google.cloud.storage.StorageException: All 3 retries failed. Waited a total of 14000 ms between attempts; 	at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleRetryForStorageException(CloudStorageRetryHandler.java:135); 	at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:115); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.fetchSize(CloudStorageReadChannel.java:253); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.<init>(CloudStorageReadChannel",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8275#issuecomment-1817434417:2600,checksum,checksum,2600,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8275#issuecomment-1817434417,1,['checksum'],['checksum']
Security,"4 05:09:31,60] [info] MaterializeWorkflowDescriptorActor [968be82c]: Call-to-Backend assignments: ValidateBamsWf.ValidateBAM -> Local; [2020-07-14 05:09:31,82] [warn] Local [968be82c]: Key/s [memory, disks] is/are not supported by backend. Unsupported attributes will not be part of job executions.; [2020-07-14 05:09:35,38] [info] Not triggering log of token queue status. Effective log interval = None; [2020-07-14 05:09:37,15] [info] WorkflowExecutionActor-968be82c-eef3-4bdb-a1ab-3d4e2ca70674 [968be82c]: Starting ValidateBamsWf.ValidateBAM; [2020-07-14 05:09:37,39] [info] Assigned new job execution tokens to the following groups: 968be82c: 1; [2020-07-14 05:09:41,61] [warn] BackgroundConfigAsyncJobExecutionActor [968be82cValidateBamsWf.ValidateBAM:0:1]: Unrecognized runtime attribute keys: disks, memory; [2020-07-14 05:09:41,71] [info] BackgroundConfigAsyncJobExecutionActor [968be82cValidateBamsWf.ValidateBAM:0:1]: /gatk/gatk \; ValidateSamFile \; --INPUT /cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/inputs/-1942028726/test.bam \; --OUTPUT test.validation_.txt \; --MODE SUMMARY; [2020-07-14 05:09:41,76] [info] BackgroundConfigAsyncJobExecutionActor [968be82cValidateBamsWf.ValidateBAM:0:1]: executing: # make sure there is no preexisting Docker CID file; rm -f /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/docker_cid; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/docker_cid \; -i \; \; --entrypoint /bin/bash \; -v /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0:/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0:delegated \; broadinstitute/gatk@sha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:4763,Validat,ValidateBAM,4763,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,4,['Validat'],"['ValidateBAM', 'ValidateBamsWf', 'ValidateSamFile']"
Security,"4 on spark2.0.0, I found they were incompatible. It seems that the interface isn't match. The error log looks like below. Exception in thread ""main"" java.lang.NoSuchMethodError: scala.collection.Seq.aggregate(Ljava/lang/Object;Lscala/Function2;Lscala/Function2;)Ljava/lang/Object;; at org.bdgenomics.adam.models.NonoverlappingRegions.mergeRegions(NonoverlappingRegions.scala:75); at org.bdgenomics.adam.models.NonoverlappingRegions.<init>(NonoverlappingRegions.scala:55); at org.bdgenomics.adam.models.NonoverlappingRegions$.apply(NonoverlappingRegions.scala:169); at org.bdgenomics.adam.util.TwoBitRecord$.apply(TwoBitFile.scala:193); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.bdgenomics.adam.util.TwoBitFile.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:311); at org.broadinstitute.hellbender.engine.spark.SparkCommandLi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2073:1064,Hash,HashMap,1064,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073,1,['Hash'],['HashMap']
Security,"45-b14; 19:53:34.607 INFO ValidateVariants - Start Date/Time: October 25, 2020 7:53:34 PM CDT; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - HTSJDK Version: 2.22.0; 19:53:34.607 INFO ValidateVariants - Picard Version: 2.22.8; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:53:34.608 INFO ValidateVariants - Deflater: IntelDeflater; 19:53:34.608 INFO ValidateVariants - Inflater: IntelInflater; 19:53:34.608 INFO ValidateVariants - GCS max retries/reopens: 20; 19:53:34.608 INFO ValidateVariants - Requester pays: disabled; 19:53:34.608 INFO ValidateVariants - Initializing engine; 19:53:35.169 INFO FeatureManager - Using codec VCFCodec to read file file://chr1-22.phased.rename.reheader.vcf.gz; 19:53:35.594 INFO ValidateVariants - Done initializing engine; 19:53:35.594 WARN ValidateVariants - IDS validation cannot be done because no DBSNP file was provided; 19:53:35.594 WARN ValidateVariants - Other possible validations will still be performed; 19:53:35.594 INFO ProgressMeter - Starting traversal; 19:53:35.595 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 19:53:35.660 INFO ValidateVariants - Shutting down engine; [October 25, 2020 7:53:35 PM CDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2114453504; java.lang.ArrayIndexOutOfBoundsException: -87; 	at org.broadinstitute.hellbender.utils.BaseUtils.convertIUPACtoN(BaseUtils.java:123); 	at org.broadinstitute.hel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911:2593,Validat,ValidateVariants,2593,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911,1,['Validat'],['ValidateVariants']
Security,"47:51.534 INFO IntelPairHmm - Requested threads: 4; 11:47:51.534 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:47:51.557 INFO ProgressMeter - Starting traversal; 11:47:51.557 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:47:52.683 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0; 11:47:52.683 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0; 11:47:52.683 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.24 sec; 11:47:52.684 INFO Mutect2 - Shutting down engine; [July 2, 2020 11:47:52 AM CEST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2511863808; java.lang.IllegalArgumentException: Read bases and read quality arrays aren't the same size: Bases: 38 vs Base Q's: 38 vs Insert Q's: 146 vs Delete Q's: 146.; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:734); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.createQualityModifiedRead(PairHMMLikelihoodCalculationEngine.java:205); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.modifyReadQualities(PairHMMLikelihoodCalculationEngine.java:268); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngine.java:236); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngine.java:164); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:246); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:299); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); 	at ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482:5242,validat,validateArg,5242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482,1,['validat'],['validateArg']
Security,"4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:09:30,69] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2020-07-14 05:09:30,72] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2020-07-14 05:09:30,83] [info] MaterializeWorkflowDescriptorActor [968be82c]: Parsing workflow as WDL 1.0; [2020-07-14 05:09:31,60] [info] MaterializeWorkflowDescriptorActor [968be82c]: Call-to-Backend assignments: ValidateBamsWf.ValidateBAM -> Local; [2020-07-14 05:09:31,82] [warn] Local [968be82c]: Key/s [memory, disks] is/are not supported by backend. Unsupported attributes will not be part of job executions.; [2020-07-14 05:09:35,38] [info] Not triggering log of token queue status. Effective log interval = None; [2020-07-14 05:09:37,15] [info] WorkflowExecutionActor-968be82c-eef3-4bdb-a1ab-3d4e2ca70674 [968be82c]: Starting ValidateBamsWf.ValidateBAM; [2020-07-14 05:09:37,39] [info] Assigned new job execution tokens to the following groups: 968be82c: 1; [2020-07-14 05:09:41,61] [warn] BackgroundConfigAsyncJobExecutionActor [968be82cValidateBamsWf.ValidateBAM:0:1]: Unrecognized runtime attribute keys: disks, memory; [2020-07-14 05:09:41,71] [info] BackgroundConfigAsyncJobExecutionActor [968be82cValidateBamsWf.ValidateBAM:0:1]: /gatk/gatk \; ValidateSamFile \; --INPUT /cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/inputs/-1942028726/test.bam \; --OUTPUT test.validation_.txt \; --MODE SUMMARY; [2020-07-14 05:09:41,76] [info] BackgroundConfigAsyncJobExecutionActor [968be82cValidateBamsWf.ValidateBAM:0:1]: executing: # make sure there is no preexisting Docker CID file; rm -f /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/docker_cid; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:4386,Validat,ValidateBAM,4386,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,1,['Validat'],['ValidateBAM']
Security,"5 INFO BlockManager:54 - BlockManager stopped; 2019-05-14 17:07:05 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-05-14 17:07:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-05-14 17:07:05 INFO SparkContext:54 - Successfully stopped SparkContext; 17:07:05.631 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [May 14, 2019 5:07:05 PM EDT] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 41.02 minutes.; Runtime.totalMemory()=23321378816; java.lang.IllegalArgumentException: Wrong FS: hdfs://scc:-1/project/casa/gcad/adsp.cc/sv/A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.contig-sam-file.sam, expected: hdfs://scc; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645); at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193); at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:105); at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:397); at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:393); at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:393); at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:337); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:775); at hdfs.jsr203.HadoopFileSystem.newOutputStream(HadoopFileSystem.java:554); at hdfs.jsr203.HadoopFileSystem.newByteChannel(HadoopFileSystem.java:395); at hdfs.jsr203.HadoopPath.newByteChannel(HadoopPath.java:558); at hdfs.jsr203.HadoopFileSystemProvider.newByteChannel(Hado",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942:1472,access,access,1472,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942,1,['access'],['access']
Security,"5 INFO FilterMutectCalls - Deflater: IntelDeflater; 14:50:59.205 INFO FilterMutectCalls - Inflater: IntelInflater; 14:50:59.205 INFO FilterMutectCalls - GCS max retries/reopens: 20; 14:50:59.205 INFO FilterMutectCalls - Requester pays: disabled; 14:50:59.205 INFO FilterMutectCalls - Initializing engine; 14:51:00.692 INFO FeatureManager - Using codec VCFCodec to read file file:///workdir/mparment/data/process/A2683/PTC2_unfiltered.vcf.gz; 14:51:01.406 INFO FilterMutectCalls - Done initializing engine; 14:51:02.360 INFO FilterMutectCalls - Shutting down engine; [December 12, 2020 2:51:02 PM CET] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2385510400; java.lang.IllegalStateException: Duplicate key 7.395307178412063E-4; at java.util.stream.Collectors.lambda$throwingMerger$138(Collectors.java:133); at java.util.stream.Collectors$$Lambda$67/403388441.apply(Unknown Source); at java.util.HashMap.merge(HashMap.java:1245); at java.util.stream.Collectors.lambda$toMap$196(Collectors.java:1320); at java.util.stream.Collectors$$Lambda$69/854719230.accept(Unknown Source); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.<init>(ContaminationFilter.java:26); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.buildFilte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6996:3688,Hash,HashMap,3688,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6996,1,['Hash'],['HashMap']
Security,5-70-gdc3237e-SNAPSHOT; 14:19:10.289 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 14:19:10.290 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:19:10.290 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 14:19:10.290 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:19:10.290 INFO PrintReadsSpark - Deflater: IntelDeflater; 14:19:10.290 INFO PrintReadsSpark - Inflater: IntelInflater; 14:19:10.290 INFO PrintReadsSpark - GCS max retries/reopens: 20; 14:19:10.290 INFO PrintReadsSpark - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 14:19:10.290 INFO PrintReadsSpark - Initializing engine; 14:19:10.290 INFO PrintReadsSpark - Done initializing engine; 17/10/11 14:19:10 INFO spark.SparkContext: Running Spark version 1.6.0; 17/10/11 14:19:10 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/11 14:19:10 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/11 14:19:10 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); users with modify permissions: Set(hdfs); 17/10/11 14:19:10 INFO util.Utils: Successfully started service 'sparkDriver' on port 43567.; 17/10/11 14:19:11 INFO slf4j.Slf4jLogger: Slf4jLogger started; 17/10/11 14:19:11 INFO Remoting: Starting remoting; 17/10/11 14:19:11 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.131.101.159:45501]; 17/10/11 14:19:11 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.131.101.159:45501]; 17/10/11 14:19:11 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 45501.; 17/10/11 14:19:11 INFO spark.SparkEnv: Registering MapOutputTracker; 17/10/11 14:19:11 INFO spark.SparkEnv: Registering BlockManagerMaster; 17/10/11 14:19,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:3303,Secur,SecurityManager,3303,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['Secur'],['SecurityManager']
Security,"5. 10:47:20.577 INFO ProgressMeter - chr1:211951736 4.0 17700000 4421996.7. 10:47:30.580 INFO ProgressMeter - chr1:220837654 4.2 18418000 4417386.9. 10:47:40.592 INFO ProgressMeter - chr1:229536819 4.3 19156000 4417642.0. 10:47:50.595 INFO ProgressMeter - chr1:237917173 4.5 19861000 4410598.8. 10:48:00.598 INFO ProgressMeter - chr1:246682719 4.7 20561000 4403066.6. 10:48:10.604 INFO ProgressMeter - chr2:6733404 4.8 21270000 4397838.6. 10:48:20.605 INFO ProgressMeter - chr2:15144861 5.0 21942000 4385607.8. 10:48:30.607 INFO ProgressMeter - chr2:24131740 5.2 22650000 4381143.4. 10:48:40.610 INFO ProgressMeter - chr2:32916253 5.3 23467000 4397382.8. 10:48:43.217 INFO LeftAlignIndels - Shutting down engine. [August 18, 2020 10:48:43 AM EDT] org.broadinstitute.hellbender.tools.LeftAlignIndels done. Elapsed time: 5.40 minutes. Runtime.totalMemory()=2076049408. java.lang.IllegalArgumentException: the range cannot contain negative indices. at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:727). at org.broadinstitute.hellbender.utils.IndexRange.validate(IndexRange.java:108). at org.broadinstitute.hellbender.utils.IndexRange.shift(IndexRange.java:73). at org.broadinstitute.hellbender.utils.IndexRange.shiftLeft(IndexRange.java:77). at org.broadinstitute.hellbender.utils.read.AlignmentUtils.leftAlignIndels(AlignmentUtils.java:735). at org.broadinstitute.hellbender.tools.LeftAlignIndels.apply(LeftAlignIndels.java:78). at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$0(ReadWalker.java:96). at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184). at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193). at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175). at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193). at java.util.Iterator.forEachRemaining(Iterator.java:116). at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801). at jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6765:6108,validat,validateArg,6108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6765,1,['validat'],['validateArg']
Security,"5.223 INFO DenoiseReadCounts - Initializing engine; 20:08:45.223 INFO DenoiseReadCounts - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 20:08:45.300 INFO DenoiseReadCounts - Reading read-counts file (BT1813.counts.hdf5)...; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fsuper.c line 277 in H5F_super_read(): file signature not found; major: File accessibilty; minor: Not an HDF5 file; 20:08:49.800 INFO DenoiseReadCounts - Shutting down engine; [May 18, 2021 8:08:49 PM EDT] org.broadinstitute.hellbender.tools.copynumber.DenoiseReadCounts done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=1789919232; org.broadinstitute.hdf5.HDF5LibException: exception when opening '/hpf/largeprojects/tabori/projects/bmmrd/CNA_project/gatk_cna/gatk/analysis/lgg/cnvponC2.pon.hdf5' with READ_ONLY mode: Not an HDF5 file; at org.broadinstitute.hdf5.HDF5File.open(HDF5File.java:490); at org.broadinstitute.hdf5.HDF5File.<init>(HDF5File.java:82); at org.broadinstitute.hdf5.HDF5File.<init>(HDF5File.java:66); at org.broadinstitute.hellbender.tools.copynumber.DenoiseReadCounts.doWork(DenoiseReadCounts.java:188); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7258:4486,access,accessibilty,4486,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258,1,['access'],['accessibilty']
Security,5062/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9Qb3dlckNhbGN1bGF0aW9uVXRpbHMuamF2YQ==) | `96.667% <100%> (+1.429%)` | `18 <7> (+3)` | :arrow_up: |; | [...tmutpileup/BasicSomaticShortMutationValidator.java](https://codecov.io/gh/broadinstitute/gatk/pull/5062/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9CYXNpY1NvbWF0aWNTaG9ydE11dGF0aW9uVmFsaWRhdG9yLmphdmE=) | `62.5% <100%> (+1.974%)` | `5 <0> (ø)` | :arrow_down: |; | [...ion/basicshortmutpileup/BasicValidationResult.java](https://codecov.io/gh/broadinstitute/gatk/pull/5062/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9CYXNpY1ZhbGlkYXRpb25SZXN1bHQuamF2YQ==) | `96.774% <100%> (+0.222%)` | `16 <2> (+1)` | :arrow_up: |; | [...tmutpileup/ValidateBasicSomaticShortMutations.java](https://codecov.io/gh/broadinstitute/gatk/pull/5062/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9WYWxpZGF0ZUJhc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zLmphdmE=) | `83.636% <100%> (+0.15%)` | `19 <0> (ø)` | :arrow_down: |; | [...dateBasicSomaticShortMutationsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5062/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9WYWxpZGF0ZUJhc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zSW50ZWdyYXRpb25UZXN0LmphdmE=) | `100% <100%> (ø)` | `5 <0> (ø)` | :arrow_down: |; | [...utils/smithwaterman/SmithWatermanIntelAligner.java](https://codecov.io/gh/broadinstitute/gatk/pull/5062/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zbWl0aHdhdGVybWFuL1NtaXRoV2F0ZXJtYW5JbnRlbEFsaWduZXIuamF2YQ==) | `50% <0%> (,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5062#issuecomment-408490831:2311,Validat,ValidateBasicSomaticShortMutations,2311,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5062#issuecomment-408490831,1,['Validat'],['ValidateBasicSomaticShortMutations']
Security,"514.10.2.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_111-b14; Version: Version:4.alpha.2-170-g8d06823-SNAPSHOT; 19:03:42.998 INFO ValidateSamFile - Defaults.BUFFER_SIZE : 131072; 19:03:42.999 INFO ValidateSamFile - Defaults.COMPRESSION_LEVEL : 1; 19:03:42.999 INFO ValidateSamFile - Defaults.CREATE_INDEX : false; 19:03:42.999 INFO ValidateSamFile - Defaults.CREATE_MD5 : false; 19:03:42.999 INFO ValidateSamFile - Defaults.CUSTOM_READER_FACTORY : ; 19:03:42.999 INFO ValidateSamFile - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 19:03:42.999 INFO ValidateSamFile - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 19:03:42.999 INFO ValidateSamFile - Defaults.REFERENCE_FASTA : null; 19:03:43.000 INFO ValidateSamFile - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_CRAM_REF_DOWNLOAD : false; 19:03:43.000 INFO ValidateSamFile - Deflater JdkDeflater; 19:03:43.000 INFO ValidateSamFile - Inflater JdkInflater; 19:03:43.000 INFO ValidateSamFile - Initializing engine; 19:03:43.000 INFO ValidateSamFile - Done initializing engine; ERROR: Record 9762, Read name 20GAVAAXX100126:7:2:8126:115177, bin field of BAM record does not equal value computed based on alignment start and end, and length of sequence to which read is aligned; ERROR: Record 24466, Read name 20FUKAAXX100202:7:46:13035:77621, bin field of BAM record does not equal value computed based on alignment start and end, and length of sequence to which read is aligned; ERROR: Record 97940, Read name 20FUKAAXX100202:5:7:21464:86224, bin field of BAM record does not equal value computed based on alignment start and end, and length of sequence to which read is aligned; ERROR: Record 97955, Read na",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-285513571:1731,Validat,ValidateSamFile,1731,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-285513571,1,['Validat'],['ValidateSamFile']
Security,52%)` | |; | [...aplotypecaller/HaplotypeCallerIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5710/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXJJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `57.339% <0%> (-30.767%)` | `89% <0%> (+4%)` | |; | [...utils/smithwaterman/SmithWatermanIntelAligner.java](https://codecov.io/gh/broadinstitute/gatk/pull/5710/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zbWl0aHdhdGVybWFuL1NtaXRoV2F0ZXJtYW5JbnRlbEFsaWduZXIuamF2YQ==) | `50% <0%> (-30%)` | `1% <0%> (-2%)` | |; | [...utils/variant/GATKVariantContextUtilsUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5710/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L0dBVEtWYXJpYW50Q29udGV4dFV0aWxzVW5pdFRlc3QuamF2YQ==) | `61.598% <0%> (-24.25%)` | `160% <0%> (ø)` | |; | [...walkers/validation/ConcordanceIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5710/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQ29uY29yZGFuY2VJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `79.444% <0%> (-20.556%)` | `3% <0%> (-3%)` | |; | [...kers/vqsr/VariantGaussianMixtureModelUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5710/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvVmFyaWFudEdhdXNzaWFuTWl4dHVyZU1vZGVsVW5pdFRlc3QuamF2YQ==) | `62.857% <0%> (-20.162%)` | `13% <0%> (ø)` | |; | [.../walkers/vqsr/TruthSensitivityTrancheUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5710/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvVHJ1dGhTZW5zaXRpdml0eVRyYW5jaGVVbml0VGVzdC5qYXZh) | `66.667% <0%> (-19.048%)` | `12% <0%> (ø)` | |; | [...stitute/hellbender/utils/nio/PathLineIterator.java](h,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5710#issuecomment-466524542:2858,validat,validation,2858,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5710#issuecomment-466524542,1,['validat'],['validation']
Security,53216; java.lang.IllegalArgumentException: Unsupported class file major version 55; 	at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166); 	at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148); 	at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136); 	at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237); 	at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49); 	at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517); 	at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500); 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); 	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134); 	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134); 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); 	at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500); 	at org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175); 	at org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238); 	at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631); 	at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:355); 	at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:307); 	at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:306); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at org.apache.spark.util.ClosureCleaner$.org$apache$sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7035:6958,Hash,HashTable,6958,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7035,1,['Hash'],['HashTable']
Security,"557000 1924140.1; 01:10:44.052 INFO ProgressMeter - 5:18554429 10.9 20887000 1924971.5; 01:10:54.053 INFO ProgressMeter - 5:23247594 11.0 21241000 1927979.5; 01:11:04.057 INFO ProgressMeter - 5:25901452 11.2 21588000 1930263.3; 01:11:14.089 INFO ProgressMeter - 5:32482380 11.4 21916000 1930729.5; 01:11:24.106 INFO ProgressMeter - 5:38674297 11.5 22249000 1931652.6; 01:11:34.133 INFO ProgressMeter - 5:49679881 11.7 22573000 1931754.3; 01:11:44.145 INFO ProgressMeter - 5:53234595 11.9 22925000 1934259.1; 04:10:15.659 INFO ProgressMeter - 6:1726401 190.4 23183000 121774.0; 04:10:25.671 INFO ProgressMeter - 6:10206926 190.5 23517000 123420.2; 04:10:31.341 INFO BaseRecalibrator - Shutting down engine; [February 22, 2021 at 4:10:31 AM PST] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 190.65 minutes.; Runtime.totalMemory()=1268776960; java.lang.IllegalStateException: cigar is completely soft-clipped; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.utils.read.CigarBuilder.make(CigarBuilder.java:129); at org.broadinstitute.hellbender.utils.read.CigarBuilder.make(CigarBuilder.java:143); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.consolidateCigar(BaseRecalibrationEngine.java:293); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:118); at org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator.apply(BaseRecalibrator.java:189)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7092:8787,validat,validate,8787,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7092,1,['validat'],['validate']
Security,5:41:49.029 INFO Funcotator - HTSJDK Version: 2.23.0; 15:41:49.029 INFO Funcotator - Picard Version: 2.22.8; 15:41:49.029 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:41:49.029 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:41:49.029 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:41:49.029 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:41:49.029 INFO Funcotator - Deflater: IntelDeflater; 15:41:49.029 INFO Funcotator - Inflater: IntelInflater; 15:41:49.029 INFO Funcotator - GCS max retries/reopens: 20; 15:41:49.029 INFO Funcotator - Requester pays: disabled; 15:41:49.029 INFO Funcotator - Initializing engine; 15:41:49.471 INFO FeatureManager - Using codec VCFCodec to read file file:///home/shiyang/Project/BGB900_101/TSO_result/TSO_somatic_vcf/112-0005-0031-B1_L1.UP12.tmb.tsv.tso.somatic.vcf; 15:41:49.489 INFO Funcotator - Done initializing engine; 15:41:49.489 INFO Funcotator - Validating Sequence Dictionaries...; 15:41:49.490 INFO Funcotator - Processing user transcripts/defaults/overrides...; 15:41:49.490 INFO Funcotator - Initializing data sources...; 15:41:49.492 INFO DataSourceUtils - Initializing data sources from directory: /home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s; 15:41:49.492 INFO DataSourceUtils - Data sources version: 1.7.2020429s; 15:41:49.492 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 15:41:49.492 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 15:41:49.496 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/achilles_lineage_results.import.txt -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/achilles/hg19/achilles_lin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6758:4657,Validat,Validating,4657,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6758,1,['Validat'],['Validating']
Security,"6.647 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 240; 17:21:06.647 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 230; 17:21:06.648 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 4488; 17:21:06.648 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 1355; 17:21:06.648 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1675; 17:21:06.648 INFO StructuralVariationDiscoveryPipelineSpark - INV: 0; 17:21:06.648 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 17:21:06.648 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 0; 18/01/25 17:21:07 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 29.0 in stage 61.0 (TID 60915, cwhelan-hg00514-1-cram-samtools-bam-feature-w-1.c.broad-dsde-methods.internal, executor 48): java.lang.IllegalArgumentException: Unexpected CIGAR format with deletion neighboring clipping; cigar elements are: [1190M, 4D, 53M, 2I, 26M, 2I, 31M, 2D, 1450S]; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); 	at org.broadinstitute.hellbender.tools.spark.sv.utils.SvCigarUtils.validateCigar(SvCigarUtils.java:134); 	at org.broadinstitute.hellbender.tools.spark.sv.utils.SvCigarUtils.getUnclippedReadLength(SvCigarUtils.java:161); 	at org.broadinstitute.hellbender.tools.spark.sv.utils.SvCigarUtils.computeAssociatedDistOnRead(SvCigarUtils.java:330); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignmentInterval.readIntervalAlignedToRefSpan(AlignmentInterval.java:634); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantDetector.extractAltHaplotypeSeq(CpxVariantDetector.java:852); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantDetector.access$300(CpxVariantDetector.java:47); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantDetector$AnnotatedContig.annotate(CpxVariantDetector.java:194); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantDet",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4260:5001,validat,validateArg,5001,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4260,1,['validat'],['validateArg']
Security,"6:08:17 UTC 2019] ValidateSamFile --INPUT CQ-NEQAS-2018.ILLUMINA.library.000000000-BCFDC.1.1.sorted.bam --MODE SUMMARY --MAX_OUTPUT 100 --IGNORE_WARNINGS false --VALIDATE_INDEX true --INDEX_VALIDATION_STRINGENCY EXHAUSTIVE --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --SKIP_MATE_VALIDATION false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. Last read position: chr9:32,633,613; INFO 2019-03-07 16:12:22 SamFileValidator Validated Read 20,000,000 records. Elapsed time: 00:03:58s. Time for last 10,000,000: 117s. Last read position: chrM:11,340; No errors found; [Thu Mar 07 16:13:05 UTC 2019] picard.sam.ValidateSamFile done. Elapsed time: 4.79 minutes.; Runtime.totalMemory()=2602041344; Tool returned:; 0; ```. But when run BaseRecalibrator got the _fromIndex toIndex_ error:; `gatk BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz`; ```; ERROR: return code 3; STDERR:; 15:46:35.795 INFO NativeLibraryLoader - Loading libgkl_compression.so from j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5807:1834,validat,validations,1834,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807,1,['validat'],['validations']
Security,6e7635897e1a6a773af5684511e2358d369af94?src=pr&el=desc) will **increase** coverage by `0.002%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #4310 +/- ##; ===============================================; + Coverage 79.065% 79.067% +0.002% ; - Complexity 16582 16583 +1 ; ===============================================; Files 1048 1048 ; Lines 59504 59504 ; Branches 9717 9717 ; ===============================================; + Hits 47047 47048 +1 ; Misses 8682 8682 ; + Partials 3775 3774 -1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4310?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...bender/tools/walkers/mutect/FilterMutectCalls.java](https://codecov.io/gh/broadinstitute/gatk/pull/4310/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9GaWx0ZXJNdXRlY3RDYWxscy5qYXZh) | `95.833% <ø> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...llbender/tools/walkers/validation/Concordance.java](https://codecov.io/gh/broadinstitute/gatk/pull/4310/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQ29uY29yZGFuY2UuamF2YQ==) | `88.542% <ø> (ø)` | `28 <0> (ø)` | :arrow_down: |; | [...itute/hellbender/tools/walkers/mutect/Mutect2.java](https://codecov.io/gh/broadinstitute/gatk/pull/4310/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QyLmphdmE=) | `92% <ø> (ø)` | `15 <0> (ø)` | :arrow_down: |; | [...ls/walkers/mutect/CreateSomaticPanelOfNormals.java](https://codecov.io/gh/broadinstitute/gatk/pull/4310/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9DcmVhdGVTb21hdGljUGFuZWxPZk5vcm1hbHMuamF2YQ==) | `87.273% <ø> (ø)` | `8 <0> (ø)` | :arrow_down: |; | [...ellbender/tools/exome/FilterByOrientationBias.java](https://codecov.io/gh/broadinstitute/gatk/pull/4310/di,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4310#issuecomment-361770954:1236,validat,validation,1236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4310#issuecomment-361770954,1,['validat'],['validation']
Security,7 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/23 20:41:38 INFO SparkContext: Running Spark version 2.2.0; 18/04/23 20:41:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/23 20:41:38 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls to: zorzan; 18/04/23 20:41:39 INFO SecurityManager: Changing modify acls to: zorzan; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls groups to:; 18/04/23 20:41:39 INFO SecurityManager: Changing modify acls groups to:; 18/04/23 20:41:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(zorzan); groups with view permissions: Set(); users with modify permissions: Set(zorzan); groups with modify permissions: Set(); 18/04/23 20:41:41 INFO Utils: Successfully started service 'sparkDriver' on port 36273.; 18/04/23 20:41:41 INFO SparkEnv: Registering MapOutputTracker; 18/04/23 20:41:41 INFO SparkEnv: Registering BlockManagerMaster; 18/04/23 20:41:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/23 20:41:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/23 20:41:41 INFO DiskBlockManager: Created local directory at /tmp/zorzan/blockmgr-994d8501-06ce-4315-84ff-3c29de358ae1; 18/04/23 20:41:41 INFO MemoryStore: MemoryStore started with capacity 4.0 GB; 18/04/23 20:41:41 INFO SparkEnv: Registering OutputCommitCoordi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:6995,Secur,SecurityManager,6995,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,7,"['Secur', 'authenticat']","['SecurityManager', 'authentication']"
Security,"78 INFO HaplotypeCaller - GCS max retries/reopens: 20; 01:13:16.078 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 01:13:16.078 INFO HaplotypeCaller - Initializing engine; 01:13:17.087 INFO HaplotypeCaller - Shutting down engine; [January 18, 2020 1:13:17 AM IST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2216689664; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getContigNames(SequenceDictionaryUtils.java:463); 	at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getCommonContigsByName(SequenceDictionaryUtils.java:457); 	at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.compareDictionaries(SequenceDictionaryUtils.java:234); 	at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:150); 	at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:98); 	at org.broadinstitute.hellbender.engine.GATKTool.validateSequenceDictionaries(GATKTool.java:621); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:563); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:160); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); 	at org.broadinstitute.hellbender.Main.main(Main.java:275). Please suggest any solution.; Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5947#issuecomment-575601220:2673,validat,validateDictionaries,2673,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5947#issuecomment-575601220,3,['validat'],"['validateDictionaries', 'validateSequenceDictionaries']"
Security,"7:30.580 INFO ProgressMeter - chr1:220837654 4.2 18418000 4417386.9. 10:47:40.592 INFO ProgressMeter - chr1:229536819 4.3 19156000 4417642.0. 10:47:50.595 INFO ProgressMeter - chr1:237917173 4.5 19861000 4410598.8. 10:48:00.598 INFO ProgressMeter - chr1:246682719 4.7 20561000 4403066.6. 10:48:10.604 INFO ProgressMeter - chr2:6733404 4.8 21270000 4397838.6. 10:48:20.605 INFO ProgressMeter - chr2:15144861 5.0 21942000 4385607.8. 10:48:30.607 INFO ProgressMeter - chr2:24131740 5.2 22650000 4381143.4. 10:48:40.610 INFO ProgressMeter - chr2:32916253 5.3 23467000 4397382.8. 10:48:43.217 INFO LeftAlignIndels - Shutting down engine. [August 18, 2020 10:48:43 AM EDT] org.broadinstitute.hellbender.tools.LeftAlignIndels done. Elapsed time: 5.40 minutes. Runtime.totalMemory()=2076049408. java.lang.IllegalArgumentException: the range cannot contain negative indices. at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:727). at org.broadinstitute.hellbender.utils.IndexRange.validate(IndexRange.java:108). at org.broadinstitute.hellbender.utils.IndexRange.shift(IndexRange.java:73). at org.broadinstitute.hellbender.utils.IndexRange.shiftLeft(IndexRange.java:77). at org.broadinstitute.hellbender.utils.read.AlignmentUtils.leftAlignIndels(AlignmentUtils.java:735). at org.broadinstitute.hellbender.tools.LeftAlignIndels.apply(LeftAlignIndels.java:78). at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$0(ReadWalker.java:96). at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184). at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193). at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175). at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193). at java.util.Iterator.forEachRemaining(Iterator.java:116). at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801). at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481). at java.util",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6765:6187,validat,validate,6187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6765,1,['validat'],['validate']
Security,"8,26:0.261:13:10:2100,793:37:29; ```. **output GatherVcfs to .vcf.gz allows for duplicate records**; ```; WMCF9-CB5:precomputed_results shlee$ java -jar $PICARD GatherVcfs I=split3_8.vcf.gz I=split2_8.vcf.gz O=../test_gathervcf_split8_overlap.vcf.gz; [Wed Jun 07 14:51:32 EDT 2017] picard.vcf.GatherVcfs INPUT=[split3_8.vcf.gz, split2_8.vcf.gz] OUTPUT=../test_gathervcf_split8_overlap.vcf.gz VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=true CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json; [Wed Jun 07 14:51:32 EDT 2017] Executing as shlee@WMCF9-CB5 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_111-b14; Picard version: 2.9.2-SNAPSHOT; INFO	2017-06-07 14:51:32	GatherVcfs	Checking inputs.; INFO	2017-06-07 14:51:32	GatherVcfs	Checking file headers and first records to ensure compatibility.; INFO	2017-06-07 14:51:32	GatherVcfs	Gathering by copying gzip blocks. Will not be able to validate position non-overlap of files.; WARNING	2017-06-07 14:51:32	GatherVcfs	Index creation not currently supported when gathering block compressed VCFs.; INFO	2017-06-07 14:51:32	GatherVcfs	Gathering /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/precomputed_results/split3_8.vcf.gz; INFO	2017-06-07 14:51:32	GatherVcfs	Gathering /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/precomputed_results/split2_8.vcf.gz; [Wed Jun 07 14:51:32 EDT 2017] picard.vcf.GatherVcfs done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=257425408; WMCF9-CB5:precomputed_results shlee$ gzcat ../test_gathervcf_split8_overlap.vcf.gz | grep -v '##' ; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	NORMAL	TUMOR; chr6	33414233	.	GT	G	.	PASS	ECNT=1;HCNT=1;MAX_ED=.;MIN_ED=.;NLOD=28.24;RPA=5,4;RU=T;STR;TLOD=154.53	GT:AD:AF:ALT_F1R2:ALT_F2R1:QSS:REF_F1R2:REF_F2R1	0/0:112,0:0.00:0:0:3730,0:62:50	0/1:66,70:0.534:25:41:2209,2350:26:40; chr6	33442919	.	A	C	.	alt_allele_i",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3061#issuecomment-306889518:2414,validat,validate,2414,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3061#issuecomment-306889518,1,['validat'],['validate']
Security,"8.0/gatk-package-4.1.8.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/zepengmu/tools/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar ValidateVariants -V ../../data/geno/phased/chr1-22.phased.rename.reheader.vcf.gz -R ../../../../index/hg19.fa.gz; 19:53:34.379 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/zepengmu/tools/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 25, 2020 7:53:34 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:53:34.606 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.606 INFO ValidateVariants - The Genome Analysis Toolkit (GATK) v4.1.8.0; 19:53:34.606 INFO ValidateVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:53:34.607 INFO ValidateVariants - Executing as zepengmu@midway2-login1.rcc.local on Linux v3.10.0-1127.8.2.el7.x86_64 amd64; 19:53:34.607 INFO ValidateVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14; 19:53:34.607 INFO ValidateVariants - Start Date/Time: October 25, 2020 7:53:34 PM CDT; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - HTSJDK Version: 2.22.0; 19:53:34.607 INFO ValidateVariants - Picard Version: 2.22.8; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:53:34.608 INFO ValidateVariants - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911:1297,Validat,ValidateVariants,1297,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911,1,['Validat'],['ValidateVariants']
Security,"80k is what I had easy access to and what I'm the most invested in; benchmarking right now. Master does fine with the same params. It's slow,; but no failures. We decided to split into 1000 shards (Eric is convinced; that there's a substantial startup cost per shard so we do better in total; cpu-hours on fewer shards) and each of those takes about 24 hours. On Thu, May 10, 2018, 11:09 AM Louis Bergelson <notifications@github.com>; wrote:. > @ldgauthier <https://github.com/ldgauthier> You're running 80k? Does that; > run using the current master version of GATK? I assumed you were rerunning; > a 20k shard with the same settings we had used for the 20k.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388082988>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLdlQoWlC8kjRvJJermDYEjltVUFks5txFgigaJpZM4TOtSm>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388183296:23,access,access,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388183296,1,['access'],['access']
Security,"86); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using exome data (#8113); - VAT Readme updates (#8090); - WDL and python scripts to use the VDS in the VAT (#8077); - VS-757 - Use JASIX to make sub-jsons of annotated output of Nirvana (#8133); - add note about permissions for P&S workflow to work (#8135); - VS-759 (and VS-760) (#8137); - VS-765. Scatter the RemoveDuplicates task. (#8144); - update delivery docs based on latest VDS delivery run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay un",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:31469,validat,validation,31469,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['validat'],['validation']
Security,"89); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:24455,validat,validate,24455,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['validat'],['validate']
Security,"90 4.1 67150 16425.7; 09:53:22.294 INFO ProgressMeter - 1:20576686 4.3 71380 16776.4; 09:53:32.681 INFO ProgressMeter - 1:21106727 4.4 73230 16538.3; 09:53:44.258 INFO ProgressMeter - 1:21270052 4.6 73820 15975.4; 09:53:54.757 INFO ProgressMeter - 1:21754504 4.8 75500 15742.8; 09:54:04.928 INFO ProgressMeter - 1:23419224 5.0 81370 16387.6; 09:54:15.956 INFO ProgressMeter - 1:23812728 5.1 82750 16070.6; 09:54:31.008 INFO ProgressMeter - 1:24023237 5.4 83470 15457.4; 09:54:33.610 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 58.921665822; 09:54:33.611 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 72.92 sec; 09:54:33.612 INFO Mutect2 - Shutting down engine; [March 7, 2019 9:54:33 AM EST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 5.51 minutes.; Runtime.totalMemory()=193003520; java.lang.IllegalArgumentException: readMaxLength must be > 0 but got 0; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); 	at org.broadinstitute.hellbender.utils.pairhmm.PairHMM.initialize(PairHMM.java:152); 	at org.broadinstitute.hellbender.utils.pairhmm.N2MemoryPairHMM.initialize(N2MemoryPairHMM.java:28); 	at org.broadinstitute.hellbender.utils.pairhmm.LoglessPairHMM.initialize(LoglessPairHMM.java:7); 	at org.broadinstitute.hellbender.utils.pairhmm.PairHMM.initialize(PairHMM.java:177); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.initializePairHMM(PairHMMLikelihoodCalculationEngine.java:242); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngine.java:177); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:229); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:232); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(Assemb",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470579844:4754,validat,validateArg,4754,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470579844,1,['validat'],['validateArg']
Security,"9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 18:11:33.871 INFO PrintReadsSpark - Initializing engine; 18:11:33.871 INFO PrintReadsSpark - Done initializing engine; 17/10/13 18:11:33 INFO spark.SparkContext: Running Spark version 2.2.0.cloudera1; 17/10/13 18:11:34 WARN spark.SparkConf: spark.master yarn-client is deprecated in Spark 2.0+, please instead use ""yarn"" with specified deploy mode.; 17/10/13 18:11:34 INFO spark.SparkContext: Submitted application: PrintReadsSpark; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing view acls groups to: ; 17/10/13 18:11:34 INFO spark.SecurityManager: Changing modify acls groups to: ; 17/10/13 18:11:34 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); groups with view permissions: Set(); users with modify permissions: Set(hdfs); groups with modify permissions: Set(); 17/10/13 18:11:34 INFO util.Utils: Successfully started service 'sparkDriver' on port 45754.; 17/10/13 18:11:34 INFO spark.SparkEnv: Registering MapOutputTracker; 17/10/13 18:11:34 INFO spark.SparkEnv: Registering BlockManagerMaster; 17/10/13 18:11:34 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 17/10/13 18:11:34 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 17/10/13 18:11:34 INFO storage.DiskBlockManager: Created local directory at /tmp/hdfs/blockmgr-ea0e0669-2981-4277-80a0-a67eddf1001d; 17/10/13 18:11:34 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB; 17/10/13 18:11:34 INFO spark.SparkEnv: Registering OutputCommitCoordinator; 17/10/13 18:11:34 INFO util.log: Logging initialized @3816ms; 17/10/13 18:11:34 INFO server.Server: jetty-9.3.z-SNAPSHOT; 17/10/13 ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:4300,authenticat,authentication,4300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,4,"['Secur', 'authenticat']","['SecurityManager', 'authentication']"
Security,": 20; 08:37:16.409 INFO GermlineCNVCaller - Requester pays: disabled; 08:37:16.409 INFO GermlineCNVCaller - Initializing engine; 08:37:21.698 INFO GermlineCNVCaller - Done initializing engine; 08:37:22.015 INFO GermlineCNVCaller - Retrieving intervals from read-count file (results/200219_X008378.counts.tsv)...; 08:37:22.119 INFO GermlineCNVCaller - No annotated intervals were provided...; 08:37:22.120 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 08:37:22.194 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 08:37:22.195 INFO GermlineCNVCaller - Shutting down engine; [February 26, 2019 8:37:22 AM GMT] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 0.29 minutes.; Runtime.totalMemory()=330301440; java.lang.IllegalArgumentException: Output directory results/190226.181217_K00178.CNVCaller does not exist.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.validateArguments(GermlineCNVCaller.java:361); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:281); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /mnt/storage/apps/software/gatk/4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compress",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:15857,validat,validateArg,15857,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398,1,['validat'],['validateArg']
Security,":///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -- --spark-runner SPARK --spark-master yarn --executor-memory 48G --driver-memory 16g --driver-cores 2 --executor-cores 8 --num-executors 8. ```; 18/03/07 13:24:26 INFO storage.BlockManagerMasterEndpoint: Registering block manager scc-q14.scc.bu.edu:42456 with 25.4 GB RAM, BlockManagerId(2, scc-q14.scc.bu.edu, 42456, None); 18/03/07 13:24:27 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 247.0 KB, free 8.4 GB); 18/03/07 13:24:28 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 25.5 KB, free 8.4 GB); 18/03/07 13:24:28 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.48.225.55:32895 (size: 25.5 KB, free: 8.4 GB); 18/03/07 13:24:28 INFO spark.SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:112; 18/03/07 13:24:28 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 7164 for farrell on ha-hdfs:scc; 18/03/07 13:24:28 INFO security.TokenCache: Got dt for hdfs://scc; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:scc, Ident: (HDFS_DELEGATION_TOKEN token 7164 for farrell); 18/03/07 13:24:28 INFO input.FileInputFormat: Total input paths to process : 1; 18/03/07 13:59:26 INFO spark.SparkContext: Starting job: aggregate at FlagStatSpark.java:73; 18/03/07 13:59:26 INFO scheduler.DAGScheduler: Got job 0 (aggregate at FlagStatSpark.java:73) with 252 output partitions; 18/03/07 13:59:26 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (aggregate at FlagStatSpark.java:73); 18/03/07 13:59:26 INFO scheduler.DAGScheduler: Parents of final stage: List(); 18/03/07 13:59:26 INFO scheduler.DAGScheduler: Missing parents: List(); 18/03/07 13:59:26 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at GATKSparkTool.java:220), which has no missing parents; 18/03/07 13:59:26 INFO memory.MemoryStore: Block broadcast_1 stored as values in mem",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371280304:1499,secur,security,1499,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371280304,1,['secur'],['security']
Security,"://gatk.broadinstitute.org/hc/en-us/community/posts/360061452132-GATK4-RNAseq-short-variant-discovery-SNPs-Indels-), but then for Haplotypecaller, and you have opened a bugreport to add a feature to ValidateVariants: https://github.com/broadinstitute/gatk/issues/6553. However, it would be nice if you could actually investigate the formatting error. Unfortunately my formatting error isn't the same as reported in the other post. I have 105 error in which the 1st alternative allele is a spanning deletion and the 2nd (and 3rd) is either an indel or snp. It's true that the 2nd and 3rd allele is actually not found in my samples. I even have 7 occurances in which the 1st allele (spanning deletion) has allele frequency 1.00. my code is the following for GenotypeGVCFs:. java -Xms32G -Xmx32G -jar ${gatk4} GenotypeGVCFs -R ${ref} -V ${pipeline}/${name}\_v4.1.6.0.g.vcf.gz -O ${vcf}/${name}\_v4.1.6.0.vcf.gz -L ${pipeline}/${name}\_intervals.list 2> ${log}/${name}\_v4.1.6.0\_genotype.log. for ValidateVariants:. java -Xms10G -Xmx10G -jar ${gatk4} ValidateVariants -R ${ref} -V ${name}\_v4.1.6.0.vcf.gz -L ${pipeline}/${name}\_intervals.list --warn-on-errors 2> ${log}/${name}\_v4.1.6.0\_genotype\_valivar.log. the warning in ValidateVariants and the site look like this:. 14:12:15.126 WARN ValidateVariants - \*\*\*\*\* Input 1st\_v4.1.6.0.vcf.gz fails strict validation of type ALL: one or more of the ALT allele(s) for the record at position chr\_1:1088200 are not observed at all in the sample genotypes \*\*\*\*\* ; ; chr\_1 1088200 . T \*,TAAAAAAAAAAAA 64.39 . AC=8,0;AF=0.667,0.00;AN=12;DP=118;ExcessHet=3.0103;FS=0.000;InbreedingCoeff=0.4286;MLEAC=7,7;MLEAF=0.583,0.583;MQ=58.73;QD=32.19;SOR=2.303 GT:AD:DP:GQ:PL ./.:9,0,0:9:.:0,0,0,0,0,0 0/0:9,0,0:9:0:0,0,113,0,113,113 ./.:10,0,0:10:.:0,0,0,0,0,0 ./.:5,0,0:5:.:0,0,0,0,0,0 1/1:0,0,1:1:0:225,15,0,15,0,0 ./.:0,0,0:0:.:0,0,0,0,0,0 ./.:12,0,0:12:.:0,0,0,0,0,0 ./.:8,0,0:8:.:0,0,0,0,0,0 0/0:3,0,0:3:0:0,0,43,0,43,43 ./.:7,0,0:7:.:0,0,0,0,0,0 ./.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6630:1608,Validat,ValidateVariants,1608,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6630,1,['Validat'],['ValidateVariants']
Security,":01,31] [info] Running with database db.url = jdbc:hsqldb:mem:c4b3296a-4b73-4053-b6bf-d4eeb71c8956;shutdown=false;hsqldb.tx=mvcc; [2019-10-01 02:53:01,85] [info] Slf4jLogger started; [2019-10-01 02:53:02,22] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-876ccf5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-10-01 02:53:02,28] [info] Metadata summary refreshing every 1 second.; [2019-10-01 02:53:02,31] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-10-01 02:53:02,31] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-10-01 02:53:02,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-10-01 02:53:02,32] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-10-01 02:53:02,40] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-10-01 02:53:02,43] [info] SingleWorkflowRunnerActor: Version 46.1; [2019-10-01 02:53:02,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-10-01 02:53:02,49] [info] Unspecified type (Unspecified version) workflow c55a06f3-abc1-4db1-8e0f-ea0303caab2c submitted; [2019-10-01 02:53:02,51] [info] SingleWorkflowRunnerActor: Workflow submitted c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,51] [info] 1 new workflows fetched by cromid-876ccf5: c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,52] [info] WorkflowManagerActor Starting workflow c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,53] [info] WorkflowManagerActor Successfully started WorkflowActor-c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,53] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2019-10-01 02:53:02,55",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6189:1661,hash,hash-lookup,1661,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6189,1,['hash'],['hash-lookup']
Security,:05.915 WARN HaplotypeCallerSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: HaplotypeCallerSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 09:38:05.915 INFO HaplotypeCallerSpark - Initializing engine; 09:38:05.915 INFO HaplotypeCallerSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 20/08/15 09:38:06 INFO SparkContext: Running Spark version 2.4.5; 09:38:06.440 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 20/08/15 09:38:06 INFO SparkContext: Submitted application: HaplotypeCallerSpark; 20/08/15 09:38:06 INFO SecurityManager: Changing view acls to: xc278; 20/08/15 09:38:06 INFO SecurityManager: Changing modify acls to: xc278; 20/08/15 09:38:06 INFO SecurityManager: Changing view acls groups to:; 20/08/15 09:38:06 INFO SecurityManager: Changing modify acls groups to:; 20/08/15 09:38:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(xc278); groups with view permissions: Set(); users with modify permissions: Set(xc278); groups with modify permissions: Set(); 20/08/15 09:38:06 INFO Utils: Successfully started service 'sparkDriver' on port 33339.; 20/08/15 09:38:06 INFO SparkEnv: Registering MapOutputTracker; 20/08/15 09:38:06 INFO SparkEnv: Registering BlockManagerMaster; 20/08/15 09:38:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 20/08/15 09:38:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 20/08/15 09:38:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e8d90ed7-8009-437a-8d5e-571b3a582f62; 20/08/15 09:38:06 INFO MemoryStore: MemoryStore started with capacity 15.8 GB; 20/08/15 09:38:06 INFO SparkEnv: Registering OutputCommitCoordinator; 2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6750:4339,Secur,SecurityManager,4339,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6750,7,"['Secur', 'authenticat']","['SecurityManager', 'authentication']"
Security,":08.688 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:49:08.688 INFO PostprocessGermlineCNVCalls - Deflater: IntelDeflater; 12:49:08.688 INFO PostprocessGermlineCNVCalls - Inflater: IntelInflater; 12:49:08.688 INFO PostprocessGermlineCNVCalls - GCS max retries/reopens: 20; 12:49:08.688 INFO PostprocessGermlineCNVCalls - Requester pays: disabled; 12:49:08.688 INFO PostprocessGermlineCNVCalls - Initializing engine; 12:49:12.598 INFO PostprocessGermlineCNVCalls - Done initializing engine; 12:49:15.678 INFO PostprocessGermlineCNVCalls - Shutting down engine; [October 29, 2020 12:49:15 PM MSK] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=2457862144; java.lang.IllegalArgumentException: Records were not strictly sorted in dictionary order.; 	at org.broadinstitute.hellbender.tools.copynumber.arguments.CopyNumberArgumentValidationUtils.validateIntervals(CopyNumberArgumentValidationUtils.java:60); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractLocatableCollection.getShardedCollectionSortOrder(AbstractLocatableCollection.java:142); 	at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.onTraversalStart(PostprocessGermlineCNVCalls.java:297); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1047); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /home/lmbs02/bio/biosoft/gatk/gatk-4.1.8.1/gat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924:9617,validat,validateIntervals,9617,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924,1,['validat'],['validateIntervals']
Security,":11:36 INFO yarn.Client: Requesting a new application from cluster with 3 NodeManagers; 17/10/13 18:11:36 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (164726 MB per container); 17/10/13 18:11:36 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead; 17/10/13 18:11:36 INFO yarn.Client: Setting up container launch context for our AM; 17/10/13 18:11:36 INFO yarn.Client: Setting up the launch environment for our AM container; 17/10/13 18:11:36 INFO yarn.Client: Preparing resources for our AM container; 17/10/13 18:11:37 INFO yarn.Client: Uploading resource file:/tmp/hdfs/spark-c7e5eece-205e-4bce-a69b-4168c9b79045/__spark_conf__2918234914787361986.zip -> hdfs://mg:8020/user/hdfs/.sparkStaging/application_1507856833944_0003/__spark_conf__.zip; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); groups with view permissions: Set(); users with modify permissions: Set(hdfs); groups with modify permissions: Set(); 17/10/13 18:11:37 INFO yarn.Client: Submitting application application_1507856833944_0003 to ResourceManager; 17/10/13 18:11:37 INFO impl.YarnClientImpl: Submitted application application_1507856833944_0003; 17/10/13 18:11:37 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1507856833944_0003 and attemptId None; 17/10/13 18:11:38 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:38 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster h",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:10633,Secur,SecurityManager,10633,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['Secur'],['SecurityManager']
Security,":30:57 INFO yarn.Client: Requesting a new application from cluster with 4 NodeManagers; 18/01/09 18:30:58 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (18432 MB per container); 18/01/09 18:30:58 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead; 18/01/09 18:30:58 INFO yarn.Client: Setting up container launch context for our AM; 18/01/09 18:30:58 INFO yarn.Client: Setting up the launch environment for our AM container; 18/01/09 18:30:58 INFO yarn.Client: Preparing resources for our AM container; 18/01/09 18:30:59 INFO yarn.Client: Uploading resource file:/tmp/sun/spark-5a3e539e-2e2b-4da2-b218-2bda166bd4c0/__spark_conf__7100950787185363106.zip -> hdfs://tele-1:8020/user/sun/.sparkStaging/application_1515493209401_0001/__spark_conf__.zip; 18/01/09 18:31:00 INFO spark.SecurityManager: Changing view acls to: sun; 18/01/09 18:31:00 INFO spark.SecurityManager: Changing modify acls to: sun; 18/01/09 18:31:00 INFO spark.SecurityManager: Changing view acls groups to: ; 18/01/09 18:31:00 INFO spark.SecurityManager: Changing modify acls groups to: ; 18/01/09 18:31:00 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(sun); groups with view permissions: Set(); users with modify permissions: Set(sun); groups with modify permissions: Set(); 18/01/09 18:31:00 INFO yarn.Client: Submitting application application_1515493209401_0001 to ResourceManager; 18/01/09 18:31:00 INFO impl.YarnClientImpl: Submitted application application_1515493209401_0001; 18/01/09 18:31:00 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1515493209401_0001 and attemptId None; 18/01/09 18:31:01 INFO yarn.Client: Application report for application_1515493209401_0001 (state: ACCEPTED); 18/01/09 18:31:01 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster hos",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:11871,Secur,SecurityManager,11871,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['Secur'],['SecurityManager']
Security,":39:44 INFO org.spark_project.jetty.server.Server: Started @3988ms; 17/11/27 20:39:44 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@7fbe38a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 17/11/27 20:39:44 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.6.1-hadoop2; 17/11/27 20:39:45 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at droazen-test-cluster-m/10.240.0.10:8032; 17/11/27 20:39:47 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1511814592376_0002; 17/11/27 20:39:52 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@7fbe38a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 20:39:52.363 INFO CountReadsSpark - Shutting down engine; [November 27, 2017 8:39:52 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.16 minutes.; Runtime.totalMemory()=630718464; code: 0; message: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; reason: null; location: null; retryable: false; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:340); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:197); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:194); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); 	at com.google.cloud.RetryHelper.run(RetryHelper.java:74); 	at com.google.cloud.RetryHelper.runWithRetries(Ret",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994:5710,secur,security,5710,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994,2,"['access', 'secur']","['access', 'security']"
Security,:50:33 PM EST] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=1065353216; java.lang.IllegalArgumentException: Unsupported class file major version 55; 	at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166); 	at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148); 	at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136); 	at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237); 	at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49); 	at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517); 	at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500); 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); 	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134); 	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134); 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); 	at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500); 	at org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175); 	at org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238); 	at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631); 	at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:355); 	at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:307); 	at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureC,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7035:6790,Hash,HashMap,6790,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7035,1,['Hash'],['HashMap']
Security,":57.036 INFO ProgressMeter - Starting traversal; 00:05:57.036 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 00:07:26.967 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr16:10185 the annotation AS_VarDP=59|115|0 was not a numerical value and was ignored; 00:07:26.967 WARN ReferenceConfidenceVariantContextMerger - Reducible annotation 'AS_VarDP' detected, add -G StandardAnnotation -G AS_StandardAnnotation to the command to annotate in the final VC with this annotation.; 00:07:26.991 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.02938786500000001,Cpu time(s),0.029037034000000003; [August 25, 2021 12:07:27 AM EDT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 1.55 minutes.; Runtime.totalMemory()=1807745024; java.lang.NullPointerException; at java.util.HashMap.putMapEntries(HashMap.java:500); at java.util.HashMap.putAll(HashMap.java:784); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.combineAnnotations(VariantAnnotatorEngine.java:211); at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.mergeAttributes(ReferenceConfidenceVariantContextMerger.java:318); at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:142); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFsEngine.callRegion(GenotypeGVCFsEngine.java:130); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.apply(GenotypeGVCFs.java:281); at org.broadinstitute.hellbender.engine.VariantLocusWalker.lambda$traverse$0(VariantLocusWalker.java:135); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7437:8055,Hash,HashMap,8055,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437,1,['Hash'],['HashMap']
Security,":; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz -O zeta_snippet_leftalign_96branch.vcf.gz; 12:55:31.964 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 06, 2018 12:55:32 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 12:55:32.083 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 12:55:32.083 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-25-g0c6f06f-SNAPSHOT; 12:55:32.083 INFO LeftAlignAndTrimVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:55:32.083 INFO LeftAlignAndTrimVariants - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 12:55:32.083 INFO LeftAlignAndTrimVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_111-b14; 12:55:32.083 INFO LeftAlignAndTrimVariants - Start Date/Time: September 6, 2018 12:55:31 PM EDT; 12:55:32.083 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 12:55:32.084 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 12:55:32.084 INFO LeftAlignAndTrimVariants - HTSJDK Version: 2.16.0; 12:55:32.084 INFO LeftAlignAndTrimVariants - Picard ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326:1356,authenticat,authentication,1356,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326,1,['authenticat'],['authentication']
Security,; 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at org.gradle.wrapper.Download.downloadInternal(Download.java:77); 	at org.gradle.wrapper.Download.download(Download.java:44); 	at org.gradle.wrapper.Install$1.call(Install.java:61); 	at org.gradle.wrapper.Install$1.call(Install.java:48); 	at org.gradle.wrapper.ExclusiveFileAccessManager.access(ExclusiveFileAccessManager.java:69); 	at org.gradle.wrapper.Install.createDist(Install.java:48); 	at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:107); 	at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:116); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3375); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3368); 	at org.gradle.wrapper.Download.downloadInternal(Download.java:62); 	... 7 more; Caused by: java.net.SocketException: Connection reset; 	at java.net.SocketInputStream.read(SocketInputStream.java:210); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401:1855,secur,security,1855,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401,1,['secur'],['security']
Security,; 	at sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at org.gradle.wrapper.Download.downloadInternal(Download.java:77); 	at org.gradle.wrapper.Download.download(Download.java:44); 	at org.gradle.wrapper.Install$1.call(Install.java:61); 	at org.gradle.wrapper.Install$1.call(Install.java:48); 	at org.gradle.wrapper.ExclusiveFileAccessManager.access(ExclusiveFileAccessManager.java:69); 	at org.gradle.wrapper.Install.createDist(Install.java:48); 	at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:107); 	at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:116); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.MeteredStream.read(MeteredStream.java:134); 	at java.io.FilterInputStream.read(FilterInputStream.java:133); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3375); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3368); 	at org.gradle.wrapper.Download.downloadInternal(Download.java:62); 	... 7 more; Caused by: java.net.SocketException: Connection reset; 	at java.net.SocketInputStream.read(SocketInputStream.java:210),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401:1721,secur,security,1721,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401,1,['secur'],['security']
Security,; ## master #3228 +/- ##; ===============================================; + Coverage 80.419% 80.427% +0.008% ; Complexity 17290 17290 ; ===============================================; Files 1165 1165 ; Lines 62596 62597 +1 ; Branches 9768 9768 ; ===============================================; + Hits 50339 50345 +6 ; + Misses 8352 8347 -5 ; Partials 3905 3905; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3228?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/3228?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `83.704% <100%> (+0.122%)` | `36 <0> (ø)` | :arrow_down: |; | [.../tools/walkers/validation/CountFalsePositives.java](https://codecov.io/gh/broadinstitute/gatk/pull/3228?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQ291bnRGYWxzZVBvc2l0aXZlcy5qYXZh) | `93.548% <100%> (ø)` | `7 <1> (ø)` | :arrow_down: |; | [.../tools/walkers/validation/FalsePositiveRecord.java](https://codecov.io/gh/broadinstitute/gatk/pull/3228?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vRmFsc2VQb3NpdGl2ZVJlY29yZC5qYXZh) | `100% <100%> (ø)` | `7 <2> (ø)` | :arrow_down: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3228?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `73.649% <0%> (+2.027%)` | `34% <0%> (ø)` | :arrow_down: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/3228?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `70% <0%> (+3.333%)` | `10% <0%> (ø)` | :arrow_down: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3228#issuecomment-314209891:1519,validat,validation,1519,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3228#issuecomment-314209891,1,['validat'],['validation']
Security,"; - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:23942,validat,validate,23942,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['validat'],['validate']
Security,"; --master yarn-client \; --driver-memory 8G \; --conf spark.driver.maxResultSize=0 \; --conf spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; --executor-memory ${execMem}g \; --num-executors $execs \; --executor-cores $cores \; bin/cleanHellbender/gatk/build/libs/gatk-all-*-spark.jar \; ReadsPipelineSpark \; --sparkMaster yarn-client \; -I hdfs:///user/akiezun/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R hdfs:///user/droazen/bqsr/human_g1k_v37.2bit \; --programName ${name} \; -O $bamout \; --knownSites hdfs:////user/akiezun/dbsnp_138.b37.excluding_sites_after_129.vcf \; --emit_original_quals \; --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES; ```. exec=24; cores=5; execMem=25. fails with . ```; java.lang.IllegalArgumentException: SimpleInterval is 1 based, so start must be >= 1, start: 0; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:58); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:33); at org.broadinstitute.hellbender.utils.baq.BAQ.getReferenceWindowForRead(BAQ.java:525); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine$BQSRReferenceWindowFunction.apply(BaseRecalibrationEngine.java:46); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine$BQSRReferenceWindowFunction.apply(BaseRecalibrationEngine.java:41); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithRefBases.lambda$addBases$c54addeb$1(BroadcastJoinReadsWithRefBases.java:52); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrapper",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1234:1070,validat,validatePositions,1070,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1234,1,['validat'],['validatePositions']
Security,; ===============================================; Files 1065 1065 ; Lines 58788 58788 ; Branches 9578 9578 ; ===============================================; + Hits 46310 46315 +5 ; + Misses 8752 8746 -6 ; - Partials 3726 3727 +1; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3989?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `66.667% <ø> (ø)` | `2 <0> (ø)` | :arrow_down: |; | [...adinstitute/hellbender/tools/IndexFeatureFile.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9JbmRleEZlYXR1cmVGaWxlLmphdmE=) | `90.323% <ø> (ø)` | `12 <0> (ø)` | :arrow_down: |; | [...r/tools/walkers/validation/RemoveNearbyIndels.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vUmVtb3ZlTmVhcmJ5SW5kZWxzLmphdmE=) | `90.476% <ø> (ø)` | `5 <0> (ø)` | :arrow_down: |; | [...oadinstitute/hellbender/tools/GatherVcfsCloud.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9HYXRoZXJWY2ZzQ2xvdWQuamF2YQ==) | `70.811% <0%> (ø)` | `40 <0> (ø)` | :arrow_down: |; | [...rg/broadinstitute/hellbender/utils/IndexUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9JbmRleFV0aWxzLmphdmE=) | `80.702% <100%> (ø)` | `16 <2> (ø)` | :arrow_down: |; | [...kers/variantutils/UpdateVCFSequenceDictionary.java](https://codecov.io/gh/broadinstitute/gatk/pull/3989/diff?src=pr&el=tree#diff-c3JjL21haW4va,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3989#issuecomment-352845785:1528,validat,validation,1528,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3989#issuecomment-352845785,1,['validat'],['validation']
Security,=) | `56.522% <0%> (-36.335%)` | `2 <0> (ø)` | |; | [...der/engine/spark/datasources/ReadsSparkSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/6010/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NvdXJjZS5qYXZh) | `63.158% <100%> (+1.825%)` | `18 <1> (+1)` | :arrow_up: |; | [...stitute/hellbender/engine/spark/GATKSparkTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/6010/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvR0FUS1NwYXJrVG9vbC5qYXZh) | `82.873% <100%> (+0.095%)` | `78 <0> (ø)` | :arrow_down: |; | [...ellbender/tools/spark/pathseq/PathSeqBwaSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/6010/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BhdGhTZXFCd2FTcGFyay5qYXZh) | `67.391% <100%> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...tools/spark/validation/CompareDuplicatesSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/6010/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay92YWxpZGF0aW9uL0NvbXBhcmVEdXBsaWNhdGVzU3BhcmsuamF2YQ==) | `89.63% <100%> (ø)` | `40 <0> (ø)` | :arrow_down: |; | [...lbender/tools/spark/pathseq/PathSeqScoreSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/6010/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BhdGhTZXFTY29yZVNwYXJrLmphdmE=) | `58.491% <100%> (+1.083%)` | `7 <0> (ø)` | :arrow_down: |; | [...lkers/varianteval/evaluators/VariantEvaluator.java](https://codecov.io/gh/broadinstitute/gatk/pull/6010/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnRldmFsL2V2YWx1YXRvcnMvVmFyaWFudEV2YWx1YXRvci5qYXZh) | `70% <0%> (-12.353%)` | `12% <0%> (ø)` | |; | [...ls/walkers/varianteval/util/EvaluationContext.java](https://codecov.io,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6010#issuecomment-503050294:2196,validat,validation,2196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6010#issuecomment-503050294,1,['validat'],['validation']
Security,"=; 10:33:06.428 INFO SparkContext - Submitted application: SortSamSpark; 10:33:06.446 INFO ResourceProfile - Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 600, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0); 10:33:06.454 INFO ResourceProfile - Limiting resource is cpu; 10:33:06.455 INFO ResourceProfileManager - Added ResourceProfile id: 0; 10:33:06.500 INFO SecurityManager - Changing view acls to: root; 10:33:06.501 INFO SecurityManager - Changing modify acls to: root; 10:33:06.501 INFO SecurityManager - Changing view acls groups to:; 10:33:06.502 INFO SecurityManager - Changing modify acls groups to:; 10:33:06.502 INFO SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); groups with view permissions: Set(); users with modify permissions: Set(root); groups with modify permissions: Set(); 10:33:06.755 INFO Utils - Successfully started service 'sparkDriver' on port 34861.; 10:33:06.784 INFO SparkEnv - Registering MapOutputTracker; 10:33:06.815 INFO SparkEnv - Registering BlockManagerMaster; 10:33:06.827 INFO BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 10:33:06.828 INFO BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up; 10:33:06.831 INFO SparkEnv - Registering BlockManagerMasterHeartbeat; 10:33:06.846 INFO DiskBlockManager - Created local directory at /raid/tmp/d6/c66ba827e22dbc38625af1cbc85adc/tmp/blockmgr-8dc41ac8-6cf4-4424-9b15-7e2cbfc9e538; 10:33:06.872 INFO MemoryStore - MemoryStore started with capacity 1076.2 GiB; 10:33:06.886 INFO SparkEnv - Registering OutputCommitCoordinator; 10:33:06.916 INFO log - Logging initialized @3948ms to org.sparkproject.jetty.util.log.Slf4j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:42048,Secur,SecurityManager,42048,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,3,"['Secur', 'authenticat']","['SecurityManager', 'authentication']"
Security,===================; Files 1059 1056 -3 ; Lines 59177 59149 -28 ; Branches 9616 9615 -1 ; ==============================================; - Hits 46750 46744 -6 ; + Misses 8689 8667 -22 ; Partials 3738 3738; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4094?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...kers/variantutils/CalculateGenotypePosteriors.java](https://codecov.io/gh/broadinstitute/gatk/pull/4094/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9DYWxjdWxhdGVHZW5vdHlwZVBvc3RlcmlvcnMuamF2YQ==) | `85.915% <ø> (ø)` | `13 <0> (ø)` | :arrow_down: |; | [...stitute/hellbender/tools/HaplotypeCallerSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4094/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9IYXBsb3R5cGVDYWxsZXJTcGFyay5qYXZh) | `82.022% <ø> (ø)` | `23 <0> (ø)` | :arrow_down: |; | [...tmutpileup/ValidateBasicSomaticShortMutations.java](https://codecov.io/gh/broadinstitute/gatk/pull/4094/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9WYWxpZGF0ZUJhc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zLmphdmE=) | `85.965% <ø> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...pipelines/metrics/CollectMultipleMetricsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4094/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvbWV0cmljcy9Db2xsZWN0TXVsdGlwbGVNZXRyaWNzU3BhcmsuamF2YQ==) | `92.593% <ø> (ø)` | `9 <0> (ø)` | :arrow_down: |; | [...s/metrics/CollectBaseDistributionByCycleSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4094/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvbWV0cmljcy9Db2xsZWN0QmFzZURpc3RyaWJ1dGlvbkJ5Q3ljbGVTcGFyay5qYXZh) | `87.037% <ø> (ø)` | `9 <0> (ø)` | :arrow_dow,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4094#issuecomment-356113187:1542,Validat,ValidateBasicSomaticShortMutations,1542,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4094#issuecomment-356113187,1,['Validat'],['ValidateBasicSomaticShortMutations']
Security,====================; Files 1089 1090 +1 ; Lines 64159 64937 +778 ; Branches 10344 10510 +166 ; ===============================================; + Hits 51600 52279 +679 ; - Misses 8498 8569 +71 ; - Partials 4061 4089 +28; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4878?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...s/read/markduplicates/sparkrecords/PairedEnds.java](https://codecov.io/gh/broadinstitute/gatk/pull/4878/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9QYWlyZWRFbmRzLmphdmE=) | `100% <ø> (ø)` | `1 <0> (ø)` | :arrow_down: |; | [...roadinstitute/hellbender/utils/read/ReadUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4878/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL1JlYWRVdGlscy5qYXZh) | `80% <100%> (+0.142%)` | `202 <3> (+3)` | :arrow_up: |; | [...tools/spark/validation/CompareDuplicatesSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4878/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay92YWxpZGF0aW9uL0NvbXBhcmVEdXBsaWNhdGVzU3BhcmsuamF2YQ==) | `84.946% <100%> (+0.502%)` | `24 <3> (ø)` | :arrow_down: |; | [...itute/hellbender/engine/spark/GATKRegistrator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4878/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvR0FUS1JlZ2lzdHJhdG9yLmphdmE=) | `100% <100%> (ø)` | `3 <0> (ø)` | :arrow_down: |; | [...icates/sparkrecords/MarkDuplicatesSparkRecord.java](https://codecov.io/gh/broadinstitute/gatk/pull/4878/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9NYXJrRHVwbGljYXRlc1NwYXJrUmVjb3JkLmphdmE=) | `100% <100%> (ø)` | `7 <3> (ø)` | :arrow_down: |; | [...ils/read/markduplicates/sparkrecords/Fragment.java](https://co,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396338920:1555,validat,validation,1555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396338920,1,['validat'],['validation']
Security,====================================; + Hits 38890 52186 +13296 ; + Misses 21482 8384 -13098 ; + Partials 4232 4024 -208; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4970?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...nce/SegmentedCpxVariantSimpleVariantExtractor.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL1NlZ21lbnRlZENweFZhcmlhbnRTaW1wbGVWYXJpYW50RXh0cmFjdG9yLmphdmE=) | `93.96% <100%> (+8.949%)` | `71 <0> (+5)` | :arrow_up: |; | [.../sv/discovery/inference/CpxVariantInterpreter.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL0NweFZhcmlhbnRJbnRlcnByZXRlci5qYXZh) | `79.839% <100%> (+74.921%)` | `26 <0> (+25)` | :arrow_up: |; | [...tmutpileup/ValidateBasicSomaticShortMutations.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9WYWxpZGF0ZUJhc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zLmphdmE=) | `85.965% <0%> (-0.94%)` | `7% <0%> (-6%)` | |; | [.../hellbender/tools/genomicsdb/GenomicsDBImport.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWljc2RiL0dlbm9taWNzREJJbXBvcnQuamF2YQ==) | `75.758% <0%> (-0.591%)` | `53% <0%> (+7%)` | |; | [...llbender/tools/genomicsdb/GenomicsDBConstants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWljc2RiL0dlbm9taWNzREJDb25zdGFudHMuamF2YQ==) | `0% <0%> (ø)` | `0% <0%> (ø)` | :arrow_down: |; | [...r/tools/walkers/annotator/ClippingRankSumTest.java](https://co,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4970#issuecomment-401503431:1658,Validat,ValidateBasicSomaticShortMutations,1658,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4970#issuecomment-401503431,1,['Validat'],['ValidateBasicSomaticShortMutations']
Security,"=================================================; 10:33:06.427 INFO ResourceUtils - No custom resources configured for spark.driver.; 10:33:06.428 INFO ResourceUtils - ==============================================================; 10:33:06.428 INFO SparkContext - Submitted application: SortSamSpark; 10:33:06.446 INFO ResourceProfile - Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 600, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0); 10:33:06.454 INFO ResourceProfile - Limiting resource is cpu; 10:33:06.455 INFO ResourceProfileManager - Added ResourceProfile id: 0; 10:33:06.500 INFO SecurityManager - Changing view acls to: root; 10:33:06.501 INFO SecurityManager - Changing modify acls to: root; 10:33:06.501 INFO SecurityManager - Changing view acls groups to:; 10:33:06.502 INFO SecurityManager - Changing modify acls groups to:; 10:33:06.502 INFO SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); groups with view permissions: Set(); users with modify permissions: Set(root); groups with modify permissions: Set(); 10:33:06.755 INFO Utils - Successfully started service 'sparkDriver' on port 34861.; 10:33:06.784 INFO SparkEnv - Registering MapOutputTracker; 10:33:06.815 INFO SparkEnv - Registering BlockManagerMaster; 10:33:06.827 INFO BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 10:33:06.828 INFO BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up; 10:33:06.831 INFO SparkEnv - Registering BlockManagerMasterHeartbeat; 10:33:06.846 INFO DiskBlockManager - Created local directory at /raid/tmp/d6/c66ba827e22dbc38625af1cbc85adc/tmp/blockmgr-8dc41ac8-6cf4-4424-9b15-7e2cbfc9e538; 10:33:0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:41912,Secur,SecurityManager,41912,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['Secur'],['SecurityManager']
Security,"=com.google.protobuf:protobuf-java&package-manager=gradle&previous-version=3.23.4&new-version=3.25.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/broadinstitute/gatk/network/alerts). </details>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9004:4437,secur,security,4437,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9004,2,"['Secur', 'secur']","['Security', 'security']"
Security,"> @colinhercus I was able to re-run your command successfully on the latest master branch (not in a release yet). I believe PR #6240 fixed the issue. @Rohit-Satyam @danielecook there's a good chance the errors you encountered are also fixed. If not, please let me know. In reference to your reply, I wish to inform you the problem still stands. > java.lang.IllegalArgumentException: Cannot construct fragment from more than two reads; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:725); at org.broadinstitute.hellbender.utils.read.Fragment.create(Fragment.java:36); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1376); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods.groupEvidence(AlleleLikelihoods.java:595); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:93); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:251); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:320); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:308); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:281); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLinePro",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6230#issuecomment-595805643:480,validat,validateArg,480,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6230#issuecomment-595805643,1,['validat'],['validateArg']
Security,> @lbergelson @gokalpcelik any chance of giving me access to the workspace for the 330 whole exomes?. Hi @nalinigans ; Unfortunately this is on my private company server but I may be able to conduct tests if you need me to. I can generate a fork of gatk and update GenomicsDB to test it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8989#issuecomment-2434590689:51,access,access,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8989#issuecomment-2434590689,1,['access'],['access']
Security,"> Hi @icemduru Looks like your slurm workload manager was configured to have a limit of 48GBs of maximum process memory size per execution. Your java instance is set with -Xmx45G which will cover most of this limit and leaves only a handful of memory space for the native GenomicsDB library. Native libraries work above the heapsize so it is better for you to set your -Xmx to a more sensible size of 8~12GB and leave rest of the memory space to the native library to use.; > ; > Keep in mind that this memory limit on slurm could be set per user not per task therefore you may need to run a single contig at a time or maybe 2 of them simultaneously. Otherwise slurm may interefere with all the tasks and cancel all your jobs.; > ; > One final reminder. We strongly recommend users to set th; [slurm-22680938.out_text.txt](https://github.com/user-attachments/files/16608314/slurm-22680938.out_text.txt); e temporary directory to somewhere else other than /tmp. Slurm workload manager interferes with this preference and sometimes results in premature termination of the gatk processes due to deletion of extracted native library and accessory files.; > ; > I hope this helps. Thank you for your help, but unfortunately it didn't resolve the issue. I've already tried allocating 10GB of memory using the -Xmx10g flag and redirecting the temporary directory away from /tmp. However, GATK is still attempting to consume more than 48GB of RAM, resulting in the termination of my run.; [slurm-22680938.out_text.txt](https://github.com/user-attachments/files/16608325/slurm-22680938.out_text.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8918#issuecomment-2287941632:1133,access,accessory,1133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8918#issuecomment-2287941632,1,['access'],['accessory']
Security,"> Hm - I don't think we can take that last change. Theres not much use in validating args after they've been used by the constructors. Let me see if there is an alternative. Well in the prior commit I just caught CommandLineException (https://github.com/broadinstitute/gatk/pull/6973/commits/814839f498cda8ce627a47229d77fb6cac7ca6e0), but this seemed hacky. . Why cant there be a separate method to create the class and validate args? . There is VariantEvalEngine.validateAndInitialize(), where some classes do analogous checks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-827857883:74,validat,validating,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-827857883,3,['validat'],"['validate', 'validateAndInitialize', 'validating']"
Security,"> How do you know ""without affecting sensitivity"" ?. I ran our validations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3988#issuecomment-352884724:63,validat,validations,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3988#issuecomment-352884724,1,['validat'],['validations']
Security,"> I don't think we've made any guarantees about the thread safety of Funcotator or the associated datasource classes.; > ; > Also, this account seems to be a bot and I can't access its listed home page…; > ; > I can audit the class at some point. https://codesafe.qianxin.com/#/home",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7376#issuecomment-894740783:174,access,access,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7376#issuecomment-894740783,2,"['access', 'audit']","['access', 'audit']"
Security,"> It seems like the patch in 4.1.6 didn't go far enough and that exception needs to be replaced with a continue in all cases. That would work, but I see where I caused the regression upstream. I chopped leading and trailing deletions from haplotype cigars, same as for read cigars, but for haplotypes we want to keep these deletions because the start and end positions need to remain pegged to the reference start and end. I have a fix + regression test branch, which is running on every M2 validation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6533#issuecomment-609115892:491,validat,validation,491,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6533#issuecomment-609115892,1,['validat'],['validation']
Security,> [...] you need to build within a full git clone of the GATK repository [...]. I build as part of the FreeBSD package build. Package builders can never build from git clones because git clones don't preserve fingerprints and fingerprints are needed to maintain security (repeatability of builds). Could you please consider adding an alternative way of determining of version through a build option?. Packages on all OSes would benefit from this. Thank you.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7143#issuecomment-799722491:262,secur,security,262,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7143#issuecomment-799722491,1,['secur'],['security']
Security,"? I can take a look a this issue. ---. @vdauwera commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275578721). Oh, they gave me access to the files but I never took the next step of figuring out which files are relevant. There are twenty thousand samples... I'm not sure what is the best way to approach this. ---. @ldgauthier commented on [Wed Mar 01 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-283365248). It would be too computationally expensive and just generally painful to get; that dropped allele. I'd suggest making a unit test with some fake data.; You'll need two positions: one upstream with a deletion to generate the *; and one for the SNP. I think the dropped allele was a 1bp deletion at the; same position that generated the representation with the extra base at the; end. Give that one a really low quality in its gvcf so it gets dropped.; PLs don't really matter as long as they jive with the quals and aren't hom; ref. You can just grab numbers from any other valid vcf. I think you can do; it with three samples: one with the upstream deletion and *, one with the; AC SNP and one with the low quality deletion. Other combinations will; probably also produce the same bug. There may be an even simpler way to reproduce the bug without the low; quality deletion but I suspect this will work. On Jan 26, 2017 10:02 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. Oh, they gave me access to the files but I never took the next step of; figuring out which files are relevant. There are twenty thousand samples...; I'm not sure what is the best way to approach this. —; You are receiving this because you commented. Reply to this email directly, view it on GitHub; <https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275578721>,; or mute the thread; <https://github.com/notifications/unsubscribe-auth/AGRhdKIgGAjH5_n3wlZ0E2A5xw1TeFg1ks5rWV5DgaJpZM4KQT_3>; .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2959:4669,access,access,4669,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2959,1,['access'],['access']
Security,@Bowen1992 I'm not sure I understand exactly what the problem is. . Is this a problem specific to running tools with genomicsDB? ; What is ParaStor? It sounds like some sort of enterprise file system? Is other software fast when writing to ParaStor? I'm sure we'll be able to debug this since I don't think we have access to a similar system.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8546#issuecomment-1758332891:315,access,access,315,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8546#issuecomment-1758332891,1,['access'],['access']
Security,"@DanishIntizar Hello! Thank you for this pr. This is great to see an official plugin from amazon available. I appreciate that you took the time to make it an optional include. I think if we're going to include it we might as well just add it as one of our normal dependencies though. Assuming there aren't any dependency conflicts it **should** (always a risky statement) be independent from everything else. . Thanks also for identifying the different issues you mentioned. It's expected that it won't work with most picard tools as you discovered, but we're actively in the process of updating more of them too support Paths instead of Files so that will slowly improve. The second issue is more worrisome. We regularly use an equivalent provider with google to read reference files through the exact same code, so I suspect there is either some sort of mismatched assumptions in the way they are handling things. Maybe something strange with the Path.resolve methods or the like. (Or in in the much worse potential case a bug in their look ahead caching.). I'd like to look into that before we'd merge this. Ideally we would have tests for this. Are there any public AWS paths we could read from without any secret authentication?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8672#issuecomment-1930094721:1218,authenticat,authentication,1218,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8672#issuecomment-1930094721,1,['authenticat'],['authentication']
Security,"@EdwardDixon Sure, here's my suggested repair process:. 1. If you haven't already, add an ""upstream"" remote to your git clone via `git remote add upstream git@github.com:broadinstitute/gatk.git` (or `https://github.com/broadinstitute/gatk.git` if you don't have ssh authentication set up with github). 2. `git fetch upstream`. 3. Copy the files you actually intended to change in this PR into a temp directory somewhere. 4. Create a new temporary branch off of `upstream/master`: `git checkout -b avxcheck_repaired upstream/master`. 5. Copy the files you saved in step 3 back into their original locations in the working tree. 6. `git commit -a`. 7. Examine the diff against upstream/master via `git diff upstream/master HEAD`. Verify that the diff is what you expect. 8. Run `git rev-parse HEAD` and save the commit ID it outputs. 9. Switch back to the broken version of the branch: `git checkout avxcheck`. 10. Run `git reset --hard commit_id_from_step_8`. This will force the branch to point to the repaired commit we created in step 6. 11. Run `git push -f origin avxcheck:avxcheck` to force-push the repaired version of the branch into your fork. Then check that it looks ok on github. For avoiding this sort of thing in the future, here's a few tips:. * Never run `git merge` or `git pull`. Always update your branch with changes from the latest gatk master branch via the command: `git fetch upstream && git rebase -i upstream/master`, followed by `git push -f` to push the rebased branch into your fork. * If you've never run `git rebase` before, read a tutorial on it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-437415495:266,authenticat,authentication,266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-437415495,1,['authenticat'],['authentication']
Security,"@Emmalynchen I wouldn't worry about the `log4j:WARN` messages discussed in this thread---they're just harmless annoyances that pop up because we haven't gotten around to making sure the HDF5Library dependency uses the same logger as the rest of the GATK. Looking at your initial post (before you edited it), it looks like DenoiseReadCounts is failing because the panel of normals contains different intervals than those in the read-count collection you are trying to denoise:. ```; 22:50:58.635 INFO SVDDenoisingUtils - Validating sample intervals against original intervals used to build panel of normals...; 22:50:59.487 INFO DenoiseReadCounts - Shutting down engine; [May 7, 2019 10:50:59 PM UTC] org.broadinstitute.hellbender.tools.copynumber.DenoiseReadCounts done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=894959616; java.lang.IllegalArgumentException: Sample intervals must be identical to the original intervals used to build the panel of normals.; ```. You might try asking for more pointers over in the GATK Forums (https://gatkforums.broadinstitute.org/gatk), if you need them. Good luck!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3763#issuecomment-491473550:520,Validat,Validating,520,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3763#issuecomment-491473550,1,['Validat'],['Validating']
Security,"@Horneth Ideally we'd just check up-front whether the bucket has requester pays enabled, and specify the user's default project as the billing project if it is. . It would also be good, I think, if we included a toggle that allows the client to tell the library to throw and refuse to proceed if an attempt is made to access requester-pays data, so that users who don't want to incur GCS access charges can get a hard guarantee that they won't.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4828#issuecomment-394839598:318,access,access,318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4828#issuecomment-394839598,2,['access'],['access']
Security,"@J-Moravec GATK and picard should both handle the same reference files. It typically requires not a gzipped reference, but a bgzipped reference to enable random access ( as I think samtools does as well.). You will need several auxiliary files with the reference. You need the .fai index as well as the .gzi index. . Could you post the stack trace you're receiving as well as additional information about your reference file to help debug the problem you're seeing?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6590#issuecomment-625875092:161,access,access,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6590#issuecomment-625875092,1,['access'],['access']
Security,"@LeeTL1220 @droazen This is ready for review. It modestly improves all of our validations except Dream challenge 4, which I suspect is because the synthetic data doesn't respect mate pairing. To account for that I added an advanced option to turn off mate-awareness. @kachulis Thanks for catching the error in finding fragments.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5831#issuecomment-490599827:78,validat,validations,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5831#issuecomment-490599827,1,['validat'],['validations']
Security,"@LeeTL1220 @katevoss @ruchim I started exposing all optional task-level parameters in the somatic workflows so that they could be specified via json when the workflows are used as subworkflows. E.g., `CNVSomaticPanelWorkflow.PreprocessIntervals.bin_length` is an optional task-level parameter that can be specified properly via json when `CNVSomaticPanelWorkflow` is the top-level workflow, but not when `CNVSomaticPanelWorkflow` is used as a subworkflow. This is because `MetaWorkflow.CNVSomaticPanelWorkflow.PreprocessIntervals.bin_length` cannot be set, correct?. However, things quickly became very messy. For example, alongside parameters like `bin_length` which are unique to the PreprocessIntervals task, we also have a lot of optional runtime parameters that are named generic things like `mem` which are not. So to expose these, we'd have to have workflow-level parameters with names like `preprocess_intervals_mem`, etc. It seems like this is exactly the problem the expected functionality would solve, if only it worked past the subworkflow level and the namespace is propagated as one would expect. Requiring that these be exposed also partially obviates the reason for having optional task-level arguments in the first place---what's the point of having them be optional if I have to add lines of code to expose all of them at the workflow level?. So again, I'm strongly against exposing all inputs for a particular workflow on the off-chance that that workflow might be used as a subworkflow. This adds a lot of unnecessary boilerplate that quickly gets very messy. I think that this problem should instead be solved by dynamically bubbling up all inputs, optional or required, at all levels. Anyway, I'm not going to try to tackle this before release, which I think was OK with @LeeTL1220. However, after release, I'd be happy to sit down and discuss how we want to do this sort of thing going forward.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3980#issuecomment-355830670:824,expose,expose,824,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3980#issuecomment-355830670,3,['expose'],"['expose', 'exposed']"
Security,"@LeeTL1220 Fixed PathSeq test BAMs so that all reads have read groups with an SM tag. This will make them pass the WellFormedReadFilter. To be thorough, I made sure they also check out with ValidateSameFile. The BAMs are now properly sorted, and also I made a small fix so that unmapped mate flags are set properly in the filter tool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3206:190,Validat,ValidateSameFile,190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3206,1,['Validat'],['ValidateSameFile']
Security,"@LeeTL1220 Having started to implement this. I have a number of design questions that would be informed by your usecases. . Firstly, is there a reason to preserve symbolic alleles? It seems as though spanning deletions could/should be dropped as in most cases there is another variant context representing that deletion elsewhere in your file? Should there be validation around dropping spanning deletion symbolic alleles to ensure we aren't dropping a spanning deletion that isn't represented anywhere else? What about nocalls? . Your example suggests that we rely on the header line counts for subsetting annotations, if there is a disagreement in the header do you want any more sophisticated behavior than just throwing? My understanding is that we are lenient with splitting in htsjdk and there have been some mislabeled header lines in the past that would make this an expected state. Furthermore, most allele specific annotators are of type string because there is no standard for ""|"" delimiters which makes them hard to handle properly. @ldgauthier do you have any suggestions as to how to detect and handle allele specific annotations?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4976#issuecomment-404949363:360,validat,validation,360,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4976#issuecomment-404949363,1,['validat'],['validation']
Security,"@LeeTL1220 I have a fast python implementation of the above. It'll take a little bit of additional code to make it output segment files. I can add that and start running some validation data, or I can just go ahead and start coding up the Java implementation, depending on how long you think it'll take to put together some validation runs up through DenoiseReadCounts. Do we want to improve the ReCapSegCaller behind CallSegments while we're at it? @davidbenjamin perhaps you can briefly remind me of the idea behind your initial caller and of the issues it had.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324128827:175,validat,validation,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324128827,2,['validat'],['validation']
Security,"@LeeTL1220 I went ahead and exposed more mem_gb parameters, which is convenient when we want to go below 250bp and the pair WDL is used as a subworkflow. Please review carefully!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4364#issuecomment-363787268:28,expose,exposed,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4364#issuecomment-363787268,1,['expose'],['exposed']
Security,"@LeeTL1220 Latest commit includes the rollback. I will create a separate branch for you that is rebased on sl_preprocess. Looking at it again, I initially described the change to you incorrectly. I thought it was ""similar CR || similar AF -> merge"" to ""similar CR && similar AF -> merge"", but that's not actually the case; it's instead ""similar according to credible interval 1 || similar according to credible interval 2 -> merge"" to ""similar according to credible interval 1 && similar according to credible interval 2 -> merge"". Probably the `&&` behavior is way too conservative, so I think rolling back to the `||` behavior would be fine for release. Let's double check with the validation just to be sure.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4046#issuecomment-355467989:684,validat,validation,684,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4046#issuecomment-355467989,1,['validat'],['validation']
Security,"@LeeTL1220 Not sure if you will absolutely need this for HCC1143 WES validation, but just be aware that this change is coming soon.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3981#issuecomment-352128732:69,validat,validation,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3981#issuecomment-352128732,1,['validat'],['validation']
Security,"@LeeTL1220 Note that I've validated with womtool, but as we've seen (#4281), changes of this sort (which deal with optional parameters, etc.) may slip through even if tests pass. You should take a careful look to make sure everything is in order!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4288#issuecomment-361293506:26,validat,validated,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4288#issuecomment-361293506,1,['validat'],['validated']
Security,"@LeeTL1220 OK, see the sl_change_model_segments_defaults_rebased branch. For validation, I'd say that sweeping the following should suffice:. Array[Float] kernel_variance_allele_fractions = [0.025, 0.05, 0.25]; Array[Float] smoothing_thresholds_allele_fraction = [2.0, 10.0, 50.0]; Array[Float] smoothing_thresholds_copy_ratio = [2.0, 10.0, 50.0]",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4046#issuecomment-355468401:77,validat,validation,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4046#issuecomment-355468401,1,['validat'],['validation']
Security,"@LeeTL1220 OK, tweaked the message a bit. I think I'm OK with this going in for the next point release. This is the sort of thing for which it will be nice to have the automatic validations, as a sanity check.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4292#issuecomment-363828979:178,validat,validations,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4292#issuecomment-363828979,1,['validat'],['validations']
Security,@LeeTL1220 The criteria in my opinion are being the best Mutect and being stable. Were you suggesting waiting for some validation like MC3?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4384#issuecomment-365660852:119,validat,validation,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4384#issuecomment-365660852,1,['validat'],['validation']
Security,"@LeeTL1220 This will make it easy to basically re-do the MC3 analysis as if M2 had been there from the beginning. The idea is:. * run M2; * merge M2 variants into MC3; * validate all variants (M2 *and* MC3); * compare all callers on equal footing. It would be nice if there were a barclay annotation for an unsupported feature, but I'm not aware of one. . .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5007:170,validat,validate,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5007,1,['validat'],['validate']
Security,"@LeeTL1220 commented on [Mon Feb 01 2016](https://github.com/broadinstitute/gatk-protected/issues/343). In `CreatePanelOfNormals` make anonymize a flag that defaults to `false`. (i.e. `--anonymize`) In other words, by default, we do _not_ produce an anonymized PoN. We could also use a separate tool that takes a pre-existing PoN and anonymizes it. . To anonymize a PoN:; - [ ] Determine which fields are private. At the very least: `fnt_control_matrix`, `log_normals`, and `log_normals_pinv`. _There may be others -- please investigate as part of this issue_; - [ ] Have `CreatePanelOfNormals` delete the fields as the last step.; - [ ] Make sure that `HDF5PoN` produces reasonable error messages if one of these fields is accessed in an anonymized PoN.; - [ ] Create CLI that can take existing PoN and delete the fields. ---. @LeeTL1220 commented on [Mon Feb 01 2016](https://github.com/broadinstitute/gatk-protected/issues/343#issuecomment-178022285). This is necessary since we may want to share PoNs and the PoN files cannot have any private data. ---. @LeeTL1220 commented on [Wed Mar 02 2016](https://github.com/broadinstitute/gatk-protected/issues/343#issuecomment-191420153). Moving this to later milestone, unless it becomes more urgent.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2835:724,access,accessed,724,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2835,1,['access'],['accessed']
Security,"@Ning-310 The error you're getting here (""Did not inflate expected amount"") implies that your input file is likely corrupt. Can you try running the tool `PrintBGZFBlockInformation` to validate the compressed blocks in your `.vcf.gz`?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7114#issuecomment-793015796:184,validat,validate,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7114#issuecomment-793015796,1,['validat'],['validate']
Security,@SHuang-Broad what is PipelineOptions needed for ... does one need it to access the reference if it stored in something that is not a ordinary file? (e.g. GS bucket?),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3476#issuecomment-325025607:73,access,access,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3476#issuecomment-325025607,1,['access'],['access']
Security,"@Sun-shan Hi, could you try running with the `--disable-sequence-dictionary-validation` command?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112#issuecomment-357043845:76,validat,validation,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112#issuecomment-357043845,1,['validat'],['validation']
Security,"@Tintest Sorry for the slow reply. I'm not sure exactly what the issue is. I've never seen this exact error before. . I have two guesses, one is that there's something really weird going on in spark that's causing that null pointer exception which is killing the heartbeat. I'm not sure how to debug that without your access to your input data . The other theory which I think is more likely, is that you're running out of memory and it's causing weird errors to occur. How much memory is available to your job? You can set the java -Xmx value with `--java-options ""-Xmx120g""` as a GATK option. I would check that you're not running out of memory on your machine, or giving the job too little. I think for BaseRecalibratorSpark you want at least 2-4g per core, but haven't tested it in a long time so I might be wrong about that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4515#issuecomment-373250134:318,access,access,318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515#issuecomment-373250134,1,['access'],['access']
Security,"@WanessaMGoes It looks like most/all of your reads are getting filtered out by GATK's `WellformedReadFilter`:. ```; 17:07:23.141 INFO DepthOfCoverage - 1031666 read(s) filtered by: WellformedReadFilter; ```. Could you try running `ValidateSamFile` on your bam, and paste the result here?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7332#issuecomment-882767886:231,Validat,ValidateSamFile,231,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7332#issuecomment-882767886,1,['Validat'],['ValidateSamFile']
Security,@Zepeng-Mu There seems to be an unsupported character in your reference fasta. Can you verify the integrity hash of your reference? Could you also try re-generating the fasta index using `samtools faidx`?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911#issuecomment-716737017:98,integrity,integrity,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911#issuecomment-716737017,2,"['hash', 'integrity']","['hash', 'integrity']"
Security,"@ahaessly Could you please take a look at this? We had a request to change this argument which wasn't previously possible since it was hardcoded into the WDL task. Ideally I'd like to expose all of the arguments but even wiring this one through the imported WDL was annoying. I tried making it an input to the task with a value like this:. ```; task M2 {; input {; Int max_reads_arg = 75; }; ...; ```; which looks a lot cleaner (don't have to make sure you wire it through from the main inputs), but when I looked in Terra it didn't actually expose the argument (I'm assuming because the M2 task is in a sub-workflow). Any thoughts on how to make this better so I can expose everything? Or is this the best way to do that at the moment?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6739:184,expose,expose,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6739,3,['expose'],['expose']
Security,@alanhoyle Can you tell us whether the 400 Bad Request error is repeatable -- did you see it more than once? Oftentimes when accessing cloud data we encounter transient errors like this that go away on their own.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-724225557:125,access,accessing,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-724225557,1,['access'],['accessing']
Security,"@ashwini06 . This bam appears to be malformed and it fails Picard ValidateSamFile. I think you'll need to examine the earlier stages of your pipeline that produce your bam to ensure you get a correctly formed bam. I'm going to close this ticket now since this doesn't appear to be an issue with Mutect2. (base) wm462-624:Downloads fleharty$ java -jar $PICARD ValidateSamFile I=concatenated_ACC5611A1_XXXXXX_consensusalign_ds.bam ; INFO	2020-07-14 11:25:52	ValidateSamFile	. ********** NOTE: Picard's command line syntax is changing.; **********; ********** For more information, please see:; ********** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition); **********; ********** The command line looks like this in the new syntax:; **********; ********** ValidateSamFile -I concatenated_ACC5611A1_XXXXXX_consensusalign_ds.bam; **********. 11:25:52.673 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/fleharty/resources/picard.jar!/com/intel/gkl/native/libgkl_compression.dylib; [Tue Jul 14 11:25:52 EDT 2020] ValidateSamFile INPUT=concatenated_ACC5611A1_XXXXXX_consensusalign_ds.bam MODE=VERBOSE MAX_OUTPUT=100 IGNORE_WARNINGS=false VALIDATE_INDEX=true INDEX_VALIDATION_STRINGENCY=EXHAUSTIVE IS_BISULFITE_SEQUENCED=false MAX_OPEN_TEMP_FILES=8000 SKIP_MATE_VALIDATION=false VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false; [Tue Jul 14 11:25:52 EDT 2020] Executing as fleharty@wm462-624 on Mac OS X 10.15.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_191-b12; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.20.4-SNAPSHOT; WARNING	2020-07-14 11:25:52	ValidateSamFile	NM validation cannot be performed without the reference. All other validations will still occur.; ERROR: Record 18321, Read name U",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:66,Validat,ValidateSamFile,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,4,['Validat'],['ValidateSamFile']
Security,"@asmirnov239 commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/1000). Right now if a user disables MAPPED filter, which is a default filter for CalculateTargetCoverage tool, it will fail with the following uninformative exception (unless somehow all reads are mapped):; ```; java.lang.IllegalArgumentException: the input location cannot be null; 	at org.broadinstitute.hellbender.utils.Utils.nonNull(Utils.java:549); 	at org.broadinstitute.hellbender.tools.exome.HashedListTargetCollection.indexRange(HashedListTargetCollection.java:152); 	at org.broadinstitute.hellbender.tools.exome.CalculateTargetCoverage.apply(CalculateTargetCoverage.java:298); ```; We should guard against it and throw an exception before the traversal starts if MAPPED filter is disabled. ---. @asmirnov239 commented on [Fri Apr 21 2017](https://github.com/broadinstitute/gatk-protected/issues/1000#issuecomment-296295294). Since there is no direct API call to access the list of resolved filters(after command line parsing) this bug fix will have to wait until [broadinstitute/barclay#38](https://github.com/broadinstitute/barclay/pull/38) is merged",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2976:500,Hash,HashedListTargetCollection,500,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2976,3,"['Hash', 'access']","['HashedListTargetCollection', 'access']"
Security,@asmirnov239 commented on [Thu Sep 29 2016](https://github.com/broadinstitute/gatk-protected/issues/727). Here is the stack trace:. ```; java.lang.IllegalArgumentException: the 'to' index must be between 'from' and the length of the data/position sequence; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:609); at org.broadinstitute.hellbender.utils.param.ParamUtils.inRange(ParamUtils.java:80); at org.broadinstitute.hellbender.utils.hmm.ForwardBackwardAlgorithm$Result.logProbability(ForwardBackwardAlgorithm.java:141); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.lambda$calculateLog10GP$6(GenotypeCopyNumberTriStateSegments.java:197); at java.util.stream.ReferencePipeline$6$1.accept(ReferencePipeline.java:244); at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545); at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); at java.util.stream.DoublePipeline.toArray(DoublePipeline.java:506); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.calculateLog10GP(GenotypeCopyNumberTriStateSegments.java:198); at org.broadinstitute.hellbender.tools.exome.germlinehmm.GenotypeCopyNumberTriStateSegments.lambda$composeVariantContext$0(GenotypeCopyNumberTriStateSegments.java:125); at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250); at java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:110); at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSeq,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2898:302,validat,validateArg,302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2898,1,['validat'],['validateArg']
Security,"@asmirnov239 looks like you are getting an NPE---remember that intervals are resolved after argument validation, so you need to do the check later. Also, good point, I think you can get a singleton after scattering if you get unlucky with your shards. ; Perhaps change the check to a filtering step in GermlineCNVCaller?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6559#issuecomment-617310137:101,validat,validation,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6559#issuecomment-617310137,1,['validat'],['validation']
Security,"@bbimber Hmn, yeah, think it needs someone who has direct write access. I'll get a thumb from a teammate. Thanks for looking at it and for the pr!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8752#issuecomment-2023092567:64,access,access,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8752#issuecomment-2023092567,1,['access'],['access']
Security,"@bbimber I was hoping this could be reduced to a single new `File, PedigreeValidationType` constructor overload, and the new `File` getter (and without any changes to the existing subclasses). Its also not a perfect solution, but I'd prefer to minimize addition of any new methods that expose founderIDs or SampleDB, since we aspire to factor out the existing code that uses those from this class completely. As for the failed test, it looks like the tests timed out for some reason, hopefully transient, but it I'm guessing its unrelated.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-855914785:286,expose,expose,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-855914785,1,['expose'],['expose']
Security,"@bbimber We just added full versions of the B37 and HG38 references to the repo a couple of days ago. You'll have to rebase this branch on current master to access them, but it might make it easier to port some of the GATK3 tests that use b37.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-430743842:157,access,access,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-430743842,1,['access'],['access']
Security,"@bensprung So I thought this would be a trivial change. It turns out that encoding the Genotype as something like `1/1` is done way down in the depths of the VCF encoder and isn't exposed in an accessible way. It's going to need a (hopefully simple) change to the underlying htsjdk library to expose that machinery. It shouldn't be hard, it just means it will take a bit longer to get to than I expected.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8160#issuecomment-1397695685:180,expose,exposed,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8160#issuecomment-1397695685,6,"['access', 'expose']","['accessible', 'expose', 'exposed']"
Security,"@bhanugandham To get around this issue, the user can run `ValidateVariants` with the `--validation-type-to-exclude ALLELES` argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6630#issuecomment-640713793:58,Validat,ValidateVariants,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6630#issuecomment-640713793,2,"['Validat', 'validat']","['ValidateVariants', 'validation-type-to-exclude']"
Security,"@bshifaw related to what Sam was saying - we also have a few standard resources needed to run the workflows that we would like to share with users. What is the standard procedure for doing so? Ideally they would be bundled with featured workspaces, but also accessible from outside of Terra",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6017#issuecomment-506504159:258,access,accessible,258,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6017#issuecomment-506504159,1,['access'],['accessible']
Security,"@byoo The easiest thing would be if you can upload it to google cloud and make it publicly visible. Then we can copy it over and you can delete it. Or if you can share your google account name I can grant you upload permission on a bucket we own. (If you want to not publish it to the world you can email it to me louisb@broadinstitute.org ) . Alternatively, if you can't use google cloud, you could upload it to the gatk ftp site. See this article here about how to connect to upload: https://gatkforums.broadinstitute.org/gatk/discussion/1215/how-can-i-access-the-gsa-public-ftp-server.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-465237693:555,access,access-the-gsa-public-ftp-server,555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-465237693,1,['access'],['access-the-gsa-public-ftp-server']
Security,"@chandrans I don't have access to dsde-docs so I can't see the ticket/test files (we asked @vdauwera to give me access last week for a different issue, but I don't have it yet).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4525#issuecomment-377963524:24,access,access,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4525#issuecomment-377963524,2,['access'],['access']
Security,"@chapmanb We were able to reproduce a failure with your command line. This looks like an issue related to JNI and garbage collection that is exposed by setting `-Xmx46965m` and `-XX:+UseSerialGC`, but it needs further debugging. To confirm, can you please try running without specifying these javaOptions? Something like this:; ```; ./gatk-launch --javaOptions '-Djava.io.tmpdir=$TEMP_DIR' \; ApplyBQSRSpark \; --sparkMaster local[16] \; --input $BAM_IN \; --output $BAM_OUT \; --bqsr_recal_file $BQSR_RECAL \; -- \; --conf spark.local.dir=$SPARK_LOCAL_DIR; ```. FYI, we see better performance from Spark when using an SSD for spark.local.dir. The `--conf ` option above shows how to set the spark.local.dir.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3605#issuecomment-332370070:141,expose,exposed,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3605#issuecomment-332370070,1,['expose'],['exposed']
Security,"@cmnbroad : first - would it be possible to kick off travis tests? i refactored this and dont seem to be able to do that. Second, yes, I was trying to reorder and condense the commits but clearly didnt work. I think the problem was trying to put your GATK3 commit first (which would seem to make sense). in any case, I just recreated this, putting a pristine GATK3 first, following a consolidated set of my commits with 1) the limited core changes, 2) the meat of the VariantEval port, and 3) A separate commit with a port of GATK3 VariantEvalIntegrationTest which is useful for validation but should not be merged. To your points:. 1) I substantially cut down the incoming large files, mostly by limiting the intervals of new large VCFs. 2) On the plugin: this was discussed above, and I initially also pointed out this should ultimately go into Barclay. You are actually the one who proposed staging it in GATK. I am not entirely sure I understand the reticence on plugins; however, my goal is to get VariantEval ported by touching as little of it as possible. This is already sucking up a ton of time. I flipped VariantEvalUtils to gather a list of classes from the appropriate package instead of a full-on plugin. That should satisfy that concern?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431913735:579,validat,validation,579,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431913735,2,['validat'],['validation']
Security,@cmnbroad @SHuang-Broad . The cluster uses Kerberos for authentication. This style of pathname works for reading the cram file which is on the hdfs file system. . Using the hadoop shell works fine.... ; hadoop fs -ls hdfs:///project/casa/gcad/adsp.cc; Found 2 items; drwxrwxr-x - zhucc casa 0 2018-04-27 14:59 hdfs:///project/casa/gcad/adsp.cc/cram; drwxrwxr-x - farrell casa 0 2018-05-08 15:21 hdfs:///project/casa/gcad/adsp.cc/sv. When I change this to a local file a similar error occurs. The program runs for 40 plus minutes and then gets the following error. . ```; 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 92.0 in stage 13.0 (TID 68093) in 1108 ms on scc-q01.scc.bu.edu (executor 24) (101/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 101.0 in stage 13.0 (TID 68102) in 1061 ms on scc-q01.scc.bu.edu (executor 6) (102/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 34.0 in stage 13.0 (TID 68035) in 1653 ms on scc-q01.scc.bu.edu (executor 24) (103/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 44.0 in stage 13.0 (TID 68045) in 1553 ms on scc-q07.scc.bu.edu (executor 7) (104/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 63.0 in stage 13.0 (TID 68064) in 1362 ms on scc-q01.scc.bu.edu (executor 24) (105/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 102.0 in stage 13.0 (TID 68103) in 1057 ms on scc-q07.scc.bu.edu (executor 7) (106/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 39.0 in stage 13.0 (TID 68040) in 1604 ms on scc-q06.scc.bu.edu (executor 23) (107/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 5.0 in stage 13.0 (TID 68006) in 2015 ms on scc-q01.scc.bu.edu (executor 24) (108/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 10.0 in stage 13.0 (TID 68011) in 1928 ms on scc-q06.scc.bu.edu (executor 23) (109/116); 2019-05-19 19:09:41 INFO TaskSetManager:54 - Finished task 15.0 in stage 13.0 (TID 68016) in 1865 ms,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-494014590:56,authenticat,authentication,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-494014590,2,['authenticat'],['authentication']
Security,"@cmnbroad @lbergelson The cram index looks like it has all the info required to generate the splits without using the CramContainerIterator to look at the cram file directly. . Could using the crai index for splits be a potential solution to the glacially slow cram split generation? ; ; CRAM index. A CRAM index is a gzipped tab delimited file containing the following columns:; 1. Sequence id; 2. Alignment start; 3. Alignment span; 4. **Container start byte offset in the file**; 5. Slice start byte offset in the container data (‘blocks’); 6. Slice bytes; Each line represents a slice in the CRAM file. Please note that all slices must be listed in index file. In Hadoop-bam this code could read the crai instead of the cram to find the container boundaries. public List<InputSplit> getSplits(List<InputSplit> splits, Configuration conf); throws IOException {; // update splits to align with CRAM container boundaries; List<InputSplit> newSplits = new ArrayList<InputSplit>();; Map<Path, List<Long>> fileToOffsets = new HashMap<Path, List<Long>>();; for (InputSplit split : splits) {; FileSplit fileSplit = (FileSplit) split;; Path path = fileSplit.getPath();; List<Long> containerOffsets = fileToOffsets.get(path);; if (containerOffsets == null) {; containerOffsets = getContainerOffsets(conf, path);; fileToOffsets.put(path, containerOffsets);; }; long newStart = nextContainerOffset(containerOffsets, fileSplit.getStart());; long newEnd = nextContainerOffset(containerOffsets, fileSplit.getStart() +; fileSplit.getLength());; long newLength = newEnd - newStart;; if (newLength == 0) { // split is wholly within a container; continue;; }; FileSplit newSplit = new FileSplit(fileSplit.getPath(), newStart, newLength,; fileSplit.getLocations());; newSplits.add(newSplit);; }; return newSplits;; }",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-373078699:1024,Hash,HashMap,1024,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-373078699,1,['Hash'],['HashMap']
Security,"@cmnbroad After thinking about this I went ahead and created VariantEvalEngine. Doing this in one PR will simplify some of the sticking points around what is a final change vs. what it expected to be fixed later. With this change, the goal is to strip most logic from VariantEval into the engine. This engine can be constructed with a VariantEvalArgumentCollection, and any kind of GATKTool as the owner. I tried to minimize the amount of context the VariantEvalEngine needed to hang on to. This means all the child classes have visibility on the VariantEvalEngine, but are no longer directly exposed to either the walker class or the argument collection. . All the logic around gathering the arguments to form DrivingVariants is moved to a static method in VariantEvalEngine. . I also rebased and fixed conflicts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-750428516:593,expose,exposed,593,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-750428516,2,['expose'],['exposed']
Security,"@cmnbroad Could you take a quick look at this again when you get a chance? I changed a few things in the untested methods to respond to @magicDGS's comments, but since they're so important and basically untested I think it would be good for someone to scan them. I change the behavior of failing customCommandLineValidation to always throw, so it's consistent failing regular command line validation as well. I also fixed some comment formatting.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2340#issuecomment-275150232:389,validat,validation,389,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2340#issuecomment-275150232,1,['validat'],['validation']
Security,"@cmnbroad I cleaned up some of the hashes and was able to create the conda environment locally. Can you try on your mac? We'll see if tests pass on Travis as well, then merge if all is good.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4061#issuecomment-355647907:35,hash,hashes,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4061#issuecomment-355647907,1,['hash'],['hashes']
Security,@cmnbroad I have added a validation/warning step to the pedigree annotations. It suffers from the issue where specifying both possibleDenovo and one of the other ped annotations will not affect warning between annotations. Since I'm choosing to only spit out warnings to the user this should probably be acceptable.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-466118658:25,validat,validation,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-466118658,1,['validat'],['validation']
Security,"@cmnbroad I saw your comment in https://github.com/broadinstitute/gatk/pull/7822. I'm trying to add WDL validation here, using the infrastructure in gradle. I was interested here in just validating the WDLs in the scripts directory.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7826#issuecomment-1116666702:104,validat,validation,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7826#issuecomment-1116666702,2,['validat'],"['validating', 'validation']"
Security,"@cmnbroad I started down this road. i wanted to make sure i follow your reasoning on some of this. I think you propose to change the tool arguments such that each input VCF is tagged by name (like -V:eval vcf1.vcf.gz -V:comp vcf2.vcf.gz), instead of different argument names. This is paired with a change to set the 'source' on each VariantContext to match the name of the source feature context. Unless I'm missing something, this basically makes everything identified by strings, with no direct FeatureInput <-> VariantContext reference, right? . Presumably, MultiVariantWalkerGroupedOnStart could implement something like:. protected Map<FeatureInput<VariantContext>, List<VariantContext>> groupVariantsByFeatureInput(List<VariantContext> variants) {; Map<String, FeatureInput<VariantContext>> sourceMap = new HashMap<>();; getDrivingVariantsFeatureInputs().forEach(x -> sourceMap.put(x.getName(), x));; ; Map<FeatureInput<VariantContext>, List<VariantContext>> ret = new HashMap<>();; variants.forEach(vc -> {; FeatureInput<VariantContext> fi = sourceMap.get(vc.getSource());; if (fi == null) {; //possibly throw? ; }; ; List<VariantContext> l = ret.getOrDefault(fi, new ArrayList<>());; l.add(vc);; ret.put(fi, l);; }); ; ; return ret;; }",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5439#issuecomment-730532600:813,Hash,HashMap,813,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5439#issuecomment-730532600,2,['Hash'],['HashMap']
Security,"@cmnbroad I updated VariantQC and identified one minor difference in behavior associated with VariantEvalEngine. Contig stratification assigns level based on all the contigs. If user-supplied contigs are given, it should defer to these. This PR addresses this, and adds a test case. Note: I put the getContigNames() method into VariantEvalEngine, but it would also be possible to keep this in Config, but expose a getter for userSuppliedIntervals. It seemed marginally better to keep that private.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7238:405,expose,expose,405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7238,1,['expose'],['expose']
Security,"@cmnbroad OK - what about this proposal? I just added a protected getter and fixed the typo in 'annotation'? We could expose a constructor based way to set PedigreeValidationType, but if you dont really want to expose more of the guts of PedigreeAnnotation to subclasses prior to splitting apart founderIds and pedigree, what about keeping this as simple and minimal as possible?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-855970929:118,expose,expose,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-855970929,4,['expose'],['expose']
Security,"@cmnbroad OK, considerable progress here. I was able to adjust behavior such that only two tests have changed behavior from GATK4/master. I think this is now correct. One instance of changed behavior is the Snpeff/overlap one we discussed above. The second is the one where we now provide the full genome as REF, not the truncated genome. I think this difference is justified since the tool now requires a reference, and the prior version was arguably too lenient on validation of contigs. Anyway, this branch now also removes by debugging code and comments. I think it is ready for a review. To some other questions you had above:. 1) The HashMap<FeatureInput<VariantContext>, HashMap<String, Collection<VariantContext>>> can be wrapped in a class with just a couple of methods, so we don't have to manifest that long type all over the place. I realize that's non-optimal, but this isnt anything I introduced here. I would really like to keep this PR as limited as we can, and address some larger refactoring in a different PR, once we've migrated to MultiVariantWalkerGroupedOnStart. 2) I know this PR still in an interim state, but passing the VariantWalker in as an argument to the comp methods doesn't seem like a step forward to me. If we can't solve that problem completely in this PR (which is fine, I'm all for trying to contain this), are those changes necessary ? Perhaps that part should just wait for the next round. As noted above, I'd like to propose this as iterative, with a second PR coming soon. I did this b/c it moved us toward not needing to pass around the walker. It minimizes the code that has access to the walker (as opposed to setting it after creating the instance of the Evaluator, etc. Yes, it exposes it for two methods, but those classes no longer hang on to it. I would like to ultimately remove this entirely. 3) To re-iterate testEvalTrackWithoutGenotypesWithSampleFields: the input file, noGenotypes.vcf, has a header dictionary with the full set of contigs, and a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-747619130:467,validat,validation,467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-747619130,3,"['Hash', 'validat']","['HashMap', 'validation']"
Security,"@cmnbroad Sorry to bug here, but I am wondering if it would be possible for someone to review. This is a limited change that basically consolidates some internal code in PedigreeAnnotation and exposes a couple protected getters. Tests are passing. Thanks in advance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-853234849:193,expose,exposes,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-853234849,1,['expose'],['exposes']
Security,"@cmnbroad Thank you for pointing out those build failures and even digging down to the apparent cause! I investigated and the issue wasn't inability to decompress gzip files (or at least wasn't only that), but XReadLines trims the lines by default and my code doesn't. The ""expected"" files have an extra tab at the end of some lines (the CHROM line for example) that this was picking up. What I've done is updated XReadLines so it can take Paths as input, so we get good matching behavior without having to duplicate code. While I was at it I also exposed XReadLines' ability to strip out comments, so assertEqualTextFiles didn't need to re-implement it anymore. Assuming Travis passes, this should be ready to review. I have the feeling we're getting close!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-456919065:548,expose,exposed,548,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-456919065,1,['expose'],['exposed']
Security,"@cmnbroad Thank you so much for the reply. I don't have a small test case for you, but I can provide some other information.; It is RNA seq data and passes validation check (`java -jar picard.jar ValidateSamFile I=S3_2.unmapped.split.bam MODE=SUMMARY`).; BaseRecalibrator cmd:; `gatk BaseRecalibrator -R Homo_sapiens_assembly38.fasta -I S3_2.unmapped.split.bam --use-original-qualities -O S3_2.unmapped.recal_data.csv -known-sites Homo_sapiens_assembly38.dbsnp138.vcf -known-sites Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --known-sites Homo_sapiens_assembly38.known_indels.vcf.gz`; ApplyBQSR cmd:; `gatk ApplyBQSR -R Homo_sapiens_assembly38.fasta -I S3_2.unmapped.split.bam -O S3_2.unmapped.aligned.duplicates_marked.recalibrated.bam -bqsr S3_2.unmapped.recal_data.csv --add-output-sam-program-record --use-original-qualities`; RecalTables in S3_2.unmapped.recal_data.csv are empty. Here is the screen dump of BaseRecalibrator and ApplyBQSR.; BaseRecalibrator; ```; Using GATK jar <XXX>/gatk-4.1.4.1-83-g031c407-SNAPSHOT/gatk-package-4.1.4.1-83-g031c407-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar <XXX>/gatk-4.1.4.1-83-g031c407-SNAPSHOT/gatk-package-4.1.4.1-83-g031c407-SNAPSHOT-local.jar BaseRecalibrator -R Homo_sapiens_assembly38.fasta -I S3_2.unmapped.split.bam --use-original-qualities -O S3_2.unmapped.recal_data.csv -known-sites Homo_sapiens_assembly38.dbsnp138.vcf -known-sites Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --known-sites Homo_sapiens_assembly38.known_indels.vcf.gz; 23:39:34.668 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:<XXX>/gatk-4.1.4.1-83-g031c407-SNAPSHOT/gatk-package-4.1.4.1-83-g031c407-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 26, 2020 11:39:34 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6242#issuecomment-592005237:156,validat,validation,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6242#issuecomment-592005237,2,"['Validat', 'validat']","['ValidateSamFile', 'validation']"
Security,"@cmnbroad rebasing is done. To summarize changes since your last review:. - I backed out the earlier changes to FeatureInput/FeatureDataSource in favor of those from #7219 ; - I dont entirely know why this didnt hit before, but I made an update to VariantStratifiers to make tests pass. See: https://github.com/broadinstitute/gatk/pull/6973/commits/1569a909d3dc2301337e46441cc0cd969843c8d1. The gist is that we now instantiate those classes and pass VariantEvalEngine. Two of these classes had validation in their constructors, and could throw a CommandLineException if the tool was executed with bad arguments. This exception was getting caught and re-thrown as GATKException with the misleading message ""Problem making an instance of ...."". This proposal is to make a separate VariantStratifier.validateArgs() method, with a default no-op validation, and to call this only after instantiation. This was already exercised under the tests, such as testMultipleEvalTracksAlleleCountWithoutMerge(). VariantEval tests pass locally for me. With luck, tests will pass here.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-827805993:494,validat,validation,494,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-827805993,3,['validat'],"['validateArgs', 'validation']"
Security,"@cmnbroad 👍 to adding an advanced command line option for it. . @magicDGS Our goal is to make it unnecessary for normal users to ever need to see a stacktrace. We're definitely not at the point yet where every UserException produces either a) the complete information necessary to debug, or even b) the correct information. We're trying to fix all those cases, but there's a lot of possible failure modes between cloud access, spark, filesystem plugins, etc, so it's going to be an issue for a while. . I don't think it will hurt the user experience to have an extra commandline argument for it. Printing the stacktrace when debug is on isn't a bad idea, but I think it's better to have finer grained control over it. . The other issue is that it's easier to explain to an unsophisticated user how to set an extra commmand line argument rather than trying to get them to set the environment variable which well be helpful for our support team when they're trying to debug someone's problem. (especially since setting the environment variables may be different on a spark cluster than on a local run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285390394:419,access,access,419,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285390394,2,['access'],['access']
Security,"@cmnbroad, sorry for the delayed response. I was in Taiwan giving a workshop last week and then I fell ill (run of the mill cold virus). The Comms team has been migrating issue tracking to a new system on Monday.com, which I am just now familiarizing myself to as I have been occupied with Taiwan workshop preparation. Forum questions are tracked in a separate system, Zendesk. The SOPs towards handling work for the two different systems are still under development so the best way to ensure you are up to date with the progress of work is to contact Robert @rcmajovski. The previous GitHub board that we used at https://github.com/broadinstitute/dsde-docs is still up and I still have my issue tickets here as I haven't had a chance to migrate these. . I do not know if you have access, but here is the link to track the issue ticket on Monday.com:; https://dsp-comms.monday.com/boards/145112271/. Just to orient you, if you click on the issue, a sideboard slides out from the right and you can comment on the work:; ![screenshot 2018-12-13 22 51 00](https://user-images.githubusercontent.com/11543866/49982483-c0040b80-ff2a-11e8-99a5-b6aae33d0311.png). It seems I've been assigned to update these documents. I'm unfamiliar with JEXL itself. I will survey the work that needs to be done and let you know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5422#issuecomment-447206700:781,access,access,781,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5422#issuecomment-447206700,1,['access'],['access']
Security,"@cwhelan I was actually debating with myself about whether to include the initialization script here, as it was living in the bucket referred to in the creation script.; So we could do this:; always store the initialization script locally with the creation script instead of referring to a script living remotely, and makes that a required argument. The good: this makes it easier to track changes; The bad: initialization script must be removed from the bucket to avoid tracking possible different versions. A non-technical issue: we are ""delivering"" SGA in the initialization script, if that comes in to this repo, legal might have a problem with it. On the other hand, if the initialization script lives in a place only we can access, we are ""installing SGA for our own use"", which is not a problem with the GPL license.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2435#issuecomment-285093289:730,access,access,730,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2435#issuecomment-285093289,1,['access'],['access']
Security,@cwhelan could you give andrei and sam access to the FC dsde-methods-sv-dev workspace can investigate the bug. I and Steve had tried without success.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5217#issuecomment-424435081:39,access,access,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5217#issuecomment-424435081,1,['access'],['access']
Security,"@davidbenjamin @ldgauthier: in #6263 you added --force-output-intervals to GenotypeGVCFs, which forces the tool to output variants based on a whitelist of sites. I believe this exposed a pre-existing, not related bug. GenotypeGVCFsEngine.removeNonRefAlleles() currently assumes the input has only one alternate allele. If the gVCF has a site with 3 or more alleles, GenotypeGVCFsEngine.removeNonRefAlleles() isnt going to work as intended. If any NON_REF is found, it *should* remove ALT allele header lines and return the new VC with NON_REF removed. It currently only does this if ""newAlleles.size() == 1"", which I assume is a proxy for not having alternates. That assumes the input had only 2 alleles, which isnt safe. This PR includes a fix for this. When I started investigating this I made a repro case (the attached VCF) and test case in GenotypeGVCFsIntegration test that uses --force-output-intervals to illustrate this. Now that the actual problem is clearer, I could understand if you dont want to add more test data to GATK. . I tried to write a unit test for removeNonRefAlleles(), but it didnt seem like it was going to be easy to make a new instance of GenotypeGVCFsEngine.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6406:177,expose,exposed,177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6406,1,['expose'],['exposed']
Security,"@davidbenjamin How's the patch coming? Did the M2 validation tests pass on your branch? We'll definitely try to expedite the code review, but I'll think we'll want some additional heavy-duty testing prior to release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6533#issuecomment-610995045:50,validat,validation,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6533#issuecomment-610995045,1,['validat'],['validation']
Security,@davidbenjamin I have asked the user that reported this issue to share their file and will let you know as soon as I can get access.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6098#issuecomment-530197163:125,access,access,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098#issuecomment-530197163,1,['access'],['access']
Security,"@davidbenjamin I made the requested changes and submitted novaseq validation jobs. They haven't failed yet, but I'll monitor the jobs and make changes to the wdl as needed. Will let you know when they finish.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4895#issuecomment-408528614:66,validat,validation,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4895#issuecomment-408528614,1,['validat'],['validation']
Security,"@davidbenjamin I've been looking at this with @nh13 and I think what's going on here is a little different that @nh13 described. Specifically when I try to reproduce this I get results very similar to those shown above, but with the MNP calling turned on I also get a second variant at `chr2:241815307`. I don't have access to the original calls @nh13 was looking at, but I suspect they may contain this call too. So I get the following with MNP support on (I'm not sure why I'm not getting them phased):. ```; chr2 241815307 . CA TG 962.73 . GT:AD:DP:GQ:PL 0/1:26,34:60:99:1000,0,758; chr2 241815308 . A G 2214.77 . GT:AD:DP:GQ:PL 1/1:1,60:61:99:1243,136,0; ```. This makes it look a lot like the issue described in #5523. I'm attaching ; ![an IGV screenshot of the region](https://user-images.githubusercontent.com/1609210/53673861-55b85880-3c47-11e9-9338-43c5ba40b5b6.png) and a SAM file, [NA24694.chr2.241815307.sam.gz](https://github.com/broadinstitute/gatk/files/2921535/NA24694.chr2.241815307.sam.gz), for the region shown that reproduces this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5696#issuecomment-468859167:317,access,access,317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5696#issuecomment-468859167,1,['access'],['access']
Security,"@davidbenjamin I've got a munrosa_bams_bugreport.tar.gz (2.1 MB) ready for you -- I'm trying to upload to the ftp side via the instructions [here](https://gatkforums.broadinstitute.org/gatk/discussion/1894/how-do-i-submit-a-detailed-bug-report), but I haven't been able to get access this morning due to the 20 user limit. Is there any other way I can send it over to you? I'd prefer not to post here.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-402257370:277,access,access,277,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-402257370,1,['access'],['access']
Security,"@davidbenjamin I've significantly refactored the production code, see the last commit. Most of this refactoring was to done make the code for the accounting of different modes (SNP/INDEL/both x BGMM/python x non/allele-specific) more minimal and straightforward. I've also combined the score/apply steps using the TwoPassVariantWalker. There's still lots of documentation, cleanup, and hardening/validation to be done, but most of the key methods and design choices have been documented, so I think it could be worth a quick review at this stage. Again, no need to nitpick code-style details, etc. (unless you really want to!) In the meantime, I'm going to do some more testing/tieout to make sure the refactor didn't break anything. This covers ~1800 LOC, which is roughly 50% of the equivalent VQSR code. Even modulo the remaining work just mentioned, which may add a few hundred LOC, I think this is a decent improvement---additional functionality, stability, etc. notwithstanding!. There's stubs for adding the truth-sensitivity conversion you proposed---should be pretty straightforward. I think it should also still be pretty easy for future pushes to add features like extraction/downsampling of unlabeled data, etc., but please do keep an eye out for design choices that may ultimately be constraining.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7659#issuecomment-1044836946:396,validat,validation,396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7659#issuecomment-1044836946,1,['validat'],['validation']
Security,"@davidbenjamin Note that this is not exposed in CollectAllelicCounts either and is set to 30 by default. Our default set of read filters is also less stringent. However, we do expose a threshold on minimum base quality. Just a few more things to consider when we unify the two tools!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4011#issuecomment-354334621:37,expose,exposed,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4011#issuecomment-354334621,2,['expose'],"['expose', 'exposed']"
Security,"@davidbenjamin again, sorry to keep bugging on this thread, but it's been a while and we're really hoping to push these changes through since they're blocking a project. I believe I addressed everything in your review. I did identify another (arguably pre-existing) issue in GenotypGVCFs that would be exposed whenever it runs in all-sites mode or in force-output mode - the lack of allele trimming. This PR addresses that, including test cases, though I havent heard back about this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6406#issuecomment-581418093:302,expose,exposed,302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6406#issuecomment-581418093,1,['expose'],['exposed']
Security,"@davidbenjamin at long last, back to you. I updated the nio wdl too, and it passes the Firecloud M2 wdl validation with the HCC sample, but not with the cram. But that's because that cram file is aligned to hg38, whereas the workspace uses hg19. I didn't touch anything related to the CramToBam task in the nio wdl so I think we're OK.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5599#issuecomment-474900012:104,validat,validation,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5599#issuecomment-474900012,1,['validat'],['validation']
Security,"@davidbenjamin commented on [Mon Mar 27 2017](https://github.com/broadinstitute/gatk-protected/issues/958). We currently have an ICE exome normal-normal analysis set up i.e. you can `cd` into `/dsde/working/davidben/mutect/validations/normalNormal` and `/Users/home/davidben/cromwell/run_sge.sh normal_normal.wdl normal_normal.json` is all you need to get the analysis. Let's set this up for a few other replicate sets, which we can grab from the Palantir wiki.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2961:223,validat,validations,223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2961,1,['validat'],['validations']
Security,"@davidbenjamin commented on [Sat May 27 2017](https://github.com/broadinstitute/gatk-protected/issues/1112). In `SomaticGenotypingEngine::callMutations` and `HaplotypeCallerGenotypingEngine::assignGenotypeLikelihoods` there is a line of code after the call is made but before the variant is annotated:; ```java; ReadLikelihoods annotationLikelihoods = prepareReadAlleleLikelihoodsForAnnotation(likelihoods...); ```; Is this really necessary? It seems quite defensible to annotate using the same likelihoods from which the variant call is derived. As far as Mutect is concerned, the standard will be whether our validations are better or at least no worse without it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3022:611,validat,validations,611,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3022,1,['validat'],['validations']
Security,"@davidbenjamin commented on [Sun May 28 2017](https://github.com/broadinstitute/gatk-protected/issues/1114). HaplotypeCaller and Mutect by default assemble reads with kmer sizes of 10 and 25. 10 seems extremely small given the low error rates of Illumina sequencing. It's worth investigating how the Mutect validations are affected by increasing these values. ---. @ldgauthier commented on [Tue May 30 2017](https://github.com/broadinstitute/gatk-protected/issues/1114#issuecomment-305032024). Investigate away, but keep in mind bigger kmers introduce more ""dangling tails"", which may end up dropping evidence at the ends of reads. If you end up diving into the assembly graphs, I'm happy to consult. It's a deep, dark rabbit hole, but I've been there before and I know the way. ;)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3024:307,validat,validations,307,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3024,1,['validat'],['validations']
Security,"@davidbenjamin commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gatk-protected/issues/903). Since most of the work is in setting up the necessary tools and pipelines to evaluate, I will lump the actual act of evaluating on specific data into this single ticket. We need to:. * CRSP specificity: apply Takuto's `mutect2-replicate-validation.wdl` on the CRSP NA12878 replicates.; * CRSP sensitivity: apply the (currently in-progress) hapmap sensitivity pipeline to CRSP data.; * cfDNA: run cfDNA samples and matched solid tumor samples (which we already have from Viktor) and run the concordance tool.; * FFPE: run FFPE and matched non-FFPE samples and run the concordance tool.; * tumor-only: run some TCGA samples with and without their matched normal and run the concordance tool. ---. @davidbenjamin commented on [Wed Mar 15 2017](https://github.com/broadinstitute/gatk-protected/issues/903#issuecomment-286795503). Update: CRSP sensitivity and specificity have been run several times, cfDNA is currently running for the first time. FFPE and tumor-only will use the same wdl as cfDNA, so we'll run those once cfDNA finishes. ---. @davidbenjamin commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gatk-protected/issues/903#issuecomment-287675783). Update: everything done except FFPE and tumor-only. FFPE will use the same wdl as cfDNA.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2942:345,validat,validation,345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2942,1,['validat'],['validation']
Security,"@davidbenjamin mutect2_pon.wdl and mutect2.wdl worked great without docker installed. Thanks!. @samuelklee @sooheelee As a user, I found a json template useful for two reasons, though it may be up to how a wdl is written.; 1) womtool generates inputs from all the dependent wdls including unnecessary ones for the workflow. (e.g. mutect2_pon.wdl); 2) womtool didn't provide default values. Looking at mutect_resources.wdl, I wondered what the good value for minimum_allele_frequency is (or what GATK team used for creating the resource bundle). In addition, I thought a test to validate every wdl would be helpful. (womtool validate [a wdl])",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4188#issuecomment-360544256:578,validat,validate,578,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4188#issuecomment-360544256,2,['validat'],['validate']
Security,"@davidbenjamin, et al. I have two recommendations:. 1) Though I prefer to work symbolically and through proofs, it might be nice to first expand on the validation by proof in the JavaDoc - including for the specific function's header - and anywhere else where necessary across the GATK code, just for sanity's sake, and for tying things together neatly and properly. This process of always going through the mathematical steps alerts me when I code that I have not missed anything. . 2) When dealing with multiple levels of transformations, it probably would be good to formulate a collection of complete set of simple tests. Since like you mentioned {phased} is a subset (⊂) of {unphased}, then the paths of phased genotypes one works with would also be ideal to test on. Does this function have any validation tests confirming the correct likelihoods, which would be performed for both phased and unphased genotypes? These can be generated tests, if original files do not exists. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221:152,validat,validation,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221,2,['validat'],['validation']
Security,"@davidbernick thanks!. I noticed that the existing jobs are now failing with a GCS error (see https://gatk-jenkins.broadinstitute.org/job/gatk-perf-test-spark-markeddupe/436/console):. ```; Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; ```. There has been a change to the GCS library (https://github.com/broadinstitute/gatk/commit/b47838c9a5fa172ed6669ed4872b04d91c962a85), but when I ran a GCS pipeline manually on my machine it worked fine, even with this change. Any thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3573#issuecomment-329446325:219,secur,security,219,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3573#issuecomment-329446325,2,"['access', 'secur']","['access', 'security']"
Security,"@doazen I need to re-review this myself, and see what more validation I can do. I hate to miss the release, but I won't be able to do that today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7402#issuecomment-952203649:59,validat,validation,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7402#issuecomment-952203649,1,['validat'],['validation']
Security,"@droazen +1 for being affected by this issue in production. As this is in production (same as @schelhorn , with big pharma which have very strict security requirements), and as 4.1.8.0 contains critical security vulnerabilities that were mitigated in subsequent releases, we are in a serious pickle here. @jhl667 how did you conclude that 4.1.8.0 performs better than newer versions? What we see is that it emits more variants, but after filtering and intersecting with other callers (i.e. Strelka), we get more variants and a ""better"" result (we can't really define ""better"" - it's merely an observation) with 4.2.4.1.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1407439000:146,secur,security,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1407439000,2,['secur'],['security']
Security,"@droazen , I was able to reproduce your result. I tried to isolate what made it work or not. I tried with two kinds of inputs:; - on the hellbender bucket, or; - on my own bucket. I tried with two choices for `GOOGLE_APPLICATION_CREDENTIALS`:; - default credentials, or; - my own. I tried with two different clusters:; - one created in the Broad project, or; - one created in my own project. With every one of those eight combinations, I got the same result: the dreaded ""Error code 404 trying to get security access token from Compute Engine metadata for the default service account."". ```; ./gatk-launch CountReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -- --sparkRunner GCS --cluster jp-test-cluster --executor-cores 2 --num-executors 2; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-352147413:501,secur,security,501,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-352147413,4,"['access', 'secur']","['access', 'security']"
Security,"@droazen - That won't be solved by the current #3447, because there is no way of fine-tune the codecs: I require to being able to add/remove concrete classes, and exclude codecs from a concrete package. An example is a custom codec implementation for some feature, to provide extra-validation for the downstream toolkit. This would be even more useful if HTSJDK is moving to an interface-based library...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-337622596:282,validat,validation,282,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-337622596,1,['validat'],['validation']
Security,@droazen : Thanks a lot for prioritizing and attending to this. The security posture has greatly improved from where we started. Community greatly benefits from your effort. I have migrated to using the 4.5 release after some regression testing. Below is a list of critical and high findings with 4.5 release. There are links to snyk version update recommendations. I know sometimes its not easy just to upgrade the library version as we could end up with run time errors. I am adding this here so that its handy when ever you look at this further. Thanks again. . packageName | version | severity | language | module_id; -- | -- | -- | -- | --; com.google.protobuf:protobuf-java | 3.7.1 | high | java | [SNYK-JAVA-COMGOOGLEPROTOBUF-2331703 ](https://security.snyk.io/vuln/SNYK-JAVA-COMGOOGLEPROTOBUF-2331703 ); com.google.protobuf:protobuf-java | 3.7.1 | high | java | [SNYK-JAVA-COMGOOGLEPROTOBUF-3167772](https://security.snyk.io/vuln/SNYK-JAVA-COMGOOGLEPROTOBUF-3167772); io.netty:netty-codec-http2 | 4.1.96.Final | high | java | [SNYK-JAVA-IONETTY-5953332](https://security.snyk.io/vuln/SNYK-JAVA-IONETTY-5953332); log4j:log4j | 1.2.17 | high | java | [SNYK-JAVA-LOG4J-2342645](https://security.snyk.io/vuln/SNYK-JAVA-LOG4J-2342645); log4j:log4j | 1.2.17 | high | java | [SNYK-JAVA-LOG4J-2342646](https://security.snyk.io/vuln/SNYK-JAVA-LOG4J-2342646); log4j:log4j | 1.2.17 | high | java | [SNYK-JAVA-LOG4J-2342647](https://security.snyk.io/vuln/SNYK-JAVA-LOG4J-2342647); log4j:log4j | 1.2.17 | critical | java | [SNYK-JAVA-LOG4J-572732](https://security.snyk.io/vuln/SNYK-JAVA-LOG4J-572732); net.minidev:json-smart | 1.3.2 | high | java | [SNYK-JAVA-NETMINIDEV-3369748](https://security.snyk.io/vuln/SNYK-JAVA-NETMINIDEV-3369748); org.apache.zookeeper:zookeeper | 3.6.3 | high | java | [SNYK-JAVA-ORGAPACHEZOOKEEPER-5961102](https://security.snyk.io/vuln/SNYK-JAVA-ORGAPACHEZOOKEEPER-5961102); org.codehaus.jettison:jettison | 1.1 | high | java | [SNYK-JAVA-ORGCODEHAUSJETTISON-3168085](https://,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1890593067:68,secur,security,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1890593067,3,['secur'],['security']
Security,"@droazen @cmnbroad @mbabadi I generally agree with the sentiments expressed in #4127, except that I think it's OK to require a conda environment (or even use of the Docker) for these particular tools. How we should validate this requirement is another question. We can discuss more with @vdauwera. @stefandiederich Hopefully once you get the conda environment set up you will be able to run the tools. We would definitely appreciate any feedback you might be able to provide. Note that the gCNV model is relatively sophisticated, so there may be some parameters (which control the priors for the model as well as how inference is performed) that you will need to adjust for your data. Depending on the number of intervals/bins you are using and your memory constraints, you may also need to scatter across multiple GermlineCNVCaller runs; see how things are done in the WDLs here: https://github.com/broadinstitute/gatk/tree/master/scripts/cnv_wdl/germline. As you noted, this pipeline is still in beta. We are currently running several evaluations and hope to soon release some Best Practices recommendations for the aforementioned parameter values that should work well for various data types generated at the Broad. We will also have some blog or forum posts that explain the new CNV pipelines in more detail coming soon---stay tuned!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357034364:215,validat,validate,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357034364,2,['validat'],['validate']
Security,"@droazen @davidbenjamin any thoughts regarding the last bullet above in https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471 on possible integration tests? Started looking at this today and was wondering if you might have any suggestions. Ideally, we'd want to test that the exposure was done correctly through the 3 affected tools: HaplotypeCaller, Mutect2, and FilterAlignmentArtifacts. I can certainly take the approach outlined above and 1) on master, pick one or more integration tests for each tool, then generate results by changing the original unexposed constants and running on the relevant test data, 2) on this branch, commit those new results, then add corresponding versions of the integration tests that change the exposed inputs and check against the results. However, not sure if we'll want to clutter the repo with more test files just for this sort of exposing of constants, and such tests don't really feel complete anyway. So alternatively, I could probably write a script to do essentially the same thing and just check consistency between the branches for a bunch of randomly generated SW parameter values, perhaps also running on more substantial test files for each tool. I can document this process and then we can move on without committing any new tests or test files once we're satisfied that the exposure was done correctly. Or if you guys have additional suggestions, would be glad to hear them!. Finally, it looks like FilterAlignmentArtifacts doesn't have any integration tests for correctness---let me know if there are auxiliary tests we'd want to run there. Anyway, probably overthinking things, but the exposure was enough of a headache that I want to make sure I did it right. But would also rather fully hash out what to do beforehand, so I don't end up having to redo things after review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-896314077:749,expose,exposed,749,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-896314077,2,"['expose', 'hash']","['exposed', 'hash']"
Security,@droazen @lbergelson -- this is blocking the JG run. I can get you access to the underlying data if necessary but hoping the stack trace will point to something obvious,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2713#issuecomment-301329590:67,access,access,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2713#issuecomment-301329590,1,['access'],['access']
Security,"@droazen @lbergelson I'm not really sure how to do this. It's easy in SVN, but not in git. We would need to insert the version info in the labels of the dockerfile when we cut a release -- the release version (e.g. 4.beta.4) not the git hash. How is GotC doing it?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3645#issuecomment-333543566:237,hash,hash,237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3645#issuecomment-333543566,1,['hash'],['hash']
Security,"@droazen Applied your steps, I hope correctly - the pull request looks clean now. Your steps were a huge help. . The travis build looks like failing now, for reasons not obviously connected with our commit:. `Error: (converted from warning) unable to access index for repository http://cran.mtu.edu/src/contrib; Execution halted; The command ""if [[ $TEST_DOCKER != true ]]; then sudo mkdir -p /usr/local/lib/R/; sudo mkdir -p site-library; sudo ln -sFv ~/site-library /usr/local/lib/R/site-library; sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9; sudo add-apt-repository ""deb http://cran.rstudio.com/bin/linux/ubuntu trusty/""; sudo apt-get update; sudo apt-get install -y --force-yes r-base-dev=3.1.3-1trusty; sudo apt-get install -y --force-yes r-base-core=3.1.3-1trusty; sudo Rscript scripts/docker/gatkbase/install_R_packages.R; fi;"" failed and exited with 1 during .`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-437922888:251,access,access,251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-437922888,1,['access'],['access']
Security,@droazen Didn't she validate the input bam though?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317047601:20,validat,validate,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317047601,1,['validat'],['validate']
Security,"@droazen I have a PR for gatk-bwa-mem that adds a footer to the image file so that we can test integrity. I also added code to test every (I think) call that returns an error indication, and pass this info up the chain. Could you review the PR or delegate, please?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3209#issuecomment-313504079:95,integrity,integrity,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3209#issuecomment-313504079,1,['integrity'],['integrity']
Security,"@droazen I have forward that question to GP and will get back to you once I get their answer. (At least, to my knowledge, I can't determine this without their help since I only have access to the output but not the pipeline.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7755#issuecomment-1099616963:182,access,access,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7755#issuecomment-1099616963,1,['access'],['access']
Security,@droazen I just looked and it seems that the only other big one I added was `--disable-artificial-haplotype-recovery` and that one is very esoteric indeed and doesn't need to be exposed I don't think.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6737#issuecomment-668197410:178,expose,exposed,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6737#issuecomment-668197410,1,['expose'],['exposed']
Security,@droazen I realized I should probably have exposed these methods for protected since we'd need to duplicated them in order to do broadinstitute/gatk-protected#1013. this includes the changes in #2630,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2631:43,expose,exposed,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2631,1,['expose'],['exposed']
Security,"@droazen I thought I had updated this months ago saying that I was finished validating, and that it was ready for review, but it appears that I didn't. But anyway its ready.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7402#issuecomment-1108948701:76,validat,validating,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7402#issuecomment-1108948701,1,['validat'],['validating']
Security,@droazen I would rather not get into why exactly the CNV tests are relying on bogus intervals that don't pass validation and what to do about it on this branch. I would rather get this version of things in now to help in at least the -L interval file use case,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7295#issuecomment-860948476:110,validat,validation,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7295#issuecomment-860948476,1,['validat'],['validation']
Security,@droazen I'm not authorized either.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3033#issuecomment-306542782:17,authoriz,authorized,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3033#issuecomment-306542782,1,['authoriz'],['authorized']
Security,@droazen No objection here. It may be that my changing the db access to read only fixed the issue.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4413#issuecomment-413247125:62,access,access,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4413#issuecomment-413247125,1,['access'],['access']
Security,@droazen The PR is #6544. James has reviewed and requested a few more tests. It's working fine on validations including ~30 exomes and ~15 genomes.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6533#issuecomment-611223334:98,validat,validations,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6533#issuecomment-611223334,1,['validat'],['validations']
Security,"@droazen Yes, please. Sorry for not catching this! Turns out that unpaired reads that pass all the M2 read filters and show evidence of a SNV are rare enough that they don't show up in any of the M2 validations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5121#issuecomment-413611150:199,validat,validations,199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5121#issuecomment-413611150,1,['validat'],['validations']
Security,"@droazen any thoughts how we should proceed here, if at all? @ldgauthier reminded me that this story was unfinished and is getting a little stale. @fleharty take note if we want to report progress on this front to our MalariaGEN collaborators. On my end, there are a couple of things to do:; - [x] rebase and resolve conflicts; - [x] change TSV input as discussed above; - [x] add doc strings for new arguments; - [x] add integration tests to make absolutely sure exposure was done correctly, perhaps? I'm open to discussion about how this should be done. Complete coverage here will be difficult and perhaps not worth the effort, but I can probably put in a few tests that make sure changing the hard-coded values in master and doing the same via the exposed parameters in this branch have the same effect on a few existing test cases. However, while I'm doing the last three, I wonder if we could run whatever canonical evaluations/optimizations we have to see whether it's worth consolidating some of the parameter sets at this stage? I think there's an argument for having at least two sets (haplotype-to-reference + read-to-haplotype), but I'm not sure how to justify having a separate set for dangling heads/tails. But also not sure which set the latter should be consolidated with---@jamesemery thoughts? Again, let me reiterate that it seems that many of these parameter values were chosen arbitrarily (or, if not, that the procedure for choosing them has been lost). As a start, you can see the results of some optimizations I did on the CHM mix on slide 15 at https://docs.google.com/presentation/d/1zGuquAZWSUQ-wNxp8D6HhGNjIaFcV0_X9WAS4LODbEo/edit?usp=sharing Here, I optimized over haplotype-to-reference + read-to-haplotype SW parameters on various metrics after variant normalization using vcfeval. These optimizations were done using the Bayesian optimization framework I prototyped long ago (see https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer and http",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471:752,expose,exposed,752,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471,1,['expose'],['exposed']
Security,"@droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473). This capability would be useful if it turns out that the CNV tools (for example) need to be released much more frequently than the GATK as a whole. We don't want a release of the CNV tools to be blocked for a long time because something else in the toolkit (like the `HaplotypeCaller`) is not ready for release. . There could be a `properties` file in the jar that controls which tools are exposed via the command-line -- this way we could publish a jar that exposes only the CNV tools, for example. An alternative approach would be to use branching and cherry-picking to do this kind of selective release, or split the GATK into even more repositories, but I'm not sure those approaches would be preferable. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215491991). This came out of a discussion between myself and @LeeTL1220 . ---. @lbergelson commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2851:492,expose,exposed,492,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851,2,['expose'],"['exposed', 'exposes']"
Security,"@droazen it looked like it was going to work but then—. ```java; @Override; final protected ReferenceDataSource directlyAccessEngineReferenceDataSource() {; throw new GATKException(""Should never directly access the engine ReferenceDataSource in walker tool classes "" +; ""outside of the engine package. Walker tools should get their data via apply() instead."");; }; ```. Also, this is really not the responsibility of the tool class—it should be handled as part of the walker.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6512#issuecomment-618429411:204,access,access,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6512#issuecomment-618429411,1,['access'],['access']
Security,@droazen sorry for a late response. I agree moving to java 17 would help. I do see that GATK itself is using the newer version of log4j but then its the transitive dependencies for the libraries used that bring in the older version of log4j. . this creates situations that the final compiled jar has both version of the log4j and this could create problems. . Gatk being a very useful tool gets integrated in multiple other tools and pipelines so in a way affecting the security posture of where its being used. The risk might be low being a standalone cli tool but its a very hard conversation with info security :) . May I ask for a ballpark ETA for the new version? Appreciate the work thats gone into this tool.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1448897264:470,secur,security,470,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1448897264,2,['secur'],['security']
Security,@droazen still giving error:. com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3592#issuecomment-330918132:102,secur,security,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3592#issuecomment-330918132,2,"['access', 'secur']","['access', 'security']"
Security,"@droazen thanks for the quick response! Just to be clear, my concerns were about testing that I didn't somehow screw up the original behavior through the exposure, not just testing that *some* behavior was exposed. But message received---will keep things on the simple side!. Also, please see the plots in #5564 to get an idea of the effect on outputs, if you haven't already. Would appreciate any thoughts you might have on that thread!. Will try to get this done in the next day or two, thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-896328697:206,expose,exposed,206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-896328697,2,['expose'],['exposed']
Security,"@droazen yes, at least to a point we have this worked out. we really needed something besides CombineGVCFs in order to scale, but GenomicsDB definitely is a new format and I hope the tool keeps getting fleshed out. One final question: is there a way to check the integrity of a GenomicsDB instance? With a VCF one could at least iterate it, or check that it's a valid (non-truncated) gzip file. . Thanks for the help - we can close the issue as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-684864223:263,integrity,integrity,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-684864223,1,['integrity'],['integrity']
Security,"@droazen, I think I have pushed most of the changes requested -. * Moved out `appendPathToDir` from BucketUtils to IOUtils; * `appendPathToDir` now uses Path.resolve() to append a given path to dir; * If a workspace already exists and `overExistingWorkspace` is false, a `UnableToCreateGenomicsDBWorkspace` exception is thrown while creating a GenomicsDB workspace.; * Made sure all paths passed to GenomicsDB are absolute.; * Introduced `gendb.hdfs:` and `gendb.gs:` URI schemes in addition to the existing `gendb:` scheme for identifying Cloud paths in GenomicsDB with unit testing for these new schemes.; * Added unit tests to test writing to GenomicsDB workspace/arrays to GCS and then reading/querying from the same GenomicsDB instance from GCS with validation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5017#issuecomment-415611303:755,validat,validation,755,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5017#issuecomment-415611303,1,['validat'],['validation']
Security,"@droazen, we have a free GCS account, so it is possible that Hadoop requires extra configuration for authenticating/connecting with the HELLBENDER travis service account. Can anyone help here? This the code we have for connecting to GCS via Hadoop. ```; hdfsFS gcs_connect(struct hdfsBuilder *builder, const std::string& working_dir) {; char *gcs_creds = getenv(""GOOGLE_APPLICATION_CREDENTIALS"");; if (gcs_creds) {; value = parse_json(gcs_creds, ""project_id""); // free value after hdfsBuilderConnect as it is shallow copied.; if (value) {; hdfsBuilderConfSetStr(builder, ""google.cloud.auth.service.account.enable"", ""true"");; hdfsBuilderConfSetStr(builder, ""google.cloud.auth.service.account.json.keyfile"", gcs_creds);; hdfsBuilderConfSetStr(builder, ""fs.gs.project.id"", value);; }; }. if (working_dir.empty()) {; hdfsBuilderConfSetStr(builder, ""fs.gs.working.dir"", ""/"");; } else {; hdfsBuilderConfSetStr(builder, ""fs.gs.working.dir"", working_dir.c_str());; }. // Default buffer sizes are huge in the GCS connector. GenomicsDB reads/writes in smaller chunks,; // so the buffer size can be made a little smaller.; hdfsBuilderConfSetStr(builder, ""fs.gs.io.buffersize.write"", ""262144"");. hdfsFS hdfs_handle = hdfsBuilderConnect(builder);; free(value);; return hdfs_handle;; }; ```. This is the error from Travis logs-; ```; Running Test: Test method testWriteToAndQueryFromGCS(org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest); hdfsBuilderConnect(forceNewInstance=1, nn=gs://hellbender-test-logs, port=0, kerbTicketCachePath=(NULL), userName=(NULL)) error:; java.io.IOException: Error getting access token from metadata server at: http://metadata/computeMetadata/v1/instance/service-accounts/default/token; at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:210); at com.google.cloud.hadoop.util.CredentialConfiguration.getCredential(CredentialConfiguration.java:75); at com.google.cloud.hadoop.fs.gcs.GoogleHadoo",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5197#issuecomment-422915888:101,authenticat,authenticating,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5197#issuecomment-422915888,1,['authenticat'],['authenticating']
Security,"@droazen, will put some debug print statements in the two tests that are failing while authenticating with GCS and issue another pull request to _nalinigans_genomicsdb_uri_support_ branch. Hope that is OK. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5197#issuecomment-422843915:87,authenticat,authenticating,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5197#issuecomment-422843915,1,['authenticat'],['authenticating']
Security,"@droazen,. Apologies for the delay in getting back to you. Given the nature of our work, it's essential that we address and remove any high and critical vulnerabilities, regardless of their real-world threat level. Ensuring our system remains secure is our top priority. Here is the pull request with the modifications to address the high and critical vulnerabilities: [#8950](https://github.com/broadinstitute/gatk/pull/8950). Please review and let me know if you have any feedback.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-2285999993:201,threat,threat,201,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-2285999993,4,"['secur', 'threat']","['secure', 'threat']"
Security,"@eddiebroad commented on [Thu Dec 01 2016](https://github.com/broadinstitute/gatk-protected/issues/806). An issue encountered with gatk-protected ""SparkGenomeReadCounts"" tool is a non-helpful ""null"" error message. A non-helpful error ""null"" message was printed by gatk-protected with the command-line below; during the course of trying to use it on/in FireCloud:. ```; + java -Xmx48g -jar fc-7ac504fc-7fe4-4bc1-89d3-7f16317b8ff4/eddie.jar SparkGenomeReadCounts --outputFile this.entity_id.coverage.tsv --reference fc-e2421839-93d5-4ed5-8861-593f00364e54/Homo_sapiens_assembly19.fasta --input firecloud-tcga-open-access/tutorial/bams/C835.HCC1143_BL.4.bam --binsize 5000; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; .....; ......; ......; proceeding with flushing remote transports.; ***********************************************************************. null. ***********************************************************************; ```. To try to make a more helpful error message appear I added a ""catch"" block after a call to runTool in instanceMainPostParseArgs in file CommandLineProgram.java and got a more helpful message about a missing dictionary file: . try {; return runTool();; } ; catch(Exception e) {; e.getStackTrace();; }. java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MissingReferenceDictFile; 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:204); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:152); Caused by: org.broadinstitute.hellbender.exceptions.UserException$MissingReferenceDictFile; 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2922:612,access,access,612,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2922,1,['access'],['access']
Security,"@fleharty @avalind ; Sorry, something happened with my previous message.; But what I wrote previously was that I couldn't reproduce the same error message using Picard ValidateSamFile. I tried validating my bam file and I don't see any errors. Even the samtools flagstat option works fine on my bam file.; Please find the attached screenshots,. <img width=""704"" alt=""picard"" src=""https://user-images.githubusercontent.com/6302819/88064375-8023f200-cb6b-11ea-960e-bab93f79ff22.png"">. <img width=""289"" alt=""flagstst"" src=""https://user-images.githubusercontent.com/6302819/88064447-9631b280-cb6b-11ea-86ee-6c49f9111507.png"">. Do you still think my bam file is malformatted?. PS: @fleharty used Picard version (2.20.4-SNAPSHOT), whereas I used v.2.23.2; for running Picard ValidateSamFile.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-661884614:168,Validat,ValidateSamFile,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-661884614,3,"['Validat', 'validat']","['ValidateSamFile', 'validating']"
Security,"@fleharty It's line 810 in that class (https://github.com/samtools/htsjdk/blob/f15bc9d2c0297a1bde6b89aa95cf2dc45dfc567f/src/main/java/htsjdk/variant/vcf/AbstractVCFCodec.java#L810). We need to switch from calling `decodeInts()` to calling a method that tolerates and preserves missing values. A decision will need to be made about whether, for AD specifically, missing values should be replaced with 0 (which @ldgauthier said she'd be ok with), or passed through to the caller as '.' or null. If we choose to propagate the missing values back to the caller, we may need to do downstream work in GATK/Picard to modify tools to handle them, and also modify the HTSJDK accessor for the AD field to return list of `Integer` instead of array of `int`. If we replace the missing values with 0, we likely wouldn't have to patch any downstream code at all.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-682016997:666,access,accessor,666,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-682016997,1,['access'],['accessor']
Security,@gbrandt6 @bhanugandham do we have access to gvcfs that reproduce this problem? just want something real to test a fix with.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-689720463:35,access,access,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-689720463,1,['access'],['access']
Security,@gmagoon I agree that it does look a lot like an off-by-one error. The genotype validator is complaining about seeing a reference allele that just happens to be the same as the previous VC. . @cwhelan can you take a look when you get a chance?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5336#issuecomment-431855929:80,validat,validator,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5336#issuecomment-431855929,1,['validat'],['validator']
Security,"@gokalpcelik I see that the bug exists in the updated code too. We can fix it, but would be good to have some dataset that can be used to validate. Any chance to ask the user to generate a small example?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8788#issuecomment-2073371655:138,validat,validate,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8788#issuecomment-2073371655,1,['validat'],['validate']
Security,"@gspowley I have neglected this for a while, to say the least. Here is a command line using publicly available data with paths on the Broad servers. Everyone at the Broad has read access to these files, FWIW. What should I do with the data?. ```bash; wgs_intervals=/seq/references/Homo_sapiens_assembly19/v1/variant_calling/wgs_calling_regions.v1.interval_list; hg19=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta; tumor_bam=/dsde/working/davidben/dream/synthetic/original_bams_symlinks/tumor_4.bam; tumor_sample=synthetic.challenge.set4.tumour; normal_bam=/dsde/working/davidben/dream/synthetic/original_bams_symlinks/normal_4.bam; normal_sample=synthetic.challenge.set4.normal; java -jar $gatk Mutect2 \; -R $hg19 \; -L $wgs_intervals \; -I $tumor_bam -tumor $tumor_sample \; -I $normal_bam -normal $normal_sample \; -O output.vcf; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2562#issuecomment-307436212:180,access,access,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2562#issuecomment-307436212,1,['access'],['access']
Security,"@gudeqing Thanks for reporting. `GatherBamFiles` is a bit of a tricky tool and it's easy to do the wrong thing with it accidentally. It's possible that there's an error in either how it was run or the input files but you definitely also could have discovered a bug. Samtools indexing it correctly would be point towards a bug, but I'd like you to check a few things first to be sure. . GatherBamFiles is dumb and just concatenates bam files so it's very picky about inputs. If the bam files in the input are not disjoint or they are specified out of order than `GatherBamFiles` will produce an invalid output which might manifest in indexing errors. It's also possible that the header specified didn't include all the contigs present in the collection of bams which could have a similar error result. Could you run ValidateSamFile on the result and report back what it says? If the file is out of order in some way or the header doesn't match it should report that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6379#issuecomment-575680528:815,Validat,ValidateSamFile,815,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6379#issuecomment-575680528,1,['Validat'],['ValidateSamFile']
Security,"@ilyasoifer Is there any way I can access the original cram (or better yet, a small subset thereof consisting of just MT) that illustrates this issue) and the reference ? It might be hard to debug without that. If thats not possible, a few suggestions: can you try using PrintReads to write the original cram (I would try just MT) first to a cram, then to a sam, and also the original cram to a sam, and see how those compare? It would also be useful to see what that read looks like if you use samtools view on the ORIGINAL cram. Do you know what software/version was used to write the original cram ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8768#issuecomment-2045130095:35,access,access,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8768#issuecomment-2045130095,1,['access'],['access']
Security,@ilyasoifer cnorman@broadinstitute.org. And don't worry about doing the PrintReads conversions I requested - if I have access to the original file and the reference I can debug this directly.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8768#issuecomment-2045185628:119,access,access,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8768#issuecomment-2045185628,1,['access'],['access']
Security,"@jamesemery @droazen I've updated this branch to ensure all read and write paths to shared state in `GenotypeLikelihoodCalculators` is synchronized. I then wrote a little [test](https://github.com/broadinstitute/gatk/commit/3bb178746b1dd286f55ba77e6939e2104ced98d0) using `AlleleSubsettingUtils` to access `GenotypeLikelihoodCalculators` 10^6 times to see the effect of adding synchronization. R session (times are in millis):; ```; > without_sync = c(10166, 10049, 10306, 10059, 10165); > with_sync = c(10700, 10384, 9923, 10097, 10190); > t.test(without_sync, with_sync, paired=TRUE). 	Paired t-test. data: without_sync and with_sync; t = -0.70447, df = 4, p-value = 0.52; alternative hypothesis: true difference in means is not equal to 0; 95 percent confidence interval:; -542.5421 322.9421; sample estimates:; mean of the differences ; -109.8 ; ```. The p-value is not less than 0.05, so we can't reject the null hypothesis (that the mean times are the same). So adding synchronization doesn't seem to make any difference in this test. BTW, I noticed that `GenotypeLikelihoods` has synchronization, so there is some precedent for thread-safety using this means.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-426338479:299,access,access,299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-426338479,1,['access'],['access']
Security,"@jamesemery Back to you, at long last. I adopted your suggestion of a proper search that doesn't revisit already-seen vertices and came up with a better way of seeding the ""good"" subgraph that is safe from your STR concern. As far as code is concerned it's a total rewrite — you can pretend the first PR commit doesn't exist. The new criterion for seeding the search is chains with good log odds on both ends and which are incident on a vertex with multiple good out-edges or multiple good in-edges. The rationale is that the adjacency of two bad edges may have good log odds (Suppose a bad edge comes in and two bad edges come out. One is a new error on top of the original error and one is the continuation of the original error) but two have two outgoing edges with good log odds requires an actual real variant. On our M2 validations this essentially no effect on sensitivity and a mild reduction in false positives. I will leave it to you (or to me when I don't have to work like a vampire) to investigate how well it interacts with junction trees. As a first step I wrote a basic unit test for the basic pathology of the old method.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6520#issuecomment-624265441:826,validat,validations,826,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6520#issuecomment-624265441,1,['validat'],['validations']
Security,"@jamesemery Can you rebase this branch onto latest master to resolve the conflicts? Recommend doing a local squash first given the number of commits here to make it less painful (by ""local squash"" I mean first `rebase -i` onto the hash of the first commit in the `git log` history that's not your own, and then rebase onto `origin/master`).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3803#issuecomment-359036049:231,hash,hash,231,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3803#issuecomment-359036049,1,['hash'],['hash']
Security,@jamesemery Could I get a 👍 on this from you. Anders reviewed it but it's not counting him since he doesn't have write access.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6668#issuecomment-646861071:119,access,access,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6668#issuecomment-646861071,1,['access'],['access']
Security,"@jamesemery I agree - all access (read and write) to `GenotypeLikelihoodCalculators` instance variables needs to be synchronized to make it safe. I think it would be sufficient to make `getInstance()` and `calculateGenotypeCountUsingTables()` synchronized. @droazen, are you concerned about performance for the Spark case? For the walker version, presumably the access is single-threaded, and hence [uncontended, which is very cheap](https://books.google.co.uk/books?id=mzgFCAAAQBAJ&pg=PA230&lpg=PA230&dq=java+uncontended+synchronization+goetz&source=bl&ots=7W4J807faW&sig=YALE1qdWoAUELPqLRhIedz-bZ20&hl=en&sa=X&ved=2ahUKEwj4jJeko8zdAhXVFsAKHazkBrcQ6AEwB3oECAIQAQ#v=onepage&q=java%20uncontended%20synchronization%20goetz&f=false). Another option would be to maintain a separate instance of `GenotypeLikelihoodCalculators` per genotyping engine. The size of the table is ploidy * alleles, so not too large?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-423546586:26,access,access,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-423546586,2,['access'],['access']
Security,@jamesemery Still some test failures https://storage.googleapis.com/hellbender-test-logs/build_reports/master_19811.2/tests/test/classes/org.broadinstitute.hellbender.tools.spark.validation.CompareDuplicatesSparkIntegrationTest.html#testOutputFile,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397343622:179,validat,validation,179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397343622,1,['validat'],['validation']
Security,"@jamesemery This is related to #6930 . The background is that PedigreeAnnotation is special-cased in GATK, which provides better command-line argument validation, and it will also be used to inject the PedigreeFile, create the SampleDB, etc. This is currently a subclass of InfoFieldAnnotation, and therefore cant be used for GenotypeFieldAnnotation. There shouldnt be this limitation, and this PR tried to address that. The way I propose to do this is to make InfoFieldAnnotation and GenotypeAnnotation into interfaces, with default methods where possible. The existing subclasses all switch from extending them to implementing them. This is generally a trivial difference, but it touches a lot of classes. . All existing classes that previously extended PedigreeAnnotation (formerly a subclass of InfoFieldAnnotation), now extend PedigreeAnnotation and implement InfoFieldAnnotation. This is a minimal difference, but it makes it possible for future classes to extend PedigreeAnnotation, and then implement GenotypeAnnotation. The only part this includes that I didnt like was the fact that the existing InfoFieldAnnotation overrides toString(), which I cant do in an interface. So I created AbstractInfoFieldAnnotation, and all existing InfoFieldAnnotation classes extend that. It's not currently clear to me how critical that override of toString() is. The weakness of this PR is that classes outside the GATK project that currently extend InfoFieldAnnotation would not inherit this. I could keep InfoFieldAnnotation a class as-is, and make a differently named interface behind it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7041:151,validat,validation,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7041,2,"['inject', 'validat']","['inject', 'validation']"
Security,"@jamesemery What about supporting an initialize() method on VariantAnnotation? This is GATK3-like, and would be non-disruptive to existing code, since the interfaces could have a default no-op implementation? . /**; * Provides an opportunity to set up context; */; public void initialize(VariantAnnotatorEngine engine) {. }. Then we could address whether any context is appropriate to expose via methods on VariantAnnotatorEngine?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-754274008:385,expose,expose,385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-754274008,1,['expose'],['expose']
Security,"@jamesemery and now the overview of the more complex changes:. - `AssemblyResultSet`: the code for adding and removing haplotypes based on pileup alleles has become a `void` method of this class, where it belongs. Here and elsewhere I introduce snappy variable and function named referring to ""good"" and ""bad"" alleles, which I find visually much clearer. The code is basically the same as before but somewhat streamified. I extracted a `makeHaplotypeWithInsertedEvent` method to eliminate some code duplication between GGA and pileup force-calling.; - `HaplotypeCallerEngine` and `Mutect2Engine`: Force-calling alleles are split into biallelic `Events`. Duplicated code for finding all pileup events, then sifting them into good event to force-call and bad events to remove is extracted as `PileupBasedAlleles.goodAndBadPileupEvents`. Computing `allVariationEvents` is much simpler because 1) it now uses `Event` instead of `VariantContext` and 2) `Event` overrides `equals` and `hashCode`.; - `PileupBasedAlleles`: `getPileupVariantContexts` and sorting into good and bad pileup variants has been unified into `goodAndBadPileupEvents()`. It has additionally been somewhat rewritten for conciseness. Also, instead of the somewhat kludgy method of making `VariantContext` with four temporary attributes, then filtering based on those attributes, it calculates the filtering status immediately and uses `Events`. Also fixed the somewhat-misleading use of the word `alt` to mean `SNP`.; - `AssemblyBasedCallerUtils`: `applyPileupEventsAsForcedAlleles`, along with several helper methods that it calls, has been moved into `AssemblyResultResult`, where it is now a void member method.; - `GATKVariantContextUtils` mainly just using `Event` instead of `VariantContext`, which simplifies the code for splitting a `VariantContext` into biallelics. After going through this exercise I realize that it's not actually so much. The diff's bark is worse than its bite. The overwhelming majority of changes are eit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8332#issuecomment-1574175702:980,hash,hashCode,980,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8332#issuecomment-1574175702,2,['hash'],['hashCode']
Security,"@jamesemery sorry to bug on this topic, but I'm hoping to make a push early this year to fully migrate my lab off GATK3 . I looked more closely at the specific annotations we need to migrate. I decided that I will implement our walker, 'DiscvrVariantAnnotator', which is basically a light wrapper around VariantAnnotation. This will make it easier to spike in custom annotations. In that walker, I will override makeVariantAnnotations(). I will make a new marker interface for EngineAwareAnnotation, and test that on all the Annotation classes, and use this to inject FeatureManager. So no core GATK changes needed. I did find one thing I'd like to propose. You probably know PedigreeAnnotation is special-cased in GATK. Annotations that use it have automatic argument validation and have the SampleDB injected. Currently, PedigreeAnnotation is a subclass of InfoFieldAnnotation, so isnt available to GenotypeAnnotations. There doesnt appear to be a solid reason why. I tried to fix that and my best idea is the proposal here: #7041 . The core idea is to convert InfoFieldAnnotation and GenotypeAnnotation to interfaces. This is generally a trivial switch in existing code. With that, it becomes possible for classes that currently extend PedigreeAnnotation (which I switched to no longer extend InfoFieldAnnotation) to simply PedigreeAnnotation and implement InfoFieldAnnotation. This makes it possible for future classes to extend PedigreeAnnotation and implement GenotypeAnnotation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-760424063:561,inject,inject,561,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-760424063,6,"['inject', 'validat']","['inject', 'injected', 'validation']"
Security,"@jamesemery, researcher has uploaded data to </humgen/gsa-scr1/pub/incoming/Exception_in_SplitNCigarReads.tgz> and has clarified a few other details within the forum thread, e.g. running ValidateSamFile `IGNORE=MISSING_TAG_NM IGNORE=MATE_NOT_FOUND` allows for validation. Thanks for looking into this. ---. Hello,. I am getting the following exception when running SplitNCigarReads on RNA-Seq data using GATK 4.0.8.1:; ```; java.lang.ArrayIndexOutOfBoundsException: 100; 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.overhangingBasesMismatch(OverhangFixingManager.java:313); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.fixSplit(OverhangFixingManager.java:252); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.addReadGroup(OverhangFixingManager.java:209); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.splitNCigarRead(SplitNCigarReads.java:270); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.firstPassApply(SplitNCigarReads.java:180); 	at org.broadinstitute.hellbender.engine.TwoPassReadWalker.lambda$traverseReads$0(TwoPassReadWalker.java:62); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5230:187,Validat,ValidateSamFile,187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5230,2,"['Validat', 'validat']","['ValidateSamFile', 'validation']"
Security,"@jason-weirather Interesting. I have no trouble accessing the FTP site from outside the Broad. What kind of error message are you getting?. There is a new version from 3/29 that has several fixes in it, in addition you'll need to make sure you have the latest GATK code (you may need to pull the source code rather than a release - I'm not sure when the last release was and some fixes required both data source changes and code changes). If you can wait a few days we're planning on doing another minor / bugfix release this week. In general it's probably not worth trying to fix errors in the data sources - you may find yourself going down a rabbit hole. That said, one of the things that was fixed was that data source line in the gencode file. There shouldn't be very many __UNKNOWN__ fields (if any at all) - at least there aren't when running with the latest version of everything.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4521#issuecomment-383404016:48,access,accessing,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521#issuecomment-383404016,1,['access'],['accessing']
Security,"@jean-philippe-martin ; Thanks for the input. I've checked both `gs://broad-dsde-methods-shuang/pb/bams/NA12892/` and `gs://broad-dsde-methods-sv/samples/G94797_CHM_MIX/WGS1/tmp`, they return something like this. ```; gs://broad-dsde-methods-shuang/pb/bams/NA12892/:; Creation time: Mon, 22 Apr 2019 16:14:50 GMT; Update time: Mon, 22 Apr 2019 16:14:50 GMT; Storage class: STANDARD; Content-Length: 11; Content-Type: text/plain; Hash (crc32c): XkI+Dw==; Hash (md5): apnFdauH+MfR7R5S5+NJzg==; ETag: CJekwKSM5OECEAE=; Generation: 1555949690032663; Metageneration: 1; ACL: [; {; ""entity"": ""project-owners-222581509023"",; ""projectTeam"": {; ""projectNumber"": ""222581509023"",; ""team"": ""owners""; },; ""role"": ""OWNER""; },; {; ""entity"": ""project-editors-222581509023"",; ""projectTeam"": {; ""projectNumber"": ""222581509023"",; ""team"": ""editors""; },; ""role"": ""OWNER""; },; {; ""entity"": ""project-viewers-222581509023"",; ""projectTeam"": {; ""projectNumber"": ""222581509023"",; ""team"": ""viewers""; },; ""role"": ""READER""; },; {; ""email"": ""shuang@broadinstitute.org"",; ""entity"": ""user-shuang@broadinstitute.org"",; ""role"": ""OWNER""; }; ]; ......; ......; ```. The line the `Content-Length: 11` seems to suggest you are right.; And if I run `gsutil ls -lh gs://broad-dsde-methods-shuang/pb/bams/NA12892/`, I get; ```; 11 B 2019-04-22T16:14:50Z gs://broad-dsde-methods-shuang/pb/bams/NA12892/; ......; ......; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5935#issuecomment-492762131:429,Hash,Hash,429,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5935#issuecomment-492762131,2,['Hash'],['Hash']
Security,"@jean-philippe-martin Can you comment on this error with your thoughts? Despite now doing a channel reopen on `UnknownHostException` in our fork of the NIO library, all reopens are failing, which implies that this error can't be recovered from via a simple retry. Could there be something wrong in our authentication setup?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412906931:302,authenticat,authentication,302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412906931,2,['authenticat'],['authentication']
Security,"@jean-philippe-martin Can you comment on this one? It looks like `google-cloud-java` recently bumped their `google-auth-library-credentials` and `google-auth-library-oauth2-http` dependencies to `0.8.0` -- was there some change that would require us to modify our authentication-related code in GATK, and/or the permissions setup in our Google Cloud project, that could explain the error:. ```; Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330936001:264,authenticat,authentication-related,264,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330936001,3,"['access', 'authenticat', 'secur']","['access', 'authentication-related', 'security']"
Security,"@jean-philippe-martin I like your counter proposal in general for testing path integration. I think writing to GCS over NIO is an important enough feature that we should have at least 1 test in gatk that actually writes to a real GCS bucket in case there's ever an issue specifically with GCS (authentication issues are one potential problem I can imagine). . It seems like we should be able to design in a way that avoids collisions. What does `Files.createTempFile()` do with gcs? My guess is that it probably doesn't do the right thing, but maybe we could fix it so it would? Or use some sort of scheme with random UUID's like the methods in BucketUtils that we have already.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-332235140:294,authenticat,authentication,294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-332235140,1,['authenticat'],['authentication']
Security,@jean-philippe-martin I think we can set up a repro by creating a new github project with a simple travis build that just does an NIO access. I don't think we can reproduce it locally since I'm pretty sure it's a bad interaction with the environment.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5928#issuecomment-516890466:134,access,access,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5928#issuecomment-516890466,2,['access'],['access']
Security,"@jean-philippe-martin The bug is in a piece of code that ISN'T using NIO, but is using some old code from the dataflow days to access the bam. I think that we can replace that code now that NIO is working and we should no longer need these special cases for GCS files.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277832993:127,access,access,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277832993,1,['access'],['access']
Security,"@jean-philippe-martin When you patch this one, could you also audit the rest of `CloudStorageReadChannel` for any other methods that could trigger a GCS access and require retries?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-314549425:62,audit,audit,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-314549425,2,"['access', 'audit']","['access', 'audit']"
Security,"@jhl667 I'm looking into this. It looks like I neglected to set the sqlite connection to read only mode when connecting to the db file. I'm going to update it to do so. I'm not sure this applies when a read-only connection is created, but it looks like sqlite has some issues with NFS / distributed file systems:; - https://stackoverflow.com/questions/9907429/locking-sqlite-file-on-nfs-filesystem-possible ; - https://github.com/CGATOxford/CGATPipelines/issues/. One post in the github thread above mentions using `-o flock` when mounting Lustre partitions so that they all have concurrent locks. This _may_ be a workaround in the meantime. . I'll try to look at it on our NFS mounts - I don't have access to a Lustre fs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4413#issuecomment-366009015:700,access,access,700,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4413#issuecomment-366009015,1,['access'],['access']
Security,"@jhl667, yep, I understand. Still, this issue is too big to be left alone. I think the Broad has to act here, since patient's lives are at stake and we didn't receive any actionable response for half a year. Mutect2 has a good reputation, and the Broad profits from that, but it is also medical software and it should be treated as such. Mutect2 will continue to be used for some time, and this has to be fixed. @droazen, is there anything else you people can do? Is the wider GATK dev team aware of the issue? Is this something I should escalate to someone else so that you get support from your management to fix it? We have pharma collabs with the Broad in place, I could try doing it that way, or via the genomics/pharma community. I'm not trying to be threatening or anything, just thinking out loud how we can help you to get the resources to solve this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1404845998:757,threat,threatening,757,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1404845998,1,['threat'],['threatening']
Security,"@jkobject Actually, we just noticed that your error is triggered by the file `gs://fc-secure-bd7b8bc9-f665-4269-997e-5a402088a369/5c2db926-3b1c-479c-9ed3-a99ce518de91/omics_mutect2/60955825-7723-4bc9-8202-bdd9975bb5c0/call-mutect2/Mutect2/7d737efc-c8be-4a6d-8803-4f786129521a/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list`, not the bam. Could you attempt the `gsutil` test on that file instead, and let us know what happens? Eg.,. ```; gsutil -u broad-firecloud-ccle cp gs://fc-secure-bd7b8bc9-f665-4269-997e-5a402088a369/5c2db926-3b1c-479c-9ed3-a99ce518de91/omics_mutect2/60955825-7723-4bc9-8202-bdd9975bb5c0/call-mutect2/Mutect2/7d737efc-c8be-4a6d-8803-4f786129521a/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list .; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1064482965:86,secur,secure-,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1064482965,2,['secur'],['secure-']
Security,"@jkobject Our testing of the nightly image with our own service account and billing project suggests that the requester pays access issue is resolved, so we're not sure what's causing it to continue to fail for you. As an experiment, could you try this: within the gatk-nightly image, try to access your requester pays file `gs://cclebams/wgs_hg38/CDS-0b4jFH.wgs_ccle.bam` using the `gsutil` command with the `-u` option. Eg., `gsutil -u broad-firecloud-ccle cp gs://cclebams/wgs_hg38/CDS-0b4jFH.wgs_ccle.bam .`, and report whether that succeeds. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1064477058:125,access,access,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1064477058,2,['access'],['access']
Security,"@jkobject That appears to be a different error: ""User project specified in the request is invalid"" instead of ""Bucket is a requester pays bucket but no user project provided"", which was the error this patch fixed. Can you confirm that the `broad-firecloud-ccle` project exists and is authorized under your service account? . @lbergelson Can you comment further on this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1064417440:284,authoriz,authorized,284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1064417440,1,['authoriz'],['authorized']
Security,"@john-alexander Just to be sure, you've checked that file is accessible to singularity?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6446#issuecomment-685813946:61,access,accessible,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6446#issuecomment-685813946,1,['access'],['accessible']
Security,"@jonn-smith I have sucessfully built GATK by these commands. git clone https://github.com/broadinstitute/gatk; gradlew bundle. I've got this zip file in folder ""build"".; gatk-4.0.4.0-34-g2cc7abd-SNAPSHOT.zip. So I unzipped and used this to replace GATK-4.0.4.0 that I downloaded from https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip. I still found errors. 21:09:33.811 INFO ProgressMeter - Starting traversal; 21:09:33.811 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 21:09:46.267 INFO ProgressMeter - chr1:24929636 0.2 3000 14453.2; 21:09:59.072 INFO ProgressMeter - chr1:64681324 0.4 6000 14251.2; 21:10:09.456 INFO ProgressMeter - chr1:156245393 0.6 9000 15149.8; 21:10:21.510 INFO ProgressMeter - chr1:206965947 0.8 12000 15094.7; 21:10:26.132 INFO Funcotator - Shutting down engine; [May 23, 2018 9:10:26 PM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 1.36 minutes.; Runtime.totalMemory()=11500781568; java.lang.IllegalArgumentException: Genomic positions must be > 0.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:722). What should I do? Can you send me one that is ready-to-use? Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391371859:1134,validat,validateArg,1134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391371859,1,['validat'],['validateArg']
Security,"@jonn-smith Is there a forum post (or other docs) on how to setup a datasource for remote, NIO access? Do we make it clear that this only supports what the GATK supports?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5425#issuecomment-439976455:95,access,access,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5425#issuecomment-439976455,2,['access'],['access']
Security,"@kachulis Thanks for the report. The fix will be slightly complicated by the fact that there is also a (GATK-tool) level arg called `masterSequenceDictionary`. So in the override, we'll want to validate/resolve that argument against the others as well. If you need a short term workaround, you can try disabling on-they-fly indexing (`--create-output-variant-index false`, and then index the output separately using `IndexFeatureFile`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5087#issuecomment-411051092:194,validat,validate,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5087#issuecomment-411051092,1,['validat'],['validate']
Security,"@kcibul Does @jean-philippe-martin's suggestion above work for you, or are you still having authentication issues when running using a service account?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2425#issuecomment-284507997:92,authenticat,authentication,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2425#issuecomment-284507997,1,['authenticat'],['authentication']
Security,"@kcibul My reasoning for doing it in WDL is to better integrate with the process that creates the VAT table, and therefore make sure that the person running it has access (and knows the location of) not only to the VAT table but also the intermediary steps (e.g. the annotation JSON files that are output from NIRVANA). Not all of the validation steps need to be all bash; the first one was because it's literally just a call to make sure a table exists and has rows with `vid` values in it. Other rules (e.g. [rule #2](https://github.com/broadinstitute/dsp-spec-ops/issues/365)) will most likely need either python or jq to run.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7352#issuecomment-883457790:164,access,access,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7352#issuecomment-883457790,2,"['access', 'validat']","['access', 'validation']"
Security,"@kcibul reports that if the CNNScoreVariants python code throws an exception during async batch processing, the GATK tool hangs (specifically, it was happening when GATK was sending a . for a missing annotation, and the python code was trying to interpret that as a number and blowing up). It looks like this happens because `StreamingPythonScriptExecutor::waitForPreviousBatchCompletion` waits for the async write thread `Future` to complete first, before checking the fifo for an `ACK`/`NCK` (which is when the exception would be propagated). If the async write thread is blocked because the fifo is full because the python code isn't retrieving data because an exception was thrown, the java side will hang waiting for the `Future` complete. The solution is to reverse the order of the `waitForPreviousBatchCompletion` checking (ack first, then validate that the async write `Future` completes). There is a [branch]( https://github.com/broadinstitute/gatk/tree/cn_async_python_exception) with a test and a fix for the StreamingPythonExecutor, and a [separate branch](https://github.com/broadinstitute/gatk/tree/cn_cnn_exception) with a test for CNNScoreVariants that also has the executor fix. I need to verify that the CNNSCoreVariants test actually fails without the fix, and then this can be turned into a PR, which I'll do when I return from vacation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7401:848,validat,validate,848,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7401,1,['validat'],['validate']
Security,"@kdatta @kgururaj It seems like we're losing rsID's in the input gvcf when we load them into genomics db. Is this deliberate to save space? Is it a bug? Is it a configuration option that isn't exposed by `GenomicsDBImport`? . I don't think it's important for production because they pass in a dbSNP at genotyping time so that can be recomputed, but it's causing issues in some of my tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2636:193,expose,exposed,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2636,1,['expose'],['exposed']
Security,@kdatta Why not use some kind of globally-unique identifier for the arrays if name collision is an issue (such as a hash or UUID)? Would that solve the problem?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3411#issuecomment-320325990:116,hash,hash,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3411#issuecomment-320325990,1,['hash'],['hash']
Security,"@kgururaj . So I found out that there are other fields that are also throwing a similar error with `vcf-validator`:. Below you will find this: `INFO tag [an_adj_exac_oth=16,16] expected different number of values (1)`. ```; # from the vcf-validator; INFO field at 4:2044128 .. INFO tag [af_exac_all=0] expected different number of values (expected 3, found 1),INFO tag [af_adj_exac_fin=0] expected different number of values (expected 3, found 1),INFO tag [an_adj_exac_oth=16,16] expected different number of values (1),INFO tag [an_adj_exac_nfe=202,202] expected different number of values (1),INFO tag [an_adj_exac_afr=20,20] expected different number of values (1),INFO tag [af_adj_exac_amr=0] expected different number of values (expected 3, found 1),INFO tag [an_exac_all=1246,1246] expected different number of values (1),INFO tag [an_adj_exac_amr=10,10] expected different number of values (1),INFO tag [af_adj_exac_oth=0] expected different number of values (expected 3, found 1),INFO tag [an_adj_exac_eas=30,30] expected different number of values (1),INFO tag [an_adj_exac_fin=2,2] expected different number of values (1),INFO tag [an_adj_exac_sas=966,966] expected different number of values (1),INFO tag [af_adj_exac_sas=0] expected different number of values (expected 3, found 1),INFO tag [af_adj_exac_afr=0] expected different number of values (expected 3, found 1),INFO tag [af_adj_exac_nfe=0] expected different number of values (expected 3, found 1),INFO tag [max_aaf_all=1] expected different number of values (expected 3, found 1),INFO tag [af_adj_exac_eas=0] expected different number of values (expected 3, found 1); ```. In this case, `an_adj_exac_oth` has > 1 values and only 1 is allowed:. ```; grep ; ##INFO=<ID=an_adj_exac_oth,Number=1,Type=Integer,Description=""Other Chromosome Count (from /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/gemini_data/ExAC.r0.3.sites.vep.tidy.vcf.gz)"">. # the corresponding variant in the vcf ; 4	2044128	.	C	T,CGCT,<NON_REF>	3476.7	.	DP=97",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407497476:104,validat,validator,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407497476,2,['validat'],['validator']
Security,"@kgururaj I ran the commands you suggested. . > [user@cedar5 bin]$ bash -x TestGenomicsDBJar/run_checks.sh; > + [[ hB != hxB ]]; > + XTRACE_STATE=-x; > + [[ hxB != hxB ]]; > + VERBOSE_STATE=+v; > + set +xv; > + unset XTRACE_STATE VERBOSE_STATE; > ++ uname -s; > + osname=Linux; > + jar xf genomicsdb--jar-with-dependencies.jar libtiledbgenomicsdb.so; > java.io.FileNotFoundException: genomicsdb--jar-with-dependencies.jar (No such file or directory); > at java.util.zip.ZipFile.open(Native Method); > at java.util.zip.ZipFile.<init>(ZipFile.java:219); > at java.util.zip.ZipFile.<init>(ZipFile.java:149); > at java.util.zip.ZipFile.<init>(ZipFile.java:120); > at sun.tools.jar.Main.extract(Main.java:1004); > at sun.tools.jar.Main.run(Main.java:305); > at sun.tools.jar.Main.main(Main.java:1288); > + jar xf genomicsdb--jar-with-dependencies.jar libtiledbgenomicsdb.dylib; > java.io.FileNotFoundException: genomicsdb--jar-with-dependencies.jar (No such file or directory); > at java.util.zip.ZipFile.open(Native Method); > at java.util.zip.ZipFile.<init>(ZipFile.java:219); > at java.util.zip.ZipFile.<init>(ZipFile.java:149); > at java.util.zip.ZipFile.<init>(ZipFile.java:120); > at sun.tools.jar.Main.extract(Main.java:1004); > at sun.tools.jar.Main.run(Main.java:305); > at sun.tools.jar.Main.main(Main.java:1288); > + '[' Linux == Darwin ']'; > + LIBRARY_SUFFIX=so; > + ldd libtiledbgenomicsdb.so; > ldd: ./libtiledbgenomicsdb.so: No such file or directory; > + md5sum libtiledbgenomicsdb.so; > md5sum: libtiledbgenomicsdb.so: No such file or directory. I'm using a compute canada server, so I don't have root access. The version of gatk4 I'm using was installed by their support team, and I load it using 'module load gatk'. I had that module loaded when I ran this test.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-357005071:1615,access,access,1615,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-357005071,1,['access'],['access']
Security,@knight2015 I'm sorry to say we don't support spark 3.0.0 a the moment. If you have access to a spark 2.4.x cluster I would try that.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6644#issuecomment-640701152:84,access,access,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6644#issuecomment-640701152,1,['access'],['access']
Security,@ksw9 can you run the vcf validator tool suggested by @komalsrathi in #5045 [here](https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407476343)? The issue is primarily caused by mismatch in the field description in the VCF header and the data lines. @droazen can you comment on the [sanity check that I suggested here](https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407501684)?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5113#issuecomment-413282882:26,validat,validator,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5113#issuecomment-413282882,1,['validat'],['validator']
Security,"@lbergelson @droazen Both of you committed changes to the Dockerfile recently, but as far as I can tell they are not security related. Should I keep this PR at [4.2.4.1](https://hub.docker.com/layers/broadinstitute/gatk/4.2.4.1/images/sha256-421d2fb2cc869249cef3f4d7a77289256d295b04ba623096228e0e5fd42939e9?context=explore)?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7611#issuecomment-1048281865:117,secur,security,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7611#issuecomment-1048281865,1,['secur'],['security']
Security,@lbergelson @gokalpcelik any chance of giving me access to the workspace for the 330 whole exomes?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8989#issuecomment-2433346908:49,access,access,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8989#issuecomment-2433346908,1,['access'],['access']
Security,"@lbergelson Hi, thank you for your kind reply. Following is the report of ValidateSamFile:; Tool returned: 3; ERROR::RECORD_OUT_OF_ORDER:Record 86076959, Read name A00583:183:HLCWVDMXX:1:1114:30897:16579, The record is out of [coordinate] order, prior read name [ST-E00159:680:H57NGCCX2:6:2201:21765:57301], prior coodinates [22:51244173]; ERROR::MATE_NOT_FOUND:Read name ST-E00159:680:H57NGCCX2:6:2208:30289:5300, Mate not found for paired read; ERROR::MATE_NOT_FOUND:Read name A00583:183:HLCWVDMXX:1:1236:31611:36793, Mate not found for paired read; ...... The Result really supprised me! However, mutect2 and haplotypecaller are ok with the input of the resulted bam indexed with samtools. I also found that the conclusion of ""Mate not found for paired read"" is True. The bam was generated by: fastq ->mergeBam( bwabam + (fastq->ubam) ) -> markdup+sortAndFixTags -> applyBQSR -> gatherBam. Steps of Bwa and BQSR were paralleled with Intervals. And, I am sure that all my inputs are in consistent order. Example of interval specified: ""-L chr21:1+ -L chr22:1+"". Looking foward to your reply.; Best Regards!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6379#issuecomment-575985383:74,Validat,ValidateSamFile,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6379#issuecomment-575985383,1,['Validat'],['ValidateSamFile']
Security,@lbergelson I added a second argument per your request. I still disagree that this is the right argument to expose because it is very dangerous.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5974#issuecomment-497383571:108,expose,expose,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5974#issuecomment-497383571,1,['expose'],['expose']
Security,"@lbergelson I added an integration test that writes to GCS... it doesn't work for me (""com.google.cloud.storage.StorageException (...) does not have storage.objects.get access to (...)""). This may be due to a misconfiguration on my end. I wonder if it'll work with Travis.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-334877523:169,access,access,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-334877523,1,['access'],['access']
Security,"@lbergelson I agree with you about moving tests up, though I would make the point that there were many tests that got pushed down in the first place because they would involve fixing bugs in MarkDuplicatesGATK that were already fixed in MarkDuplicatesSpark, I will audit the ones I did and didn't push up so i'm more confident there is a reason to have tests pushed down or not.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5166#issuecomment-419550902:265,audit,audit,265,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5166#issuecomment-419550902,1,['audit'],['audit']
Security,"@lbergelson I disagree -- it's very clear to me that those tests will trigger Google authentication, just by tracing through the code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2706#issuecomment-300806909:85,authenticat,authentication,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2706#issuecomment-300806909,2,['authenticat'],['authentication']
Security,"@lbergelson I updated to the latest 2.x Mockito and that fixed the problem. The only failing test now is FuncotatorIntegrationTest#nonTrivialLargeDataValidationTest. The output VCF differs in the `FUNCOTATION` annotation, and it’s to do with ordering of the fields. E.g. it will be. ```; 1_%7C_1|false_%7C_false|false_%7C_false; ```; not; ```; false_%7C_false|1_%7C_1|false_%7C_false; ``` . It looks like it could be a case of using HashSet not LinkedHashSet, or HashMap not LinkedHashMap - but a quick replace throughout the GATK and HTSJDK codebase didn’t fix the problem.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532593427:433,Hash,HashSet,433,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532593427,2,['Hash'],"['HashMap', 'HashSet']"
Security,"@lbergelson I was referring more to the middle part of the StackOverflow by Daniel Chapman - specifically the [4.1 The ObjectStreamClass Class](http://docs.oracle.com/javase/8/docs/platform/serialization/spec/class.html#a5082) and [4.6 Stream Unique Identifiers](http://docs.oracle.com/javase/8/docs/platform/serialization/spec/class.html#a4100):. _If not specified by the class, the value returned is a hash computed from the class's name, interfaces, methods, and fields using the Secure Hash Algorithm (SHA) as defined by the National Institute of Standards._. Now when I look at the `java.io.ObjectStreamClass.java` file for 64-bit JDK7 and JDK8 - from src.zip - both have the same code for the following parts after performing a `diff` - I didn't list all of the lines of code since they are quite long:. ```; public long getSerialVersionUID() {; // REMIND: synchronize instead of relying on volatile?; if (suid == null) {; suid = AccessController.doPrivileged(; new PrivilegedAction<Long>() {; public Long run() {; return computeDefaultSUID(cl);; }; }; );; }; return suid.longValue();; }; ... private static long computeDefaultSUID(Class<?> cl) {; ...very long code which can be inspected via the src.zip file...; }; ```. So looking at the code portions of `computeDefaultSUID()` and I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:404,hash,hash,404,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,1,['hash'],['hash']
Security,"@lbergelson Louis, I run into the similar trouble when I enable GATK to access oss fs of Aliyun, our oss of hadoop-fs impl (org.apache.hadoop.fs.{FileSystem, Path}) work well with spark perfectly but oss provider (java.nio.file.spi.FileSystemProvider) is not available today. (Not included in gatk package gatk-4.beta.6. . After I researched on [SparkContext](; https://github.com/apache/spark/blob/1c9f95cb771ac78775a77edd1abfeb2d8ae2a124/core/src/main/scala/org/apache/spark/SparkContext.scala) impl and [GATK rg.seqdoop.hadoop_bam.AnySAMInputFormat](https://github.com/broadinstitute/gatk/search?utf8=%E2%9C%93&q=java.nio.file.FileSystem&type=Code) impl. [IOUtils](https://github.com/broadinstitute/gatk/blob/94ac626218e073b77156a3eff076003d26be318c/src/main/java/org/broadinstitute/hellbender/utils/io/IOUtils.java#L535). Today, org.apache.hadoop.fs.{FileSystem, Path} is much broadly used in the Big Data world, and most of vendors of distribution storage provider already provide impl of org.apache.hadoop.fs.{FileSystem, Path} include AWS, Google and Alibaba. There are huge customers of Hadoop already work on hadoop.fs for years, if GAKT on spark could rely on org.apache.hadoop.fs.{FileSystem, Path} , I guess GAKT could acquire more existing customers of Hadoop on Cloud much faster . . Maybe we could consider migrating java.nio.file.FileSystem impl to org.apache.hadoop.fs.{FileSystem, Path} impl in [SparkContextFacto]r(https://github.com/broadinstitute/gatk/blob/73f2a62bee52518b57a985717770ed3a64d83243/src/main/java/org/broadinstitute/hellbender/engine/spark/SparkContextFactory.java), otherwise we could support both nio and hadoop thru env variable, Let me know your thought!. ```; scala> stringRdd.saveAsTextFile(""oss://eric-new/testwrite10""). scala> val stringRdd = sc.parallelize(Seq(""Test String"")); stringRdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[4] at parallelize at <console>:24. scala> stringRdd.saveAsTextFile(""oss://eric-new/testwrite11""); ```. ``` oss",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-354989381:72,access,access,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-354989381,1,['access'],['access']
Security,"@lbergelson You may recall that we've encountered things like malformed block-compressed input that validates and can be read without error, and yet appears to have fewer records than it should.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317048503:100,validat,validates,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317048503,1,['validat'],['validates']
Security,@lbergelson changed it to `validate`. Anything else?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2543#issuecomment-290780224:27,validat,validate,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2543#issuecomment-290780224,1,['validat'],['validate']
Security,@lbergelson thank you for the comment and sorry for my bit late response. I excluded the dependency to the jsr203-s3a and tested that both local- and spark-gatk can access s3a files by dynamically loading it. I also added a new directory `scripts/s3a` for documentation and simple tests for s3a demonstration.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-665484597:165,access,access,165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-665484597,2,['access'],['access']
Security,"@lbergelson thanks for following up. To the first part:. ```; jar tvf GenomeAnalysisTK4.jar; echo $?; ```; returns 0. and this is:; ```; jar tvf ../bin/GenomeAnalysisTK4.jar | grep -i FileTruncatedException; 765 Wed Mar 17 12:09:12 PDT 2021 htsjdk/samtools/FileTruncatedException.class; ```; so seems ok. I will talk to the group that manages the cluster. one out there possibility is that this is based on a lustre filesystem, and there could be some cryptic cluster-specific access issue. i have no specific reason to believe this, but weird things have happened.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1042022576:477,access,access,477,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1042022576,1,['access'],['access']
Security,"@lbergelson, I don't think that this solution will help in this case, because another error when trying to use `CommandLineProgramTest`is that it extends `BaseTest`, which loads directly a `GenomeLocParser` for a reference that is not present and it blows up in every test. Regarding the `Main` class, because you point it out here, I would like to have some control over `Main` and how it manages things like errors or logging header. Basically all the things that I'm facing at the moment are, apart of this error using the testing framework, is that the framework have tons of mentions to the GATK itself (error messages pointing to the GATK manual page or bundle tools), and little control over which of them should be expose to the final user. Only as an example, I would like to output a line with the name and version of my software and a short notice about the usage of the GATK framework and which version I'm using (for easier maintenance, and contribution if a bug is found).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242802278:723,expose,expose,723,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242802278,1,['expose'],['expose']
Security,"@lbergson's instructions above are good, but somehow they did not work for me. I was able to follow the [application default credentials](https://developers.google.com/identity/protocols/application-default-credentials) instructions, though. Here are the steps I took:. 1. create a new service account on the Google Cloud web page and download the JSON key file.; 2. gcloud auth activate-service-account --key-file ""$PATH_TO_THE_KEY_FILE""; 3. export GOOGLE_APPLICATION_CREDENTIALS=""$PATH_TO_THE_KEY_FILE"". I cleared my credentials first to make sure that the access worked because of the above steps, not because of other credentials. After those steps, gatk was able to run from my desktop and access files using the service account credentials. `gsutil ls` worked as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2425#issuecomment-282900470:559,access,access,559,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2425#issuecomment-282900470,4,['access'],['access']
Security,"@ldgauthier Concordance with XHMM would definitely be useful for validating calls on clusters for which we do not have WGS data. ; We need to modify code for having a fixed common CNV regions, but that should be straightforward.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4738#issuecomment-387828741:65,validat,validating,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4738#issuecomment-387828741,1,['validat'],['validating']
Security,"@ldgauthier Feel free to open a ticket describing your dream sequence dictionary compatibility check -- we can make the existing check stricter if you think it's too permissive, since users are always free to run with `--disable-sequence-dictionary-validation`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3754#issuecomment-495336889:249,validat,validation,249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3754#issuecomment-495336889,1,['validat'],['validation']
Security,"@ldgauthier If you feel that you need some validation, but less strict/expensive than the default, then I'd suggest turning off the default validation, writing your own scaled-down dictionary validation routine, and calling it from `onTraversalStart()` in your tool. Then if it seems like the scaled-down validation might be generally useful, we could hook it up to the `SequenceDictionaryValidationArgumentCollection` as a third engine-level option.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6589#issuecomment-625438251:43,validat,validation,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6589#issuecomment-625438251,4,['validat'],['validation']
Security,"@ldgauthier This is why the `--disable-sequence-dictionary-validation` argument exists in `GATKTool`. If you're confident in the compatibility of your inputs, and the checks are too expensive, you can run with that option and (optionally) perform some less strict validation of your own in your `onTraversalStart()` method.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6589#issuecomment-625366653:59,validat,validation,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6589#issuecomment-625366653,2,['validat'],['validation']
Security,"@ldgauthier and @jonn-smith . As discussed during the gatk office hours, this error traces back to ValidateVariants in GVCF mode being unable to handle variants with a lower start position than the previous contig.; Example:; Super-Scaffold_1 9238114 . T <NON_REF> . . END=9238123 GT:DP:GQ:MIN_DP:PL 0/0:12:0:11:0,0,0; Super-Scaffold_2 1 . G <NON_REF> . . END=4 GT:DP:GQ:MIN_DP:PL 0/0:31:93:31:0,93,1141",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6023#issuecomment-507376213:99,Validat,ValidateVariants,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6023#issuecomment-507376213,1,['Validat'],['ValidateVariants']
Security,@ldgauthier has volunteered to open a PR to expose this parameter -- should be part of the next GATK release,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1662611627:44,expose,expose,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1662611627,1,['expose'],['expose']
Security,"@ldgauthier this finishes what we started in #4858 and is necessary for the pileup-calls-on-bamouts MC3 validation. The cause is the same, in that Pair-HMM has a tiny bias in favor of shorter haplotypes and thus it prefers deletion haplotypes when reads end inside STRs. In #4858 we broke near-ties in favor of the reference; this PR fixes the case where two alt haplotypes share a SNV and one of them has a spurious deletion. One important sanity check was that when I set `cigarTerm` to zero in `AssemblyBasedCallerUtils.java` no tests broke. This means that the refactoring needed to set up the change didn't affect behavior. I looked at most of the sites where `PL`s and/or `DP`s changed in the integration test vcfs and in every case the difference was from a fake deletion that this PR fixed. I also went through the diff of the bamouts in IGV and found the same thing. Finally, the changes to test vcfs in `GenotypeGVCFsIntegrationTest` and `GenomicsDBImporterIntegrationTest` are a consequence of changes to the `HaplotypeCallerIntegrationTest` vcfs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5359:104,validat,validation,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5359,1,['validat'],['validation']
Security,"@lucidtronix Are the environment variables that you added to the Docker env essential to realize the speed 2x improvement ? I'm reluctant to just add them to the Docker env without understanding what they're doing and whether/how they impact other components. i.e., changing OPEN_MP thread affinity/pinning params etc. might impact the native Intel PairHMM implementation (also @samuelklee will these impact CNV) ? Another option is reduce the scope of them and set them only for the specific tool(s), possibly exposed as command line arguments. The ScriptExecutor has control over the python process' environment and could easily propagate them to the so they only affect the particular Python process. But the values would have to be provided somehow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5725#issuecomment-475614790:511,expose,exposed,511,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5725#issuecomment-475614790,1,['expose'],['exposed']
Security,"@magiDGS My preference would be to do a single PR with all of the fixes for the enable/disable validation rules and allowed values changes, and only those changes. Then we can do a second one with the extensibility changes. You can decide if you want to close this PR, or use it as the basis for the second one - either way is fine with me. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-278328215:95,validat,validation,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-278328215,1,['validat'],['validation']
Security,"@magicDGS Args like the config file that are truly optional (have no default value at all) do not show up in the command line or headers unless they're populated with some value. It should be pretty easy for ReadTools (which I think already has a common base class for its tools), to ensure a config file is never accepted by just precluding it via custom command line validation, or arg preprocessing. BTW, all tools built with GATK already have numerous common args that may or may not apply in a given tool context. For example, all of the ReadWalkers have a `--lenientVCFProcessing` arg. So I'm not even sure we need to make this hidden, since it will hide it from gatk users. My 2 cents. Others may feel differently.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371819413:369,validat,validation,369,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371819413,1,['validat'],['validation']
Security,"@magicDGS Good question! The main requirement is that the APIs need to allow you to *optionally* pass in URIs/Paths for all of the ""companion files"" for a particular input. For example, the `fai` and `dict` files for a fasta, or the `bai` file for a bam. When using signed URIs for authentication, these would all have separate signed URIs that would need to be provided explicitly. Of course, there should also be API methods that don't require you to pass in all of the companion files, and instead infer them automatically from the Path to the primary input, as htsjdk currently does. We would use these whenever possible (eg., when using account-level authentication rather than signed URIs, or when no authentication is necessary).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5269#issuecomment-429064032:282,authenticat,authentication,282,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5269#issuecomment-429064032,3,['authenticat'],['authentication']
Security,"@magicDGS I glanced at the tests and it appears there may be a scientific validation component, which I am unfamiliar with (I'm in an intro to Java course currently). Is this the case? If scientific validation is needed, then it is best to involve someone familiar with validation, e.g. Laura or Yossi. If all you need is data that can be run through these commands, I can put this together. Let me know. I'm late to these efforts, but I'd like to check one thing. Because of the way GRCh38 contigs are parsed, e.g. the HLAs that contain colons in their names, I believe we now prefer Picard-style intervals lists that tab-separate values instead of the `10:96000399-96000421` format that RealignerTargetCreator produces. I'm not certain of the status of GATK-style intervals lists, but I do know that the CNV developers have swiched to Picard-style. Is this what is produced by the new RealignerTargetCreator?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371506127:74,validat,validation,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371506127,3,['validat'],['validation']
Security,"@magicDGS I'd strongly prefer not to introduce a read filter descriptor hierarchy if we can avoid it, as it will be tricky to get right, and add complexity. We definitely need to be able to extend the package list used by the descriptor to find plugins, but as you point out we'll be able to use the configuration mechanism for that. For before/after-analysis filters, I expect that we'll just add that directly to the existing plugin once we resolve https://github.com/broadinstitute/gatk/pull/2085 (which I hope to get to this week). I think the rest of the cases can be addressed by overriding makeReadFilter and providing custom behavior of filter merging. If this turns out to be something truly common, we could consider allowing the tool to inject an argument collection into the plugin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-274970451:748,inject,inject,748,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-274970451,1,['inject'],['inject']
Security,"@magicDGS My apologies for the long delay on this. It looks like there are still quite a few standard porting issues that need to be addressed in this PR (brackets, finals, multiple top-level public classes, outdated GATK3 usage example, remove references to RODs, kebabify, etc., etc.). There is also the bigger issue of testing and validation - ideally at a minimum we'd reproduce the existing GATK3 tests, but since these are dependent on large, private files, those tests will have to be replaced with new tests, and validated/compared against GATK3. These same issues will come up with IndelRealigner. @vdauwera @sooheelee Even with @magicDGS graciously volunteering to do the work of porting the code, retaining the indel realignment tools will require internal review, helping with test development and validation, and support. Before we commit to that, I guess I want to make sure that this is indeed a high priority, and that you think porting this to GATK4 is a better option than relying on GATK3, or letting @magicDGS port it to ReadTools ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-363451643:334,validat,validation,334,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-363451643,3,['validat'],"['validated', 'validation']"
Security,"@magicDGS Sorry for the delay on these AssemblyRegion-related PRs. There is an effort at the Broad right now to validate the GATK4 `HaplotypeCaller` against the GATK3 version. Until this is complete, we're not accepting even minor changes to code on the critical path for the `HaplotypeCaller`, except for bug fixes that arise from the validation work. It's still possible that as a result of this validation work `AssemblyRegionWalker` may get refactored/altered to address problems discovered, so until we have a final version that produces acceptable results for `HaplotypeCaller` (and we're not quite there yet) other changes to that part of the codebase will have to wait. Sorry for the inconvenience -- once GATK4's `HaplotypeCaller` gets the official stamp of approval we will certainly find a way to get all of your changes in. In the mean time we have to ask you to be patient a little longer!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2371#issuecomment-287447471:112,validat,validate,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2371#issuecomment-287447471,3,['validat'],"['validate', 'validation']"
Security,"@magicDGS The problem with exposing the datasources to walkers is that they would be able to invalidate the entire traversal. For example, a `ReadWalker` could alter the traversal intervals on the reads datasource mid-way through traversal from within `apply()`, or it could cause the reads iterator used by the engine to get closed by issuing a separate `iterator()` call on the datasource, which would cause the rest of the traversal to fail. This is why I feel strongly that the datasource objects should not be directly accessible to walker-based tools. Note that it's still possible for walkers to create their own, separate datasources without reaching into the ones used by the engine, or a tool author can extend `GATKTool` directly rather than one of the walker base classes and have the freedom to access everything (which was not possible before this PR).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4964#issuecomment-401423305:524,access,accessible,524,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4964#issuecomment-401423305,2,['access'],"['access', 'accessible']"
Security,"@magicDGS Yes, I think it would be much simpler if we had one PR with all of the fixes for the validation rules (and related help issues). The extensibility changes we've been discussing should be a separate PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-278330318:95,validat,validation,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-278330318,2,['validat'],['validation']
Security,"@magicDGS, after much discussion with @droazen, we will make this test set even smaller. Also, the current test set will be temporary, to unimpede you, until updates to the main test set can be made (discussion to start next week). @cmnbroad says we need to keep an eye out for coverage in the tests, given the need to make this dataset extremely small. So I removed chr17 from the mini-reference, such that only four small snippets of reference from various chromosomes remained. However, it appears there are only two usable indel realignments going on with GATK3. Note thate these are targeted exome samples with comparatively low coverage. [for_magicDGS.zip](https://github.com/broadinstitute/gatk/files/1820785/for_magicDGS.zip). There are two samples, so as to enable testing nWayOut, and an artificial reference `hg38_Shl01`. The two sites to hone in on are chr11:177568 and chr11:207134. Note also that the BAMs are in an invalid state according to ValidateSamFile. However, GATK3 RealignerTargetCreator and IndelRealigner did not seem to mind. I think these tools should allow processing of BAMs in any validation state. Apologies for the meager state of the data. On the bright side, the data set including the ref is only 23MB and will meet with @droazen's approval in terms of size. . Good luck @magicDGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600:1112,validat,validation,1112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-373845600,3,"['Validat', 'validat']","['ValidateSamFile', 'validation']"
Security,"@marcopessoa, the fixes for this issue are not in master yet - here is the pending PR #6305. You could use the [genomicsdb_120_1](https://github.com/broadinstitute/gatk/tree/genomicsdb_120_1) branch to test out your scenario and post what you find to provide further validation for the changes in the PR. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-573447804:267,validat,validation,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-573447804,1,['validat'],['validation']
Security,@mbabadi Can you let me know about the validation tools today?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3887#issuecomment-348487741:39,validat,validation,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3887#issuecomment-348487741,1,['validat'],['validation']
Security,"@mbabadi I know for the hg38 runs you are using CalculateTargetCoverage, but in case you used SparkGenomeReadCounts for any other WGS runs you're looking at, here's another thing to be aware of when considering the duplicates issue. I'm also running into frequent errors when running SparkGenomeReadCounts for the WGS CNV validation that seem to be related to hadoop bam or binning errors. Let's move to @asmirnov239's new tool ASAP.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3367#issuecomment-324935273:322,validat,validation,322,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3367#issuecomment-324935273,1,['validat'],['validation']
Security,"@mbabadi I've updated my PR to use miniconda3. @mbabadi @lucidtronix @samuelklee I think we should aim for tools that at least run out-of-box, without depending on any out-of-band configuration other than the conda env. On top of that we can provide guidance/configs for users on how to enable further optimizations, like g++. Does that sound like an achievable goal ?. As for the docker, we're going to have strike the right balance between image bloat and performance(including test performance). I think we're around 4+ gig now, and counting. Before the Python integration we were at 1.9G, and trying to find ways to reduce it. So lets see where we wind up but keep that in mind. Finally, we need to find a way to install the (GATK) python package(s) without depending on access to the GATK repo. Right now I think the gCNV branch has a ""pip install from source"" added to the conda env .yml. That will work on the docker at the moment (and thus on travis), but that won't work for non-docker users how don't have source/repo access. Also, one of the proposals to reduce the size of the docker is to remove the repo clone that is currently there. My proposal is that we change the gradle build to create an archive/zip of the python source (this would include the VQSR-CNN package code as well as gCNV kernel). We can then copy that on to the docker image, and pip-install it from the copy. That would retain the ability to always run travis tests based on the code in the repo, and also keep the nightly docker image in sync. We'll also have deliver the archive as an artifact somehow (perhaps including PyPi) for non-docker users.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350303277:775,access,access,775,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350303277,4,['access'],['access']
Security,"@mbabadi commented on [Thu Jan 05 2017](https://github.com/broadinstitute/gatk-protected/issues/842). - [ ] carefully document the exposed parameters of gCNV, mark the tricky ones as advanced and document use case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2928:131,expose,exposed,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2928,1,['expose'],['exposed']
Security,"@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1058). - [ ] good choice of default parameters; - [ ] double-check assertion coverage in `CoverageModelArgumentCollection.validate()`; - [ ] if a model is provided, ARD and number of PCs must be overridden (currently, an exception is thrown if there is a discrepancy between model parameters and arguments). Relevant discussion:; **Mehrtash**: We may be able to get rid of a number of these parameters. Though, generally speaking, I'd rather expose more than less, with good default values and bold advanced disclaimers w/ proper documentation as you suggested. This is the case with sophisticated tools like HaplotypeCaller, StarAligner, etc. Soon enough, we will get strange errors from various users many of which can be resolved by changing a certain advanced parameter. Without exposing them, we will have to create patches for them and/or build custom jars.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2995:218,validat,validate,218,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2995,2,"['expose', 'validat']","['expose', 'validate']"
Security,@mbabadi commented on [Tue May 02 2017](https://github.com/broadinstitute/gatk-protected/issues/1021). - [ ] factor I/O methods out of `CoverageModelEMWorkspace` and to a new class; - [ ] shrink the exposed API; - [ ] rename/refactor `CopyRatioCallingMetadata` appropriately; - [ ] rename/move `MathObjectAsserts` to test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2979:199,expose,exposed,199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2979,1,['expose'],['exposed']
Security,@mcovarr @RoriCremer I have modified this now to fail outright if one of the validations fail. It now calls a GenerateFinalReport task as its last task and that will summarize the results in a way that (hopefully) makes it easier to understand the validation failure.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7850#issuecomment-1131987266:77,validat,validations,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7850#issuecomment-1131987266,2,['validat'],"['validation', 'validations']"
Security,"@mcovarr Hi Miguel - I just took a quick look at this branch, and it seems that it nicely addresses most of our needs for PGEN extract - specifically it would allow us to make an `ExtractCohortToPgen` that is just a slight variation of `ExtractCohortToVcf`. . There is one other thing we need though, which is a way to determine how many variants will be traversed *before* we traverse them. PGEN needs that up front, so we'd need it when we create the PGEN writer (currently it looks like that would be in the `ExtractCohortToPgen` `onStart` method). Is there any way to do that, and/or does it require access to the ExtractCohortEngine, which currently looks to private in the `ExtractTool` base class ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8344#issuecomment-1570383483:604,access,access,604,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8344#issuecomment-1570383483,1,['access'],['access']
Security,"@meganshand Could you review this? It fixes most of the homoplasmic missed calls in broadinstitute/dsp-spec-ops#116. I reviewed all of the false positive that were introduced to our somatic validations when attempting to make this the default in non-mitochondria mode. Everything was due to mapping error, which I do not expect to be an issue in mitochondria, and not an inherent problem with recovering more dangling ends. You will still want to run this branch through some of your validations, however. In mitochondria mode it's the same as the dangling-1-29.jar that I shared earlier with you and Sarah.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5693:190,validat,validations,190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5693,2,['validat'],['validations']
Security,"@meganshand Here's a quick example:. ![image](https://user-images.githubusercontent.com/11076296/158385742-20a3303b-d8ce-4335-b42f-622da9bfa8d3.png); ![image](https://user-images.githubusercontent.com/11076296/158385777-6174f8b8-7abb-4b31-92d1-11cc8064854b.png). Note that the malaria data used was pretty small: chr1-2 training (~20k positive training/truth variants, ~50k negative training variants; note also that the threshold for determining negative training was not tuned---a threshold corresponding to a 98% truth sensitivity was arbitrarily chosen), chr3 validation (~50k variants), and chr4-6 test (~150k variants). The LL score is calculated from a validation set held out from the training/truth positives used to train the model, while the F1 score is calculated using ""orthogonal truth"" positives/negatives determined using 3 families of ~30 trios each. However, there's some arbitrariness in how we define the boundary for the latter positives/negatives, and hence some arbitrariness in the F1 score itself. But I'd expect using gold-standard GIAB truth would be more straightforward. Not sure how much we can conclude, but that the validation and test F1s are similar and that the validation LL score isn't *too* far off are encouraging. That said, there is a pretty big drop in recall when optimizing LL. But we should also expect some discrepancy between LL and F1, according to one of the papers linked above. I would hope that with more variants or reliable training/truth (as in your data), things might stabilize or line up better. I'll try running with more malaria data, as well. The following trios x sites heatmap (top plot) for the validation set might better illustrate the arbitrariness in F1 (click to enlarge):. ![image](https://user-images.githubusercontent.com/11076296/158385585-1a0dfe8e-d4b7-4770-aed0-19ad81162c92.png). Here, yellow = het errors (since these are supposed to be clonal malaria samples), red = Mendelian errors, grey = no calls, green = Mendelian con",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431:564,validat,validation,564,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431,4,['validat'],['validation']
Security,"@meganshand I ran the ""Full Pipeline"" workflows in a clone of your FC workspace: https://portal.firecloud.org/#workspaces/broad-firecloud-dsde/copy-of-megans-m2-mito-validations. I did not run any of the things that generate graphs because they were harder for me to understand. To compare the new results to your previous ones, I took all variants that were either PASS or had only the contamination filter applied, extracted just the locus and alleles columns, then manually inspected the diff. For the 5% and 50% spike-ins there were usually no differences at all, while for the 1% spike-in the difference was usually 2-5 variants that straddled the LOD threshold.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5473#issuecomment-443745103:166,validat,validations,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5473#issuecomment-443745103,1,['validat'],['validations']
Security,"@meganshand There is a warning in the docs for `ReadCoordinateComparator` that it should not be used for bam file output that needs to match the ordering of `SAMRecordCoordinateComparator` exactly, since it sorts all unmapped reads after all mapped reads. `ReadCoordinateComparator` is a comparator for `GATKRead`, and that interface does not allow unmapped reads to have a position. Ie., even if an unmapped `SAMRecord` is assigned the position of its mapped mate, calling `getContig()`/`getStart()` on the unmapped read via the `GATKRead` interface will return null/0. This was done mainly for consistency reasons and to simplify client code. Whenever we need bam file order for reads in GATK4, we operate on SAMRecords directly and use either the `SAMRecordCoordinateComparator` from htsjdk or the `HeaderlessSAMRecordCoordinateComparator` (for headerless Spark reads) that produces the same ordering. I recommend addressing this for this tool via `presorted = false` for now, since the GATK3 version has it set to false as well with the comment: ""**we don't want to assume that reads will be written in order by the manager because in deep, deep pileups it won't work**"". This suggests that even if you were to change the comparator used by this tool to behave like `SAMRecordCoordinateComparator`, you'd still have ordering issues in deep coverage areas. It's worthwhile, though, to open a separate ticket to explore whether `ReadCoordinateComparator` could be changed to exactly match bam file order. Eg., perhaps we could add `getAssignedContig()`, `getAssignedStart()`, etc. methods to `GATKRead` to expose the positions that unmapped reads with mapped mates get assigned for sorting purposes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1853#issuecomment-224668518:1608,expose,expose,1608,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1853#issuecomment-224668518,1,['expose'],['expose']
Security,@mepowers Nice to meet you. . This issue isn't resolved. What was resolved was uploading a core dump that exhibits the problem. Is it possible for you to take a look into what's the causing the invalid pointer? Let us know what additional information we can provide. The core dump is located at `gs://hellbender/bugs/5690/core.tar.gz` and should be publicly accessible.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-465272533:358,access,accessible,358,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-465272533,1,['access'],['accessible']
Security,"@michaelgatzen ; in the meantime, if you need a samtools docker that can read from the bucket, you can ; ```; docker pull us.gcr.io/broad-dsde-methods/samtoolscloud:bucket.access. docker run us.gcr.io/broad-dsde-methods/samtoolscloud:bucket.access \; /bin/bash -c \; ""export GCS_OAUTH_TOKEN=`gcloud auth application-default print-access-token`; samtools view -H gs://your_bucket""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6148#issuecomment-531017117:172,access,access,172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6148#issuecomment-531017117,3,['access'],"['access', 'access-token']"
Security,"@micknudsen The splitting-bai is a different index from the bai. It's used to determine where spark should split the bam into shards when it's distributing work across the cluster. It doesn't provide random access support to the file, so it's a supplement to the bai instead of a replacement. Ideally we'd also output a normal bai as well, but due to the way the work is sharded it's not trivial to do so. . You can create the bai with `samtools index` if you use samtools, or `CreateHadoopBamSplittingIndex` has an option to output a bai. . The spark tools really should be creating it on the fly but we haven't gotten a chance to implement it yet. I opened a new ticket to track that since I didn't see one anywhere #4226",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4219#issuecomment-359544765:207,access,access,207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4219#issuecomment-359544765,1,['access'],['access']
Security,"@mohitmathew Thanks for the report! We are currently in the process of updating GATK to Java 17, which necessarily involves updating many of our dependencies. We are also updating our docker image to be based off of the latest Ubuntu LTS release. This should greatly reduce the number of critical vulnerabilities in our release image. After the Java 17 switchover we can revisit this and see what security issues remain.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1442245408:397,secur,security,397,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1442245408,1,['secur'],['security']
Security,@mrizkypw I've reported these abusive PRs to github as well as the Broad's security team.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6194#issuecomment-537530173:75,secur,security,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6194#issuecomment-537530173,1,['secur'],['security']
Security,"@munrosa @ldgauthier Possible breakthrough. . First, what's definitely true about the het at 169510380 in 55_55003_F5region.bam when I reproduce the bug with `-L chr1:169510380 -ip 100`:. * The variant is considered active and triggers assembly, as it should.; * For every kmer size there are non-unique kmers in the reference, so it increases up to k = 85, the last attempt at which the engine relaxes the unique kmers requirement. (See `ReadThreadingAssembler` line 425).; * Once it reaches this kmer size, there are cycles in the graph and so no assembly is returned. (See `ReadThreadingAssembler` line 464). Thus no alt haplotype is discovered and the variant is missed. I believe there are two possible solutions.; * The assembly engine looks for cycles before pruning, but this order could be switched with no ill effects. In the case of this het there are no cycles after pruning because the apparent cycle was a poorly-supported path due to sequencing error. Here regular pruning works but the new `--adaptive-pruning` option would give a bit more security against false cycles.; * We don't actually have to check for cycles, especially in the last, desperate kmer attempt. Well, we do with the current recursive implementation of `KBestHaplotypeFinder`, but we *don't* in the Dijkstra's algorithm implementation currently under review: #5462. (Technical note: @ldgauthier I know I promised that this PR gives entirely equivalent results to the existing implementation, but technically this is only true if the existing implementation finishes in finite time. Due to the greedy -- but optimal -- nature of Dijkstra's algorithm, cycles do not cause issues). Personally, I am in favor of *both* solutions -- looking for cycles after pruning, and waiving the no-cycle requirement on the last attempt. They are complementary.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-446465913:1056,secur,security,1056,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-446465913,1,['secur'],['security']
Security,"@mwalker174 Ok, I've asked our Google collaborator @jean-philippe-martin to comment on https://github.com/broadinstitute/gatk/issues/3591. It looks like there were some authentication-related changes in the newer gcloud releases that could explain the error. It may be that we just need to update our client code and/or project IAM settings.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3592#issuecomment-330937174:169,authenticat,authentication-related,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3592#issuecomment-330937174,1,['authenticat'],['authentication-related']
Security,"@mwalker174 This is just FYI... you do not need to do anything. You cannot assume that the user running the test doesn't have root access. Also, I cleaned up the test a bit to use standard TestNG conventions (note the ``@ Test``):; ```; @Test(expectedExceptions = UserException.CouldNotCreateOutputFile.class); @SuppressWarnings(""unchecked""); public void testWriteTwoKryo() throws Exception {; final File tempFile = createTempFile(""test"", "".dat"");; final Integer int_in = 29382;; final String str_in = ""test string"";; PSUtils.writeKryoTwo(tempFile.getPath(), int_in, str_in);. final Kryo kryo = new Kryo();; kryo.setReferences(false);; final Input input = new Input(BucketUtils.openFile(tempFile.getPath(), null));; final Integer int_out = (Integer) kryo.readClassAndObject(input);; final String str_out = (String) kryo.readClassAndObject(input);; input.close();. Assert.assertEquals(int_in, int_out);; Assert.assertEquals(str_in, str_out);. // Point to a subdir that does not exist, so that we get a FNF exception; PSUtils.writeKryoTwo(tempFile.getAbsolutePath() + ""/bad_dir/bad_subdir/"", int_out, str_out);; }; ```. Please note that this is not in ``master`` at the time of this writing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2708#issuecomment-300832489:131,access,access,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2708#issuecomment-300832489,1,['access'],['access']
Security,@nalinigans @mlathara Is there an integrity checker tool for GenomicsDB (or a programmatic way to check the integrity of a GenomicsDB instance)?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-684910131:34,integrity,integrity,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-684910131,2,['integrity'],['integrity']
Security,"@nh13 Thank you for clarifying. As of today GKL does not auto-detect when there's a read that's ""too long,"" ie a read length we haven't validated with GKL. We should be able to build that into our pending release. I agree we should also make sure that if the GKL pairHMM fails, the JAVA version is called instead. @Kmannth @droazen let's discuss this in our next sync.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-674250782:136,validat,validated,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-674250782,1,['validat'],['validated']
Security,@nh13 The tool as it currently stands does not appear to do any validation on the VCF header. Agree that this is something the tool ought to do. . @ldgauthier I heard that you had a branch that improves this tool -- did you by any chance add header validation in that branch?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6762#issuecomment-679294356:64,validat,validation,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6762#issuecomment-679294356,2,['validat'],['validation']
Security,"@niyomiw That's not a `NullPointerException` -- it's a `NumberFormatException`, and is different from the issue reported in this ticket. It's possible that one or more of your VCFs are malformed in some way. Could you try running `ValidateVariants` on the inputs?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-716675461:231,Validat,ValidateVariants,231,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-716675461,1,['Validat'],['ValidateVariants']
Security,"@niyomiw That's not a `NullPointerException` -- it's a `NumberFormatException`, and is different from the issue resolved in https://github.com/broadinstitute/gatk/issues/6766. It's possible that one or more of your VCFs are malformed in some way. Could you try running `ValidateVariants` on the inputs?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6913#issuecomment-716675049:270,Validat,ValidateVariants,270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6913#issuecomment-716675049,1,['Validat'],['ValidateVariants']
Security,"@pettyalex I don't think the Spark tools such as `MarkDuplicateSpark` are a likely candidate for stdin/stdout support. As you point out, they achieve parallelism by partitioning and then randomly accessing serialized input files. Even if it they could read from stdin, the benefits would be minimal, since they can't begin processing until they've seen the entire input stream, and they can't begin assembling the output until all of the worker nodes have finished processing their individual shards. So it would still require serializing the input, andI think the coarse grained process-parallelism you usually get from pipelining would be pretty minimal.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6749#issuecomment-1096818715:196,access,accessing,196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6749#issuecomment-1096818715,1,['access'],['accessing']
Security,"@rahulg603 This last time this was reported, we were unable to reproduce on our end, and the issue mysteriously ""went away"" on its own for @ldgauthier. Could you please report whether you're still getting the same error today? Are you able to access the same bucket using `gsutil` ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6179#issuecomment-1048012280:243,access,access,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6179#issuecomment-1048012280,1,['access'],['access']
Security,"@rdbremel for ""mystery 1"" see issue #5447. This should be an innocuous warning that it can't initialize the Google Cloud Storage code and shouldn't cause a failure unless you try to access paths that start with ""gs://"". Going through the Cloud initialization steps described in the README should remove the warning (though again, this isn't required if you don't need to read files from the cloud). Mystery 2: For what it's worth, ""GC overhead limit exceeded"" indicates that the VM was spending too much time in GC. Running low on memory is a possible cause but generating too many small objects or being stuck in an infinite loop of allocation/deallocation are others. In the past these have been caused by inputs that were malformed in some way. This isn't the place for this discussion though, please file a separate issue since it's a separate bug.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548955879:182,access,access,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548955879,1,['access'],['access']
Security,"@ronlevine commented on [Fri Jul 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438). ### Instructions. Follow up to #1432.; Remove the following code from `IntervalUtils. intervalFileToList()` when a new exome, correctly converted interval list (with no -1 length intervals) is released :. ```; if (interval.getStart() - interval.getEnd() == 1 ) { ; logger.warn(""Possible incorrectly converted length 1 interval : "" + interval);; }; ```. ---; ## Feature request; ### Tool(s) involved. Any tool using `IntervalUtils. intervalFileToList()` ; ### Description. Once this change is made, -1 length intervals will be validated and an exception will be thrown. ---. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260495927). From what I understand of the referenced thread, the ""incorrect"" interval list may always be around, so we may never be able to just blow up on it. Would it perhaps be more viable to add an option to toggle the level of stringency, ie choose in the command line whether to blow up or skip on these invalid intervals? . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260496001). @yfarjoun will want to opine on this, I think. . ---. @yfarjoun commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260513266). I hope that when we move exomes to hg38 we will correct this silly thing; and a few decades later we will no need this code (hehe). Y. On Mon, Nov 14, 2016 at 6:19 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > From what I understand of the referenced thread, the ""incorrect"" interval; > list may always be around, so we may never be able to just blow up on it.; > Would it perhaps be more viable to add an option to toggle the level of; > stringency, ie choose in the command line whether to blow up or skip on; > these invalid intervals?; > ; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2520:629,validat,validated,629,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2520,1,['validat'],['validated']
Security,@ruchim would you be able to run the centaur tests on an arbitrary hash? That way we don't release bad WDL.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4319#issuecomment-362086043:67,hash,hash,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4319#issuecomment-362086043,1,['hash'],['hash']
Security,"@samuelklee @asmirnov239 @mbabadi I tried to run a 30-sample cohort through gCNV on all canonical chromosomes with 250bp bins sharded in 10k-interval blocks, but PostprocessGermlineCNVCalls gave the following error:. ```...; 19:26:14.967 INFO PostprocessGermlineCNVCalls - Analyzing shard 223...; 19:26:15.107 INFO PostprocessGermlineCNVCalls - Analyzing shard 224...; 19:26:15.259 INFO PostprocessGermlineCNVCalls - Analyzing shard 225...; 19:26:15.260 INFO PostprocessGermlineCNVCalls - Shutting down engine; [May 29, 2018 7:26:15 PM UTC] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 3.34 minutes.; Runtime.totalMemory()=39753089024; ***********************************************************************. A USER ERROR has occurred: Bad input: Validation error occurred on line %d of the posterior file: Posterior probabilities for at at least one posterior record do not sum up to one.; ```. After inspecting the output from shard 225, it seems that the model starts producing nan values after ~1600 warmup iterations (looking at the ELBO log). This shard corresponds to a pericentromeric region chr3:91540501-94090250. . It would be nice to have the option to bypass this error in PostprocessGermlineCNVCalls. Here is the model config for the shard:. ```""p_alt"": 1e-06,; ""p_active"": 0.01,; ""cnv_coherence_length"": 10000.0,; ""class_coherence_length"": 10000.0,; ""max_copy_number"": 5,; ""num_calling_processes"": 1,; ""num_copy_number_states"": 6,; ""num_copy_number_classes"": 2; ""max_bias_factors"": 5,; ""mapping_error_rate"": 0.01,; ""psi_t_scale"": 0.001,; ""psi_s_scale"": 0.0001,; ""depth_correction_tau"": 10000.0,; ""log_mean_bias_std"": 0.1,; ""init_ard_rel_unexplained_variance"": 0.1,; ""num_gc_bins"": 20,; ""gc_curve_sd"": 1.0,; ""q_c_expectation_mode"": ""hybrid"",; ""active_class_padding_hybrid_mode"": 50000,; ""enable_bias_factors"": false,; ""enable_explicit_gc_bias_modeling"": false,; ""disable_bias_factors_in_active_class"": false; ""version"": ""0.7""; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4824:797,Validat,Validation,797,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824,1,['Validat'],['Validation']
Security,"@samuelklee @davidbenjamin I'm not authorized to merge PRs, so one of yous will have to.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3033#issuecomment-306541526:35,authoriz,authorized,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3033#issuecomment-306541526,1,['authoriz'],['authorized']
Security,@samuelklee Can we just expose `bin_length` since it determines what is WGS vs. Exome? Punt the rest?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3980#issuecomment-355997554:24,expose,expose,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3980#issuecomment-355997554,1,['expose'],['expose']
Security,@samuelklee Feel free to merge regardless of your decision. This PR is actually blocking a validation.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3550#issuecomment-331029649:91,validat,validation,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3550#issuecomment-331029649,1,['validat'],['validation']
Security,"@samuelklee I am inclined toward dropping all nd4j-related things. Given that we have access to tf, theano and numpy, I personally do not intend to do any heavy lifting in Java in the foreseeable future. Feel free to clean up and issue PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2929#issuecomment-358097583:86,access,access,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2929#issuecomment-358097583,1,['access'],['access']
Security,"@samuelklee I support exposing these parameters via the command line, but I'd be opposed to any consolidation of parameters that changes the HaplotypeCaller output prior to the initial DRAGEN-GATK release in November, as the evaluations in that project are difficult enough as it is. If you want to do an evaluation to find the best set of SW parameters now, that's fine of course -- but we wouldn't be able to actually merge any breaking HaplotypeCaller changes until after the November DRAGEN-GATK release, and we'd also have to check whether the proposed changes affect the functional equivalence of GATK and DRAGEN (we're developing tests now that can check this). If you want to expose the SW parameters on the CLI now, I think 12 arguments is fine. Just give each argument a clear prefix indicating what it applies to (eg., `--read-to-haplotype-mismatch-penalty`). If a user has gotten to the point where they feel the need to mess with the SW parameters, their command line is probably already long and complex as it is, so adding a few additional arguments won't ruin their day.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705081291:684,expose,expose,684,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705081291,2,['expose'],['expose']
Security,"@samuelklee I wrote some benchmarks for the exact combinatorics and you were right, my optimization was pointless. Although the `CombinatoricsUtils` method does explicitly multiply out instead of using cached factorials 1) the number of multiplications is only min(ploidy, (allele count - 1)), and 2) it actually takes quite a while (much larger than reasonable ploidy and allele count) for multiplication to take longer than the memory access of stored factorials. . I have removed this error in judgment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1066873168:437,access,access,437,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1066873168,1,['access'],['access']
Security,"@samuelklee If there are CNV tools that can't comfortably extend `GATKTool` as things stand now, then I think that we should adjust `GATKTool` to be more flexible until they can do so. This would help with certain long-term goals that the engine team has (such as all tools supporting NIO for all inputs, consistent sequence dictionary validation, etc.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358040921:336,validat,validation,336,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358040921,1,['validat'],['validation']
Security,@samuelklee We can easily expose the `IntervalMergingRule` in `IntervalArgumentCollection` (which is where `-L` is defined) as an argument to prevent the merging of adjacent intervals. We could also add a way for individual tools to set a default value for `IntervalMergingRule` themselves.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3246#issuecomment-314496762:26,expose,expose,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3246#issuecomment-314496762,1,['expose'],['expose']
Security,"@samuelklee Yes, this looks similar to what happened in master yesterday. In that case it looked like a transient remote access issue during the docker build. Having said that, its strange that the two data points we have were both on M2 WDL build.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4118#issuecomment-356761950:121,access,access,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4118#issuecomment-356761950,1,['access'],['access']
Security,@samuelklee the problem is that I need something pretty quickly here. I'm guessing that changing the GMM algorithm is going to require a ton of validation...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3230#issuecomment-313891523:144,validat,validation,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3230#issuecomment-313891523,1,['validat'],['validation']
Security,"@schaluva Could I get access to a bam for that chromosome or some smaller interval that exhibits the bug and the original, non-simplified germline resource VCF? I think the error in filtering has been fixed, so I'm focusing on the first error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6098#issuecomment-530090255:22,access,access,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098#issuecomment-530090255,2,['access'],['access']
Security,"@schelhorn We have a large clinical ""truth"" set we utilize during workflow validations. We also utilize spike-in samples from SeraCare and perform dilutions using a couple of the common Coriell cell lines. We noticed the Mutect2 calling inconsistency while validating a small targeted panel.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1171672027:75,validat,validations,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1171672027,2,['validat'],"['validating', 'validations']"
Security,"@slzhao This is definitely a reasonable thing to do. Please feel free to issue a PR to do this. That said, I have to give you a warning. Some of the datasources rely on `sqlite3` and therefore have issues on some distributed filesystems (see this [post on Lustre/NFS errors](https://github.com/CGATOxford/CGATPipelines/issues/39)). There are a few posts in the GATK forums about this as well. So you may want to do some testing before using one centralized copy of the data sources. . As a heads-up - I do not have access to a Lustre filesystem so I am unable to do any debugging with it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6731#issuecomment-671508424:515,access,access,515,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6731#issuecomment-671508424,1,['access'],['access']
Security,"@sooheelee I can't speak for CNV, but there isn't any general reason to prefer Picard interval lists in GATK. There was previously an issue with parsing interval queries that used contig names that contained "":"", but thats fixed now. The only time we prefer a Picard list is the theoretical case were you use a query interval against a sequence dictionary that contains contigs that make that query ambiguous (hg38 is not one of those). GATK will detect and reject such a query and suggest using a Picard interval file to disambiguate it. @magicDGS I'm not sure how/if writing tests against existing files in the repository will be useful. I want to restate that we don't want to take ports of these tools if they're marked `@Experimental `or `@Beta` because they haven't been validated, or don't have good test coverage. We need to find a way need to have valid tests so they'll be production ready.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371528161:777,validat,validated,777,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371528161,1,['validat'],['validated']
Security,"@sooheelee I'm not sure I understand this issue. Is someone running the gatk docker image and then having people use that as a shared server? If they are, then it seems like it's their responsibility to control ssh access to the server, set up permissions, etc. That's outside of the scope of what we can do. If someone is running docker and starting up their own instance of the gatk container, then they have root access to that container by definition. . Could you explain the exact use case you want to support? I read the thread you pointed at but I also didn't really follow EADG's reasoning.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-333650137:215,access,access,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-333650137,2,['access'],['access']
Security,"@takeshi-yoshimura Native access to s3a::// seems useful. In order to include this though we need some tests to show that it works/ demonstrate how to use it. I'm a bit concerned that it's version 0.0.1 (although it looks like there is a [0.0.2](https://search.maven.org/artifact/net.fnothaft/jsr203-s3a) out, should that be the one incorporated instead?) and there doesn't seem to be any activity on the library's [github](https://github.com/fnothaft/jsr203-s3a) in the last two years-ish. I'm wondering how stable/supported it is. . Maybe @fnothaft can comment on it. What's the status of this library? Do you recommend incorporating it or is there a different solution you've moved on to?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-658863506:26,access,access,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-658863506,1,['access'],['access']
Security,"@takutosato @LeeTL1220 As mentioned, this change scraps all the p values and replaces it with a simple and cheap probabilistic model. All our validations either improve or stay the same and speed is much better. * Spurious active regions are reduced by almost 50%.; * DREAM 4 goes from 40 hours total CPU time to 20 hours. All DREAM genomes now take less than a day.; * Hapmap sensitivity is the same.; * DREAM sensitivities for SNVs and indels all go up a bit.; * Upon manual review we no longer make any obviously bad inactive calls, except for very long deletions, which remain an issue. @takutosato This is a higher priority review than either of the documentation PRs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3304:142,validat,validations,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3304,1,['validat'],['validations']
Security,"@takutosato Based on all of our validations I added a commit to make this the default for M2. Because M2 shares a nested argument collection with HaplotypeCaller, this was pretty awkward. Louis told me this was the best among bad options.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5473#issuecomment-444360492:32,validat,validations,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5473#issuecomment-444360492,1,['validat'],['validations']
Security,@takutosato Can you review this PR?. This is a community request and a useful feature for our MC3 validation.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4601:98,validat,validation,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4601,1,['validat'],['validation']
Security,"@takutosato I was checking the filter analysis outputs of every M2 validation and this filter hurts much, much more than it helps, probably because other developments have made it less necessary. Let's essentially turn it off by default.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5487:67,validat,validation,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5487,1,['validat'],['validation']
Security,"@takutosato If you look at the previous code for realignment to a read's best haplotype it assumes that the read start within the reference haplotype byte array is the same as its start within the best haplotype byte array (see the coordinate passed to leftAlignIndels). This means that left alignment would effectively be deactivated (since the bases didn't line up correctly) whenever the best haplotype contained indels before the read start. This also creates a rare but possible edge case bug where if a read cigar ends in an indel and we miscalculated the read's start in the reference we might get an array out of bounds exception within leftAlignIndels. The recent PR #6427, which fixed some bugs involving left alignment, actually exposed this bug, because the previous code simply skipped left alignment when it encountered an out of bounds index. The fix is in the line `final int readStartOnReferenceHaplotype = readStartOnReferenceHaplotype(rightPaddedHaplotypeVsRefCigar, readToHaplotypeSWAlignment.getAlignmentOffset());` The idea is that we know where the read starts on its best haplotype from the SW alignment. In order to find the corresponding reference base, we follow the haplotype-to-reference cigar up to the read start and count the number of reference bases consumed. For example, suppose the haplotype-to-reference cigar is 30M5D100M and the read starts at (0-indexed) position 50 on the haplotype. We want to know how many reference bases are consumed in order to consume 50 alt haplotype bases in this cigar. That is, we count the reference bases in the 30M5D20M leading sub-cigar, which is 55. Thus the reference start is 55. Conversely, if the haplotype-to-reference cigar were 30M5I100M the read would start after 30M5I15M, with 45 reference bases consumed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6461:740,expose,exposed,740,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6461,1,['expose'],['exposed']
Security,"@takutosato Since this is an unsupported script that I have already tested to make sure results are the same, don't spend much time on it. Here's the summary:. * Put sub-sampling of hapmap (the most expensive part and a one-time cost because the samples are the same every time) into its own wdl.; * Put the rest of generating the truth into the same wdl as the sensitivity validation. This will make things simpler for the TAG team.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3379:374,validat,validation,374,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3379,1,['validat'],['validation']
Security,"@takutosato The extra strength of normal reads informing the ref allele's annotations improves results (very) slightly in all of our validations. The deeper reason for this change is in anticipation of multi-sample mode, where filtering based on a single INFO field will be simpler and probably statistically more powerful than filtering on a bunch of separate genotype fields.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5518:133,validat,validations,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5518,1,['validat'],['validations']
Security,@takutosato These are the changes I made for the most recent GP validation.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3171:64,validat,validation,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3171,1,['validat'],['validation']
Security,@takutosato This change doesn't hurt sensitivity in our validations and made M2 25% faster. We were getting a lot of active regions based on single substitution errors in overlapping reads.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5078:56,validat,validations,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5078,1,['validat'],['validations']
Security,"@takutosato This dramatically improves `CalculateContamination` by giving more care to distinguishing hom alts from hets. It makes an especially big difference in our tumor-only HCC1143 validations, where the accuracy is now very good (and BTW, ContEst gets these all completely wrong even *with* a matched normal). It also makes the tool work better in targeted panels where there might not be enough hom alt sites by adding a backup hom ref mode that gets triggered automatically. This is based on a Broadie request. I will file a separate issue to update the docs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5413:186,validat,validations,186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5413,1,['validat'],['validations']
Security,@takutosato This lets `ValidateBasicSomaticShortMutations` optionally annotate the `eval` vcf with validation `INFO` fields in addition to the standard output of a tsv. Having a vcf is more convenient for some things in MC3.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4999:23,Validat,ValidateBasicSomaticShortMutations,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4999,2,"['Validat', 'validat']","['ValidateBasicSomaticShortMutations', 'validation']"
Security,"@takutosato This uses minor allele fraction segmentation, which was already done internally in `CalculateContamination`, to improve tumor-only calling a lot. I also sw modest improvements in some tumor-normal validations. Also, @chandrans @sooheelee this hopefully does away with the problems with `af-of-alleles-not-in-resource` by deriving a defensible default that doesn't result in all calls in tumor-only mode getting filtered.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4509:209,validat,validations,209,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4509,1,['validat'],['validations']
Security,@takutosato Two quick edge cases and new unit tests. These were exposed when fixing other bugs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6518:64,expose,exposed,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6518,1,['expose'],['exposed']
Security,@takutosato We don't need to do this. The read orientation model is sufficiently well-validated.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5654#issuecomment-572203804:86,validat,validated,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5654#issuecomment-572203804,1,['validat'],['validated']
Security,"@tedsharpe @cwhelan please review. - Adds MarkDuplicatesReadFilter (to replace MarkedOpticalDuplicateReadFilter). MarkedOpticalDuplicateReadFilter will be removed in a subsequent PR because the Filter tool currently uses it.; - Changed some types (short to int, float to double) in the DUST algorithm; - Adds HostAlignmentReadFilter for filtering sufficiently well-mapped host reads. The helper function is there to run the test on supplementary alignments. I chose not to expose this as a GATK filter because the definitions of coverage and identity used here could be different than what some users would expect. @lbergelson Addressed your comments from the other branch:; - Added docstring to AmbiguousBaseReadFilter argument; - Made filterOpticalOnly an argument; - Argument variables changed from uppercase to lowercase; - See above regarding the duplicates filters",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2665:473,expose,expose,473,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2665,1,['expose'],['expose']
Security,"@tedsharpe please review. - SVKmerizer takes in an integer specifying the spacing between between kmers. This is is an effective way to reduce the kmer set size without affecting sensitivity much.; - SVKmerShort - added masking function that returns a copy of the current kmer after deleting bases at the specified positions; - Reworded some error messages about kmer length; - Moved and added some hashing functions to SVUtils, which will be used in another PR for the long-typed set classes and de-duplication filter.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2662:399,hash,hashing,399,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2662,1,['hash'],['hashing']
Security,@thebkaufman1995 encountered the following warning when trying to use TSV count files in the gCNV pipeline:. ```; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fsuper.c line 277 in H5F_super_read(): file signature not found; major: File accessibilty; minor: Not an HDF5 file; ```. My guess is this is because we first try to open counts files as HDF5 and then fall back to TSV (catching the relevant exception). Perhaps slightly different versions of the HDF5 library result in these warnings being emitted.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4482:307,access,accessibilty,307,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4482,3,['access'],['accessibilty']
Security,"@theisaacwong Thanks for reporting this. it might be hard to debug this without access to the input file. It looks like the issue might be with the cram file itself - do you know how the file was created (GATK, etc. ?). Also, it would be interesting to know if GATK is able to consume the file without specifying query intervals (without -L).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6865#issuecomment-704956160:80,access,access,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6865#issuecomment-704956160,1,['access'],['access']
Security,"@tomwhite After spending some time searching for this feature for my testing purposes, it would be helpful to expose the NIO adapter toggle directly from the command line in this branch.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5138#issuecomment-418494235:110,expose,expose,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5138#issuecomment-418494235,1,['expose'],['expose']
Security,@tomwhite I think this might have been from the first run of the jenkins tests since the disq change over - not sure though. You should have access to the `broad-gatk-test-jenkins-robust` bucket now if you need it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5545#issuecomment-449440744:141,access,access,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545#issuecomment-449440744,1,['access'],['access']
Security,"@vdauwera @droazen @lbergelson @cmnbroad Hey, this pull request will fail as long as the docker hub repo for broadinstitute/gatk has restricted read access. Any objections to making reading of the gatk (not gatk-protected) dockerhub repo public?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2726#issuecomment-302212757:149,access,access,149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2726#issuecomment-302212757,1,['access'],['access']
Security,"@vdauwera reported this here #950 :. > Can't seem to do git clone https://github.com/broadinstitute/hellbender/. ```; wmd16-c9e:codespace vdauwera$ git clone http://github.com/broadinstitute/hellbender/; Cloning into 'hellbender'...; remote: Counting objects: 22221, done.; remote: Compressing objects: 100% (142/142), done.; remote: Total 22221 (delta 47), reused 4 (delta 4), pack-reused 22046; Receiving objects: 100% (22221/22221), 36.63 MiB | 3.58 MiB/s, done.; Resolving deltas: 100% (9903/9903), done.; Checking connectivity... done.; Downloading src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam (76.16 MB); Username for 'http://github.com': vdauwera; Password for 'http://vdauwera@github.com': ; Error accessing media: src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam (6b1304800e60c0ac0358df137bdad48b7857a36465b04fef3fbbb09380f04746). Errors logged to /Users/vdauwera/codespace/hellbender/.git/lfs/objects/logs/20151005T220016.510795175.log.; Use `git lfs logs last` to view the log.; Downloading src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam.bai (11.25 KB); Username for 'http://github.com': ; ```. > Looks like I'm failing to download large test files. Do I need to be on VPN for this to work? Or is it expected and I should ignore it?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/952:675,Password,Password,675,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/952,2,"['Password', 'access']","['Password', 'accessing']"
Security,"@vdauwera we should modify the M2 WDLs. ; @davidbenjamin this will improve your sensitivity. Currently Mutect2 uses the MateOnSameContigOrNoMappedMateReadFilter filter that filters out any paired read whose mate maps to a different contig. This filter, if I recall correctly, used to be the hidden filter in HaplotypeCaller code that could not be turned off. It necessitated that I remove 0x1 flags in the GRCh38 tutorial (see section 6.1 of <https://gatkforums.broadinstitute.org/gatk/discussion/8017/>) so as to be able to call variants associated with a sample with an alternative haplotype. This filter is now exposed so that users can disable it. In addition to disabling this filter for ALT-aware data, I recommend we turn it off by default for somatic analyses, for any reference. This allows us (i) to call on ALT-aware mappings if data is such and (ii) call on SNPs and indels generated by putative structural variants that go _across contigs_. I know that this filter is active in the GATK4.beta.3-Mutect2 (see last line):; ```; WMCF9-CB5:align shlee$ gatk-launch Mutect2 -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -I hcc1143_N_subset500.bam -tumor HCC1143_normal -O 1_normalforpon.vcf.gz; Using GATK jar /Applications/genomicstools/gatk/gatk-4.latest/gatk-package-4.beta.3-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -jar /Applications/genomicstools/gatk/gatk-4.latest/gatk-package-4.beta.3-local.jar Mutect2 -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -I hcc1143_N_subset500.bam -tumor HCC1143_normal -O 1_normalforpon.vcf.gz; 19:26:43.105 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Applications/genomicstools/gatk/gatk-4.latest/gatk-package-4.beta.3-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; [August 24, 2017 7:26:43 PM EDT] Mutect2 --tumorSampleNam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3514:614,expose,exposed,614,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3514,1,['expose'],['exposed']
Security,"@vdauwera, examples of long arguments from the `StandardArgumentDefinitions`:. * `--disable-tool-default-read-filters`; * `--disable-sequence-dictionary-validation`; * `--add-output-sam-program-record`; * `--add-output-vcf-command-line`. This arguments are long anyway, but from my point of view it is more readable in the camel-case format; in addition, for the two last I have a question: is the upper-case format extension (SAM/VCF) going to be in lower-case? If not, there is still a mixture of upper/lower-case that may be confusing (and difficult to enforce).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2596#issuecomment-324038691:153,validat,validation,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2596#issuecomment-324038691,1,['validat'],['validation']
Security,"@vidprijatelj , I can't reproduce the issue on `MacOS` and `Centos 7`. Can you provide us with more information with respect to the system you are on? What is the OS? Are there any [access control lists](https://man7.org/linux/man-pages/man5/acl.5.html) setup?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-1470386063:182,access,access,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-1470386063,1,['access'],['access']
Security,@vilay-nference Thank you for your pull request. I've incorporated your suggestions and closed out many vulnerabilities from our transitive dependencies. Hadoop/spark have finally stopped incorporating log4j1 so that one is closed out for good. . I've also rebuilt our base docker to incorporate recent patches from ubuntu. We've implemented some additional security scanning into our build process which will help keep us more up to date going forward.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-2432287402:358,secur,security,358,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-2432287402,1,['secur'],['security']
Security,"@vilay-nference Were you able to get this configuration to pass tests on your end? I've attempted to incorporate your changes into https://github.com/broadinstitute/gatk/pull/8998, but I'm running into issues with hadoop and protobuf incompatibilities. I see the same problem with your branch when I try to run tests on it. (I also can't run tests without disabling -Werror on your branch since there are still some unresolved deprecation and other minor issues). Errors look like this:. ```; Caused by: java.lang.ExceptionInInitializerError: Exception java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.security.proto.SecurityProtos [in thread ""IPC Server handler 1 on default port 64812""]; 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos.<clinit>(ClientNamenodeProtocolProtos.java); ```. and you can easily trigger one by running `ParallelCopyGCSDirectoryIntoHDFSSparkIntegrationTest`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8950#issuecomment-2412827680:630,secur,security,630,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8950#issuecomment-2412827680,2,"['Secur', 'secur']","['SecurityProtos', 'security']"
Security,"@vilay-nference You are always very welcome to submit a pull request on github with any proposed changes to GATK!. Most of the remaining vulnerabilities are in dependencies-of-dependencies which can be difficult to update, but we are slowly chipping away at them. For example, log4j 1.x is a dependency of the latest release of Apache Spark 3.x, and 4.x is still in preview (and note again that the log4j 1.x vulnerabilities are not the same as the infamous and very serious vulnerability that affected log4j 2.x some years ago). We don't believe that any of the remaining library vulnerabilities pose a real-world threat to GATK in practice, but it would still be good to eliminate them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-2223132298:615,threat,threat,615,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-2223132298,1,['threat'],['threat']
Security,"@vruano ; Tests are failing, could you resolve this?; It appears to be in:; org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorIntegrationTest > testAgainstMutect2 FAILED; java.lang.IllegalArgumentException: Dirichlet parameters may not be negative; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); at org.broadinstitute.hellbender.utils.Dirichlet.<init>(Dirichlet.java:26); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticLikelihoodsEngine.getEffectiveCounts(SomaticLikelihoodsEngine.java:56)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7303#issuecomment-859204971:314,validat,validateArg,314,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7303#issuecomment-859204971,1,['validat'],['validateArg']
Security,"@yfarjoun @vdauwera I've refined the tool categorization based on feedback on the tentative categorization. Thank you @yfarjoun for the review and feedback. The refinement is reflected in the new tabbed sheet in the shared Google Spreadsheet:`1217Changes_categorization-and-assignments`. I've separated out GATK vs Picard tools for each of the categories. . Here is a summary of the changes. 1. New 11 to `Diagnostics and QC`:; AnalyzeCovariates (from Alignment, Duplicate flagging and BQSR); GatherBQSRReports (from Alignment, Duplicate flagging and BQSR); FlagStat (from Read Data Manipulation); FlagStatSpark (from Read Data Manipulation); GetSampleName (from Read Data Manipulation); Picard BamIndexStats (from Read Data Manipulation); Picard CalculateReadGroupChecksum (from Read Data Manipulation); Picard CheckTerminatorBlock (from Read Data Manipulation); Picard CompareSAMs (from Read Data Manipulation); Picard ValidateSamFile (from Read Data Manipulation); Picard ViewSam (from Read Data Manipulation). 2. Merge 14 tools remaining in `Alignment, Duplicate flagging and BQSR` with 37 tools in `Read Data Manipulation`. Keep latter name. 	51 tools. 3. Move these out of `Read Data Manipulation`:; CompareDuplicatesSpark (to DxQC); ConvertHeaderlessHadoopBamShardToBam (to Other); CreateHadoopBamSplittingIndex (to Other). 4. Move ValidateVariants into `Variant Evaluation`. Also:; `Variant Evaluation and Refinement` --> `Variant Evaluation`; `VCF Manipulation` --> `Variant Manipulation` . Let us know your thoughts. Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-352313248:921,Validat,ValidateSamFile,921,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-352313248,2,['Validat'],"['ValidateSamFile', 'ValidateVariants']"
Security,"@yfarjoun How do you want to proceed with this PR, given that there are downstream issues even after this fix? Is the strategy going to be to keep developing patches like this, or throw in the towel and sanitize the problematic bases early on in the pipeline?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6625#issuecomment-642045845:203,sanitiz,sanitize,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6625#issuecomment-642045845,1,['sanitiz'],['sanitize']
Security,"@yfarjoun I believe you always have opinions about validation, wdyt?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2129#issuecomment-243281529:51,validat,validation,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2129#issuecomment-243281529,1,['validat'],['validation']
Security,"@yfarjoun Right, the intention of this ticket was to implement the codec in htsjdk, then add a GATK integration test proving that we can now access interval_list files as tribble features.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5788#issuecomment-472566997:141,access,access,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5788#issuecomment-472566997,1,['access'],['access']
Security,"@yfarjoun Well, as I said in person I believe that there are benefits to running with asynchronous prefetching turned on even in the random-access case. @jean-philippe-martin can confirm.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5882#issuecomment-482681722:140,access,access,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5882#issuecomment-482681722,1,['access'],['access']
Security,"@yl-h I have created a new branch [genomicsdb_6744](https://github.com/broadinstitute/gatk/tree/genomicsdb_6744) that exposes GenomicsDBArgument Collection to CreateSomaticPanelOfNormals. Can you please run the following to help us narrow down the issues?. 1. The default for GenomicsDB exports/queries changed from BCFCodec streaming in 4.1.7.0 to VCFCodec in 4.1.8.0. Run `gatk CreateSomaticPanelOfNormal` with `--genomicsdb-use-bcf-codec true` to override this default. If the expected PoN records is still missing variants, can you also run (2)?; 2. Run `gatk SelectVariants -O out.vcf -V gendb://...` on a small region with this branch to verify the number of variants is the same as from 4.1.7.0. If not, would you be able to distill and post any line that is missing now?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-672322785:118,expose,exposes,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-672322785,1,['expose'],['exposes']
Security,"@zhanyinx Just merged a fix to this, which exposes a new CLI parameter. Tonight's nightly build will have the new option for you to try.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8276#issuecomment-1674597532:43,expose,exposes,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8276#issuecomment-1674597532,1,['expose'],['exposes']
Security,"A few recent bugs, which are all entirely my fault, came about because the liftover of gnomAD to hg38 (there is no official hg38 gnomAD yet) exposed some new edge cases, such as `AF=.` and `AF=0`, that caused errors. I suspect that you are seeing one that we hadn't found yet. Unfortunately, we do not have nearly validation on hg38. Here's what I will do: 1) correct our hg38 gnomAD to fix liftover artifacts and put this new resource in the GATK bucket. 2) Create a Firecloud workspace with a few hg38 samples in order to reproduce the error and to make sure future changes don't create new problems 3) try to fix the error because even if 1) works it's sloppy to rely on the fact that gnomAD won't have these edge cases. I hope 1) succeeds because it will be available immediately without waiting for the next release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5821#issuecomment-478305154:141,expose,exposed,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5821#issuecomment-478305154,2,"['expose', 'validat']","['exposed', 'validation']"
Security,A quick and dirty implementation. The on thing that needs extra scrutiny is going to be the various conditions/failure states in the validate method. I think i caught most of the cases but there might be a gap somewhere.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8724:133,validat,validate,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8724,1,['validat'],['validate']
Security,"A quick patch to help out the Variants team, which is struggling with a problematic callset. Note that a similar regularization to the effective number per component probably should have been applied to solve the issue in https://github.com/broadinstitute/gatk/pull/6425. I'm not sure if the lack of this regularization will still lead to convergence issues, but I would hope that the fix that was implemented instead (treating vanishing components as a special case and skipping computation) suffices. As discussed there, we may also want to eventually remove the idiosyncratic finalize step; it’s likely this is the source of issues here, since the correct Bayesian M step is already regularized by the prior. The covariance regularization term added here is standard (c.f. e.g. https://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/mixture/_gaussian_mixture.py#L154), but it may result in non-negligible changes to VQSLODs. As just discussed with the Variants team, we can probably use the WARP validation to convince ourselves that results are functionally equivalent. I updated the exact-match tests without much close examination (by simply forcing IntegrationTestSpec.assertEqualTextFiles to overwrite the old expected files), so someone may want to sanity check them. There were also a few more interactions between the integration tests for different tools than I anticipated. Some tests use output generated by an upstream tool as input and break encapsulation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7709:1043,validat,validation,1043,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709,1,['validat'],['validation']
Security,"A seemingly large change PR, but most changes are trivial.; The non-trivial part:. * a new tool `StructuralVariantionDiscoveryPipelineSpark` to run the whole process of SV discovery, by delegating works to `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSAMSpark`, both of which are refactored to accommodate the new tool;; * class `AlignmentRegion` is effectively moved into a new class `AlignedAssembly` (named quite close to the existing class `AlignedAssemblyOrExcuse` but will be moved into a different sub-package in a sequential PR).; * integration tests (local mode and on MiniClusters/hdfs) for all 5 major tools `FindBreakpointEvidenceSpark`, `DiscoverVariantsFromContigAlignmentsSAMSpark`, `StructuralVariantionDiscoveryPipelineSpark`, `AlignAssembledContigsSpark` and `DiscoverVariantsFromContigAlignmentsSGASpark`; a draw back is these integration tests do not test correctness of results but simple tests if these tools run.; * various unit tests. The two paths involving use of Fermi-lite are tested to be running and generating compatible results. The path involves using SGA as the assembler is also running but generates significantly less variants. (see attached run logs).; [differentVersions.txt](https://github.com/broadinstitute/gatk/files/956271/differentVersions.txt). The access levels of the various classes and methods are not optimal now because a serial PR that simply repackaging these classes (hence access levels must be changed) is expected to be generated immediately after this PR is approved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2621:1321,access,access,1321,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2621,2,['access'],['access']
Security,"A user rightly [points out](http://gatkforums.broadinstitute.org/gatk/discussion/comment/32631#Comment_32631) that different versions of HaplotypeCaller may produce GVCFs that are not directly compatible, causing weirdness when you joint-genotype them with GenotypeGVCFs. . Obviously this is primarily a data management problem (user should control what's in their pipeline) -- but it would be good to provide an additional safety layer by having GenotypeGVCFs, CombineGVCFs or whatever demon is used to invoke TileDB at least emit a WARN message if they see GVCFs produced by different versions of HC within the same input cohort. . Note that the VCF version number is not directly useable for this purpose since changes in the contents of GVCFs can arise within the same version of VCF spec. Also, one could argue that the GVCFs really should all be produced using exactly the same command line arguments -- but validating the entire command line would probably be overkill...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2129:914,validat,validating,914,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2129,1,['validat'],['validating']
Security,"A12878_S1_md.bam --output hc_variants_7.vcf --bam-output realigned_slice_7.bam --max-reads-per-alignment-start 1000 --min-base-quality-score 0 --minimum-mapping-quality 0 --disable-read-filter MappingQualityReadFilter --disable-read-filter MappingQualityAvailableReadFilter --disable-read-filter NotSecondaryAlignmentReadFilter --disable-read-filter NotDuplicateReadFilter --disable-read-filter PassesVendorQualityCheckReadFilter --disable-read-filter NonZeroReferenceLengthAlignmentReadFilter --disable-read-filter GoodCigarReadFilter --disable-read-filter WellformedReadFilter`; [January 10, 2018 2:39:19 PM EST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 91.81 minutes.; Runtime.totalMemory()=7215251456; java.lang.IllegalArgumentException: Invalid interval. Contig:chr5 start:71357769 end:71357768; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:49); at org.broadinstitute.hellbender.engine.AssemblyRegion.add(AssemblyRegion.java:335); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.fillNextAssemblyRegionWithReads(AssemblyRegionIterator.java:230); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:194); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:135); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:34); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:290); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:271); at org.broadinstitute.hellbender.engine.GATKTool.doWo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4120:1350,validat,validatePositions,1350,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4120,1,['validat'],['validatePositions']
Security,"ATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Sep 16, 2019 2:33:15 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Mon Sep 16 02:33:15 UTC 2019] Executing as user@server on Linux 3.10.0-693.21.1.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_192-b01; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.3.0; [Mon Sep 16 02:33:22 UTC 2019] picard.analysis.CollectWgsMetrics done. Elapsed time: 0.16 minutes.; Runtime.totalMemory()=6996099072; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp; java.lang.IllegalArgumentException: The requested position is not covered by this StartEdgingRecordAndOffset object.; at htsjdk.samtools.util.AbstractRecordAndOffset.validateOffset(AbstractRecordAndOffset.java:109); at htsjdk.samtools.util.EdgingRecordAndOffset$StartEdgingRecordAndOffset.getBaseQuality(EdgingRecordAndOffset.java:112); at picard.analysis.FastWgsMetricsCollector.excludeByQuality(FastWgsMetricsCollector.java:189); at picard.analysis.FastWgsMetricsCollector.processRecord(FastWgsMetricsCollector.java:144); at picard.analysis.FastWgsMetricsCollector.addInfo(FastWgsMetricsCollector.java:105); at picard.analysis.WgsMetricsProcessorImpl.processFile(WgsMetricsProcessorImpl.java:93); at picard.analysis.CollectWgsMetrics.doWork(CollectWgsMetrics.java:231); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Using",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6163:2502,validat,validateOffset,2502,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6163,1,['validat'],['validateOffset']
Security,ATK. ### Affected version(s); 4.1.4.1; ### Description ; when trying to compile with .gradlew bundle get the error above. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/usr/bin/gatk/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --info or --debug option to get more log output. Run with --scan to get full insights. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':gatkDoc'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$3.accept(ExecuteActionsTaskExecuter.java:166); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$3.accept(ExecuteActionsTaskExecuter.java:163); at org.gradle.internal.Try$Failure.ifSuccessfulOrElse(Try.java:191); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:156); at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:62); at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:108); at org.gradle.api.internal.tasks.execution.ResolveBeforeExecutionOutputsTaskExecuter.execute(ResolveBeforeExecutionOutputsTaskExecuter.java:67); at org.gradle.api.internal.tasks.execution.ResolveAfterPreviousExecutionStateTaskExecuter.execute(ResolveAfterPreviousExecutionStateTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute(CleanupStaleOutputsExecuter.java:94); at org.gradle.api.internal.tasks.execution.FinalizePropertiesTaskExecuter.execute(FinalizePropertiesTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.ResolveTaskExecutionModeExecuter.execute(ResolveTaskExecutionModeExecuter.java:95); at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:57); at org.gradle.api.internal.tasks,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466:1093,Validat,ValidatingTaskExecuter,1093,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466,1,['Validat'],['ValidatingTaskExecuter']
Security,AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:300); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at shaded.cloud-nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:347); ... 17 more; Caused by:; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1316); at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1291); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250); at shaded.cloud-nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:77); at shaded.cloud-nio.com.google.api.client.http.HttpRequest,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2514:3795,secur,security,3795,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514,1,['secur'],['security']
Security,"Actually, I'm going to go ahead and add some exact match tests to guard against this sort of thing. Behavior for key somatic CNV tool modules (i.e., kernel segmentation and MCMC) is unit tested to within statistical noise (so, not exact match) on simulated data, but most integration tests just check for plumbing and not correctness. The idea was always that this sort of thing would be covered by what eventually became CARROT, since such tests would probably have to be long running and require more resources than are available in the repo to be useful. See the high priority but long dormant issues https://github.com/broadinstitute/gatk/issues/4122 and https://github.com/broadinstitute/gatk/issues/4123, as well as https://github.com/broadinstitute/gatk/issues/4630. In fact, I think the original idea was that Lee's validation would be the first to go into CARROT. Note also that I did some work to set up transition of all existing CNV tests (also including the somatic CNV validation against TCGA SNP calls that I put together on Terra) before going on leave and moving off CNVs, but during all that, we managed to 1) lose TCGA access, 2) delete the test files on which Lee based his validation after he left, and 3) reassign at least one of the people that was going to help with the transition. Again, the resulting differences here are minor and it's unlikely that future non-CNV code changes will have similar effects, since the CNV code is relatively well encapsulated, but the exact-match checks will hopefully give us some peace of mind until CARROT tests are ready. @jonn-smith @KevinCLydon looping you in just in case you're not aware of all of this history. Would love to chat about where CARROT is at and where you'd like it to go---feel free to ping me anytime!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7649#issuecomment-1023354175:824,validat,validation,824,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7649#issuecomment-1023354175,4,"['access', 'validat']","['access', 'validation']"
Security,"Actually, I'm noticing that while using NIO for the BAM for read/allelic-count collection is usually much more efficient, using NIO for the reference in other tasks can be much slower. Perhaps the access patterns for the reference (hitting ~10^5 intervals for WES PreprocessIntervals/AnnotateIntervals and ~10^6 sites for WES/WGS CollectAllelicCounts, respectively) make localization a better strategy? @droazen does that sound right to you?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-392046938:197,access,access,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-392046938,1,['access'],['access']
Security,"Actually, Jessica Hekman just confirmed she ran into an issue when running the GATK SV WDLs on-prem with Singularity, namely being unable to access the GATK jar in the root directory.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6525#issuecomment-1006929885:141,access,access,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6525#issuecomment-1006929885,1,['access'],['access']
Security,Add CheckForNullColumns to and fixed ClinvarSignificance in VAT validation [VS-1156],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8669:64,validat,validation,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8669,1,['validat'],['validation']
Security,Add NIO test that accesses public GCS data while not being authenticated,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5340:18,access,accesses,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5340,2,"['access', 'authenticat']","['accesses', 'authenticated']"
Security,Add VAT Validation check that aa_change and exon_number are consistentally set.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7850:8,Validat,Validation,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7850,1,['Validat'],['Validation']
Security,Add VAT validation rule #2 [VS-19],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7374:8,validat,validation,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7374,1,['validat'],['validation']
Security,Add VAT validation rule #5 [VS-16],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7365:8,validat,validation,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7365,1,['validat'],['validation']
Security,Add VAT validation rule #6 [VS-15],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7373:8,validat,validation,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7373,1,['validat'],['validation']
Security,Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7379:8,validat,validation,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7379,2,['validat'],['validation']
Security,Add VDS Validation to the hail integration split,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8343:8,Validat,Validation,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8343,1,['Validat'],['Validation']
Security,Add a github action to run womtool validation on all WDLs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7822:35,validat,validation,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7822,1,['validat'],['validation']
Security,Add a loop to the validation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8732:18,validat,validation,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8732,1,['validat'],['validation']
Security,"Add a test that validates than an ambiguous interval query can be disambiguated by the user by providing the interval in a bed file; changes the error message to recommend this alternative; fixes an issue where the error message was displaying the entire interval query multiple times, rather than the specific contigs which make the query ambiguous.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4183:16,validat,validates,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4183,1,['validat'],['validates']
Security,Add a test to validate WDLs in the scripts directory.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7826:14,validat,validate,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7826,1,['validat'],['validate']
Security,"Add additional validation around duplicated rows in the VAT; <img width=""1418"" alt=""duplicate_AN_or_AC_values"" src=""https://user-images.githubusercontent.com/6863459/220667710-a416ab64-4f9b-475b-9268-ef7b86bfa81e.png"">. This has a successful run (except for one failure that is because it's being run on way less data); https://job-manager.dsde-prod.broadinstitute.org/jobs/07ddde58-ac0d-4229-9f96-d093f5c11682; The failed test is:; SpotCheckForAAChangeAndExonNumberConsistency. Perhaps we want to update this to not run this test if there are less than 10k samples?; Yes we do:; Here's the ticket for that:; https://broadworkbench.atlassian.net/browse/VS-878",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8175:15,validat,validation,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8175,1,['validat'],['validation']
Security,Add an argument to GATKTool and GATKSparkTool that allows sequence dictionary validation to be turned off,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1145:78,validat,validation,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1145,1,['validat'],['validation']
Security,Add annotation checks to ValidateVariants,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6272:25,Validat,ValidateVariants,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6272,1,['Validat'],['ValidateVariants']
Security,"Add argument to disable sequence dictionary validation, off by default",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1136:44,validat,validation,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1136,1,['validat'],['validation']
Security,"Add large runtime resource directory to lfs, and expose it to the Docker build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4530:49,expose,expose,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4530,1,['expose'],['expose']
Security,Add optional summary table output to ValidateBasicSomaticShortMutations,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4982:37,Validat,ValidateBasicSomaticShortMutations,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4982,1,['Validat'],['ValidateBasicSomaticShortMutations']
Security,Add task for VAT validation #3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7360:17,validat,validation,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7360,1,['validat'],['validation']
Security,Add task for VAT validation #4,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7363:17,validat,validation,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7363,1,['validat'],['validation']
Security,Add task for VAT validation #8 & 9,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7364:17,validat,validation,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7364,1,['validat'],['validation']
Security,"Add the gcs-connector as a GATK dependency, and write a test showing that GCS access with the Spark local runner works",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3125:78,access,access,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3125,1,['access'],['access']
Security,Add validation to SparkSharder to get better information about sequence dictionary errors,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2615:4,validat,validation,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2615,1,['validat'],['validation']
Security,"Addded abstract class MachineLearningUtils to provide an interface and; handle common tasks. These include loading data, splitting data into; training and test sets, cross-validation, and optimizing classifier; hyperparameters. Also added XGBoostUtils which provides a concrete implemention of; MachineLearningUtils (by wrapping xgboost4j) and serves as an example; of how to provide access to a 3rd-party machine learning library. Finally, added an example tool: ExampleTrainXGBoostClassifier. This; demonstrates a typical training use case of loading data, training a; classifier, assessing accuracy, and saving the classifier. It also; demonstrates a typical filtering use case of loading a saved classifer,; and using it to calculate probabilities or class labels. This is working towards issue 4922 by providing the tools necessary to; train classifiers in general, but does not provide tools to train a; BreakpointEvidence filter, so does not resolve it. Additionally, this; framework should eventually be extended to provide a bayesian; hyperparameter optimizer. One outstanding problem with these changes is that xgboost4j threading; does not appear to work on OSX, resulting in slower training. However,; it does work on linux.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5146:172,validat,validation,172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5146,2,"['access', 'validat']","['access', 'validation']"
Security,Added `#` as a character to be sanitized by `VCFOutputRenderer`.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5817:31,sanitiz,sanitized,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5817,1,['sanitiz'],['sanitized']
Security,"Added a `PrintReads` based test (`IntelInflaterIntegrationTest.testIntelInflaterDeflaterWithPrintReads`) to test integration of `IntelInflater` and `IntelDeflater`. Removed the previous `IntelInflater` and `IntelDeflater` integration tests, which were basically copies of GKL unit tests. . **Note:** ; The `PrintReads` test is using `CEUTrio.HiSeq.WGS.b37.NA12878.20.21.tiny.md.bam` for input. When using `CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam` for input, we see the exception below when comparing `PrintReads` input and output. **This is true when using `IntelInflater`/`IntelDeflater`, as well as JDK `Inflater`/`Deflater`.** Is this a problem?. ```; htsjdk.samtools.SAMFormatException: SAM validation error: ERROR: Record 9762, Read name 20GAVAAXX100126:7:2:8126:115177, bin field of BAM record does not equal value computed based on alignment start and end, and length of sequence to which read is aligned; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-284551832:695,validat,validation,695,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-284551832,1,['validat'],['validation']
Security,"Added a few comments of my own -- requested that you refactor to check the index modification time in the `FeatureDataSource` constructors, rather than in the sequence dictionary validation routines.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3063#issuecomment-320948324:179,validat,validation,179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3063#issuecomment-320948324,1,['validat'],['validation']
Security,Added a github action to run womtool validation on all WDLs; Also fixed two wdls that were failing validation.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7822:37,validat,validation,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7822,2,['validat'],['validation']
Security,Added a test to validate WDLs in the scripts directory. (#7826),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7829:16,validat,validate,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7829,1,['validat'],['validate']
Security,"Added a unit test. To do so I had to make `BaseRecalibrationEngine.calculateKnownSites()` static. This wasn't a problem because I don't think it accesses any instance attributes but if there's some reason it shouldn't be static, I can do an integration test instead.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6389#issuecomment-576898683:145,access,accesses,145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6389#issuecomment-576898683,1,['access'],['accesses']
Security,"Added a workflow file for enabling the GitHub Action which processes PR comments to determine if they are meant to trigger and CARROT test, and then processes them if they are formatted in that way. BIG IMPORTANT NOTE: Before this is merged, we need to set two secrets for this repo:; - `CARROT_TOPIC_NAME`, which is the name of the Google Cloud PubSub topic that messages will be sent to if a comment should trigger a run, and; - `CARROT_SA_KEY`, which is the service account key JSON for the service account that has write access to the PubSub topic.; If this is merged before those are set, I'm fairly confident we're just gonna get an email saying the action failed each time someone posts a PR comment (CARROT or otherwise), which would be less than ideal. Closes #6916",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6917:525,access,access,525,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6917,1,['access'],['access']
Security,Added experimental tool and exposed some of the AllelicCNV file extension constants. I am adding the tools as requested. Minor additional changes.; @gtiao and @sooheelee . Closes #3196,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3198:28,expose,exposed,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3198,1,['expose'],['exposed']
Security,Added in code to change how the best transcript is determined.; Added `#` as a character to be sanitized by `VCFOutputRenderer`. (#5817); Fixes #5822,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5834:95,sanitiz,sanitized,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5834,1,['sanitiz'],['sanitized']
Security,Added map-style accessors to all concrete Funcotation classes (Funcotation.`getField`). Fixes #3919,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4176:16,access,accessors,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4176,1,['access'],['accessors']
Security,Added requester pays option to Mutect2 tasks that access bams,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6879:50,access,access,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6879,1,['access'],['access']
Security,Added the following methods to `GATKTool`:. - `getReferenceDataSource()`; - `getReadsDataSource()`; - `getFeatureManager()`. `Walker` inherits directly from `GATKTool` and overrides these methods to throw an exception if they are called. No walker should need to directly access the data.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4964:272,access,access,272,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4964,1,['access'],['access']
Security,Added validateSampleNameMap command line parameter; Added a unit test; Updated genomicsdb version to 0.6.3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2733:6,validat,validateSampleNameMap,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2733,1,['validat'],['validateSampleNameMap']
Security,"Adding a new method `getVariantCacheLookAheadBases` to `VariantWalkerBase` which allows subclasses to set how far to look ahead when caching variants. This may help reduce memory use in GenotypeGVCFs. This also changes the side inputs to use FeatureDataSource.DEFAULT_QUERY_LOOKAHEAD_BASES which is `1000`, this is the value used by the other tools. I'm not sure if that's the right thing to do, but it makes variant walkers more consistent with other tools. Alternatively we could add a separate configuration method that lets tools change the side input value. We could also expose an optional parameter in the feature input that lets you set that on a per input basis if we need it. . This doesn't seem to have any negative effect on performance for genotypegvcfs, but it's hard to tell from short runs. It's also hard to tell if it's improving memory usage. It doesn't seem to make an appreciable difference at random places in the genome, but I'm hoping it will make a difference in very bad locations that have a lot of variation. Ideally our caches would be based on size rather than number of variants, but that's a more complicated change. fixes #3471",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3480:577,expose,expose,577,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3480,1,['expose'],['expose']
Security,"Adding validation to the SimpleInterval(String) constructor; Making GenomeLoc implement Locatable; Replacing all instances of SimpleInterval( locatable.getContig(), locatable.getStart(), locatable.getEnd()) with the new constructor. fixes #438 and #436",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/441:7,validat,validation,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/441,1,['validat'],['validation']
Security,"Adds a new ReadWalker tool, `RealignSoftClippedReads`, that realigns soft-clipped fragments using BWA. This tool is motivated by a specific artifact produced by Illumina DRAGEN v3.7.8 in which reads containing small indels are erroneously soft-clipped, often within mobile element contexts (LINE, SINE, ALU, SVA, etc). This is particularly problematic for mobile element insertion callers such as [Scramble](https://github.com/GeneDx/scramble) that rely on soft-clips for identifying potential insertion sites but do not perform a local assembly. In some cases, these soft-clipped reads are aligned to the incorrect region (confirmed by BLAT query and comparison to BWA alignments). An example of a false positive site produced by Scramble is shown below. <img width=""1008"" alt=""Screenshot 2023-11-16 at 2 09 45 PM"" src=""https://github.com/broadinstitute/gatk/assets/5686877/9d2c1dfd-9673-49f0-9372-c4c9cf6ffd9f"">. This PR includes the new tool and unit/integration tests and some minor refactoring to expose non-Spark BWA read mapping. This tool should be considered experimental until thorough benchmarking and analysis can be performed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8588:1002,expose,expose,1002,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8588,1,['expose'],['expose']
Security,Adds a tool to sanitize reads. This tool will convert any bases that do not match the reference into bases that do match the reference using the CIGAR as a key for which bases to change. The qualities for bases that match the reference are preserved.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6653:15,sanitiz,sanitize,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6653,1,['sanitiz'],['sanitize']
Security,"Adds support for online documentation generation through Barclay (https://github.com/broadinstitute/gatk/issues/2211) via the gatkDoc gradle task. The first commit contains only annotation updates (to target selected classes as documentation targets). A more complete audit/update pass will need to be done; but we need some for now in order to be able to exercise the doc generation process. The second commit contains that actual code and templates for documentation, and the final one upgrades to a Barclay snapshot that has the necessary dependent classes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2327:268,audit,audit,268,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2327,1,['audit'],['audit']
Security,"Adds updated tools for creating the host reference kmer set (PathSeqBuildKmers) and filtering reads that are low-quality, low-complexity, or come from the host (PathSeqFilterSpark). Sorry for the especially large size on this PR. **PathSeqBuildKmers tool**. Note this has been renamed from PathSeqKmerSpark. Input:; 1) Host reference FASTA; 2) False positive probability (0 create a hash set, >0 to create a Bloom filter); 3) Kmer length (1-31); 4) Kmer base indices to mask (optional). Output:; 1) Serialized kmer Hopscotch set (.hss) or Bloom filter (.bfi) file. For each reference record, the tool generates a list of long's containing the canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3115:383,hash,hash,383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115,1,['hash'],['hash']
Security,"Adressed all comments, @cmnbroad. Still the question on how to allow the user to provide custom validation. Back to you and thanks for reviewing!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-289361595:96,validat,validation,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-289361595,1,['validat'],['validation']
Security,"After #5887 goes in. PreprocessIntervals should still allow the use of IntervalMergingRule.OVERLAPPING_ONLY, and we should validate early that intervals are non-overlapping elsewhere.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5891:123,validat,validate,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5891,1,['validat'],['validate']
Security,"Agree that a separate README for the binary distribution is a good idea, but the document linked to above lacks basic instructions on things like running on a cluster and setting up GCS authentication. I think the doc should be based on the repo README instead with some sections omitted. It would actually be best if it could be generated automatically somehow from the repo README, so that we don't have to edit two documents whenever we make a change.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3199#issuecomment-311986024:186,authenticat,authentication,186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3199#issuecomment-311986024,1,['authenticat'],['authentication']
Security,Ah no we should just give them direct access to our dev repo. Will do now. Thanks though @ronlevine,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2308#issuecomment-291033418:38,access,access,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2308#issuecomment-291033418,1,['access'],['access']
Security,"Ah, that's a good reason not to use it. How about just plain old `validate`?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2543#issuecomment-290773651:66,validat,validate,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2543#issuecomment-290773651,1,['validat'],['validate']
Security,"All that needs to be done here is prove that GenomicsDBImport can run on an 11k sample callset over a single interval without exploding or running out of memory. We don't need to hyper-optimize memory usage, or optimize the instance types for cost, etc. We should also probably pair it with GenotypeGVCFs in a single WDL script, to make sure that tool doesn't blow up either. Once done, we should share the settings we used with red team. Details on how to access the 11k sample set are in a Google doc that has been shared privately.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2633:457,access,access,457,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2633,1,['access'],['access']
Security,"All walkers now have comprehensive sequence dictionary validation performed on their inputs (via the `GATKTool` base class, which is aware of all primary tool inputs and so is able to perform this check automatically -- see `GATKTool.validateSequenceDictionaries()`). At present, we need to do this validation manually in dataflow tools, but it would be nice if we could get it to happen automatically in a base class as it does on the walker side of things.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/669:55,validat,validation,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/669,3,['validat'],"['validateSequenceDictionaries', 'validation']"
Security,"Alright, I think I am in agreement with you @lbergelson about this behavior. Furthermore, all I needed out of this branch was an exposed mechanism for getting back the un-merged intervals so that I can track them myself in subtools. To that end I think I'm going to keep this branch and its tests and get rid of the merging rule argument in favor of leaving the logic for merging in place to be accessed by tools.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5887#issuecomment-567666730:129,expose,exposed,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5887#issuecomment-567666730,2,"['access', 'expose']","['accessed', 'exposed']"
Security,"Also just realized there’s *yet another* implementation in htsjdk, HardyWeinbergCalculation at https://github.com/samtools/htsjdk/blob/master/src/main/java/htsjdk/tribble/util/popgen/HardyWeinbergCalculation.java, so just a reminder to myself to check against that. Looks like a two-sided p-value of sorts is calculated there—I think this is P_{2\alpha} from Wigginton, although I need to double check. EDIT: Yup, it is, and furthermore the implementation appears to be correct. Phew! Added one more test to guard against a possible overflow issue that came up with that implementation, although it doesn't appear we have the same issue here. Will also note that 1) tests for the htsjdk implementation are pretty slim and don't actually cover very much, and 2) I don't see why we need to have two copies of this implementation, when all that essentially differs is the choice of p-value returned---we could certainly consolidate and expose the option of which p-value to return. Finally, I will also note that there is an implementation in bcftools. I have not checked it for correctness, but it appears to allow the calculation of both the one-sided p-value intended by ExcessHet, as well as what Wigginton calls P_{HWE}. So with that, the aforementioned implementations have covered every p-value discussed by that paper—and then one!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-892893837:933,expose,expose,933,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-892893837,1,['expose'],['expose']
Security,"Also note that I decided to append `_for_oncotator` to `additional_args`, since this is sufficiently vague without the suffix. However, analogous suffixes were not appended to exposed optional arguments for other tasks, since their names were less ambiguous. This is the sort of grossness we can do away with once Cromwell handles this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4324#issuecomment-362128625:176,expose,exposed,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4324#issuecomment-362128625,1,['expose'],['exposed']
Security,Also refactored the `VcfOutputRenderer` sanitization code. Fixes #5671,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5817:40,sanitiz,sanitization,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5817,1,['sanitiz'],['sanitization']
Security,Also when I validated the bam I got no errors. . I'll try rerunning this BQSR step a bunch of times to see if I can get the error again and see if it happens on the same shard.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317000181:12,validat,validated,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317000181,1,['validat'],['validated']
Security,"Also, MQ filtering results in stochastic coverage dropout. It is likely that low MQ regions significantly overlap across samples, in which case, downstream CNV can learn such biases and correct the coverage. Will test this in validations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4551#issuecomment-375722179:226,validat,validations,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4551#issuecomment-375722179,2,['validat'],['validations']
Security,"Also, it may be prudent for me to run the data through the commands the tests use, as the data I will make comes from an external source and may not validate in its current state, depending on the tool.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371500028:149,validat,validate,149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371500028,1,['validat'],['validate']
Security,"Also, the sooner Cromwell handles exposure of subworkflow task-level parameters, the better. I had to make these changes to bubble up and expose all optional parameters for all subworkflows, a process which adds an enormous amount of boilerplate and is (obviously) prone to errors. I plan to revert the changes when Cromwell is ready (#4287), so let me know!. I did not file an issue yet. @LeeTL1220 may have?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4319#issuecomment-362072883:138,expose,expose,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4319#issuecomment-362072883,1,['expose'],['expose']
Security,"Also, update code to use Hellbender's IOUtils instead of htsjdk's IOUtil; for these checks. We have both, presumably there's a reason Hellbender has their own and we should use them (for example, we can only add the hinting in our own). Sample error now:. A USER ERROR has occurred: Couldn't read file gs://foo/sam/m54113_160913_184949.scraps.beginning.sam. Error was: Error 403: jp-testing@redacted.iam.gserviceaccount.com does not have storage.objects.get access to foo/sam/m54113_160913_184949.scraps.beginning.sam. Potential cause: incorrect Google Cloud configuration; see instructions in the README. Fixes: #5468",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5477:458,access,access,458,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5477,1,['access'],['access']
Security,Also:; - Cleaned up headers in some test resources.; - Made sequence-dictionary checking more uniform across all CNV tools.; - Fixed an NPE bug in PlotModeledSegments input validation.; - Improved documentation regarding sex chromosomes in the ModelSegments pipeline.; - Miscellaneous boy-scout activities. Closes #3916.; Closes #3951.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4268:173,validat,validation,173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4268,1,['validat'],['validation']
Security,"Although there is a workaround, ideally we'd remove the assumption from our docker that it can access the root user's home dir.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-434049374:95,access,access,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-434049374,1,['access'],['access']
Security,Always have git hash in Docker tags to avoid collisions [VS-1086],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8549:16,hash,hash,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8549,1,['hash'],['hash']
Security,"An optimization introduced in https://github.com/broadinstitute/gatk/pull/5466 was removed in https://github.com/broadinstitute/gatk/pull/6885. The latter exposed Smith-Waterman parameters, allowing them to be changed from their default values and thus to possibly violate conditions assumed by the former. We could restore the optimization if we added explicit checks of these conditions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7441:155,expose,exposed,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7441,1,['expose'],['exposed']
Security,"And for cosmic:. /home/robby/Tools/NGS/gatk-4.2.6.1-src/scripts/funcotator/data_sources/cosmic/; getCosmicDataSources.sh; This script creates the cosmic data sources for the Funcotator GATK tool. For usage information run with the '-h' option. To retrieve the COSMIC data sources you must have a COSMIC account.; Please enter your COSMIC account credentials:; Enter your email address: ***@***.***; Enter your password: ; Creating folders: ...; mkdir: created directory 'cosmic'; mkdir: created directory 'cosmic/hg19'; mkdir: created directory 'cosmic/hg38'; mkdir: created directory 'cosmic_fusion'; mkdir: created directory 'cosmic_fusion/hg19'; mkdir: created directory 'cosmic_fusion/hg38'; mkdir: created directory 'cosmic_tissue'; mkdir: created directory 'cosmic_tissue/hg19'; mkdir: created directory 'cosmic_tissue/hg38'; Getting files ... ; get: cosmic/grch37/cosmic/v84/; CosmicCompleteTargetedScreensMutantExport.tsv.gz: ssh: Could not resolve ; hostname sftp-cancer.sanger.ac.uk: Name or service not known; get: cosmic/grch37/cosmic/v84/CosmicFusionExport.tsv.gz: ssh: Could not ; resolve hostname sftp-cancer.sanger.ac.uk: Name or service not known; get: cosmic/grch38/cosmic/v84/; CosmicCompleteTargetedScreensMutantExport.tsv.gz: ssh: Could not resolve ; hostname sftp-cancer.sanger.ac.uk: Name or service not known; get: cosmic/grch38/cosmic/v84/CosmicFusionExport.tsv.gz: ssh: Could not ; resolve hostname sftp-cancer.sanger.ac.uk: Name or service not known; ***@***.***:~/Tools/NGS> . -- ; ; r-engelmann.de - Ihre Seite für die Auswertung und Visualisierung von Daten ; aus den Bereichen Biomedizin, Finanzen, Sozioökonomie und weitere. On Dienstag, 30. August 2022 16:36:24 CEST Jonn Smith wrote:; > @robby81 Which scripts are you running and what are the errors you see? The ; data sources scripts; > are unsupported, but should work out of the box (they did last time I tried ; them).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7427#issuecomment-1274207002:410,password,password,410,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7427#issuecomment-1274207002,1,['password'],['password']
Security,"Annotations in VCF are a nightmare due to format requirements. I'd recommend against using VCF to store annotations unless it's absolutely necessary. The mechanism to do so is too unwieldy - either you add annotations by name per allele as real annotations (i.e. accounted for in the header and then added to the INFO field as applicable with per-allele annotations separated by commas), or you add them to the INFO field as a pipe-delimited single annotation field, with commas separating this long annotation for each allele (this is currently what Funcotator does). Both are kind of gross, with the former taking a LOT of extra space and the latter being basically unreadable by eye. You also need to make sure annotations are sanitized for illegal characters (such as commas). Funcotator has an open issue for this. A tabular format for annotations makes more sense, and, as much as it pains me to suggest it, MAF may be a quick answer here.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386121869:730,sanitiz,sanitized,730,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386121869,1,['sanitiz'],['sanitized']
Security,Another argument against this: the map function of a tool should clearly articulate its inputs in its signature. A map() that takes no parameters and relies on reflection/injection into members for its inputs would be supremely bad design.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/242#issuecomment-76739266:171,inject,injection,171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/242#issuecomment-76739266,1,['inject'],['injection']
Security,"Any chance we could break off legacy CNV tools into their own group? There are *many* more of them than there will be in the new pipelines---and many of them are experimental, deprecated, unsupported, or for validation only---that I think it makes sense to hide them and perhaps be less stringent about their documentation requirements. Anything we can do to reduce the support burden before release would be great.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346125341:208,validat,validation,208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346125341,1,['validat'],['validation']
Security,"Any objections to exposing SW parameters to the command line? This looks like something we will want to explore for malaria. I'm also not convinced that our current parameters have been justified and/or optimized in any documented way. A few questions:. 1) There are 3 sets of parameters used in various ways, a) haplotype-to-reference alignment, b) read-to-haplotype alignment, and c) dangling ends. Any chance we can evaluate the effect of consolidating at least c), if not all sets? @emeryj I was told that you might be the one to ask about c) in particular; @davidbenjamin speculated that these might effectively yield STR-specific parameters. In general, if there are any quick and readily available evaluations (which ideally include variant normalization), I'd appreciate pointers to them. 2) Any suggestions on what the resulting command line should look like? I don't want to add 12 parameters, in the worst case. I also think that using integer arrays might be clunky. Perhaps I can suggest the use of args files in the doc string---although I don't think that those are expanded in the `##GATKCommandLine`, right?. 3) Should I touch `SWOverhangStrategy` at all? See e.g. https://github.com/broadinstitute/gatk/issues/6576. It looks like we thread both this and the `SWParameters` through many methods and classes, so the code could stand quite a bit of refactoring, but for now I will stick to the minimal changes required to expose. @droazen @ldgauthier any thoughts?. In some simple experiments of changing the a) parameters (from the somewhat questionable `NEW_SW_PARAMETERS = new SWParameters(200, -150, -260, -11)` back to `STANDARD_NGS = new SWParameters(25, -50, -110, -6)`), I've seen that there are non-negligible differences in the calls (beyond representation) at the few percent level, as well as changes in annotations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863:1437,expose,expose,1437,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863,1,['expose'],['expose']
Security,Any test that tries to access a bucket seems to stall indefinitely. I think this has to do with gcloud not accepting our credentials file on travis. I suspect it's trying to open a web browser. I suspect it may need to be reconfigured with a service account key and an explicit authorization step before the build.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/444:23,access,access,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/444,2,"['access', 'authoriz']","['access', 'authorization']"
Security,"Apologies for re-opening, this is becoming an increasing issue for those looking to run GATK via Docker or singularity in a multi-tenant environment. Currently:; Docker creation and images provided run with a default user root within the container. Dropping privileges within the instance to a gatk user, would reduce the risk of inadvertent data access or harm when run in a multi-user environment. A possible solution:; Add something like the following within the Dockerfile:; RUN useradd -ms /bin/bash dev; WORKDIR /home/dev; USER dev. Providing:; Making changes like the above would bring the GATK docker container into line with best practice and greatly assist sites which are also looking to apply minimum standards enforcable through 3rd party applications, i.e. Aqua etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5959:347,access,access,347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5959,1,['access'],['access']
Security,"Apple is going to turn on Software Signing with OSX Catalina very soon (sometime this fall or so). While signing GATK will be fine, theoretically we have to sign all of the dynamic libraries that we leverage. OpenJDK did a release a while ago with these security features on and it was a major fiasco. We need to research what the signing requirements are and how they will affect the GATK release process.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6756:254,secur,security,254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6756,1,['secur'],['security']
Security,"Are the errors below part of this, when starting BwaSpark with spark-submit?; I activated ""--disable-sequence-dictionary-validation true"", but that doesn't help. It is very unclear, why a BAM is not recognized as a BAM file. I have tried all kinds of ways to make sure that it is a BAM and not a SAM file.; The documentation for BwaSpark also says ""BAM/SAM/CRAM file containing reads"", so if SAM files are really not possible, that should probably be changed.; ...; Even on verbosity DEBUG, the comments are not at all helpful to get at the problem.; E.g. ""Cannot retrieve file pointers within SAM text files.""; Is that a general statement about SAM files? Or does it only say, that in this specific SAM file (which is actually a BAM file), file pointers cannot be found?; What pointers are meant exactly?; How could this be fixed?. ```; ""SamReaderFactory	Unable to detect file format from input URL or stream, assuming SAM format.""; Which URL?; Which stream?; Why would this happen? What could be the error?; The SAM/BAM distinction seems very unclear. It would be more helpful, if some specific missing aspect (e.g. not queryname sorted) would be clearly declared as the culprit.; ...; 00:29 DEBUG: [kryo] Write: SAMFileHeader{VN=1.5, SO=queryname}; ...; WARNING	2018-01-16 02:11:25	SamReaderFactory	Unable to detect file format from input URL or stream, assuming SAM format.; ...; java.lang.UnsupportedOperationException: Cannot retrieve file pointers within SAM text files.; 	at htsjdk.samtools.SAMTextReader.getFilePointerSpanningReads(SAMTextReader.java:185); ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4131#issuecomment-357838062:121,validat,validation,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4131#issuecomment-357838062,2,['validat'],['validation']
Security,Are these functions exposed to jexl?. https://github.com/samtools/htsjdk/blob/335f2c1d70fe922c1bedfcb2d7d7751d5adb723c/src/main/java/htsjdk/variant/variantcontext/VariantContext.java#L737,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5916#issuecomment-489349581:20,expose,exposed,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5916#issuecomment-489349581,1,['expose'],['exposed']
Security,ArrayIndexOutOfBoundsException: -87 in ValidateVariants,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911:39,Validat,ValidateVariants,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911,1,['Validat'],['ValidateVariants']
Security,"As @droazen and I have been investigating, providing the `--gcs-project-for-requester-pays` argument when accessing buckets where the user does not have storage.bucket.get permission will cause failures. This clarifies the argument usage.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6594:106,access,accessing,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6594,1,['access'],['accessing']
Security,As I discovered when making the fix in PR #2021 bam files will fail validation if overhang clipping is used when running SplitNCigarRead because the mate reference start position might be changed. The tool can be refactored to perform a second walker pass over the reads in order to identify locations where this will be a problem by checking for sites where the primary read gets clipped by OverhangClippingManager.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2075:68,validat,validation,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2075,1,['validat'],['validation']
Security,"As a compromise fix, I have added a check to the validation code that asserts the dictionaries actually exist to save ourselves the potential null-pointer exceptions. @droazen . Fixes #6142",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6147:49,validat,validation,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6147,1,['validat'],['validation']
Security,"As a stopgap solution to allow `gs://` access on Spark with the local runner, let's add the `gcs-connector` as a project dependency, and craft a test case the runs a simple Spark tool like `PrintReadsSpark` using the local runner with GCS inputs and outputs. I've already started this in the branch https://github.com/broadinstitute/gatk/compare/dr_fix_gcs_spark_writing, but it's not working yet since the gcs-connector requires some extra authentication-related setup.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3125:39,access,access,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3125,2,"['access', 'authenticat']","['access', 'authentication-related']"
Security,"As an addendum to this task, we would also want to improve the test coverage to the underlying methods by hitting the newly exposed API.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5910#issuecomment-499224455:124,expose,exposed,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5910#issuecomment-499224455,1,['expose'],['exposed']
Security,"As discussed during GATK office hrs, we need to improve ValidateVariants message to say this is not a invalid vcf. ; **Solution**: separate validation argument that goes beyond vcf specs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6630#issuecomment-637245840:56,Validat,ValidateVariants,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6630#issuecomment-637245840,2,"['Validat', 'validat']","['ValidateVariants', 'validation']"
Security,"As discussed in https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358040921, we need to refactor `GATKTool` so that all non-Spark tools can comfortably extend it rather than extending `CommandLineProgram` directly, as some tools currently do. In particular, we need to:. * Provide a mechanism for subclasses to selectively disable engine-wide arguments such as `-I` completely (and also the ability to override with their own version of an argument). * Access necessary datasources outside of the engine package. * Add the ability to register input metadata such as sequence dictionaries, so that standard validation rules can be enforced across the toolkit. * Add the ability for each tool to change the defaults for engine arguments such as `--interval-merging-rule`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4341:467,Access,Access,467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4341,2,"['Access', 'validat']","['Access', 'validation']"
Security,As it turns out some of the `createGenomeLoc()` instances are being used by the GermlineCNVPipeline tests (evidently in ValidateVariants) and will fail if genome validation is enabled. This should be investigated.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7300#issuecomment-861713884:120,Validat,ValidateVariants,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7300#issuecomment-861713884,2,"['Validat', 'validat']","['ValidateVariants', 'validation']"
Security,"As part of #8083 we are drastically rewriting the entire Pileup-Caller infrastructure for DRAGEN-GATK. In doing so we have largely neglected its original functionality in Mutect2 and some of the changes (namely the re-factoring of that code to now happen after trimming like in with the GGA code) are going to impact the overall results for pileupcalling. It seems that we never added a real test of this functionality and its unclear to me currently what the meterics are that we want to assure ourselves that its working as intended. In #8083 I have checked that the code is hooked up manually, but its not clear to me what a proper test looks like for mutect without re-hashing the test samples that were being used in the bacterial project. I'm a little skeptical about adding a test that just asserts ""these results were different somehow"" and yet thats essentially the sort of test i would like and that would have saved me here. I would really like to have something better in place, especially if we are going to keep sharing the pileup-calling code between HC and M2 going forward.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8242:673,hash,hashing,673,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8242,1,['hash'],['hashing']
Security,"As part of my work in the Pipeline Dev team, I created 2 GATK images to address issue discussed [here](https://github.com/broadinstitute/gatk/issues/8684) (ie. having too many docker layers, we hit ACR limits very quickly). The images are in terrapublic, a premium-tier ACR and is publicly accessible. I made two images, one is squashed to just 1 layer, the other is reduced to just 12 layers (from the original 45). With these changes and the fact that terrapublic is on [premium](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-skus#registry-throughput-and-throttling) tier, the maximum docker pulls per minute becomes 833 (ie. 10k readOps / 12 layers) for the reduced-layers image and 10,000 for the squashed one. We have yet to test these in our pipelines but I anticipate the squashed version to be slower since it won’t be able to take advantage of any parallel pulls or caching, hence the two versions to allow pipeline devs to decide which one is better for their use-case.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8808:290,access,accessible,290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8808,1,['access'],['accessible']
Security,"As per the discussion in https://github.com/broadinstitute/gatk/issues/3246, we need to expose the `IntervalArgumentCollection.IntervalMergingRule` setting as a command-line argument in `IntervalArgumentCollection`. This was exposed in GATK3 as `--interval_merging`/`-im` (see GATK3's `IntervalArgumentCollection`)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3251:88,expose,expose,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3251,2,['expose'],"['expose', 'exposed']"
Security,"As reported by @jkobject testing our latest gatk-nightly image, certain non-requester-pays accesses fail with the latest google-cloud-nio version (0.123.23) when `--gcs-project-for-requester-pays` is specified. . The specific issue appears to be checks for the existence of non-existent files in non-requester-pays buckets when `--gcs-project-for-requester-pays` is set, resulting in a ""User project specified in the request is invalid"" error:. ```; code: 400; message: User project specified in the request is invalid.; reason: invalid; location: null; retryable: false; com.google.cloud.storage.StorageException: User project specified in the request is invalid.; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:233); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.list(HttpStorageRpc.java:376); 	at com.google.cloud.storage.StorageImpl.lambda$listBlobs$11(StorageImpl.java:391); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); 	at com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at com.google.cloud.storage.Retrying.run(Retrying.java:51); 	at com.google.cloud.storage.StorageImpl.listBlobs(StorageImpl.java:388); 	at com.google.cloud.storage.StorageImpl.list(StorageImpl.java:359); 	at com.google.cloud.storage.contrib.nio.CloudStoragePath.seemsLikeADirectoryAndUsePseudoDirectories(CloudStoragePath.java:118); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:743); 	at java.nio.file.Files.exists(Files.java:2385); 	at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:418); 	at htsjdk.tribble.TribbleIndexedFeatureReader.loadIndex(TribbleIndexedFeatureReader.java:162); 	at htsjdk.tribble.TribbleIndexedFeatureReader.hasIndex(TribbleIndexedFeatureReader.java:228); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSourc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7716:91,access,accesses,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7716,1,['access'],['accesses']
Security,"At the Helsinki workshop someone explained to me they couldn't use Dockers on the server because folks don't typically have root access. I cannot say I understand the details, but I can get you in touch with someone who does if this is something you want to follow up on.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-333936510:129,access,access,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-333936510,1,['access'],['access']
Security,"At the moment, ReadTools only documents and use `ReadFilters` but I am planning to probably add pack a couple of tools from the GATK/Picard tools at some point. In addition, I am working on another toolkit based on the GATK code, and it will include also annotations for variants (and probably some VCF tools). I just thought that it will be useful to been able to pull out the super-category map to re-use the GATK docgen code. If a downstream project with extra-categories wants to use the GATK templates and DocGen code might get into troubles without being able to access that. I am still working on how to document better my toolkits, but it is not a problem yet. Anyway, I don't really have any strong feeling about this; I just wanted to reduce a bit the complexity of the GATK code and do the same with my downstream projects. If it is something that you anticipate that it is going to change the contract often, feel free to close (the RNA Strings can be removed in other PR).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4247#issuecomment-360856661:569,access,access,569,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4247#issuecomment-360856661,1,['access'],['access']
Security,Audit CRAM code in htsjdk for Path/NIO support,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5209:0,Audit,Audit,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5209,1,['Audit'],['Audit']
Security,Audit Funcotator FeatureCache access patterns,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5143:0,Audit,Audit,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5143,2,"['Audit', 'access']","['Audit', 'access']"
Security,Audit GenomeLocParser overloads for validation arguments.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7300:0,Audit,Audit,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7300,2,"['Audit', 'validat']","['Audit', 'validation']"
Security,Audit MuTect2 headers to describe fragment vs read annotations,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7904:0,Audit,Audit,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7904,1,['Audit'],['Audit']
Security,Audit Mutect2/HC FeatureCache access patterns,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5148:0,Audit,Audit,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5148,2,"['Audit', 'access']","['Audit', 'access']"
Security,Audit R package dependencies,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3047:0,Audit,Audit,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3047,1,['Audit'],['Audit']
Security,Audit ReadClipper code (and clients) to make sure that it can handle empty reads,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4204:0,Audit,Audit,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4204,1,['Audit'],['Audit']
Security,Audit ReadsDataSource for bugs related to header merging and contig indices,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1674:0,Audit,Audit,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1674,1,['Audit'],['Audit']
Security,Audit tools with tests with suspicious FeatureCache misses,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5895:0,Audit,Audit,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895,1,['Audit'],['Audit']
Security,Audit use of Utils random generators and refactor if necessary.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6112:0,Audit,Audit,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6112,1,['Audit'],['Audit']
Security,Authenticate with dockerhub when running tests on travis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7102:0,Authenticat,Authenticate,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7102,1,['Authenticat'],['Authenticate']
Security,Authenticate with github from Travis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3179:0,Authenticat,Authenticate,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3179,1,['Authenticat'],['Authenticate']
Security,Authenticating to dockerhub in travis build,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7204:0,Authenticat,Authenticating,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7204,1,['Authenticat'],['Authenticating']
Security,Authentication to access private GCS buckets,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2394:0,Authenticat,Authentication,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2394,2,"['Authenticat', 'access']","['Authentication', 'access']"
Security,"Authorization settings for the connector are described here: https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/conf/gcs-core-default.xml#L50. @jamesemery have you been able to get the connector working?. @droazen what configuration improvements did you have in mind?. Also, I'm not sure what the difference between `google.cloud.auth.service.account.json.keyfile` and `fs.gs.auth.service.account.json.keyfile` is (if any).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5996#issuecomment-500755373:0,Authoriz,Authorization,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5996#issuecomment-500755373,1,['Authoriz'],['Authorization']
Security,Automatic sequence dictionary validation for spark tools,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/669:30,validat,validation,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/669,1,['validat'],['validation']
Security,"Back to @droazen. Also in the last round, I didn't actually add the code in PositionalDownSampler to reject `submit` after `signalEndOfInput` was called (I added the state to keep track of it, but left out the actual validate call in submit). Letting tests run then, back to you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5594#issuecomment-457759988:217,validat,validate,217,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5594#issuecomment-457759988,1,['validat'],['validate']
Security,"Back to @meganshand. I put in a simple mitochondrial integration test. Given that our MC3 validation already covers this particular bug I actually don't think it needs a new test for mitochondria. Also, for later, are any of your spike-in bams public (or rather, public + public)? I noticed that the NA12878 truth doesn't have very low AFs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5057#issuecomment-408649991:90,validat,validation,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5057#issuecomment-408649991,2,['validat'],['validation']
Security,Back to you @cmnbroad. Your commit and some minor changes are included. There are still two questions:. * Why not using a `LinkedHashMap` instead of a `HashMap`/`ArrayList` for the default filters?; * Maybe it is better to make the fields final to do not override them in future changes or being aware of the change of state.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-283311142:152,Hash,HashMap,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-283311142,1,['Hash'],['HashMap']
Security,"Back to you @takutosato. You caught a couple of whoppers. Fortunately, the validations still look good after fixing them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4832#issuecomment-394917958:75,validat,validations,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4832#issuecomment-394917958,1,['validat'],['validations']
Security,"Based on the TODO that was in ReadsDataSource.java, I exposed a SamReaderFactory parameter for ReadsDataSource rather than limit it to just validation stringency. Whats the right protocol for adding a test that uses a test file from another package (I'm reaching into the picard test data for a data file for an engine test). Alternatively, is there a better way to test this change ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/565:54,expose,exposed,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/565,2,"['expose', 'validat']","['exposed', 'validation']"
Security,Basic Validator,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3755:6,Validat,Validator,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3755,1,['Validat'],['Validator']
Security,Basic testing of mutect2-replicate-validation.wdl still incomplete,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2948:35,validat,validation,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2948,1,['validat'],['validation']
Security,"Before our cromwell/WDL tests even start to build the docker image, we could run womtool to validate the WDL. This will catch some obvious errors in much less time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4802:92,validat,validate,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4802,1,['validat'],['validate']
Security,"Better sample name validation is done on the java side of plotting in #2858, but otherwise I don't really see a way around hardcoding column names in the R code that isn't more trouble than it's worth.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2862#issuecomment-335623002:19,validat,validation,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2862#issuecomment-335623002,1,['validat'],['validation']
Security,"Bumping this since I ran into the same error as I was helping QC a colleagues data, running GATK 4.1.8.1 produces the following:. https://www.dropbox.com/s/2uleabl53dmg9y3/Screenshot%202020-07-28%2000.35.45.png. And this is on targeted capture data (Twist custom capture) ran through our core facility's sentieon pipeline, using the 'consensus' reads mapped to 1kg_grch37, using the raw reads works fine. Im not very familiar with sentieons pipelines but the steps to generate the UMI consensus reads are described at https://support.sentieon.com/appnotes/umi/. At first I though that discrepancy between @fleharty's ValidateSam and yours @ashwini06, could be that in the the newer version of Picard uses an updated version of htsjdk (v 2.23.0), but it's the same version of htsjdk that's included in GATK 4.1.8.1, so it seems unlikely. Walking through the commits between Picard 2.22.8 (the one bundled with GATK 4.1.8.1) and 2.23.2 doesn't (at least at first glance for me) show any commits changing code that could explain the differences in behaviour.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-664683562:617,Validat,ValidateSam,617,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-664683562,1,['Validat'],['ValidateSam']
Security,"Bumps commons-io:commons-io from 2.7 to 2.14.0. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=commons-io:commons-io&package-manager=gradle&previous-version=2.7&new-version=2.14.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); You ca",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9003:296,secur,security-vulnerabilities,296,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9003,2,['secur'],"['security-updates', 'security-vulnerabilities']"
Security,BwaSpark does inappropriate validation of bam sequence dictionary against reference,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2121:28,validat,validation,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2121,1,['validat'],['validation']
Security,"BwaSpark is using WELLFORMED as it's read filter. This includes a number of checks that the reads validate against the header. However, most unaligned sam files are likely to have no or incomplete headers. This causes many reads to be unexpectedly filtered. We should define a better filter criteria BwaSpark.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2120:98,validat,validate,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2120,1,['validat'],['validate']
Security,"By the way, I thought @vdauwera was opposed to using optional inputs in this way at some point (see #3657). Was that question ever decided? (I'm still of the opinion that they *should* be used in this way, but this is one of the reasons I didn't for this iteration of the WDL.). To be clear, the pair WDL right now does not allow all of the workflow paths (tumor-only, no PoN, etc.) that the new tools make possible. It only allows the one that we will most likely run in production (matched-normal + PoN). We should probably make the WDL a little more flexible to cover the most common use cases, but I'm fine if it doesn't completely expose all of the possible workflow paths---this would probably just make the WDL harder to maintain. Users can write their own WDLs in this case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362696132:636,expose,expose,636,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362696132,2,['expose'],['expose']
Security,C6C62656E6465722F746F6F6C732F77616C6B6572732F67656E6F74797065722F47656E6F747970696E67456E67696E652E6A617661) |; | 0% | [...nstitute/hellbender/engine/AssemblyRegionWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/2293/compare#diff-7372632F6D61696E2F6A6176612F6F72672F62726F6164696E737469747574652F68656C6C62656E6465722F656E67696E652F417373656D626C79526567696F6E57616C6B65722E6A617661) |; | 0% | [...titute/hellbender/engine/spark/LocusWalkerSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2293/compare#diff-7372632F6D61696E2F6A6176612F6F72672F62726F6164696E737469747574652F68656C6C62656E6465722F656E67696E652F737061726B2F4C6F63757357616C6B6572537061726B2E6A617661) |; | 0% | [...broadinstitute/hellbender/utils/GenomeLocParser.java](https://codecov.io/gh/broadinstitute/gatk/pull/2293/compare#diff-7372632F6D61696E2F6A6176612F6F72672F62726F6164696E737469747574652F68656C6C62656E6465722F7574696C732F47656E6F6D654C6F635061727365722E6A617661) |; | 0% | [...ellbender/tools/validation/CompareBaseQualities.java](https://codecov.io/gh/broadinstitute/gatk/pull/2293/compare#diff-7372632F6D61696E2F6A6176612F6F72672F62726F6164696E737469747574652F68656C6C62656E6465722F746F6F6C732F76616C69646174696F6E2F436F6D70617265426173655175616C69746965732E6A617661) |; | 0% | [...hellbender/tools/walkers/bqsr/AnalyzeCovariates.java](https://codecov.io/gh/broadinstitute/gatk/pull/2293/compare#diff-7372632F6D61696E2F6A6176612F6F72672F62726F6164696E737469747574652F68656C6C62656E6465722F746F6F6C732F77616C6B6572732F627173722F416E616C797A65436F76617269617465732E6A617661) |; | 0% | [...lections/RequiredVariantInputArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/2293/compare#diff-7372632F6D61696E2F6A6176612F6F72672F62726F6164696E737469747574652F68656C6C62656E6465722F636D646C696E652F617267756D656E74636F6C6C656374696F6E732F526571756972656456617269616E74496E707574417267756D656E74436F6C6C656374696F6E2E6A617661) |; > [Review all 203 files changed](https://codecov.io/gh/bro,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2293#issuecomment-265198632:2834,validat,validation,2834,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2293#issuecomment-265198632,1,['validat'],['validation']
Security,"CAL_DIRS in YARN).; 10:33:06.427 INFO ResourceUtils - ==============================================================; 10:33:06.427 INFO ResourceUtils - No custom resources configured for spark.driver.; 10:33:06.428 INFO ResourceUtils - ==============================================================; 10:33:06.428 INFO SparkContext - Submitted application: SortSamSpark; 10:33:06.446 INFO ResourceProfile - Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 600, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0); 10:33:06.454 INFO ResourceProfile - Limiting resource is cpu; 10:33:06.455 INFO ResourceProfileManager - Added ResourceProfile id: 0; 10:33:06.500 INFO SecurityManager - Changing view acls to: root; 10:33:06.501 INFO SecurityManager - Changing modify acls to: root; 10:33:06.501 INFO SecurityManager - Changing view acls groups to:; 10:33:06.502 INFO SecurityManager - Changing modify acls groups to:; 10:33:06.502 INFO SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); groups with view permissions: Set(); users with modify permissions: Set(root); groups with modify permissions: Set(); 10:33:06.755 INFO Utils - Successfully started service 'sparkDriver' on port 34861.; 10:33:06.784 INFO SparkEnv - Registering MapOutputTracker; 10:33:06.815 INFO SparkEnv - Registering BlockManagerMaster; 10:33:06.827 INFO BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 10:33:06.828 INFO BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up; 10:33:06.831 INFO SparkEnv - Registering BlockManagerMasterHeartbeat; 10:33:06.846 INFO DiskBlockManager - Created local directory at /raid/tmp/d6/c66ba827e22dbc38625af1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:41845,Secur,SecurityManager,41845,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['Secur'],['SecurityManager']
Security,CNNPipelineIntegration tests needs expected results validation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4537:52,validat,validation,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4537,1,['validat'],['validation']
Security,"CNNVariant Update models, validate scores, cleanup training",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5175:26,validat,validate,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5175,1,['validat'],['validate']
Security,CNV OncotateSegments does not expose bootDiskInGb runtime parameter,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3566:30,expose,expose,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3566,1,['expose'],['expose']
Security,COMPRESSION_LEVEL : 1; 14:19:10.290 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:19:10.290 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 14:19:10.290 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:19:10.290 INFO PrintReadsSpark - Deflater: IntelDeflater; 14:19:10.290 INFO PrintReadsSpark - Inflater: IntelInflater; 14:19:10.290 INFO PrintReadsSpark - GCS max retries/reopens: 20; 14:19:10.290 INFO PrintReadsSpark - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 14:19:10.290 INFO PrintReadsSpark - Initializing engine; 14:19:10.290 INFO PrintReadsSpark - Done initializing engine; 17/10/11 14:19:10 INFO spark.SparkContext: Running Spark version 1.6.0; 17/10/11 14:19:10 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/11 14:19:10 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/11 14:19:10 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); users with modify permissions: Set(hdfs); 17/10/11 14:19:10 INFO util.Utils: Successfully started service 'sparkDriver' on port 43567.; 17/10/11 14:19:11 INFO slf4j.Slf4jLogger: Slf4jLogger started; 17/10/11 14:19:11 INFO Remoting: Starting remoting; 17/10/11 14:19:11 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.131.101.159:45501]; 17/10/11 14:19:11 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.131.101.159:45501]; 17/10/11 14:19:11 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 45501.; 17/10/11 14:19:11 INFO spark.SparkEnv: Registering MapOutputTracker; 17/10/11 14:19:11 INFO spark.SparkEnv: Registering BlockManagerMaster; 17/10/11 14:19:11 INFO storage.DiskBlockManager: Created local directory at /tmp/hdfs/bloc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:3378,Secur,SecurityManager,3378,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['Secur'],['SecurityManager']
Security,CREDENTIALS`). Here's an example test case: https://github.com/broadinstitute/gatk/commit/8b217f82352ceb55d21d7a5236e879818910d9c9. and the stacktrace:. ```; java.util.ServiceConfigurationError: java.nio.file.spi.FileSystemProvider: Provider com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider could not be instantiated; at java.util.ServiceLoader.fail(ServiceLoader.java:232); at java.util.ServiceLoader.access$100(ServiceLoader.java:185); at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384); at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); at java.util.ServiceLoader$1.next(ServiceLoader.java:480); at java.nio.file.spi.FileSystemProvider.loadInstalledProviders(FileSystemProvider.java:119); at java.nio.file.spi.FileSystemProvider.access$000(FileSystemProvider.java:77); at java.nio.file.spi.FileSystemProvider$1.run(FileSystemProvider.java:169); at java.nio.file.spi.FileSystemProvider$1.run(FileSystemProvider.java:166); at java.security.AccessController.doPrivileged(Native Method); at java.nio.file.spi.FileSystemProvider.installedProviders(FileSystemProvider.java:166); at java.nio.file.Paths.get(Paths.java:141); at org.broadinstitute.hellbender.engine.spark.datasources.NioProviderExceptionUnitTest.test(NioProviderExceptionUnitTest.java:12). Caused by:; java.lang.IllegalArgumentException: A project ID is required for this service but could not be determined from the builder or the environment. Please set a project ID using the builder.; at shaded.cloud-nio.com.google.common.base.Preconditions.checkArgument(Preconditions.java:122); at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:208); at com.google.cloud.HttpServiceOptions.<init>(HttpServiceOptions.java:153); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:69); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:27); at com.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:64); at com.google.c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2110:1164,secur,security,1164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2110,1,['secur'],['security']
Security,"CS max retries/reopens: 20; 04:59:43.046 INFO ApplyBQSR - Requester pays: disabled; 04:59:43.047 INFO ApplyBQSR - Initializing engine; WARNING: BAM index file /scratch/ddo/markedsam/C18-436P.sort.rmdup.bam.bai is older than BAM /scratch/ddo/markedsam/C18-436P.sort.rmdup.bam; 04:59:43.556 INFO ApplyBQSR - Done initializing engine; 04:59:43.592 WARN ApplyBQSR - This tool has only been well tested on ILLUMINA-based sequencing data. For other data use at your own risk.; 04:59:43.592 INFO ProgressMeter - Starting traversal; 04:59:43.592 INFO ProgressMeter - Current Locus Elapsed Minutes Reads Processed Reads/Minute; 04:59:45.014 INFO ApplyBQSR - Shutting down engine; [November 8, 2021 at 4:59:45 a.m. PST] org.broadinstitute.hellbender.tools.walkers.bqsr.ApplyBQSR done. Elapsed time: 0.05 minutes.; Runtime.totalMemory()=557842432; java.lang.IllegalStateException: **The covariates table is missing ReadGroup V300019285_L2_ in RecalTable0**; 	at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:750); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.ReadGroupCovariate.keyForReadGroup(ReadGroupCovariate.java:81); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.ReadGroupCovariate.recordValues(ReadGroupCovariate.java:53); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.StandardCovariateList.recordAllValuesInStorage(StandardCovariateList.java:133); 	at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.computeCovariates(RecalUtils.java:546); 	at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.computeCovariates(RecalUtils.java:527); 	at org.broadinstitute.hellbender.transformers.BQSRReadTransformer.apply(BQSRReadTransformer.java:145); 	at org.broadinstitute.hellbender.transformers.BQSRReadTransformer.apply(BQSRReadTransformer.java:27); 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipelin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7549:3519,validat,validate,3519,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7549,1,['validat'],['validate']
Security,"Calling `hashCode` directly on an instance of a Java enum produces a value that depends on the object's memory location and is therefore not stable across machines in a distributed environment. This causes issues when using these objects (or objects that contain them) as keys in Spark RDDs, which by default use `HashPartitioner` to parition by hash code. See this blog post for a summary:. http://dev.bizo.com/2014/02/beware-enums-in-spark.html. This PR modifies all places within the sv tools where we call `hashCode` on an enum, instead computing the hash of the ordinal of the enum value. For @SHuang-Broad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4621:9,hash,hashCode,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4621,5,"['Hash', 'hash']","['HashPartitioner', 'hash', 'hashCode']"
Security,Can you gain access to the original gvcfs for those samples? if not I don't think there is a standard way to do such a thing in GATK following best practices. Perhaps there is some general VCF merging tool that would do the trick sort-of but it won't be the same as if you had run that sample with the rest thru the single sample + joint-genotyping pipeline.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7292#issuecomment-855333445:13,access,access,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7292#issuecomment-855333445,1,['access'],['access']
Security,"Can you point out where in the log you see that? I'm looking at it but I don't see anything about memory in the log you provided. (Except the Runtime.totalMemory()=4523032576 which is just standard output spam from gatk when it shutsdown) Sequence dictionary validation usually happens first, it's strange that a failure in the middle of a run would be effected by it. I'm no very curious what weird thing is happening that's causing this...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548114772:259,validat,validation,259,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548114772,1,['validat'],['validation']
Security,"Can you run the GATK tool `ValidateSamFile` on `CEUTrio.HiSeq.WGS.b37.NA12878.20.21.tiny.md.bam`, and see if you get the same validation error as you do after roundtripping through the inflater/deflater, @gspowley?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-285104828:27,Validat,ValidateSamFile,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-285104828,2,"['Validat', 'validat']","['ValidateSamFile', 'validation']"
Security,Can you setup your temporary folder to a location where you have read and write access? Slurm might interfere with temporary files. ; [How to setup and use temporary folder for GATK local execution](https://gatk.broadinstitute.org/hc/en-us/articles/18965297287067-How-to-setup-and-use-temporary-folder-for-GATK-local-execution),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8873#issuecomment-2168101103:80,access,access,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8873#issuecomment-2168101103,1,['access'],['access']
Security,Cannot access pull request for Owner configuration,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3945:7,access,access,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3945,1,['access'],['access']
Security,Change CompareSAMs to obey validation stringency; re-enable two integ…,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/604:27,validat,validation,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/604,1,['validat'],['validation']
Security,Changed ValidateVariants Doc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2759:8,Validat,ValidateVariants,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2759,1,['Validat'],['ValidateVariants']
Security,Check UUID in read adapter equals() and hashCode() methods,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/653:40,hash,hashCode,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/653,1,['hash'],['hashCode']
Security,Clean up the fail fast validation a little.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7821:23,validat,validation,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7821,1,['validat'],['validation']
Security,"Closes #4893. Closes #5086. Closes #5684. Closes #4500. Makes #4933, #4958, and #5085 possible. @takutosato Failing tests are superficial. You can begin reviewing. . This is a big PR:. * Refactor of all M2 filtering. Each filter has its own class, and the filtering engine ties it all together.; * Learn allele fraction clustering and somatic SNV and indel priors.; * More probabilistic filters.; * All filters have a common probabilistic threshold.; * M2 determines threshold automatically.; * Rewrite of all M2 documentation.; * Several filters, including strand bias and normal artifact, learn their own parameters. @LeeTL1220 M2 validations look really, really good. @meganshand Once this goes in mitochondria best practices will need to be tweaked again. We can merge the dangling tails homoplasmic fix before merging this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5688:633,validat,validations,633,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5688,1,['validat'],['validations']
Security,"Closes #5085. This improves validations a bit immediately but more importantly will enable more intelligent filtering on the level of haplotypes, which I think is critical to some of the messy data we have seen. This will also benefit HaplotypeCaller some day. @LeeTL1220 Could you review as far as changes to Mutect2 are concerned?. @droazen Could you review the refactoring of `ReadLikelihoods` / extracting `MoleculeLikelihoods`?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5831:28,validat,validations,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5831,1,['validat'],['validations']
Security,"Closes #6235 . This PR allows users to more easily clobber individual theano flags. This can be used to change the location of the theano compilation directory (see the above issue and #4782 for context). These flags are set upon import of the theano module; see http://deeplearning.net/software/theano/library/config.html for details. This solution is a little hacky, hence the code duplication. Ideally, we'd be able to specify this directory (and potentially other flags) as a parameter to the tools. As discussed with @cmnbroad, probably the cleanest solution would be to modify the PythonScriptExecutor to allow environment variables to be specified, e.g. via ProcessSettings. This solution would also cover the initial import of the `gcnvkernel` package for validation purposes, rather than only the imports in the resource scripts modified in this PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6244:764,validat,validation,764,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6244,1,['validat'],['validation']
Security,Closes #6829; @mwalker174 Could you please review? . I only added `gcs_project_for_requester_pays` input to tasks that need access to BAMs. Do we also need it for the ones that require reference such as `PreprocessIntervals`?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6870:124,access,access,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6870,1,['access'],['access']
Security,Closes #7672. @ldgauthier This is the bug fix for Sarah Calvo. The allele was lost when trimming caused its haplotype to start with a deletion. The solution is to inject force-calling alleles after trimming. Are you able to review?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7679:163,inject,inject,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7679,1,['inject'],['inject']
Security,"Closes https://github.com/broadinstitute/dsp-spec-ops/issues/366 by putting into SQL what is in English in that ticket:; > All variants in the region, chr19:35,740,407-35,740,469, overlap transcripts with multiple genes and those genes are always IGFLR1 and AD000671.2. Do not consider rows that include a consequence of downstream_gene_variant or upstream_gene_variant in this validation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7360:378,validat,validation,378,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7360,1,['validat'],['validation']
Security,"Closing this ancient PR -- this is hard to test/would take a lot of work to get in, and it's a somewhat exotic use case. We do already have tests covering access to private files with default credentials, which seems sufficient.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2879#issuecomment-453564368:155,access,access,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2879#issuecomment-453564368,1,['access'],['access']
Security,"Coming from https://gatkforums.broadinstitute.org/gatk/discussion/9358/gatk-runtime-error-read-max-length-must-be-0-but-got-0-with-1000g-bam#latest. There seems to be a bug somewhere in the implementation of pair hmm, which multiple users have run into. The most recent user reported running Mutect2 on two different machines with the same inputs, and same versions of GATK. One run was successful, while the other failed with ; ``` ; java.lang.IllegalArgumentException: readMaxLength must be > 0 but got 0; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); 	at org.broadinstitute.hellbender.utils.pairhmm.PairHMM.initialize(PairHMM.java:152); 	at org.broadinstitute.hellbender.utils.pairhmm.N2MemoryPairHMM.initialize(N2MemoryPairHMM.java:28); 	at org.broadinstitute.hellbender.utils.pairhmm.LoglessPairHMM.initialize(LoglessPairHMM.java:7); 	at org.broadinstitute.hellbender.utils.pairhmm.PairHMM.initialize(PairHMM.java:177); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.initializePairHMM(PairHMMLikelihoodCalculationEngine.java:242); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngine.java:177); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:207); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:212); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:979); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadins",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5543:554,validat,validateArg,554,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543,1,['validat'],['validateArg']
Security,Command line validation stringency argument for GATKTool.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1439:13,validat,validation,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1439,1,['validat'],['validation']
Security,CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:233); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; 	at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.refreshAccessToken(ComputeEngineCredentials.java:137); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:160); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:146); 	at shaded.cloud_nio.com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:96); 	at com.google.cloud.http.HttpTransportOptions$1.initialize(HttpTransportOptions.java:157); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:93); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:300); 	at shaded.cloud_nio.com.google.api.client.googleapis.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591:3649,secur,security,3649,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591,2,"['access', 'secur']","['access', 'security']"
Security,CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:137); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:158); 	at org.broadinstitute.hellbender.Main.main(Main.java:239); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; 	at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.refreshAccessToken(ComputeEngineCredentials.java:152); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:175); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:161); 	at shaded.cloud_nio.com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:96); 	at com.google.cloud.http.HttpTransportOptions$1.initialize(HttpTransportOptions.java:157); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:93); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:300); 	at shaded.cloud_nio.com.google.api.client.googleapis.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994:9215,secur,security,9215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994,2,"['access', 'secur']","['access', 'security']"
Security,CommandLinePrograms should not be instantiated until arguments are parsed and injected,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/107:78,inject,injected,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/107,1,['inject'],['injected']
Security,CompareSAMs ignores validation stringency,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/419:20,validat,validation,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/419,1,['validat'],['validation']
Security,"CompareSAMs ignores validation stringency. Running this. ```; build/install/hellbender/bin/hellbender CompareSAMs src/test/resources/org/broadinstitute/hellbender/tools/BQSR/NA12878.chr17_69k_70k.dictFix.bam src/test/resources/org/broadinstitute/hellbender/tools/BQSR/NA12878.chr17_69k_70k.dictFix.bam --VALIDATION_STRINGENCY SILENT; ```. results in this. ```; htsjdk.samtools.SAMFormatException: SAM validation error: ERROR: Record 130, Read name 809R9ABXX101220:5:6:17918:145992, Mate Alignment start should be 0 because reference name = *.; at htsjdk.samtools.SAMUtils.processValidationErrors(SAMUtils.java:439); at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:643); at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:628); at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:598); at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:544); at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:518); at htsjdk.samtools.util.PeekIterator.peek(PeekIterator.java:67); at htsjdk.samtools.SecondaryOrSupplementarySkippingIterator.skipAnyNotprimary(SecondaryOrSupplementarySkippingIterator.java:36); at htsjdk.samtools.SecondaryOrSupplementarySkippingIterator.advance(SecondaryOrSupplementarySkippingIterator.java:31); at org.broadinstitute.hellbender.utils.read.SamComparison.compareCoordinateSortedAlignments(SamComparison.java:111); at org.broadinstitute.hellbender.utils.read.SamComparison.compareAlignments(SamComparison.java:68); at org.broadinstitute.hellbender.utils.read.SamComparison.<init>(SamComparison.java:44); at org.broadinstitute.hellbender.tools.picard.sam.CompareSAMs.doWork(CompareSAMs.java:34); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:94); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:144); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardComm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/419:20,validat,validation,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/419,2,['validat'],['validation']
Security,"Completing https://github.com/broadinstitute/hellbender/issues/673 will take care of the case of ""missing reference for CRAM"", but we also need to make sure we're handling the case of ""wrong reference"" elegantly (where elegantly means ""throw a `UserException` with a descriptive error message). We want a test case with a reference that is the wrong reference for a CRAM, but has a compatible sequence dictionary (so that it won't be caught by the sequence dictionary validation). Both the wrong and missing reference cases should have a simple integration test that runs, eg., `PrintReads` with `expectedExceptions = UserException.class`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374:468,validat,validation,468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/677#issuecomment-126031374,1,['validat'],['validation']
Security,"Comprises the last ~~6 commits~~ 7 commits. (Had to fix a test file dependency. From now on, I'd like to request more encapsulation of test resources. Certainly we should have a common pool of general, rarely changed resources, but sharing of specific resources across packages breaks encapsulation.). @lbergelson I removed a few R dependencies. We should update the base Docker image accordingly and make sure I didn't break anything.; @davidbenjamin I had to change one use of HashedListTargetCollection in CalculateContamination. Also note that FilterByOrientationBias is the sole survivor in the exome package, so you may want to move it somewhere else.; @LeeTL1220 @vruano This removes a lot of your code. Please speak up if there are any utility classes, etc. that you'd like to keep. (For example, I kept the HMM code.) I removed the Target codec and associated classes.; @sooheelee I will update the list of tools for doc updates accordingly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3935:479,Hash,HashedListTargetCollection,479,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3935,1,['Hash'],['HashedListTargetCollection']
Security,"Could it be possible to read the file from a `java.nio.Path` in the [gatk-bwamem-jni](https://github.com/broadinstitute/gatk-bwamem-jni), @SHuang-Broad? It looks that it's a constraint of the native code, but it will be nice to be able to have just one index image in HDFS accessible for all the nodes...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-311908959:273,access,accessible,273,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-311908959,1,['access'],['accessible']
Security,Could you also run `docker images --digests` and paste the sha256 hash value for the image you're running? For `broadinstitute/gatk:4.4.0.0` it should be `044112d3d70603732d4a654ecaee33919cf9d45332d47268f5f1697b6ed558ed`,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8402#issuecomment-1648498439:66,hash,hash,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8402#issuecomment-1648498439,1,['hash'],['hash']
Security,CountVariants in Spark. exposed Loading VariantContexts in parallel,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1496:24,expose,exposed,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1496,1,['expose'],['exposed']
Security,Create WDL to validate VAT and add first test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7352:14,validat,validate,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7352,1,['validat'],['validate']
Security,Create a checksum to validate individual data sources based on the size of the data source files and some other meta attributes. Calculate the checksum for each data source in each data source package (somatic/germline). Store these checksums in the GATK jar and validate the data sources at runtime. Default to an exception if the checksums do not match. Have a command-line option to reduce the exception to a warning.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4380#issuecomment-415085087:9,checksum,checksum,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4380#issuecomment-415085087,6,"['checksum', 'validat']","['checksum', 'checksums', 'validate']"
Security,Create separate ValidateVariants validation that go beyond vcf specs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6630:16,Validat,ValidateVariants,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6630,2,"['Validat', 'validat']","['ValidateVariants', 'validation']"
Security,CreateSequenceDictionary needs to expose a utility method to create a sequence dictionary programmatically.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7628:34,expose,expose,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7628,1,['expose'],['expose']
Security,Created NioFileCopier that copies files using nio paths (includes; optional progress indicator and integrity validation).; Updated an error message in funcotator to make it more descriptive. Fixes #4549,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5150:99,integrity,integrity,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5150,2,"['integrity', 'validat']","['integrity', 'validation']"
Security,Created VAT here: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/b5c03ce6-cc74-462a-b81d-8ea102be314e; Validated VAT here: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/e6511960-8340-4e74-8f06-6c1a69d848bd (confirming with Rori that the failures are not surprising given it's only 10 samples),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8665:145,Validat,Validated,145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8665,1,['Validat'],['Validated']
Security,Creating tools and simple command-line for validation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1240:43,validat,validation,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1240,2,['validat'],['validation']
Security,"Cromwell switched to Java 11 starting with version 60 so the Java 8 sub-builds are probably going to error like. ```; Caused by: java.lang.UnsupportedClassVersionError: wdl/draft3/parser/WdlParser$Ast has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:473); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7824#issuecomment-1116073144:497,secur,security,497,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7824#issuecomment-1116073144,3,"['Secur', 'secur']","['SecureClassLoader', 'security']"
Security,"CrosscheckFingerprints cannot access to ""Requester Pays"" buckets, can it be changed to support the ""--gcs-project-for-requester-pays"" option as in the GenomicsDBImport tool?. ```; code: 400; message: Bucket is requester pays bucket but no user project provided.; reason: required; location: null; retryable: false; com.google.cloud.storage.StorageException: Bucket is requester pays bucket but no user project provided.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:229); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:439); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:242); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:239); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); at shaded.cloud_nio.com.google.cloud.RetryHelper.run(RetryHelper.java:76); at shaded.cloud_nio.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:238); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:736); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:497); at htsjdk.samtools.util.IOUtil.assertPathsAreReadable(IOUtil.java:525); at picard.fingerprint.CrosscheckFingerprints.doWork(CrosscheckFingerprints.java:449); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; {; ""code"" : ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7489:30,access,access,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7489,1,['access'],['access']
Security,"Currently `GATKRead.copy()` is unable to guarantee a deep copy, since we only have a deep copy method for Google `Read`s (`GenericData.clone()`, which it inherits), not `SAMRecord`s. We should write a deep copy method for `SAMRecord`, hook it up to the `GATKRead.copy()` implementation in `SAMRecordToGATKReadAdapter`, and change the method contract to guarantee that a deep copy will be performed. This is not a huge priority, since `GATKRead` already guarantees that defensive copies will be made of all mutable reference types returned from accessor methods (which means that shallow copies should be safe to use freely), but would be nice for consistency and peace of mind.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/623:544,access,accessor,544,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/623,1,['access'],['accessor']
Security,"Currently `SamAssertionUtils.assertSamsEqual` fails with `""SAM file output differs from expected output""`. It uses `SamComparison`, which prints a lot of helpful information about how the files differ to stdout. This output is often hidden when running it in a test suite though. It's also a bit strange to print error messages to stdout. `SamComparison` should capture this information and provide an accessor to retrieve it instead of dumping it to stdout.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/375:402,access,accessor,402,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/375,1,['access'],['accessor']
Security,"Currently `ValidateSamFile` has a limited list of platforms which does not include `BGI`. We should add `BGI` to the list of valid platforms, and we should review our list of supported platforms to make sure there are no others we are missing.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5517:11,Validat,ValidateSamFile,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5517,1,['Validat'],['ValidateSamFile']
Security,Currently build_docker will run for a long time and then fail at the end if you are not authenticated to gcloud. We should have an upfront test for it.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5353:88,authenticat,authenticated,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5353,1,['authenticat'],['authenticated']
Security,"Currently in master, intervals from the IAC that have been modified by the engine are further padded, overlapping intervals are merged, and bins are created. WES should be run by specifying bin_length = 0, and the total number of intervals (targets, in this case) should remain fixed. However, current behavior does not prevent intervals that overlap after padding from being merged. Note also that in sl_gcnv_ploidy_cli, bins that contain only Ns are also dropped. We should add validation of the IAC to check for minimal modification by the engine (as is done by other CNV CLIs), and then pad without introducing overlaps or merging if bin_length = 0. If bin length != 0, then the current behavior is fine (since it is conceivable that one might want to bin when running WES).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3981:480,validat,validation,480,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3981,1,['validat'],['validation']
Security,Currently it seems like the pull request build on travis is running the wrong commit's tests in docker. The docker build script takes a commit hash as part of it's inputs and then performs a checkout of that when building the docker. This is failing for the PR builds because the .travis.yml is currently getting the hash from calling rev-parse on the current branch which gives the commit number of master. It should be using $TRAVIS_COMMIT as the hash.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3216:143,hash,hash,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3216,3,['hash'],['hash']
Security,"Currently one can indicate if a program argument is optional by setting the Attribute annotation property `optional` to `true`. . Then, it is up to the programmer to indicate a default value for the parameter explicitly initializing the corresponding field with such a default value. It seems to me that we would gain versatility/expressibility if we use java.util.Optional as the type of the field:; - The code can then know wether the user actually gave a value to the parameter using `isPresent()`; currently there is no way to do so as the value provided by the user happens to be the default value.; - The default value may be defined dynamically based on other argument values or input data using `orElse(dynamicDefault)` or `orElseGet(dynamicDefaultLamda)`.; - The current solution prompts to use arbitrary marker constant value to deactivate the functionality; behind the user argument. eg.`maximumDepth = -1`. This constant may in fact be outside the valid ; range for the argument which makes validating it a bit more difficult.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1445:1003,validat,validating,1003,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1445,1,['validat'],['validating']
Security,"Currently our default reads validation stringency as defined in `GATKTool` is SILENT -- should it be STRICT, or is STRICT too impractical/annoying for read-world data?. Prompted by a discussion in https://github.com/broadinstitute/gatk/pull/1439",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1457:28,validat,validation,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1457,1,['validat'],['validation']
Security,Currently the only way to create a sequence dictionary from within a GATK tool is to call into the CreateSequenceDictionary tool as if it was being executed from the command-line. This is a hack. We need to expose a method that other tools can call into that will create a sequence dictionary for files.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7628:207,expose,expose,207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7628,1,['expose'],['expose']
Security,"Currently the script fails when configured to run tests, since it doesn't have access to the large files in the docker image. These need to be downloaded and mounted for the tests to pass, as we do in the docker tests on travis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3191:79,access,access,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3191,1,['access'],['access']
Security,"Currently we have a dependency on having gcloud and gsutil installed and configured in a certain way, but we don't have any documentation about it. . We're getting authentication partially from gcloud auth login, which is being propagated in a way I don't fully understand through the dataflow pipeline options. . We need to understand exactly what's happening and then write an explanation of what a user needs to do to have it work.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1051:164,authenticat,authentication,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1051,1,['authenticat'],['authentication']
Security,"Currently, `IntervalArgumentCollection` still uses `GenomeLocs` internally during parsing, despite the rest of the engine using `SimpleIntervals`. This forces us to provide a sequence dictionary when interval arguments are present even if the only input is an interval list (eg., an `IntervalWalker` that purely processes/transforms intervals). We should provide a mode in `IntervalArgumentCollection` in which intervals can be parsed into `SimpleIntervals` without a sequence dictionary. This will require us to fill in a special value such as `Integer.MAX_VALUE` for the stop position of intervals that don't contain a stop position (eg., ""chr1"" or ""chr1:1+""), since we won't know the true contig lengths, but our future query interfaces should all be robust to requests for locations outside of contig boundaries (and not blow up given such requests). This will also require adding some way of determining whether or not an interval has been validated against a sequence dictionary -- perhaps `ValidatedInterval` could be a subclass of `SimpleInterval`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/298:945,validat,validated,945,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/298,2,"['Validat', 'validat']","['ValidatedInterval', 'validated']"
Security,"Currently, if the GATK doesn't have permission to check whether a GCS bucket is Requester Pays (which is a separate permission from access to the bucket itself!), we get a cryptic error message along the lines of:. ```; User does not have storage.buckets.get access to bucket_name; ```. This is the same error the gsutil client gives in the same situation:. ```; $ gsutil requesterpays get gs://gatk-best-practices; AccessDeniedException: 403 droazen@broadinstitute.org does not have storage.buckets.get access to gatk-best-practices.; ```. Ideally we should detect this situation upfront in the GATK and emit a more informative error message.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6349:132,access,access,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6349,4,"['Access', 'access']","['AccessDeniedException', 'access']"
Security,"Currently, in order to create a GenomeLoc, you need to create a GenomeLocParser, which requires either a reference fasta or a sequence dictionary for validation. Sometimes, however, you don't want this level of validation (perhaps you have already checked that the interval is within bounds, making the extra validation wasteful -- this happens in a few places in the new engine). There should be a way to instantiate a GenomeLoc directly, without the need to pass around a GenomeLocParser.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/100:150,validat,validation,150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/100,3,['validat'],['validation']
Security,"Currently, we need to copy files on S3 to local storage before using; them. This patch enables gatk local and spark modes to access s3a://; files directly to reduce copy overhead and local disk usages. s3a file accesses require additional configuration of core-site.xml; located in CLASSPATH as well as other hadoop applications. Spark; already has hadoop dependencies but local modes need to add hadoop; jars in the classpath. Example core-site.xml:. ```; <configuration>; <property>; <name>fs.s3a.access.key</name>; <value>{Your AWS_ACCESS_KEY_ID}</value>; </property>; <property>; <name>fs.s3a.secret.key</name>; <value>{Your AWS_SECRET_ACCESS_KEY}</value>; </property>; </configuration>; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6698:125,access,access,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6698,3,['access'],"['access', 'accesses']"
Security,"Currently, we're instantiating our CommandLineProgram before we've parsed its arguments and injected them into the appropriate member variables. This means that the constructors for our tools (eg., the ReadWalker constructor) cannot use argument values during initialization, which is a big problem. We need to delay instantiation until after arguments are parsed and injected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/107:92,inject,injected,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/107,2,['inject'],['injected']
Security,"DATA_SOURCES/data_source_8/hg38/dnaRepairGenes.20180524T145835.csv; 16:01:43.979 INFO Funcotator - Initializing Funcotator Engine...; 16:01:43.983 INFO Funcotator - Creating a VCF file for output: file:/home/deepak/software_library/gatk-4.1.7.0/variants.funcotated.vcf; 16:01:44.020 INFO ProgressMeter - Starting traversal; 16:01:44.020 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 16:01:44.068 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr1:1-10454 due to alternate allele: <NON_REF>; 16:01:44.116 INFO VcfFuncotationFactory - dbSNP 9606_b150 cache hits/total: 0/0; 16:01:44.121 INFO Funcotator - Shutting down engine; [12 May, 2020 4:01:44 PM IST] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.14 minutes.; Runtime.totalMemory()=2889875456; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:-9 end:10464; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:733); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35); at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.createReferenceSnippet(FuncotatorUtils.java:1439); at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.getBasesInWindowAroundReferenceAllele(FuncotatorUtils.java:1468); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationForSymbolicAltAllele(GencodeFuncotationFactory.java:2560); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFlankFuncotation(GencodeFuncotationFactory.java:2465); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createGencodeFuncotationOnSingleTranscript(GencodeFuncotationFactory.java:953); at org.broadinstitute.hellbender.tools.func",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-664565036:6401,validat,validateArg,6401,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-664565036,1,['validat'],['validateArg']
Security,"DBImport - Callset Map JSON file will be written to /home/test/Software/gatk-4.4.0.0/test/./02/callset.json; 10:19:39.951 INFO GenomicsDBImport - Complete VCF Header will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vcfheader.vcf; 10:19:39.951 INFO GenomicsDBImport - Importing to workspace - /home/test/Software/gatk-4.4.0.0/test/./02; 10:19:40.060 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 10:19:40.075 INFO GenomicsDBImport - Shutting down engine; org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; java.lang.NumberFormatException: For input string: ""G""; 	at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67); 	at java.base/java.lang.Integer.parseInt(Integer.java:668); 	at java.base/java.lang.Integer.parseInt(Integer.java:786); 	at htsjdk.tribble.readers.TabixReader.getIntv(TabixReader.java:337); 	at htsjdk.tribble.readers.TabixReader.access$500(TabixReader.java:48); 	at htsjdk.tribble.readers.TabixReader$IteratorImpl.next(TabixReader.java:438); 	at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:46); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.readNextRecord(TabixFeatureReader.java:170); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.<init>(TabixFeatureReader.java:159); 	at htsjdk.tribble.TabixFeatureReader.query(TabixFeatureReader.java:133); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$1.query(GenomicsDBImport.java:971); 	at org.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:167); 	at org.genomicsdb.importer.GenomicsDBImporter.lambda$null$4(GenomicsDBImporter.java:732); 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Thr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8517:3552,access,access,3552,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517,1,['access'],['access']
Security,"DP=1399;ECNT=1;MBQ=36,36;MFRL=209,211;MMQ=60,60;MPOS=34;POPAF=7.3;TLOD=42.57	GT:AD:AF:DP:F1R2:F2R1:SAAF:SAPP	0/1:1327,27:0.019:1354:672,12:655,15:0.02,0.02,0.02:0.0007239,0.005058,0.994; 11	108175462	.	G	A	.	.	DP=972;ECNT=1;MBQ=36,36;MFRL=209,206;MMQ=60,60;MPOS=37;POPAF=7.3;TLOD=15.55	GT:AD:AF:DP:F1R2:F2R1:SAAF:SAPP	0/1:898,12:0.013:910:446,6:452,6:0.01,0.01,0.013:0.002966,0.0009292,0.996; 12	12037318	.	C	G	.	.	DP=975;ECNT=1;MBQ=36,36;MFRL=205,218;MMQ=60,60;MPOS=48;POPAF=7.3;TLOD=15.41	GT:AD:AF:DP:F1R2:F2R1:SAAF:SAPP	0/1:893,12:0.013:905:438,4:455,8:0.01,0.01,0.013:0.004146,0.0007942,0.995; 15	66679819	.	G	C	.	.	DP=870;ECNT=1;MBQ=36,36;MFRL=215,211;MMQ=60,60;MPOS=52;POPAF=7.3;TLOD=25.62	GT:AD:AF:DP:F1R2:F2R1:SAAF:SAPP	0/1:797,17:0.022:814:384,11:413,6:0.02,0.02,0.021:0.004622,0.001165,0.994; 18	42532923	.	T	C	.	.	DP=1402;ECNT=1;MBQ=36,36;MFRL=197,222;MMQ=60,60;MPOS=51;POPAF=7.3;TLOD=41.21	GT:AD:AF:DP:F1R2:F2R1:SAAF:SAPP	0/1:1312,25:0.019:1337:681,13:631,12:0.02,0.01,0.019:0.0009167,0.002842,0.996; 20	31019360	.	AT	A	.	.	DP=1530;ECNT=1;MBQ=36,36;MFRL=198,199;MMQ=60,60;MPOS=44;POPAF=7.3;RPA=7,6;RU=T;STR;TLOD=30.65	GT:AD:AF:DP:F1R2:F2R1:SAAF:SAPP	0/1:1424,34:0.022:1458:673,20:751,14:0.02,0.02,0.023:0.003063,0.0009654,0.996; ```. #### Steps to reproduce; ```; # Call without --alleles option does not produce the variants.; gatk Mutect2 --reference GRCh37.fa --read-validation-stringency LENIENT -I subset_properheader.bam -L enrichment.bed --interval-set-rule INTERSECTION -O unfiltered_noalleles.vcf.gz; # Call with --alleles option produces the variants with presumably high quality scores; gatk Mutect2 --reference GRCh37.fa --read-validation-stringency LENIENT -I subset_properheader.bam -L enrichment.bed --interval-set-rule INTERSECTION -O unfiltered_alleles.vcf.gz --alleles truth_small_variants_NA12878-NA24385-mix_sorted.vcf.gz; ```; The BAM, BED and output VCF files are available for download [here](https://gatk-validation.s3-eu-west-1.amazonaws.com/mutect2-4.1.9.0.zip).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7015:5837,validat,validation-stringency,5837,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7015,3,['validat'],"['validation', 'validation-stringency']"
Security,Data is sensitive and bug is recapitulated in https://github.com/broadinstitute/dsde-docs/issues/3026. CombineGVCFs gives the following error message:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:HLA-DRB1*15:03:01:02 start:11569 end:11005; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.onTraversalSuccess(CombineGVCFs.java:415); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:895); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:288); ```; Here are the dictionary lines for two consecutive HLA-DRB1 contigs:; ```; @SQ SN:HLA-DRB1*15:03:01:02 LN:11569 M5:4e0d459b9bd15bff8645de84334e3d25 AS:38 UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta SP:Homo sapiens; @SQ SN:HLA-DRB1*16:02:01 LN:11005 M5:4a972df76bd3ee2857b87bd5be5ea00a AS:38 UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta SP:Homo sapiens; ```; Notice the `LN` lengths match up. It appears that our tool is mistaking contig information.; Note that `HLA-DRB1*16:02:01` is the very last contig in GRCh38.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4572:308,validat,validateArg,308,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4572,2,['validat'],"['validateArg', 'validatePositions']"
Security,"Dear @davidbenjamin and @droazen,. please find [here](https://github.com/broadinstitute/gatk/files/12196051/2023-06-21.ISMB.Poster.on.XOP.Variant.Calling.Workflow.and.Mutect2.SES.v3.pdf) the poster that we presented this Monday at ISMB 2023 in Lyon. It sheds some additional light on this issue and explains why we believe that fine-tuning of the error model introduced in Mutect2 4.1.9.0 led to overcalling of variants in samples with increased (but not pathologic) DNA degradation. Presentation of the poster led to several interesting discussions with other users of Mutect2 at the conference. Given these results, I would propose that you could expose the parameter you newly set in commit [a304725](https://github.com/broadinstitute/gatk/commit/a304725a60f5000ec6381040137043a557fc3dc1) `private static final int ONE_THIRD_QUAL_CORRECTION = 5;` as a user-facing command line parameter. In this manner, you could leave the parameter at `5` by default but allow users who work on clinical (especially FFPE) samples to change it to `0` instead, thus effectively restoring the behaviour shown (and benchmarked) prior to 4.1.9.0. Would that be an acceptable compromise?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1655902557:649,expose,expose,649,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1655902557,1,['expose'],['expose']
Security,Dear GATK staff. We are now forbidden access to any webpages starting with the address https://gatkforums.broadinstitute.org/gatk/discussion/ by cloudflare. We can still access pages starting with https://gatk.broadinstitute.org/hc/en-us/ . We are connecting from various computers and netwrk in France using Firefox but it seems that it is the server itself that is blocked by Cloudflare. Have you already been notified of this problem and do you think you can solve it with cloudflare?. Thanks for your help. Best regards. Thierry Grange,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5869#issuecomment-577016491:38,access,access,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869#issuecomment-577016491,2,['access'],['access']
Security,"Dear all, thank you for the quick reply! Yes, I tried this on the latest release (4.0.8.0), and it is still an issue. vcf-validator validates the input VCFs, however, in the output VCF, I get errors associated with the field descriptions for the VAF annotation (which I believe may be specific to DeepVariant), a few of which are pasted below:. The header tag 'reference' not present. (Not required but highly recommended.). column CDC1551_clean_mutated_9.fasta_1_1 at NC_000962.3:580772; \ .. FORMAT tag [VAF] expected different number of values (expected 1, found 2); column CDC1551_clean_mutated_2.fasta_1_1 at NC_000962.3:580772 .. FORMAT tag [VAF] expected different number of values (expected 1, found 2). I tried the above after changing the VAF field description label from Number=R to Number=A in my unmerged gVCF files. This did not solve the problem. I'm attaching two example VCF files which I am attempting to merge. My commands are below: . ```. # Create GenomicsDBImport; gatk GenomicsDBImport \; -R ${REF_DIR}${ref} \; -V CDC1551_clean_mutated_1.fasta_1_1.bwa_deep.g.vcf.gz \; -V CDC1551_clean_mutated_2.fasta_1_1.bwa_deep.g.vcf.gz \; --reader-threads 8 \; --genomicsdb-workspace-path ${mapper}_${caller}_comb \; --intervals 'NC_000962.3' \; --java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true' \; --overwrite-existing-genomicsdb-workspace true . ## Joint Genotype VCF; gatk GenotypeGVCFs \; -R ${REF_DIR}${ref} \; -V gendb://${mapper}_${caller}_comb \; -O ${mapper}_${caller}_joint.vcf; ```. [test_vcfs.zip](https://github.com/broadinstitute/gatk/files/2295977/test_vcfs.zip). Thank you for your support on this problem!; Best,",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5113#issuecomment-413755772:122,validat,validator,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5113#issuecomment-413755772,2,['validat'],"['validates', 'validator']"
Security,"Default changed from 250 -> 20 in #5699 and exposed in #7450. Unfortunately, I don't recall if we did any benchmarking to spot check runtime/output changes, but I would expect 20 samples to be sufficient for estimating posterior means and standard deviations to the level needed for most downstream use cases.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5754#issuecomment-910220751:44,expose,exposed,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5754#issuecomment-910220751,1,['expose'],['exposed']
Security,"Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 17:43:53.293 INFO ValidateVariants - Done initializing engine; 17:43:53.294 INFO ProgressMeter - Starting traversal; 17:43:53.294 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 17:43:53.302 INFO ValidateVariants - Shutting down engine; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants don",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2509:3329,Validat,ValidateVariants,3329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509,2,"['Validat', 'validat']","['ValidateVariants', 'validationExampleGood']"
Security,"Did you clone the GATK repo ?; **Yes**; What JDK/version are you using ?; **openjdk version ""11.0.6""**. Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/usr/bin/gatk/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --info or --debug option to get more log output. Run with --scan to get full insights. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':gatkDoc'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$3.accept(ExecuteActionsTaskExecuter.java:166); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$3.accept(ExecuteActionsTaskExecuter.java:163); at org.gradle.internal.Try$Failure.ifSuccessfulOrElse(Try.java:191); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:156); at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:62); at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:108); at org.gradle.api.internal.tasks.execution.ResolveBeforeExecutionOutputsTaskExecuter.execute(ResolveBeforeExecutionOutputsTaskExecuter.java:67); at org.gradle.api.internal.tasks.execution.ResolveAfterPreviousExecutionStateTaskExecuter.execute(ResolveAfterPreviousExecutionStateTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute(CleanupStaleOutputsExecuter.java:94); at org.gradle.api.internal.tasks.execution.FinalizePropertiesTaskExecuter.execute(FinalizePropertiesTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.ResolveTaskExecutionModeExecuter.execute(ResolveTaskExecutionModeExecuter.java:95); at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:57); at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.exec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:951,Validat,ValidatingTaskExecuter,951,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Validat'],['ValidatingTaskExecuter']
Security,"Discussed with @kcibul (and others) offline. Conclusion: the majority of the validation code might end up being non-WDL with just one or two WDL tasks that call pthyon....etc., but it will be divided up into separate WDL tasks for now for development, and once it's done (or mostly) done, it can always be restructured to have the code in whatever place/structure makes the most sense going forward.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7352#issuecomment-883526280:77,validat,validation,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7352#issuecomment-883526280,1,['validat'],['validation']
Security,Disk space variables inconsistently exposed in gCNV WDLs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6995:36,expose,exposed,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6995,1,['expose'],['exposed']
Security,"Disq uses different code for finding container offsets compared to Hadoop-BAM, so that might be where the problem is coming from. However, I need access to the file that it's failing with to diagnose the issue. @jjfarrell how can I get a copy?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451880631:146,access,access,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451880631,1,['access'],['access']
Security,"Docker instance running as root, possible security issue for local data access. REVISITED",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5959:42,secur,security,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5959,2,"['access', 'secur']","['access', 'security']"
Security,Document how to access spark master page when running hellbender on GCE/dataproc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/975:16,access,access,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/975,1,['access'],['access']
Security,Document how to authenticate to Google Cloud Storage,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2495:16,authenticat,authenticate,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2495,1,['authenticat'],['authenticate']
Security,Does the bam pass `ValidateSamFile`?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6065#issuecomment-516480503:19,Validat,ValidateSamFile,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6065#issuecomment-516480503,1,['Validat'],['ValidateSamFile']
Security,"Does the problem go away if you use an output path with the 'hdfs://' scheme? E.g. _hdfs://namenode:8020/user/yaron/output.bam_ (where _namenode_ is the hostname of the namenode). There are two libraries being used internally for accessing the filesystem - the Hadoop filesystem API, and the NIO API - and they have slightly different behaviour if no scheme is provided. So to avoid problems it's best to give full paths with URI schemes for all input and output paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066#issuecomment-407791242:230,access,accessing,230,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066#issuecomment-407791242,1,['access'],['accessing']
Security,Don't check contig ordering during sequence dictionary validation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1178:55,validat,validation,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1178,1,['validat'],['validation']
Security,Don't check contig ordering in sequence dictionary validation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1176:51,validat,validation,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1176,1,['validat'],['validation']
Security,Don't forget to expose the file in the WDL.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5114#issuecomment-413203475:16,expose,expose,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5114#issuecomment-413203475,1,['expose'],['expose']
Security,"Done. Full results are on the internal presentation slides. The summary table follows:. | speedup vs async on a 1-core machine | slice | whole | intervals |; |-------------|----------|---------|----------|; | vcf | 0.91| 1.40| 0.57|; | bam (exome)| 40.42| 1.06| 1.02|; | bam (wgs)| 111.18| 1.21| 0.99|. This table compares the execution time of a single machine running PrintReads or SelectVariants, getting its input either directly from the Google bucket (NIO), or by first copying it with gsutil and then running off the local disk (with the async option turned on, allowing eager decompression of the stream - a feature the NIO code does not have). Each experiment is run four times, and each number here represents the ratio of two such experiments. Numbers larger than 1 indicate that NIO was faster. Each row is a different input file: vcf, small bam (exome), large bam (whole genome). Each column is a selection of what to read from the file (via the `-L` argument): a megabase slice, the whole file, or a long list of intervals. The NIO code relies heavily on prefetching, so it doesn't perform well with the many disjoint accesses of the right column. When processing only a small part of the (already small) vcf file, NIO loses out to copy + local processing. Everywhere else the direct-to-bucket ""NIO"" code performs quite well, up to 111x faster than the ""copy then process"" approach. I also ran the full set with async disabled. It makes a difference but NIO still wins and loses at the same places by similar margins (in particular the 111x win remains).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2424#issuecomment-284056956:1132,access,accesses,1132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2424#issuecomment-284056956,1,['access'],['accesses']
Security,Double-check validation test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/557:13,validat,validation,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/557,1,['validat'],['validation']
Security,"Dsamjdk.compression_level=2 -jar /Applications/genomicstools/gatk/gatk-4.0.11.0/gatk-package-4.0.11.0-local.jar LearnReadOrientationModel -alt-table 13_tumor-alt.tsv -ref-hist 13_tumor-ref.metrics -alt-hist 13_tumor-alt-depth1.metrics -O 13_tumor-artifact-prior-table.tsv; 12:16:19.960 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Applications/genomicstools/gatk/gatk-4.0.11.0/gatk-package-4.0.11.0-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; Nov 26, 2018 12:16:20 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 12:16:20.176 INFO LearnReadOrientationModel - ------------------------------------------------------------; 12:16:20.177 INFO LearnReadOrientationModel - The Genome Analysis Toolkit (GATK) v4.0.11.0; 12:16:20.177 INFO LearnReadOrientationModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:16:20.177 INFO LearnReadOrientationModel - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 12:16:20.177 INFO LearnReadOrientationModel - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_111-b14; 12:16:20.177 INFO LearnReadOrientationModel - Start Date/Time: November 26, 2018 12:16:19 PM EST; 12:16:20.177 INFO LearnReadOrientationModel - ------------------------------------------------------------; 12:16:20.177 INFO LearnReadOrientationModel - ------------------------------------------------------------; 12:16:20.178 INFO LearnReadOrientationModel - HTSJDK Version: 2.16.1; 12:16:20.178 INFO LearnReadOrientationModel - Picard Version: 2",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441721615:1500,authenticat,authentication,1500,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441721615,1,['authenticat'],['authentication']
Security,DuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:30:54.424 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 18:30:54.424 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 18:30:54.424 INFO BwaAndMarkDuplicatesPipelineSpark - GCS max retries/reopens: 20; 18:30:54.424 INFO BwaAndMarkDuplicatesPipelineSpark - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 18:30:54.424 INFO BwaAndMarkDuplicatesPipelineSpark - Initializing engine; 18:30:54.424 INFO BwaAndMarkDuplicatesPipelineSpark - Done initializing engine; 18/01/09 18:30:54 INFO spark.SparkContext: Running Spark version 2.2.0.cloudera1; 18/01/09 18:30:54 INFO spark.SparkContext: Submitted application: BwaAndMarkDuplicatesPipelineSpark; 18/01/09 18:30:54 INFO spark.SecurityManager: Changing view acls to: sun; 18/01/09 18:30:54 INFO spark.SecurityManager: Changing modify acls to: sun; 18/01/09 18:30:54 INFO spark.SecurityManager: Changing view acls groups to: ; 18/01/09 18:30:54 INFO spark.SecurityManager: Changing modify acls groups to: ; 18/01/09 18:30:54 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(sun); groups with view permissions: Set(); users with modify permissions: Set(sun); groups with modify permissions: Set(); 18/01/09 18:30:55 INFO util.Utils: Successfully started service 'sparkDriver' on port 38793.; 18/01/09 18:30:55 INFO spark.SparkEnv: Registering MapOutputTracker; 18/01/09 18:30:55 INFO spark.SparkEnv: Registering BlockManagerMaster; 18/01/09 18:30:55 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/01/09 18:30:55 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/01/09 18:30:55 INFO storage.DiskBlockManager: Created local directory at /tmp/sun/blockmgr-b03058d,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:5443,Secur,SecurityManager,5443,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['Secur'],['SecurityManager']
Security,During alpha we should work out the process of releasing builds to maven central. This may require coordinating with ops in order to handle signing keys and passwords.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1188:157,password,passwords,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1188,1,['password'],['passwords']
Security,"EDIT: Parameters are now exposed as individual arguments, so the following quoted text is outdated; see below for more details. > Adds the parameters `--dangling-end-smith-waterman-parameters-table <GATKPath>`, `--haplotype-to-reference-smith-waterman-parameters-table <GATKPath>`, and `--read-to-haplotype-smith-waterman-parameters-table <GATKPath>` to HaplotypeCaller and Mutect2. This allows for input via a TSV containing the column headers `MATCH_VALUE\tMISMATCH_PENALTY\tGAP_OPEN_PENALTY\tGAP_EXTEND_PENALTY` and one row of integers. Enables investigation of #2498 and #5564. Closes #6863 . Just opening this in case anyone wants to play around with it. I'll do some further testing on human and malaria data, but we have already found some cases in the latter for which changing some of the quizzical values to more reasonable ones yields immediate benefits. If anyone has any suggestions for possible evaluations, I'm all ears!. A few notes:. - I still need to add doc strings for the new arguments.; - Per https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705081291, we can wait until after the DRAGEN-GATK dust settles to review/reevaluate/merge.; - At that time, I'll add a few simple integration tests to check that I've properly bubbled up each set of parameters.; - The reviewer might find the diagram at https://github.com/broadinstitute/gatk/issues/6863#issuecomment-707919816 useful.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885:25,expose,exposed,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885,1,['expose'],['exposed']
Security,ERROR] [system.err] [bwt_restore_sa] SA-BWT inconsistency: seq_len is not the same. Abort!; ... 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':test'.; 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:98); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:68); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); 11:54:40.433 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54); 11:54:40.434 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAt,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:1180,Validat,ValidatingTaskExecuter,1180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['Validat'],['ValidatingTaskExecuter']
Security,"EXHAUSTIVE --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --SKIP_MATE_VALIDATION false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. Last read position: chr9:32,633,613; INFO 2019-03-07 16:12:22 SamFileValidator Validated Read 20,000,000 records. Elapsed time: 00:03:58s. Time for last 10,000,000: 117s. Last read position: chrM:11,340; No errors found; [Thu Mar 07 16:13:05 UTC 2019] picard.sam.ValidateSamFile done. Elapsed time: 4.79 minutes.; Runtime.totalMemory()=2602041344; Tool returned:; 0; ```. But when run BaseRecalibrator got the _fromIndex toIndex_ error:; `gatk BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz`; ```; ERROR: return code 3; STDERR:; 15:46:35.795 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:46:42.808 INFO BaseRecalibrator - ------------------------------------------------------------; 15:46:42.810 INFO BaseR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5807:2078,Validat,Validated,2078,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807,1,['Validat'],['Validated']
Security,"E_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. Last read position: chr9:32,633,613; INFO 2019-03-07 16:12:22 SamFileValidator Validated Read 20,000,000 records. Elapsed time: 00:03:58s. Time for last 10,000,000: 117s. Last read position: chrM:11,340; No errors found; [Thu Mar 07 16:13:05 UTC 2019] picard.sam.ValidateSamFile done. Elapsed time: 4.79 minutes.; Runtime.totalMemory()=2602041344; Tool returned:; 0; ```. But when run BaseRecalibrator got the _fromIndex toIndex_ error:; `gatk BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz`; ```; ERROR: return code 3; STDERR:; 15:46:35.795 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:46:42.808 INFO BaseRecalibrator - ------------------------------------------------------------; 15:46:42.810 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.0.0; 15:46:42.810 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:46:42.813 INFO BaseRecalibrator - Execut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5807:2262,Validat,ValidateSamFile,2262,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807,1,['Validat'],['ValidateSamFile']
Security,Early Subset VDS during validation for validation check,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8728:24,validat,validation,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8728,2,['validat'],['validation']
Security,"Empirical testing has shown that this tool performs best at scale with cloud buffering; disabled. With cloud buffering on and thousands of concurrent GenomicsDBImport tasks,; we do too many simultaneous GCS accesses (since the prefetcher spawns a new thread for each; reader upon a query) and start seeing intermittent failures, even with aggressive retries.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3110:207,access,accesses,207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3110,1,['access'],['accesses']
Security,Enable direct s3a:// file accesses,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6698:26,access,accesses,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6698,1,['access'],['accesses']
Security,Enable setting validation stringency in ReadsDataSource.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/565:15,validat,validation,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/565,1,['validat'],['validation']
Security,Error getting access token for service account,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2514:14,access,access,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514,1,['access'],['access']
Security,Error running gatk seq-format-validation workflow,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:30,validat,validation,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,1,['validat'],['validation']
Security,"Error: A JNI error has occurred, please check your installation and try again; Exception in thread ""main"" java.lang.SecurityException: Invalid signature file digest for Manifest main attributes; 	at sun.security.util.SignatureFileVerifier.processImpl(SignatureFileVerifier.java:314); 	at sun.security.util.SignatureFileVerifier.process(SignatureFileVerifier.java:268); 	at java.util.jar.JarVerifier.processEntry(JarVerifier.java:316); 	at java.util.jar.JarVerifier.update(JarVerifier.java:228); 	at java.util.jar.JarFile.initializeVerifier(JarFile.java:383); 	at java.util.jar.JarFile.getInputStream(JarFile.java:450); 	at sun.misc.JarIndex.getJarIndex(JarIndex.java:137); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:839); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:831); 	at java.security.AccessController.doPrivileged(Native Method); 	at sun.misc.URLClassPath$JarLoader.ensureOpen(URLClassPath.java:830); 	at sun.misc.URLClassPath$JarLoader.<init>(URLClassPath.java:803); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:530); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:520). This error seems to be resulting from signed jars and there are recommendations on the web such as exclude 'META-INF/*.RSA', 'META-INF/*.SF','META-INF/*.DSA'; I will greatly appreciate if someone could tell me where to add the aforementioned line in 'build.gradle' file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2676:116,Secur,SecurityException,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2676,5,"['Access', 'Secur', 'secur']","['AccessController', 'SecurityException', 'security']"
Security,Errors in VariantFiltration - possibly due to (or exposed by) async tribble reading,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1638:50,expose,exposed,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638,1,['expose'],['exposed']
Security,Escape table names properly in ValidateVat WDL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8116:31,Validat,ValidateVat,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8116,1,['Validat'],['ValidateVat']
Security,"Evaluation of THCA/STAD/LUAD TCGA WGS/WES CR concordance with SNP arrays was implemented on FC last summer and showed good performance. For WES, comparisons against GATK CNV and CODEX showed comparable to highly improved performance, respectively, with minimal parameter tuning. WGS comparisons were unavailable due to limitations of competing tools. This evaluation will be expanded to include CR/MAF concordance against PanCanAtlas ABSOLUTE results. Some curation of the samples could be performed; some batch effects were observed in some LC WGS LUAD samples. Comparisons to other tools will probably be removed for ease of maintenance. Will be adapted to fit into whatever framework arises from #4630; same goes for HCC1143 and CRSP validations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4122#issuecomment-459833697:737,validat,validations,737,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4122#issuecomment-459833697,1,['validat'],['validations']
Security,"Even if you know that only some tools are enabled for Spark, its not obvious how to find them. And the tool list has more than one Spark program group, which I didn't notice at first:. Spark Validation tools: Tools written in Spark to compare aspects of two different files; Spark pipelines: Pipelines that combine tools and use Apache Spark for scaling out (experimental); Spark tools: Tools that use Apache Spark for scaling out (experimental)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1291:191,Validat,Validation,191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1291,1,['Validat'],['Validation']
Security,Every travis build will upload it's test results to gs://hellbender/test/build_reports/<somepath>. The end of the build log shows the publicly accessible url for the test results.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/704:143,access,accessible,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/704,1,['access'],['accessible']
Security,"Evidently docker is changing their policy around anonymous users pulling from all docker repos (regardless of the tier for the owner of the repository being pulled from). This might or might not affect us since it looks like travis is pulling from our GCR repo for builds but we should be mindful of workflows that might rely on pulling hundreds of docker images from docker-hub through anonymous web VMs:; ```; On Monday, November 2, 2020 at 9am Pacific Standard Time, Docker will begin enforcing rate limits on container pulls for Anonymous and Free users. Anonymous (unauthenticated) users will be limited to 100 container image pulls every six hours, and Free (authenticated) users will be limited to 200 container image pulls every six hours, when enforcement is fully implemented. Docker Pro and Team subscribers can pull container images from Docker Hub without restriction, as long as the quantities are not excessive or abusive.; In addition, we are pausing enforcement of the changes to our image-retention policies until mid-2021, when we anticipate incorporating them into usage-based pricing. Two months ago, we announced an update to Docker image-retention policies. As originally stated, this change, which was set to take effect on November 1, 2020, would result in the deletion of images for free Docker account users after six months of inactivity. Today's announcement means Docker will not enforce image expiration on November 1, 2020.; ```; This is farther clarified on their FAQ https://www.docker.com/pricing/resource-consumption-updates:; ```; Rate limits for Docker image pulls are based on the account type of the user requesting the image - not the account type of the image’s owner. These are defined on the pricing page.; The highest entitlement a user has, based on their personal account and any orgs they belong to, will be used. Unauthenticated pull requests are “anonymous” and will be rate limited based on IP address rather than user ID. For more information on aut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6922:665,authenticat,authenticated,665,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6922,1,['authenticat'],['authenticated']
Security,Example Run (AoU) [here](https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20Echo%20RD/job_history/f4e85f7d-d8bf-45e1-a564-0dd2ae9f3761); New output file here: gs://fc-secure-4c3976f3-d84d-4243-876f-baa9f9a4256f/submissions/f4e85f7d-d8bf-45e1-a564-0dd2ae9f3761/GvsCalculatePrecisionAndSensitivity/85938f82-4731-437e-96a1-140a8ba7fcb7/call-CollateReports/stdout,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8633:182,secur,secure-,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8633,1,['secur'],['secure-']
Security,Expose BucketUtils.NIO_MAX_REOPENS as an engine-level argument,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3315:0,Expose,Expose,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3315,1,['Expose'],['Expose']
Security,"Expose Feature headers, and provide convenient VCFHeader access in VariantWalker",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/308:0,Expose,Expose,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/308,2,"['Expose', 'access']","['Expose', 'access']"
Security,Expose GenomicsDBArgumentCollection with CreateSomaticPanelOfNormals,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6746:0,Expose,Expose,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6746,1,['Expose'],['Expose']
Security,Expose HaplotypeCaller `READ_QUALITY_FILTER_THRESHOLD` on the command line,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7034:0,Expose,Expose,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7034,1,['Expose'],['Expose']
Security,Expose IntervalArgumentCollection.IntervalMergingRule as a command-line argument,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3251:0,Expose,Expose,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3251,1,['Expose'],['Expose']
Security,Expose `splitting-index-granularity` to all Spark tools outputting BAMs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6418:0,Expose,Expose,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6418,1,['Expose'],['Expose']
Security,Expose mapred.max.split.size as a parameter to GATKSparkTool,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1064:0,Expose,Expose,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1064,1,['Expose'],['Expose']
Security,Expose max-alt-alleles constant as user parameter before we don't emit PLs in GenotypeGVCFs and CombineGVCFs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2956:0,Expose,Expose,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2956,1,['Expose'],['Expose']
Security,Expose maximum-training-variants VQSR parameter [VS-634],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8029:0,Expose,Expose,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8029,1,['Expose'],['Expose']
Security,Expose more CNN training parameters to the command line,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8483:0,Expose,Expose,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8483,1,['Expose'],['Expose']
Security,Expose more optional parameters for CreatePanelOfNormals in CNV WDL.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3356:0,Expose,Expose,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3356,1,['Expose'],['Expose']
Security,Expose number of samples for emitting denoised copy ratios in gCNV.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5754:0,Expose,Expose,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5754,1,['Expose'],['Expose']
Security,Expose or decrease CHUNK_DIVISOR in HDF5SVDReadCountPanelOfNormals.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4365:0,Expose,Expose,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4365,1,['Expose'],['Expose']
Security,Expose or hide all task-level parameters in CNV WDLs consistently.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3980:0,Expose,Expose,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3980,1,['Expose'],['Expose']
Security,Expose point size in somatic CNV plotting tools.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5748:0,Expose,Expose,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5748,1,['Expose'],['Expose']
Security,Expose spark attributes so they can be set by users,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1107:0,Expose,Expose,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1107,1,['Expose'],['Expose']
Security,Expose the attributes we set automatically in `SparkContextFactory` so they can be overridden by the user if necessary.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1107:0,Expose,Expose,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1107,1,['Expose'],['Expose']
Security,Expose the maximum-training-variants parameter for both INDEL and SNP model creation in the `GvsCreateFilterSet` WDL. Closes https://broadworkbench.atlassian.net/browse/VS-634,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8029:0,Expose,Expose,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8029,1,['Expose'],['Expose']
Security,Expose timeout arg in StreamingPythonScriptExecutor,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4221:0,Expose,Expose,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4221,1,['Expose'],['Expose']
Security,Expose vid_mapping_file JSON in GenomicsDB,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3689:0,Expose,Expose,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3689,1,['Expose'],['Expose']
Security,Exposed Linked-Debrujin-Graph arguments to the help options,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6737:0,Expose,Exposed,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6737,1,['Expose'],['Exposed']
Security,"Exposed Smith-Waterman parameters in HaplotypeCaller, Mutect2, and FilterAlignmentArtifacts.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885:0,Expose,Exposed,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885,1,['Expose'],['Exposed']
Security,Exposed ability to blacklist intervals in CNV WDLs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5027:0,Expose,Exposed,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5027,1,['Expose'],['Exposed']
Security,Exposed bounds for copy-neutral segments in CallCopyRatioSegments.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4263:0,Expose,Exposed,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4263,1,['Expose'],['Exposed']
Security,Exposed maximum chunk size in CNV panel of normals.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4528:0,Expose,Exposed,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4528,1,['Expose'],['Exposed']
Security,Exposed maximum copy ratio and point size for CNV plotting tools.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6482:0,Expose,Exposed,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6482,1,['Expose'],['Exposed']
Security,Exposed more mem_gb parameters and fixed mem_gb_for_model_segments.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4364:0,Expose,Exposed,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4364,1,['Expose'],['Exposed']
Security,Exposed number of samples used for estimating denoised copy ratios in gCNV,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7450:0,Expose,Exposed,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7450,1,['Expose'],['Exposed']
Security,Exposed optional task-level parameters and standardized runtime parameters in CNV WDLs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4288:0,Expose,Exposed,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4288,1,['Expose'],['Exposed']
Security,Exposes an argument in the mitochondria WDL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6739:0,Expose,Exposes,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6739,1,['Expose'],['Exposes']
Security,Extract GenotypeGVCFs engine into publicly accessible class/function,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5910:43,access,accessible,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5910,1,['access'],['accessible']
Security,FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkTabComplete'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/vsc-hard-mounts/leuven-data/304/vsc30484/git/gatk/build/tmp/gatkTabComplete/javadoc.options'. * Try:; Run with --info or --debug option to get more log output. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':gatkTabComplete'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:69); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:35); at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:64); at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:52); at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:52); at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:53); at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43); at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:233); at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:215); at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:74); at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPla,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155:1505,Validat,ValidatingTaskExecuter,1505,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155,1,['Validat'],['ValidatingTaskExecuter']
Security,FILTER_READS_WITH_N_CIGAR_ARGUMENT_FULL_NAME; ValidationExclusion.TYPE.ALLOW_N_CIGAR_READS. See TODO in MalformedReadFilter::checkCigarIsSupported,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/186:46,Validat,ValidationExclusion,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/186,1,['Validat'],['ValidationExclusion']
Security,"FO DenoiseReadCounts - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:08:45.223 INFO DenoiseReadCounts - Initializing engine; 20:08:45.223 INFO DenoiseReadCounts - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 20:08:45.300 INFO DenoiseReadCounts - Reading read-counts file (BT1813.counts.hdf5)...; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fsuper.c line 277 in H5F_super_read(): file signature not found; major: File accessibilty; minor: Not an HDF5 file; 20:08:49.800 INFO DenoiseReadCounts - Shutting down engine; [May 18, 2021 8:08:49 PM EDT] org.broadinstitute.hellbender.tools.copynumber.DenoiseReadCounts done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=1789919232; org.broadinstitute.hdf5.HDF5LibException: exception when opening '/hpf/largeprojects/tabori/projects/bmmrd/CNA_project/gatk_cna/gatk/analysis/lgg/cnvponC2.pon.hdf5' with READ_ONLY mode: Not an HDF5 file; at org.broadinstitute.hdf5.HDF5File.open(HDF5File.java:490); at org.broadinstitute.hdf5.HDF5File.<init>(HDF5File.java:82); at org.broadinstitute.hdf5.HDF5File.<init>(HDF5File.java:66); at org.broadinstitute.hellbender.tools.copynumber.DenoiseReadCounts.doWork(DenoiseReadCounts.java:188); at org.broadinstitute.hellbender.cmdline.CommandLineProgram",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7258:4296,access,accessibilty,4296,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258,1,['access'],['accessibilty']
Security,"FO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:17:06.850 INFO IndexFeatureFile - Deflater: IntelDeflater; 00:17:06.855 INFO IndexFeatureFile - Inflater: IntelInflater; 00:17:06.856 INFO IndexFeatureFile - GCS max retries/reopens: 20; 00:17:06.858 INFO IndexFeatureFile - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 00:17:06.859 INFO IndexFeatureFile - Initializing engine; 00:17:06.860 INFO IndexFeatureFile - Done initializing engine; 00:17:07.292 INFO FeatureManager - Using codec VCFCodec to read file file://bad.vcf; 00:17:07.310 INFO IndexFeatureFile - Shutting down engine; [January 26, 2018 12:17:07 AM GMT] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=512229376; java.lang.IllegalStateException: the progress meter has not been started yet; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:697); at org.broadinstitute.hellbender.engine.ProgressMeter.stop(ProgressMeter.java:230); at org.broadinstitute.hellbender.utils.codecs.ProgressReportingDelegatingCodec.isDone(ProgressReportingDelegatingCodec.java:104); at htsjdk.tribble.index.IndexFactory$FeatureIterator.readNextFeature(IndexFactory.java:522); at htsjdk.tribble.index.IndexFactory$FeatureIterator.<init>(IndexFactory.java:440); at htsjdk.tribble.index.IndexFactory.createDynamicIndex(IndexFactory.java:326); at org.broadinstitute.hellbender.tools.IndexFeatureFile.createAppropriateIndexInMemory(IndexFeatureFile.java:122); at org.broadinstitute.hellbender.tools.IndexFeatureFile.doWork(IndexFeatureFile.java:71); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4269:2968,validat,validate,2968,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4269,1,['validat'],['validate']
Security,FO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:39:19 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(username); groups with view permissions: Set(); users with modify permissions: Set(username); groups with modify permissions: Set(); 18/04/24 17:39:20 INFO Utils: Successfully started service 'sparkDriver' on port 46576.; 18/04/24 17:39:20 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:39:20 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:39:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/24 17:39:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/24 17:39:20 INFO DiskBlockManager: Created local directory at /tmp/username/blockmgr-24b23f43-effa-45a6-8539-b9de234fa346; 18/04/24 17:39:20 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 18/04/24 17:39:20 INFO SparkEnv: Registering OutputComm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:7602,authenticat,authentication,7602,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,8,"['Secur', 'authenticat']","['SecurityManager', 'authentication']"
Security,FO PathSeqPipelineSpark - Inflater: IntelInflater** ; **09:27:44.735 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20** ; **09:27:44.735 INFO PathSeqPipelineSpark - Requester pays: disabled** ; **09:27:44.735 INFO PathSeqPipelineSpark - Initializing engine** ; **09:27:44.736 INFO PathSeqPipelineSpark - Done initializing engine** ; **Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties** ; **20/03/05 09:27:46 INFO SparkContext: Running Spark version 2.4.3** ; **09:27:47.141 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable** ; **20/03/05 09:27:47 INFO SparkContext: Submitted application: PathSeqPipelineSpark** ; **20/03/05 09:27:47 INFO SecurityManager: Changing view acls to: phenomata** ; **20/03/05 09:27:47 INFO SecurityManager: Changing modify acls to: phenomata** ; **20/03/05 09:27:47 INFO SecurityManager: Changing view acls groups to: ** ; **20/03/05 09:27:47 INFO SecurityManager: Changing modify acls groups to: ** ; **20/03/05 09:27:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(phenomata); groups with view permissions: Set(); users with modify permissions: Set(phenomata); groups with modify permissions: Set()** ; **20/03/05 09:27:48 INFO Utils: Successfully started service 'sparkDriver' on port 49119.** ; **20/03/05 09:27:49 INFO SparkEnv: Registering MapOutputTracker** ; **20/03/05 09:27:49 INFO SparkEnv: Registering BlockManagerMaster** ; **20/03/05 09:27:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information** ; **20/03/05 09:27:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up** ; **20/03/05 09:27:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-990bc09b-b5cf-4d67-8fec-87bfa4a6fa94** ; **20/03/05 09:27:49 INFO MemoryStore: MemoryStore started with capacity 106.5 GB** ; **20/03/05,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:4482,Secur,SecurityManager,4482,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,7,"['Secur', 'authenticat']","['SecurityManager', 'authentication']"
Security,FeatureManager provide access to a feature iterator without querying for a particular interval. This must work without the need for an index file present.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/647:23,access,access,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/647,1,['access'],['access']
Security,FeatureManager provide access to a feature iterator.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/647:23,access,access,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/647,1,['access'],['access']
Security,Figure out azure authentication in tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8612:17,authenticat,authentication,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8612,1,['authenticat'],['authentication']
Security,"FilterMutectCalls failed with ; ```; java.lang.IllegalArgumentException: beta must be greater than 0 but got -87566.7500301585; ```; ""this error only comes after the first pass of filtermutectCalls completed."". ValidateVarinats shows no errors when run on VCF.; ""The stats file was created by mutect2 for each shard and then joined with MergeMutectStats. Similar the read orientation model was built with the f1r2 files from all shards."". @davidbenjamin. --------------; Hi there,. I have a simulated dataset of related samples and currently running Mutect2 on it (10 tumor samples WGS with 130x); I managed to run everything through and now FilterMutectCalls crashes after the first pass through the variants with. ```; [October 1, 2019 12:16:16 PM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 370.68 minutes.; Runtime.totalMemory()=20597702656; java.lang.IllegalArgumentException: beta must be greater than 0 but got -87566.7500301585; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); at org.broadinstitute.hellbender.tools.walkers.readorientation.BetaDistributionShape.<init>(BetaDistributionShape.java:14); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.BinomialCluster.getFuzzyBinomial(BinomialCluster.java:42); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.BinomialCluster.learn(BinomialCluster.java:33); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.lambda$learnAndClearAccumulatedData$7(SomaticClusteringModel.java:131); at org.broadinstitute.hellbender.utils.IndexRange.forEach(IndexRange.java:116); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:131); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:156); at org.broadinstitute.hellbender.tools.wa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6202:211,Validat,ValidateVarinats,211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6202,1,['Validat'],['ValidateVarinats']
Security,"Finished refactoring the production code as detailed above, just need to add some tests. Results are exactly the same as before in single-sample mode---except for allele-fraction-only mode. This is because I refactored all existing segmenters (there were separate ones for copy-ratio-only, allele-fraction-only, and copy-ratio + allele-fraction) as special cases of a single multisample segmenter; however, the Gaussian kernel in the old allele-fraction-only segmenter was missing a normalization factor that is now present in the new multisample segmenter. Thus, users who previously ran in allele-fraction-only mode will have to retune parameters to achieve the same level of sensitivity. I expect that this will be a very small number of users (if any---note that allele-fraction-only mode isn't even exposed in the WDL), but we can probably mention it in the release notes. Might need to update a figure, etc. as well in the tutorial.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-611833344:804,expose,exposed,804,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-611833344,1,['expose'],['exposed']
Security,Finished up the import of sequence dictionary validation.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/629:46,validat,validation,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/629,1,['validat'],['validation']
Security,"Finished, [2D CNN inference](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/tools/walkers/vqsr/CNNScoreVariants.java) and training merged, many new issues spawned.:; - [X] A cool(?) name CNNScoreVariants; - [x] Model training script (in Python, eventually in Java); - [x] Pretrained model for WGS; - [x] Pretrained model for WEx (still being validated and was only trained on NA12878); - [x] Model inference and VCF annotation (in Java); - [x] Solution for applying filters based on CNN score cutoff (tranches.py script); - [x] Currently just re-filtering. Still no joint calling solution...; - [x] Hyperparameters optimized for small 2d model similar performance but .25 params.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4225#issuecomment-377623706:390,validat,validated,390,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4225#issuecomment-377623706,1,['validat'],['validated']
Security,First cut at a python notebook to validate inputs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7845:34,validat,validate,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7845,1,['validat'],['validate']
Security,Fix BQSR Dataflow test that was failing due to lack of sequence dictionary validation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/668:75,validat,validation,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/668,1,['validat'],['validation']
Security,Fix Input Validation python notebook,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7853:10,Validat,Validation,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7853,1,['Validat'],['Validation']
Security,Fix requester pays access by updating the google-cloud-nio library to the latest release,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7700:19,access,access,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7700,1,['access'],['access']
Security,Fix requester pays access harder. #7716,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7730:19,access,access,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7730,1,['access'],['access']
Security,Fix tests in ValidateVariantsIntegrationTest,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/659:13,Validat,ValidateVariantsIntegrationTest,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/659,1,['Validat'],['ValidateVariantsIntegrationTest']
Security,Fixed a bug where force calling alleles were lost upon trimming by placing allele injection after trimming,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7679:82,inject,injection,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7679,1,['inject'],['injection']
Security,"Fixes #7068 . - When adding AC, AF, AN, DP header lines, SelectVariants now checks if these lines are in the original header already and if so, overwrites these lines with the respective standard lines; - Without this check, an issue in htsjdk causes duplicate header lines with the same ID if the description differs. This should be fixed there but this fix provides a downstream workaround; - Modified the integration test validation files, which have been invalid VCF files with duplicate header lines; - Removed addition of AC, AF, AN if `--set-filtered-gt-to-nocall` is set, because these lines will be added later anyway",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7069:425,validat,validation,425,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7069,1,['validat'],['validation']
Security,"Fixes a bug that was exposed in https://gatk.broadinstitute.org/hc/en-us/community/posts/360075631171-BQSR-no-output-table-found. The issue is that bytes are signed in java, and yet we were treating them as unsigned and directly indexing an array with them.....this works as long as you don't have weird characters in your bam/reference...but if you do you get an access error that is non-informative.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7010:21,expose,exposed,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7010,2,"['access', 'expose']","['access', 'exposed']"
Security,"Fixes https://github.com/broadinstitute/gatk/issues/611. Uses ""validationStringency"" as the argument; PicardCommandLineProgram currently uses ""VALIDATION_STRINGENCY""; should we align all of these to use the same name?. I've done the same work for ReadSparkSource and GATKSparkTool but it requires a Hadoop-BAM upgrade so its in a separate PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1439:63,validat,validationStringency,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1439,1,['validat'],['validationStringency']
Security,"Fixes https://github.com/broadinstitute/gatk/issues/6173. I added a single line test that shows the problem, and regenerated the other test files. I didn't write a specific unit test that proves it's solved. Feel free to do so if you think it's necessary.; I validated the output by adding the field name to the output value and checking it by eye against the header lines. This could fairly simply made into a unit test if desired. I'm not sure if there are other large files that need to be regenerated. I had initially said this bug only affects vcf but I think it happens to Maf output as well. I removed a weakly typed method that allowed this bug to occur.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6178:259,validat,validated,259,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6178,1,['validat'],['validated']
Security,"Fixes issue identified by @kgururaj when investigating GenomicsDB; dropping calls (https://github.com/broadinstitute/gatk/issues/3429#issuecomment-324188028); which is due to incorrect VCF header length descriptions. It looks like this mismatch was reported early by @LeeTL1220; in #3296 when validating VCFs. @davidbenjamin provided a partial fix in #3351, generalizing the output; to include the option of specifying R instead of A using; `includeRefAllele`, fixing MFRL and removing MCL. This fixes the 3 remaining cases, MMQ (which breaks the GenomicsDB; example), MPOS and MBQ. Fixes #3296; Fixes #3429",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3490:293,validat,validating,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3490,1,['validat'],['validating']
Security,Fixing hash collision issue when adding filter value to the variant context,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3305:7,hash,hash,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3305,1,['hash'],['hash']
Security,"For GVS Feature Extract, ~Cohort Extract~ and Prepare Callset we should add a bq labels to indicate the query and tool being. gvs_tool_name (e.g. feature-extract); gvs_query_name (e.g. read-sample-table). Python Prepare Callset:. <img width=""334"" alt=""Screen Shot 2021-05-19 at 9 31 04 AM"" src=""https://user-images.githubusercontent.com/6863459/118821262-1193eb80-b885-11eb-8456-71225b40192b.png"">. Java GVS Feature Extract:; <img width=""346"" alt=""Screen Shot 2021-05-19 at 9 31 25 AM"" src=""https://user-images.githubusercontent.com/6863459/118821247-0e006480-b885-11eb-9d95-99441c6dbebd.png"">. for Feature Extract; update the wdl to take in a query_labels optional string; update the GATK tool to take in a query_labels param; update the GATK tool to validate labels; update the GATK tool to add constant kv labels: ""gvs_tool_name"", ""extract-features"" and ""gvs_query_name"", ""extract-features"" (is there a way to get more explicit in the queries? isn't it just one query?); test that this works with and without a label param passed in. for Prepare Callset; update the wdl to take in a query_labels optional string; update the python script to take in a query_labels param; update the python scrip to validate passed in labels; update the python script to add constant kv labels for ever single query individually and as a default; test that this works with and without a label param passed in. <img width=""717"" alt=""Screen Shot 2021-05-05 at 2 40 30 PM"" src=""https://user-images.githubusercontent.com/6863459/117192554-dd61fa80-adaf-11eb-8996-be1dc266dcc2.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7233:752,validat,validate,752,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7233,2,['validat'],['validate']
Security,"For `byte[]` attributes, the `GATKSAMRecordToGATKReadAdapter.getAttributesAsString()` uses the default `Object.toString()` (className@hashCode). This is something unexpected for my point of view, because the following code may fail:. ```java; public void testGATKReadGetAttributeAsString(final GATKRead read) {; read.setAttribute(""BC"", new byte[]{'A', 'C', 'T', 'G'});; // this will fail with the current implementation; Assert.assertEquals(read.getAttributesAsString(""BC"").getBytes(Charset.forName(""UTF-8"")),; read.getAttributeAsByteArray(""BC""));; }; ```. This PR fixes the issue by identifying `byte[]` attributes and converting them to Strings by using the default GATKRead charset (UTF-8).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3223:134,hash,hashCode,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3223,1,['hash'],['hashCode']
Security,"For a PrintReads command that requires accessing the index, e.g. when specifying `-L` intervals, running locally works but running on `gs://` CRAM fails even if index is explicitly specified. ## Local run (server); A command that accesses a local CRAM runs fine. ```; /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-launch PrintReads \; -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; -I /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram \; -O /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/test_decram_20171002.bam \; -L chr17; ...; Using GATK jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar PrintReads -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa -I /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram -O /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/test_decram_20171002.bam -L chr17; 14:52:10.763 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; [October 5, 2017 2:52:10 PM EDT] PrintReads --output /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/test_decram_20171002.bam --intervals chr17 --input /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram --reference /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --readValidat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3669:39,access,accessing,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3669,2,['access'],"['accesses', 'accessing']"
Security,"For example, contig-ploidy disk space is exposed only in case mode. Variable names are somewhat inconsistent, too (`disk_space_*` vs. `disk_space_gb_*`).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6995:41,expose,exposed,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6995,1,['expose'],['exposed']
Security,"For germline CNV:; It would be interesting to know which details (e.g. read group tags such as platform, model or also average insert size) are important for choosing additional Fastq/BAM files for creating a Panel of Normals (PON), when one does not have access to a batch. Which results from the GATK quality metrics would be most useful for choosing such datasets?. I'm thinking especially about e.g. choosing other Fastq/BAM files from the Personal Genomes Project. It would also be interesting to know if something like a ""Panel of Unnormals"" could be created, e.g. by choosing datasets from similar patients according to participant survey results from the Personal Genomes Project (https://my.pgp-hms.org/google_surveys). If a Panel of Normals can be used to reject spurious read counts, then a Panel of Unnormals could help to not reject rare read counts. (Hypermobile) Ehlers-Danlos-Syndrome as an unsolved and probably multi-gene case might be a perfect example, because it is already known that many genes exist with similar effects on connective tissue, i.e. hypermobility.; There are 115 hits grepping the above surveys for Ehlers-Danlos and at least a few of these might offer full fastq/bam files.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2881#issuecomment-358092692:256,access,access,256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2881#issuecomment-358092692,1,['access'],['access']
Security,"For ingesting into the aou security boundary, use the optional service account json argument.; This also needs to localize some files manually.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7164:27,secur,security,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7164,1,['secur'],['security']
Security,"For now, the tool will just supply the count and not make any decision as to whether the variant validated.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5059#issuecomment-408433167:97,validat,validated,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5059#issuecomment-408433167,1,['validat'],['validated']
Security,"For repeated operators (whether xIyI or xMyM), I think GATK3 has/had a function to simplify cigars to ""sanitize"" that situation. In terms of desired behavior, we don't want to filter out the reads, we want to transform them to be processable without difficulty. I think xIyD should be considered valid.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/428#issuecomment-95056320:103,sanitiz,sanitize,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/428#issuecomment-95056320,1,['sanitiz'],['sanitize']
Security,"For testing #3069, creating a Java test that shows that the GKL messages are controlled by the [log4j logLevel](https://www.tutorialspoint.com/log4j/log4j_logging_levels.htm).; Also, this branch should be rebased. A fix for the test failures, due to an issue with 2 factor authentication for [GIt Large File Storage](https://git-lfs.github.com/) of test data was recently merged.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3177#issuecomment-312404141:273,authenticat,authentication,273,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3177#issuecomment-312404141,1,['authenticat'],['authentication']
Security,"For what it's worth, this works but I should improve the error message. If you don't have the right permissions, currently you get an error that's something like this:. ```; htsjdk.samtools.util.RuntimeIOException: Error opening file: file:///home/jpmartin//gatk/gs:/bobs-bucket-you-cant-write-to/test-output/printtogcs.bam; ```. This is both; - confusing: it makes it look like the path is wrong even though the code actually tried the correct path; - improvable: it'd be nice if we gave the reason there was an error (403 Forbidden: user jpmartin doesn't have write access)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2422#issuecomment-342255531:568,access,access,568,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2422#issuecomment-342255531,1,['access'],['access']
Security,"From @tomwhite . ""Isilon exposes a Hadoop filesystem interface which makes it possible; to use it as a source or sink for Spark. (There are some notes here:; http://www.cloudera.com/documentation/enterprise/latest/topics/cm_mc_isilon_service.html). Note however that you lose the benefits of locality, so it won't be as; fast as HDFS. Definitely worth a try. Also, for a pipeline with; multiple steps, you could use HDFS to store intermediate data, only; reading from Isilon for the source files and writing to Isilon with; the final result.""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1508:25,expose,exposes,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1508,1,['expose'],['exposes']
Security,"From our normal-normal validation. Some irrelevant annotations removed. ```; 3	124464215	.	ATTT	A,ATTTT	.	PASS	N_ART_LOD=-1.573e+00,7.21;RPA=15,12,16;RU=T;STR;TLOD=5.65,4.81	GT:AD:AF 0/1/2:91,12,6:0.183,0.174 0/0:100,13,8:0.166,0.179; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4689:23,validat,validation,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4689,1,['validat'],['validation']
Security,Full scientific validation via end to end comparison of filtered results between WARP and BQ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7179:16,validat,validation,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7179,1,['validat'],['validation']
Security,Fully validate HaplotypeCaller walker for production (with help of palantir and/or short variants team),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1640:6,validat,validate,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1640,1,['validat'],['validate']
Security,Funcotator - Added map-style accessor for fields.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4176:29,access,accessor,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4176,1,['access'],['accessor']
Security,Funcotator - Need a validation tool for data sources,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4380:20,validat,validation,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4380,1,['validat'],['validation']
Security,Funcotator - Refactor Funcotation class to use a HashMap for each field,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3919:49,Hash,HashMap,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3919,1,['Hash'],['HashMap']
Security,Funcotator should validate datasource version at startup.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5660:18,validat,validate,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5660,1,['validat'],['validate']
Security,"G: [kryo] Read: WrappedArray([]); 02:25 DEBUG: [kryo] Read: WrappedArray([]); 02:25 DEBUG: [kryo] Write: scala.Tuple3[]; ...; 02:42 DEBUG: [kryo] Write object reference 1941: HLA-A*24:152; 02:42 DEBUG: [kryo] Write object reference 1945: chrUn_JTFH01001224v1_decoy; 02:42 DEBUG: [kryo] Write object reference 1949: HLA-B*14:01:01; 02:42 DEBUG: [kryo] Write object reference 1953: chr5_GL949742v1_alt; ...; 02:42 DEBUG: [kryo] Write object reference 1942: SAMSequenceRecord(name=HLA-A*24:152,length=3176,dict_index=2919,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1946: SAMSequenceRecord(name=chrUn_JTFH01001224v1_decoy,length=1051,dict_index=2066,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1950: SAMSequenceRecord(name=HLA-B*14:01:01,length=3312,dict_index=2999,assembly=null,alternate_names=[]); 02:42 DEBUG: [kryo] Write object reference 1954: SAMSequenceRecord(name=chr5_GL949742v1_alt,length=226852,dict_index=241,assembly=null,alternate_names=[]); ...; 02:42 DEBUG: [kryo] Write: Array[java.lang.Object]; 02:42 DEBUG: [kryo] Write: Object[]; 02:42 DEBUG: [kryo] Write: byte[]; 02:42 DEBUG: [kryo] Write: WrappedArray([]); ...; 03:20 DEBUG: [kryo] Read: CompressedMapStatus; 03:20 DEBUG: [kryo] Write: Array[java.lang.Object]; 03:20 DEBUG: [kryo] Write: Object[]; 03:20 DEBUG: [kryo] Write: byte[]; 03:20 DEBUG: [kryo] Read: Array[java.lang.Object]; 03:20 DEBUG: [kryo] Read: Object[]; 03:21 DEBUG: [kryo] Write: TaskCommitMessage; 03:21 DEBUG: [kryo] Read: TaskCommitMessage; ```. #### Steps to reproduce; **Command:**; ```; gatk --java-options ""-Djava.io.tmpdir=/scratch"" MarkDuplicatesSpark --input C19CUACXX.1.1-1.sorted.bam --output /scratch/C19CUACXX.1.1.sorted.Spark.Strigency-strict.bam --conf 'spark.local.dir=/scratch' --tmp-dir /scratch --read-validation-stringency STRICT; ```. #### Expected behavior; Finish MarkDuplicatesSpark successfully and output a valid bam file. #### Actual behavior; The bam file is empty.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8134:2892,validat,validation-stringency,2892,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8134,1,['validat'],['validation-stringency']
Security,"GATK consumes hdf5, but doesn't expose an API for it. https://github.com/broadinstitute/hdf5-java-bindings/issues/15 is a more appropriate place for that question.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5953#issuecomment-494358138:32,expose,expose,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5953#issuecomment-494358138,1,['expose'],['expose']
Security,GATK exposes Picard tools thru its command line interface. The usage syntax is different between these two tool kits and so picard code examples in javadocs don't make that much sense when displayed in GATK's own docs. For now code examples embeded in the Picard repo should stick to picard syntax as a matter of principle. So the solution is to transform those snippets on-the-fly when we generate the gatk-docs. . The is a good opportunity to do so in ```GATKHelpDocWorkUnitHandler``` by overloading the ```getDescription(DocWorkUnit)``` so that any occurrence of a picard.jar example is translated into gatk's version:; ```. <pre>java -jar picard.jar toolName \; ARG1=VAL1 \; ARG2=VAL2 ; </pre>; ```. to this:. ```; <pre>gatk toolName \; -ARG1 VAL1 \; --ARG2 VAL2; </pre>; ```. The difficult part is to find-out the mapping of picard argument names and gatk's exposed picard argument names and whether these need single or double dash. To this end we can use some reflection.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3932:5,expose,exposes,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3932,2,['expose'],"['exposed', 'exposes']"
Security,GATK4 ValidateVariants Doc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2759:6,Validat,ValidateVariants,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2759,1,['Validat'],['ValidateVariants']
Security,GATKSparkTool should use stricter validation for CRAMs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1179:34,validat,validation,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1179,1,['validat'],['validation']
Security,"GATKVariantContextUtils.createVCFWriter attempts to determine the output vcf type based on the file extension provided by the user, and defaults to vcf if the extension isn't a recognized type. There is code in VariantContextWriterBuilder (determineOutputTypeFromFile) in htsjdk that attempts to do the same mapping, but isn't public. We should expose that in htsjdk and use it in GATKVariantConextUtils so we can get rid of the redundant code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2128:345,expose,expose,345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2128,1,['expose'],['expose']
Security,"GRCh37, and this will be fine in the vast majority of cases.  There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references. ; ; 12:37:55.679 INFO  ProgressMeter - Starting traversal ; ; 12:37:55.679 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Features Processed  Features/Minute ; ; 12:37:56.198 WARN  FuncotatorUtils - Reference allele is different than the reference coding sequence (strand: -, alt = G, ref G != T reference coding seq) @\[chr1:13839497\]!  Substituting given allele for sequence code (TTC->GTC) ; ; 12:37:56.213 INFO  FuncotateSegments - Shutting down engine ; ; \[February 9, 2022 12:37:56 PM EST\] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.24 minutes. ; ; Runtime.totalMemory()=3139436544 ; ; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:29534 end:14501 ; ;     at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804) ; ;     at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59) ; ;     at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.findInclusiveExonIndex(SegmentExonUtils.java:95) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.determineSegmentExonPosition(SegmentExonUtils.java:63) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2938) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2914) ; ;     at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsOnSegment(GencodeF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7676:2393,validat,validateArg,2393,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676,1,['validat'],['validateArg']
Security,GVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwaW5nRW5naW5lLmphdmE=) | `49.123% <0%> (-0.442%)` | `45 <0> (+1)` | |; | [...ute/hellbender/utils/variant/GATKVCFConstants.java](https://codecov.io/gh/broadinstitute/gatk/pull/2655?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L0dBVEtWQ0ZDb25zdGFudHMuamF2YQ==) | `50% <0%> (-16.667%)` | `3 <2> (+2)` | |; | [...itute/hellbender/tools/walkers/vqsr/ApplyVQSR.java](https://codecov.io/gh/broadinstitute/gatk/pull/2655?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQXBwbHlWUVNSLmphdmE=) | `75% <100%> (ø)` | `55 <0> (ø)` | :arrow_down: |; | [...roadinstitute/hellbender/engine/ProgressMeter.java](https://codecov.io/gh/broadinstitute/gatk/pull/2655?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUHJvZ3Jlc3NNZXRlci5qYXZh) | `93.75% <0%> (-1.563%)` | `21% <0%> (-1%)` | |; | [...r/tools/walkers/variantutils/ValidateVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/2655?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9WYWxpZGF0ZVZhcmlhbnRzLmphdmE=) | `81.081% <0%> (+0.484%)` | `24% <0%> (+6%)` | :arrow_up: |; | [...org/broadinstitute/hellbender/utils/MathUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2655?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9NYXRoVXRpbHMuamF2YQ==) | `81.009% <0%> (+1.711%)` | `170% <0%> (+30%)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/2655?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `74.026% <0%> (+1.948%)` | `35% <0%> (ø)` | :arrow_down: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/2655?src=pr&el=tree#diff-c3JjL21,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2655#issuecomment-299088185:2403,Validat,ValidateVariants,2403,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2655#issuecomment-299088185,1,['Validat'],['ValidateVariants']
Security,Gatk validate variants doesn't report an error on non-spec-compliant headers,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6762:5,validat,validate,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6762,1,['validat'],['validate']
Security,Gave SortSam lenient validation in M2 wdl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4844:21,validat,validation,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4844,1,['validat'],['validation']
Security,Generate Avro Files run: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/577e77f2-4174-46c0-9d8a-9fdbbd2a27ed. Generate VDS run: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/27299041-7779-49d7-b7d3-6c0349f5ffc3. Generate VAT run: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/ad1b2687-7df0-4e92-b4e2-b61fcb77cd19. Validate VAT run: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/dae4874f-a619-40d4-84b4-066295b76cb2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8702:442,Validat,Validate,442,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8702,1,['Validat'],['Validate']
Security,"GenomicsDBImport currently doesn't support multi-sample input vcfs. We've had a number of usesr request the ability to import precombined multi-sample vcfs. . People may have old call-sets which they would like to combine with new data, and do not have access to the original single sample gvcfs needed to recombine them in GenomicsDBImport. . It would be good if we could import these directly into GenomicsDB.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6530:253,access,access,253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6530,1,['access'],['access']
Security,"Glad to hear you were able to make progress. We're open to suggestions around improving the tooling for this. For instance, you mentioned wanting to redo samples -- we already have support in GenomicsDB for querying by sample. We should be able to expose that at the GATK level. As long as you're okay with renaming the sample when you re-generate the gVCFs that should work. . Technically we could expose support to modify existing samples, but that get's a bit hairy because of the way data is retrieved. . I'm not sure why the queries for intact chromosomes take so much longer. Since you were able to replicate with a single sample, ~7m interval is there any chance you can share just that bit (workspace, or even better that portion of the gvcf) and we can take a deeper look?. To your question about whether GenomicsDBImport includes variants that span the specified import interval: it will definitely include variants that start in those intervals, but it won't always store variants that start before the import interval. For deletions, we have some special handling for variants that start before the interval - they should show up represented by the star allele, but I don't think this is the case for insertions starting before the import interval.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1221066848:248,expose,expose,248,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1221066848,2,['expose'],['expose']
Security,"Go the following:. ```; Current git hash does not match GATK git hash. Run anyway?yes; error: malformed object name 1; usage: git branch [<options>] [-r | -a] [--merged | --no-merged]; or: git branch [<options>] [-l] [-f] <branch-name> [<start-point>]; or: git branch [<options>] [-r] (-d | -D) <branch-name>...; or: git branch [<options>] (-m | -M) [<old-branch>] <new-branch>; or: git branch [<options>] [-r | -a] [--points-at]; ...; ```; Not sure the first error message about the ""hashes"" means perhaps that is the cause though. Assigned to @TedBrookings because I think this is you beast.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3593:36,hash,hash,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3593,3,['hash'],"['hash', 'hashes']"
Security,"Goal was to get WGS coverage collection at 100bp at ~15 cents per sample. Since this is I/O bound (takes ~2 hours to stream or localize a BAM, or about the same to decompress a CRAM), cost reduction can be most easily achieved by reducing the memory requirements and moving down to a cheaper VM. . Memory requirements at 100bp are dominated by manipulations of the list of ~30M intervals. There were a few easy fixes to reduce requirements that did not require changing the collection method (which can be easily modified for future investigations, see #4551):. -removed WellformedReadFilter. See #5233. EDIT: We decided after PR review to retain this filter by default and disable it at the WDL level when Best Practices is released. Leaving the issue open.; -initialized HashMultiSet capacity; -removed unnecessary call to OverlapDetector.getAll; -avoided a redundant defensive copy in SimpleCountCollection; -used per-contig OverlapDetectors, rather than a global one. This brought the cost down to ~9 cents per sample using n1-standard-2's with 7.5GB of memory when collecting on BAMs with NIO. Note that I didn't optimize disk size, which accounts for ~50% of the total cost and is unused when running with NIO, so we are closer to ~5 cents per sample. It is possible that using CRAMs with or without NIO and with or without SSDs might be cheaper. Note that OverlapDetectors may be overkill for our case, since bins are guaranteed to be sorted and non-overlapping and queries are also sorted. We could probably roll something that is O(1) in memory. However, since we are I/O bound, as long as we are satisfied with the current cost, I am willing to sacrifice memory for implementation and maintenance costs, as well as the option to change strategies easily. In any case, @lbergelson found some easy wins in OverlapDetector that may further bring the memory usage down, and will issue a fix in htsjdk soon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5715:773,Hash,HashMultiSet,773,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715,1,['Hash'],['HashMultiSet']
Security,"Good catch @EdwardDixon. From @cmnbroad 's Oct 12 post:; >We may need to provide a fallback environment for those (I'll try to get resolution on that). If it turns out we do, I'm actually not suggesting the fallback be automatic (3 in your list), just that we have a **graceful failure mode and an instructive error message.**. We have this now, right? Just want to be sure. If there's more work to be done, let's hash it out on another PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-438735712:414,hash,hash,414,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-438735712,1,['hash'],['hash']
Security,"Good catch. I think hc7b2577_8 might be specific for linux64. We can try removing that hash, but I don't have a mac, so I can't test this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4061#issuecomment-355645250:87,hash,hash,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4061#issuecomment-355645250,1,['hash'],['hash']
Security,"Google is deprecating and removing their implementation of the old style GA4GH read and reference API's. . > ; > Reads API functionality is now replaced by the htsget protocol ; > ; > This year, the GA4GH team introduced the htsget protocol to allow users to download read data for subsections of the genome in which they are interested. This is a richer and more flexible approach to working with reads data. It allows you to keep your genomics data in a common BAM file format on Google Cloud Storage and work with it efficiently from your computation pipelines, using standard bioinformatics tools. We have already launched our own open source implementation of this protocol, which you can use to access your reads data. Many popular tools such as samtools and htslib have been updated by the community to support htsget. Documentation is provided here. The Reads API is now deprecated, and will be decommissioned after one year, or after there has been no API activity for one month by those receiving this notice, whichever comes first. ; > ; > Variants API is now replaced by htsget and Variant Transforms ; > ; > The GA4GH team also plans to extend the htsget protocol to cover variant data, and we will extend our implementation of htsget to cover this use case. ; > ; > After analyzing usage of the Variants API, we found that users primarily used it to import variant data and then export it to BigQuery. To save time and effort, we created Variant Transforms, an open source tool for directly importing VCF data into BigQuery. Variant Transforms and its documentation are published here. Variant Transforms is more scalable than the legacy Variants API, and it has a robust roadmap with a dedicated team. We also welcome collaborators on this project as it advances. ; > ; > The Variants API is now deprecated, and will be decommissioned after one year, or after there has been no API activity for one month, whichever comes first. ; > ; > We are excited to move in step with the global ge",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4166:701,access,access,701,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4166,1,['access'],['access']
Security,Got the go-ahead and the access has been changed.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2726#issuecomment-302518914:25,access,access,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2726#issuecomment-302518914,1,['access'],['access']
Security,"Great! And yes, LL will be optimized separately for SNPs and INDELs. How about this for a first workflow to target?. 1) Run ExtractVariantAnnotations on a training set of chromosomes. You can keep training/truth labels as in Best Practices, for now.; 2) Run TrainVariantAnnotationsModel on that. We'll use the truth scores generated here for any sensitivity conversions---i.e., we'll be calibrating scores only to the truth sites that are contained in the training set of chromosomes.; 3) Use the trained model to run a single shard of ScoreVariantAnnotations on a validation set of chromosomes.; 4) Run some variation of the above script on the resulting outputs to determine SNP and INDEL score thresholds for optimizing the corresponding LL scores. We can also add some code to the script to use the truth scores from step 2 to convert these score thresholds into truth-sensitivity thresholds.; 5) Provide these truth-sensitivity thresholds to ScoreVariantAnnotations and use them to hard filter. Evaluate on a test set of chromosomes. If all looks good, we can later move steps 3-4 into the train tool and automate the passing of sensitivities in 5 via outputs in the model directory. This will let us keep the basic interface of ScoreVariantAnnotations the same, but we'll have to add a few basic parameters to TrainVariantAnnotationsModel to control the train/validation split. So I think all this branch is missing is step 5---we'll simply need to add command-line parameters for the SNP/INDEL sensitivity thresholds and then do the hard filtering in the VCF writing method highlighted above. Do you think you can handle implementing that in this branch, and then the rest at the WDL level? I can help with the python script for the LL stuff (or anything else), if needed. Not sure if you got a chance to check out what your collaborators are doing in the methods you're looking to compare against, but it would be good to understand if this basic scheme for train/validation/test splitting can",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1068345084:565,validat,validation,565,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1068345084,2,['validat'],['validation']
Security,"Great! Now that we know it works, it can wait to be reviewed. It's a small change but this feature had stayed untested for too long. In fact an even better way of doing this, if we have the energy & desire, would be to set up a separate bucket with a separate project. I *think* if we do it right we may then be able to have a fully automated test of the explicit credentials, as the default credentials would have access to the ""normal"" test data but we'd make sure the default user is not authorized to access the separate bucket (so we have to use the explicit credentials).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2879#issuecomment-306295687:415,access,access,415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2879#issuecomment-306295687,3,"['access', 'authoriz']","['access', 'authorized']"
Security,"Great, thanks!. On Mon, Dec 3, 2018 at 10:15 AM David Benjamin <notifications@github.com>; wrote:. > @meganshand <https://github.com/meganshand> I ran the ""Full Pipeline""; > workflows in a clone of your FC workspace:; > https://portal.firecloud.org/#workspaces/broad-firecloud-dsde/copy-of-megans-m2-mito-validations.; > I did not run any of the things that generate graphs because they were; > harder for me to understand. To compare the new results to your previous; > ones, I took all variants that were either PASS or had only the; > contamination filter applied, extracted just the locus and alleles columns,; > then manually inspected the diff. For the 5% and 50% spike-ins there were; > usually no differences at all, while for the 1% spike-in the difference was; > usually 2-5 variants that straddled the LOD threshold.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/5473#issuecomment-443745103>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdMwCcuQyzMweZjxWrXBODTCBaOSIks5u1T_-gaJpZM4Y9STI>; > .; >. -- ; Laura Doyle Gauthier, Ph.D.; Associate Director, Germline Methods; Data Sciences Platform; gauthier@broadinstitute.org; Broad Institute of MIT & Harvard; 320 Charles St.; Cambridge MA 0214",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5473#issuecomment-443751026:305,validat,validations,305,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5473#issuecomment-443751026,1,['validat'],['validations']
Security,"Grr, looks like this slipped by us in #4288. See my comments there---why does this validate in womtool? We got bit by a similar issue in #4281.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4319#issuecomment-362071370:83,validat,validate,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4319#issuecomment-362071370,1,['validat'],['validate']
Security,"HG00731 fails with a similar error:. ```; 18/04/11 16:08:40 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 98.0 in stage 42.0 (TID 49620, cwhelan-hg00731-cram-samtools-bam-feature-w-4.c.broad-dsde-methods.internal, executor 43): java.lang.IllegalArgumentException: Invalid interval. Contig:chr6 start:34662153 end:34662143; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:163); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575:380,validat,validateArg,380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575,2,['validat'],"['validateArg', 'validatePositions']"
Security,HI @droazen I see you were on this issue and generated a PR but could not merge because test case failures. I wanted to check if you were able to make progress on this. Within my organization infosec independently reviewed and have denied use of GATK :( . Let me know if you have an ETA for security fix update. Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1678986791:291,secur,security,291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1678986791,1,['secur'],['security']
Security,"HI @jonn-smith ; Creating this issue ticket as discussed. ; Issue: The header of that particular file has a hash mark # before it, because of that the parser wont work properly. A bug seems to have been introduced in the tribble indexing code for TSV files.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6223#issuecomment-545117559:108,hash,hash,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6223#issuecomment-545117559,1,['hash'],['hash']
Security,"HI, ; the following commands were included at my bash script .; after I ran them, I got a log file with ""Tool returned:; 6785087"", I am not sure why the return code is 6785087 , not 0 ???. Anything wrong with my commands ?; Thanks ! . gatk-4.1.2.0/gatk BaseRecalibrator -R $fasta -I $tumor_bam --known-sites a.vcf --known-sites b.vcf ; --intervals t.bed --interval-padding 50 --read-validation-stringency SILENT -O recal_data.table . gatk-4.1.2.0/gatk ApplyBQSR -R $fasta -I $tumor_bam --bqsr-recal-file recal_data.table ; --intervals t.bed --interval-padding 50 --read-validation-stringency SILENT -O recal_data.bam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6056:383,validat,validation-stringency,383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6056,2,['validat'],['validation-stringency']
Security,Hand HaplotypeCaller snapshot with first-pass validation issues addressed off to palantir for second-pass validation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3233:46,validat,validation,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3233,2,['validat'],['validation']
Security,HashedListTargetCollection has unnatural contig order,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1754:0,Hash,HashedListTargetCollection,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1754,1,['Hash'],['HashedListTargetCollection']
Security,"Hello @dagsbio and thank you for your question. It sounds like there was an issue with your bam file, possibly with the header version line. We support up through bam v1.6. You can also try running ValidateSamFile on the bam. I would recommend however that you direct your questions/troubleshooting requests to the new gatk support forum here: https://gatk.broadinstitute.org/hc/en-us/community/topics",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6350#issuecomment-571636329:198,Validat,ValidateSamFile,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6350#issuecomment-571636329,1,['Validat'],['ValidateSamFile']
Security,"Hello @jackycsie. I would request that you direct this sort of question to the GATK Support forums https://gatkforums.broadinstitute.org/gatk/categories/gatk-support-forum. Regarding the BAM size there are a number of reasons the file could have shrunk, the most likely answer is that the GATK applied a different level of compression to the output bam than was applied to your inputs. I would recommend you run CountReads and calling ValidateSamFile on the outputs for your run and if there appears to be an issue with the output you post a question about it on the Support Forums.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6236#issuecomment-547452764:435,Validat,ValidateSamFile,435,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6236#issuecomment-547452764,1,['Validat'],['ValidateSamFile']
Security,"Hello @liuyihhha, I'm sorry to hear that. We should probably include a more prominent warning that something like this could happen if you disable all of the readfilters for the tool some of them are there just to make sure everything is formatted correctly and doesn't crash. If you `--disable-tool-default-read-filters` it disables all of them including the basic ""can the GATK even parse this read"" style readfilters. I would recommend adding back in some of the more basic read filters and seeing if you still get this error. Specifically I would recommend trying with the following arguments:; ```; --disable-tool-default-read-filters true; -RF WellformedReadFilter; -RF GoodCigarReadFilter; ```; As for what about SplitNCigarReads could be causing the reads to be invalid I would like to have some idea of what the issue is. You could try running `ValidateSamFile` on your input to Mutect2 and hopefully that will spit back some readnames that are invalid which should make it easier to diagnose. Given the operations in SplitNCigarReads and the nature of your stacktrace it wouldn't surprise me if there is something wrong/messy about the cigars output by that tool in some cases.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8224#issuecomment-1450266657:854,Validat,ValidateSamFile,854,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8224#issuecomment-1450266657,1,['Validat'],['ValidateSamFile']
Security,"Hello, Could you tell me the exact source websites of funcotator_dataSources.v1.7.20200521g? I did not find it in your Google Cloud (genomics-public-data).; Besides, I used this code to download`./gatk-4.1.9.0/gatk FuncotatorDataSourceDownloader --germline --validate-integrity --extract-after-download; `, but the error appeared as following:`Nov 18, 2023 1:15:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:15:05.202 INFO FuncotatorDataSourceDownloader - ------------------------------------------------------------; 13:15:05.203 INFO FuncotatorDataSourceDownloader - The Genome Analysis Toolkit (GATK) v4.1.9.0; 13:15:05.203 INFO FuncotatorDataSourceDownloader - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:15:05.203 INFO FuncotatorDataSourceDownloader - Executing as yaoxq@mu01 on Linux v3.10.0-693.el7.x86_64 amd64; 13:15:05.203 INFO FuncotatorDataSourceDownloader - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 13:15:05.203 INFO FuncotatorDataSourceDownloader - Start Date/Time: November 18, 2023 1:15:04 PM CST; 13:15:05.203 INFO FuncotatorDataSourceDownloader - ------------------------------------------------------------; 13:15:05.203 INFO FuncotatorDataSourceDownloader - ------------------------------------------------------------; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Version: 2.23.0; 13:15:05.204 INFO FuncotatorDataSourceDownloader - Picard Version: 2.23.3; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:15:05.204 I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8275#issuecomment-1817434417:259,validat,validate-integrity,259,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8275#issuecomment-1817434417,1,['validat'],['validate-integrity']
Security,"Hello, more information on the parameters and runtime can be found here: #7492 . the stacktrace is now:; ```; ...; 22:14:59.985 INFO ProgressMeter - chrUn_JTFH01001653v1_decoy:301 116.6 2161460 18530.7; 22:15:11.142 INFO ProgressMeter - chrUn_JTFH01001673v1_decoy:301 116.8 2161540 18501.9; Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://cclebams/hg38_wes/CDS-02waxZ.hg38.bam -tumor TUHR14TKB --germline-resource gs://depmapomicsdata/gnomad.genomes.r3.0.sites.vcf.bgz -pon gs://depmapomicsdata/1000g_pon.hg38.vcf.gz -L gs://fc-secure-d2a2d895-a7af-4117-bdc7-652d7d268324/cec2a1a6-ffc3-4f1b-ba94-27ae918c56e9/Mutect2/b389d86b-8b0b-4d77-8224-a5a3e3a0b4e5/call-SplitIntervals/cacheCopy/glob-0fc990c5ca95eebc97c4c204e3e303e1/0004-scattered.interval_list -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --gcs-project-for-requester-pays broad-firecloud-ccle; ln: failed to access '/cromwell_root/*normal-pileups.table': No such file or directory; ln: failed to access '/cromwell_root/*tumor-pileups.table': No such file or directory; 2021/10/05 22:15:24 Starting delocalization.; ...; ```. I run mutect2 in tumor only mode. ; Interestingly, this error always only happen at the last shard only (every other shard runs to completion). Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7494:858,secur,secure-,858,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7494,3,"['access', 'secur']","['access', 'secure-']"
Security,"Hello,. There was at least one prior conversation about migrating or not migrating GATK3 CombineVariants to GATK4. My understanding is that there was a decision in GATK not to migrate CombineVariants, and instead push people to use Picard MergeVcfs. As you know, Picard MergeVcfs is somewhat similar; however, it doesnt merge genotypes. That is a pretty big difference in function. . CombineVariants is one of the few GATK3 tools my lab is still using. I'd like to move us off GATK3 in the coming months. Given GATK has already decided not to migrate it, I would first like to propose that we could port and take it over in my lab's DISCVRseq project (https://github.com/bimberlab/discvrseq). I'm happy to give attribution to GATK, etc. I would likely rename it MergeVcfsAndGenotypes (this is more intuitive to me), but I would otherwise not change functionality much. I'd prefer to do this instead of porting to GATK4 because porting to GATK is going to throw up a lot more obstacles and probably require that I modernize/update a good amount of that tool's code. I appreciate why this is required, but it takes a lot more work from us. If you did not like this, I'm open to considering porting to GATK4. In my initial review, it looked like CombineVariants was fairly self-contained and that most of the accessory code (merging genotypes is the most complex thing) was already migrated to GATK4. Some of you may have already done a more thorough review of it. . What do you think?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7038:1306,access,accessory,1306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7038,1,['access'],['accessory']
Security,"Here is a recap of what we discussed today during the CNV meeting:. For the first round of evaluations we decided to run Germline CNV pipeline on TCGA exomes using a range of key hyperparameters (namely psi-t-scale and p-alt) and establish the base level performance metrics using output of GenomeSTRiP on matched WGS samples as ground truth. . @mbabadi could you come up with a good range of hyperparameter values that you think should be cross-validated?. In particular we need to:. - Dockerize tools we will be evaluating against (XHMM, CODEX2, CLAMMS, GenomeSTRiP); - Write a WDL that runs Germline CNV that scatters across range of key hyperparameters and outputs array of VCFs; - Write VCF processors for output of CLAMMS and CODEX2 ; - Write WDLs for running XHMM, CODEX2, CLAMMS and GenomeSTRiP that output VCFs; - Write WDL that takes results of the above and uses @mbabadi 's gCNV evaluation python modules(located here /dsde/working/mehrtash/gCNV_theano_eval) to output performance metrics; - Decide on an automatic evaluation framework. For the next round of evaluations we need to:. - Decide on appropriate metrics for evaluating performance on trios and write scripts that implement them; - Expand the range of hyperparameters in search space (possibly include different bin sizes, GC vs no GC correction, and fragment mid point coverage collection vs largest fragment overlap coverage collection); - Use gnomAD subset of matched WES/WGS pairs for validation",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-362075071:446,validat,validated,446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-362075071,2,['validat'],"['validated', 'validation']"
Security,"Here it is. An overview of what's been added:; - metrics package; - a few general metrics classes (e.g. MultiLevelMetrics); - may want to push these down into HTSJDK later; - added some utils; - utils.gene: gene annotation; - utils.illumina: general Illumina-related utils (adapters, etc); - utils.text.parsers: text parsing; - utils.variant: added dbSNP stuff; - MathUtils: added a few basic things (mean, stddev, etc) with unit tests; - tools; - three major packages:; - analysis: metrics + analyses (including necessary Rscripts); - illumina: Illumina parsing + validation; - vcf: VCF manipulation + GenotypeConcordance; - also two smaller packages, fastq and intervals, containing a few tools each; - tests; - all existing tests were ported; still, overall test coverage goes down by ~6%; - all CLP integration tests have been ported to the new argument system; - test data has also been carried over, and is neatly organized; - there are no huge files, and very few above 100KB (just a few VCFs I think); - however, the Illumina test data is pretty big - ~6MB spread over ~1700 files",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/347:565,validat,validation,565,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/347,1,['validat'],['validation']
Security,"Here's an idea. It may be too much work, for now, and I'm fine with this code as it stands, but perhaps others are not. Anyway, the idea is this:. The constructor gets one extra argument: A lambda that serves as a validation function of the form; ```; void validate( int contig, int start, int end );; ```; It either throws an exception or returns silently.; Every client of SVInterval thus documents its intended conventions, and makes certain that those conventions are being adhered to.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5157#issuecomment-418868380:214,validat,validation,214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5157#issuecomment-418868380,2,['validat'],"['validate', 'validation']"
Security,"Here's how these scripts are organized and why they take the form it is now:. How to run; * `manage/project.sh` is the ""executable""; * paths for VCF files (zipped or not) from PacBio callsets on CHM haploids, and Manta's VCF on the mixture should be provided to `manage/project.sh`, and; * paths for two versions of GATK-SV callsets; one is fine, but scripts in the sub-directory `manage` must be modified. Two GATK-SV vcf files are requested because this would allow one to compare if a supposedly improvement would make our raw sensitivity/specificity better or worse, that was the use case [here](https://github.com/SHuang-Broad/GATK-SV-callset-regressionTest), and; * paths to where results are to be stored, one for each GATK-SV callset must be given and ; * path to where to store the results of comparing the two callsets; * several GNU bash utilities are expected, `guniq` and `gsort`, when run on a Mac, as well as `bedtools`. and what to expect; * the scripts checks the VCF files, prints to screen a slew of information that one can pipe, or simplely browse through.; * the scripts also outputs the ID's of variants from each of the two GATK callsets that are ""validated"" by PacBio haploid calls. Misc points:; * watch out for ""duplicated"" records, as sometimes different assembly contigs mapped to the same locations have slightly different alleles (SNP, for example) hence both would be output, but there aren't many such records based on experience; * there are also some variants that we output to the VCFs having size <50 or >50K, both of which are filtered upfront and saved separately.; * The scripts started when we first call insertions, deletions, inversions and small duplications, and back then PacBio call sets on the CHM haploids were not available, so Manta's calls were used as ""reference"", that explains why they are referred to throughout the project",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4406#issuecomment-365730030:1172,validat,validated,1172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4406#issuecomment-365730030,2,['validat'],['validated']
Security,"Here's some old code that uses SamLocusIterator (from tfennel) that AllelicCapseg can adapt for now. From Tim:; ""They key to making this nice and simple is the SamLocusIterator class, which given a BAM file and a list of intervals, will give you pileups at each position in the intervals, filtered how you want them, and even provide convenience methods to access the exact base per read that is piled up at the site etc. The really nice things about doing it this way is that the constructor to SamLocusIterator takes a simple parameter to tell it whether to use an index/query mechanism (similar to what you're doing now) or to just read the BAM serially up until the last interval is reached and output the loci of interest. Running the below with ~100k sites on a standard exome (15GB or so) without using the index took only about 15 minutes."". ```; public void pileup(final File bam, final IntervalList intervals, final int minQ, final File outputFile) {; final int MAX_INTERVALS_FOR_INDEX = 25000; // just a guess, not sure what the right number is. final SamLocusIterator iterator = new SamLocusIterator(new SAMFileReader(bam), intervals, intervals.size() < MAX_INTERVALS_FOR_INDEX);; iterator.setEmitUncoveredLoci(false);; iterator.setQualityScoreCutoff(minQ);. final BufferedWriter out = IoUtil.openFileForBufferedWriting(outputFile); // will automatically gzip if filename ends with .gz; try {; while (iterator.hasNext()) {; final SamLocusIterator.LocusInfo locus = iterator.next();; int a=0, c=0, g=0, t=0;. for (final SamLocusIterator.RecordAndOffset rec : locus.getRecordAndPositions()) {; switch (rec.getReadBase()) {; case 'A' : ++a; break;; case 'C' : ++c; break;; case 'G' : ++g; break;; case 'T' : ++t; break;; }; }. out.append(locus.getSequenceName() + ""\t"" + locus.getPosition() + ""\t"" + a + ""\t"" + c + ""\t"" + g + ""\t"" + t + ""\t"");; }. out.close(); ; }; catch (IOException ioe) { throw new RuntimeIOException(ioe); }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420:357,access,access,357,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420,1,['access'],['access']
Security,Hey @droazen can you please give me authorization to merge PRs? Or can you please merge this PR? Thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2812#issuecomment-306374281:36,authoriz,authorization,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2812#issuecomment-306374281,1,['authoriz'],['authorization']
Security,"Hey @lbergelson, @droazen or @jamesemery: I'm hoping we could finalize this issue. I am unable to commit directly to this branch. To respond to the suggestions from @droazen I made this PR which i am hoping someone can merge into this feature branch (https://github.com/broadinstitute/gatk/pull/8871). . To recap:; - This PR is driven by a request from some of DISCVRseq's users to restore a GATK3 behavior where all source IDs are saved for variants when multiple VCFs are merged.; - This PR would expose this as an opt-in feature, and should not change any default GATK4 behavior. Therefore existing variant merge code should be unchanged.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8752#issuecomment-2206804137:499,expose,expose,499,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8752#issuecomment-2206804137,1,['expose'],['expose']
Security,"Hey @rsasch -- Just thinking out loud here… but does WDL seem like the right ""language"" to implement these sort of checks? It just feels like there is a ton of boilerplate, the WDL/bash constructs are a bit hard to follow, and I'm sure the development cycles were pretty slow waiting for cromwell to spin up all those VMs, etc. An alternative would be to write all these validations as a python script, which could make better use of structure (ie authenticating once, etc) and I bet would be quite a bit more readable. You could also run it locally if needed for development, debugging and then ultimately wrap it into a single-task WDL so it could be run easily from Terra. Happy to chat more",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7352#issuecomment-883435010:371,validat,validations,371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7352#issuecomment-883435010,2,"['authenticat', 'validat']","['authenticating', 'validations']"
Security,"Hey folks,. I have a test dataset that interestingly core-dumps or JVM errors with `--smith-waterman FASTEST_AVAILABLE` but not with `--smith-waterman JAVA`. The only thing I can think of is somehow Intel's HMM has a length limitation, as I am using `--assembly-region-padding 1000` to GATK to call 100-1000bp indels (and it works!). I cannot share the test BAM unfortunately. What can I do to help debug further?. I'm using `gatk4-4.1.8.1-0` from `conda create -n debug-gatk4 -c defaults -c conda-forge -c bioconda gatk4`. ```; $gatk ... -version; The Genome Analysis Toolkit (GATK) v4.1.8.1; HTSJDK Version: 2.23.0; Picard Version: 2.22.8; $ java -version; openjdk version ""1.8.0_152-release""; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12); OpenJDK 64-Bit Server VM (build 25.152-b12, mixed mode); ```. First error motif:; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010efa9dc2, pid=23946, tid=0x000000000000a503; #; # JRE version: OpenJDK Runtime Environment (8.0_152-b12) (build 1.8.0_152-release-1056-b12); # Java VM: OpenJDK 64-Bit Server VM (25.152-b12 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # V [libjvm.dylib+0x3a9dc2] PhaseIdealLoop::set_ctrl(Node*, Node*)+0x10; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; #; # Compiler replay data is saved as:; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; ```. Second error motif:; ```; java(24057,0x7000035bd000) malloc: Incorrect checksum for freed object 0x7fd8a8193600: probably modified after being freed.; Corrupt value: 0x2e4630002e47e; java(24057,0x7000035bd000) malloc: *** set a breakpoint in malloc_error_break to debug; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733:1703,checksum,checksum,1703,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733,1,['checksum'],['checksum']
Security,"Hi ; I am using GATK 4.1.7.0, and have faced an issue running the seq-format-validation workflow. basically the process is ran but ends in failed state and the main output does not appear. I have attached here the exact command, stderr,validate-bam-inputs.json and validate-bam.wdl. The stdout just says 230. ; I would be grateful for any help on how to solve the issue. ; Best ; zara. <img width=""1280"" alt=""Screen Shot 2020-07-16 at 12 01 12 AM"" src=""https://user-images.githubusercontent.com/61913000/87845900-e6491480-c8e0-11ea-8dc0-ec5b3fbc15a8.png"">; <img width=""1280"" alt=""Screen Shot 2020-07-16 at 12 01 19 AM"" src=""https://user-images.githubusercontent.com/61913000/87845901-e77a4180-c8e0-11ea-8c6a-fb5783949ba3.png"">; <img width=""353"" alt=""Screen Shot 2020-07-16 at 12 00 30 AM"" src=""https://user-images.githubusercontent.com/61913000/87845902-ea753200-c8e0-11ea-96bc-caecee2ccb39.png"">; <img width=""1249"" alt=""stderr1"" src=""https://user-images.githubusercontent.com/61913000/87845904-eea14f80-c8e0-11ea-90bd-235c9205f72f.png"">. (gatk) root@bc3c6aca6231:/gatk/my_data/tools# java -jar cromwell-51.jar run /gatk/my_data/seq-format-validation/validate-bam.wdl --inputs /gatk/my_data/seq-format-validation/validate-bam.inputs.json; [2020-07-14 05:09:22,78] [info] Running with database db.url = jdbc:hsqldb:mem:f10b64bd-d8ca-4428-917b-311fca24c372;shutdown=false;hsqldb.tx=mvcc; [2020-07-14 05:09:29,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-07-14 05:09:29,37] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-07-14 05:09:29,47] [info] Running with database db.url = jdbc:hsqldb:mem:e337a356-2f0c-4389-92c5-255465180f24;shutdown=false;hsqldb.tx=mvcc; [2020-07-14 05:09:29,89] [info] Slf4jLogger started; [2020-07-14 05:09:30,10] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-ca5c695"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:77,validat,validation,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,3,['validat'],"['validate-bam', 'validate-bam-inputs', 'validation']"
Security,Hi ; We have a forum post asking help for getting GATK 4.1.0.0 conda environment installed using the yml file. ; [https://gatk.broadinstitute.org/hc/en-us/community/posts/18332470602523-Install-GATK-version-4-1-0-0-using-Conda-](url). Looks like restructuring of the default repository under conda took out some of these packages and they are no longer directly accessible. They can be accessed from the forge repo with certain flags. This issue seems to deprecate some of the older but still usable versions of GATK (due to various reasons). Directing people to use the docker version or upgrading to the latest GATK version seems to be the only solution left for now. Any other ideas of how we should pursue this issue? @lbergelson @droazen ?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8504:362,access,accessible,362,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8504,2,['access'],"['accessed', 'accessible']"
Security,"Hi @aderzelle. Thanks for your interest in NeuralNetInference. The tool currently has pre-beta`Experimental` status, so for now it should be used for evaluation purposes only. We expect to release another version of GATK, probably within the next week, that will include an updated version of the tool that will include a default architecture file, along with some additional tools for things like training. The tools will still be `Experimental`, but should be a bit easier to use. In the meantime, there is a bit more information about how to access the existing hd5 file [here](https://github.com/broadinstitute/gatk/issues/4511). Note that in the next release, the name of the tool will have changed to CNNScoreVariants. @lucidtronix anything else to add here?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4559#issuecomment-375647411:545,access,access,545,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4559#issuecomment-375647411,1,['access'],['access']
Security,Hi @cmnbroad - I removed a space in the path and re-ran ValidateVariants. It went through throwing no errors. Many thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4657#issuecomment-380989518:56,Validat,ValidateVariants,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4657#issuecomment-380989518,1,['Validat'],['ValidateVariants']
Security,"Hi @dislek, this type of question might be more appropriate for the GATK forum (and has actually been previously asked there, see https://gatkforums.broadinstitute.org/gatk/discussion/12543/determinegermlinecontigploidy and the thread linked in the answer). The priors file is a TSV file that is described in the tool documentation for DetermineGermlineContigPloidy. You should manually construct this TSV file, with contig names appropriate to the reference being used and prior probabilities appropriate for the quality of your data set---probably the suggestions in the example in the tool documentation are a reasonable place to start. However, you may want to validate the ploidy calls output by DetermineGermlineContigPloidy using samples where the truth is known to ensure that the model is trained correctly using your choice of priors and other model parameters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5719#issuecomment-467252385:665,validat,validate,665,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5719#issuecomment-467252385,1,['validat'],['validate']
Security,Hi @icemduru ; Looks like your slurm workload manager was configured to have a limit of 48GBs of maximum process memory size per execution. Your java instance is set with -Xmx45G which will cover most of this limit and leaves only a handful of memory space for the native GenomicsDB library. Native libraries work above the heapsize so it is better for you to set your -Xmx to a more sensible size of 8~12GB and leave rest of the memory space to the native library to use. . Keep in mind that this memory limit on slurm could be set per user not per task therefore you may need to run a single contig at a time or maybe 2 of them simultaneously. Otherwise slurm may interefere with all the tasks and cancel all your jobs. . One final reminder. We strongly recommend users to set the temporary directory to somewhere else other than /tmp. Slurm workload manager interferes with this preference and sometimes results in premature termination of the gatk processes due to deletion of extracted native library and accessory files. . I hope this helps.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8918#issuecomment-2283694332:1010,access,accessory,1010,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8918#issuecomment-2283694332,1,['access'],['accessory']
Security,Hi @jonn-smith I had some success getting outputs with this. However ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/ is no longer accessible. The most recent version of I accessed on 3/19/2018 still contained at least one error. I'm trying to correct them myself as I go using a more recent build of GATK but it would be helpful if the data files required by this program were available. The one I found is in `gencode_xrefseq.config` where it references a source that doesn't exist and I fixed that. After that I was able to get outputs with hg38. Thanks for your work on this!. I'd also point out there are a lot of fields in the MAF with `__UNKNOWN__` as the entry,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4521#issuecomment-383387645:150,access,accessible,150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521#issuecomment-383387645,2,['access'],"['accessed', 'accessible']"
Security,"Hi @praneetha92 ,. Where did this file come from? GATK doesn't produce GLs anymore. It looks like there are the wrong number of likelihoods for one of your genotypes. Unfortunately our validation tool doesn't check this, but can you try vcf-validator? You can find it here: https://vcftools.github.io/perl_module.html#vcf-validator",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6897#issuecomment-710135263:185,validat,validation,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6897#issuecomment-710135263,3,['validat'],"['validation', 'validator']"
Security,"Hi @tushu1232. The index image is a new feature that's not fully exposed in a friendly. It's the 5 indexes that bwa requires baked into a single file for easy distribution. ( .amb, .ann, .bwt, .pac, .sa ) We're going to add an official tool to generate it in the near future, but at present the only way to do so is to write a tool yourself that calls into org.broadinstitute.hellbender.utils.bwa.BwaMemIndex.createIndexImage();",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2414#issuecomment-282128828:65,expose,exposed,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2414#issuecomment-282128828,1,['expose'],['exposed']
Security,"Hi Adam,. Maybe I'm thinking naively here - and I don't have access to a complete and proper Spark cluster for rigorous testing - but just as a simple test of loading a VCF via Spark, I took [PrintReadsSpark.java](https://github.com/broadinstitute/gatk/blob/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:61,access,access,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857,1,['access'],['access']
Security,"Hi Chris,. Would you please review this?; And @tedsharpe, feel free to look at the getters in `InsertSizeMetricsCollectorSpark.java` and see if you need more access functions. Thank you.; Steve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1566:158,access,access,158,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1566,1,['access'],['access']
Security,"Hi James, that definitely shouldn't be happening. Any chance I could get access to that cram file or a 10 kB chunk around chr1:1914706 for debugging?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6473#issuecomment-593049062:73,access,access,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6473#issuecomment-593049062,1,['access'],['access']
Security,"Hi Jose,; Your system ulimit setting is too low. Please do this with root at; /etc/security/limits.conf; * soft memlock unlimited; * hard memlock unlimited; * hard nofile 20480; * soft nofile 20480; * hard nproc 40960; * soft nproc 40960; * soft stack unlimited; * hard stack unlimited. Ruzhu; -------------------------------------------; Ruzhu Chen, PhD (845) 433-8426(T/L 293-8426); Email: ruzhuchen@us.ibm.com, Mobile: (845) 337-7238; Sr. Technical Solution Architect, HPC / Genomics & Life Sciences; IBM Systems, 2455 South Road, Poughkeepsie, NY 12601. From:	Jose Sergio Hleap <notifications@github.com>; To:	broadinstitute/gatk <gatk@noreply.github.com>; Cc:	ruzhuchen <ruzhuchen@us.ibm.com>, Mention; <mention@noreply.github.com>; Date:	03/12/2020 11:37 AM; Subject:	[EXTERNAL] Re: [broadinstitute/gatk] Got ""Too many open files""; when use BaseRecalibratorSpark (#5316). Apologies on the poor report. There are no other users in these compute; nodes (I am the tester) and for all intents and purposes the ulimit is; pretty high (hard limit of 8192 max files). I am using GATK version; 4.1.4.1, although it might be the one that has been optimised for IBM; power9 systems by @ruzhuchen. Currently I am waiting for the sys admin to; increase the max files further, but I believe that this is far from ideal.; Here is the (simplified) command:. gatk --java-options ""-Xmx40g; -Djava.library.path=/bio/apps/gatk_4.1.4/gatk-4.1.4.1/libs; -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" Mutect2 -R; Homo_sapiens_assembly38.fa -I illuminaN_hg38.br.recal.bam; --max-mnp-distance 0 -O illuminaN.vcf.gz. May be I am running it wrong?. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-598269062:83,secur,security,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-598269062,2,['secur'],['security']
Security,"Hi all,. Below error occurs trying to access Funcotator data source directory installed on lustre file system. We have a non-lustre mounted fs for cases like this, but I thought it was worth bringing up. ```; org.broadinstitute.hellbender.exceptions.GATKException: Unable to query the database for geneName: null; 	at org.broadinstitute.hellbender.tools.funcotator.dataSources.cosmic.CosmicFuncotationFactory.createFuncotations(CosmicFuncotationFactory.java:244); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.enqueueAndHandleVariant(Funcotator.java:404); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.apply(Funcotator.java:316); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:110); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:108); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4413:38,access,access,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4413,1,['access'],['access']
Security,"Hi all;; When validation runs on the GATK 4.0.0 release (congrats!) we're running into segfault issues on some `GenomicsDBImport` runs which look to be due to the length of the database path:; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f99a7642c5b, pid=7446, tid=0x00007f99fbfa0700; #; # JRE version: OpenJDK Runtime Environment (8.0_121-b15) (build 1.8.0_121-b15); # Java VM: OpenJDK 64-Bit Server VM (25.121-b15 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libtiledbgenomicsdb8843204539247232071.so+0x4fdc5b] std::string::compare(char const*) const+0x1b; ```; Here is a self-contained test case that reproduces the issue:. https://s3.amazonaws.com/chapmanb/testcases/gatk/gatk4_genomicsdb_length.tar.gz. A standard small name and longer name of 105 characters work fine:; ```; gatk-launch --java-options '-Xms1g -Xmx2g' GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path short_genomicsdb -L chr22:15069-15500 --variant Test1.vcf.gz --variant Test2.vcf.gz; gatk-launch --java-options '-Xms1g -Xmx2g' GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path long_aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa_genomicsdb/works_aaaaaaaaaaaaaaaaaaaaaaaaaa -L chr22:15069-15500 --variant Test1.vcf.gz --variant Test2.vcf.gz; ```; But when you add an additional character, you trigger the segfault:; ```; gatk-launch --java-options '-Xms1g -Xmx2g' GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path long_aaaaaa; aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa_genomicsdb/fails_aaaaaaaaaaaaaaaaaaaaaaaaaaa -L chr22:15069-15500 -; -variant Test1.vcf.gz --variant Test2.vcf.gz; ```; Thank you for looking at this and please let me know if I can provide any other information to help debug.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4160:14,validat,validation,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4160,1,['validat'],['validation']
Security,"Hi everyone,. I'm facing a similar issue with GATK v4.1.0.0 (HTSJDK v2.18.2 and Picard v2.18.25). I'm using GATK Docker image broadinstitute/gatk:4.1.0.0. Following what I read here, I checked the bam file and everything seems fine:; `gatk ValidateSamFile --INPUT sorted.bam --MODE SUMMARY`; ```; Using GATK jar /gatk/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.0.0-local.jar ValidateSamFile --INPUT CQ-NEQAS-2018.ILLUMINA.library.000000000-BCFDC.1.1.sorted.bam --MODE SUMMARY; 16:08:17.382 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu Mar 07 16:08:17 UTC 2019] ValidateSamFile --INPUT CQ-NEQAS-2018.ILLUMINA.library.000000000-BCFDC.1.1.sorted.bam --MODE SUMMARY --MAX_OUTPUT 100 --IGNORE_WARNINGS false --VALIDATE_INDEX true --INDEX_VALIDATION_STRINGENCY EXHAUSTIVE --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --SKIP_MATE_VALIDATION false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. La",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5807:240,Validat,ValidateSamFile,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807,3,['Validat'],['ValidateSamFile']
Security,"Hi folks. @chandrans and I have laid out some plans towards updating GATK4 docs for the January 9 release. Our approach is to prioritize documentation around stable Best Practice Workflows. On the docket currently is the single stable workflow--germline SNP and indel calling from DNA data. We will of course update tool docs (excluding Spark and BWA tools) and supporting tutorials. Even for tools we are unfamiliar with, we aim to have at the least a basic description and an example command. Thanks for the documentation you have already done and the help you give us in updating these. If you are certain your workflow will be ready for the release, then please let us know immediately so we can plan accordingly. If your workflow will be ready later, then can you still give us an estimate for your release so we can plan ahead? Thanks. - @davidbenjamin, did I hear you correctly that you think somatic SNV and indel calling will be ready for the Jan 9 release?; - @samuelklee, I know major changes are currently afoot for somatic CNV. Will you make the Jan 9 release for the targeted exomes use-case? What about WGS?; - @mbabadi, is March, 2018 still the plan?; - @jonn-smith, what is the status on the Tool-That-Must-Not-Be-Named?; - @cwhelan @tedsharpe @SHuang-Broad, is SV on for next year or thereafter?. It would be most helpful to users if we also have validation of our workflows as applied to real data. Are there plans to make benchmarking stats available for each of your workflows?. Sheila and I have 30-man days we can give between us towards updating documentation by December 14. Besides Geraldine, we will rely on some of you to review further refinements to documentation from now to December 14. Thanks again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3769:1365,validat,validation,1365,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3769,1,['validat'],['validation']
Security,"Hi, ; for those looking to run containers within a multi-user HPC environment, running a container with default root privileges presents a potential data security risk. Adding something like :. RUN useradd -ms /bin/bash gatk; WORKDIR /home/gatk; USER gatk. to the Docker file would greatly reduce the risk and bring the current containers in line with general best practice, e.g https://medium.com/@mccode/processes-in-containers-should-not-run-as-root-2feae3f0df3b. There should be no downsides to running in this manner. Singularity could help but the current configuration will be picked up and prevented from running by any site using a container security scanner, e.g. Aqua.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-494457377:154,secur,security,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-494457377,2,['secur'],['security']
Security,"Hi, I encountered the following error while running GATK.; It is hard for me to say what exactly is wrong, and extensive searching has not been helpul. Thanks in advance!. ```; gatk ValidateVariants -V ../../data/geno/phased/chr1-22.phased.rename.reheader.vcf.gz -R ../../../../index/hg19.fa.gz; Using GATK jar ~/tools/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/zepengmu/tools/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar ValidateVariants -V ../../data/geno/phased/chr1-22.phased.rename.reheader.vcf.gz -R ../../../../index/hg19.fa.gz; 19:53:34.379 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/zepengmu/tools/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 25, 2020 7:53:34 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:53:34.606 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.606 INFO ValidateVariants - The Genome Analysis Toolkit (GATK) v4.1.8.0; 19:53:34.606 INFO ValidateVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:53:34.607 INFO ValidateVariants - Executing as zepengmu@midway2-login1.rcc.local on Linux v3.10.0-1127.8.2.el7.x86_64 amd64; 19:53:34.607 INFO ValidateVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14; 19:53:34.607 INFO ValidateVariants - Start Date/Time: October 25, 2020 7:53:34 PM CDT; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - HTSJDK Version: 2.22.0; 19:53:34.607 INFO ValidateVarian",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911:182,Validat,ValidateVariants,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911,2,['Validat'],['ValidateVariants']
Security,"Hi, since there is DOS (Denial of Service) threat for log4j 2.16.0(https://logging.apache.org/log4j/2.x/security.html),; is that possible to update GATK with log4j_2.17.0? Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7603#issuecomment-998102555:43,threat,threat,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7603#issuecomment-998102555,2,"['secur', 'threat']","['security', 'threat']"
Security,"Hi,. @gbrandt6 can you let me know how to access the bug report files that the user pushed to [ftp](https://gatk.broadinstitute.org/hc/en-us/community/posts/360072797951/comments/360012763332)? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793#issuecomment-686806185:42,access,access,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793#issuecomment-686806185,1,['access'],['access']
Security,"Hi,. I had used ""ASEReadCounter"" with the GRCh37 Genome on my samples and I got results even when I had this warning:; ""IndexDictionaryUtils - Track sitesVCFFile doesn't have a sequence dictionary built in, skipping dictionary validation "". The problem is that now I'm using just the canonical chromosomes as a reference and I'm getting the same warning but the output file is empty. Could someone help me? . Thanks,; Cristian",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6540:227,validat,validation,227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6540,1,['validat'],['validation']
Security,"Hi,. I'm trying to validate the performance of BwaSpark (I'm running it locally). The input ubam file size is 5.1 GB. It takes 65 minutes for GATK's BwaSpark to complete which is exactly same as bwa-mem. Below is the command that I used to run BwaSpark. Is there any way to make BwaSpark run faster while running it locally or will the performance increase only while running on spark cluster? Please let me know if I had to modify or add any parameter. . Also, please let me know where can I find the complete list of --conf parameters for BwaSpark? (I couldn't find these options in gatk BwaSpark --help). `time gatk BwaSpark --bwa-mem-index-image GRCh37.fasta.img --spark-master local[*] --bam-partition-size 4000000 --conf 'spark.executor.num=5' --conf 'spark.executor.cores=16' --conf 'spark.executor.memory=15G' --conf 'spark.driver.memory=30G' --conf 'spark.dynamicAllocation.enabled=true' -I unmapped_input.bam -O output.bam -R GRCh37.fasta 2> Log_file.log`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8897:19,validat,validate,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8897,1,['validat'],['validate']
Security,"Hi,. Not sure if it is applicable but I just ran vcf-validator on the input gvcf file (also not sure if you can run vcf-validator on a gvcf): . ```; vcf-validator Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; ```. Here are the first few lines from the logs:. ```; The header tag 'reference' not present. (Not required but highly recommended.); INFO field at 1:14599 .. INFO tag [max_aaf_all=0.2096] expected different number of values (expected 2, found 1); INFO field at 1:14604 .. INFO tag [max_aaf_all=0.2096] expected different number of values (expected 2, found 1); INFO field at 1:14930 .. INFO tag [max_aaf_all=0.5231] expected different number of values (expected 2, found 1); INFO field at 1:15211 .. INFO tag [max_aaf_all=0.7316] expected different number of values (expected 2, found 1); INFO field at 1:15274 .. INFO tag [max_aaf_all=0.7205] expected different number of values (expected 3, found 1); INFO field at 1:16949 .. INFO tag [max_aaf_all=0.0227] expected different number of values (expected 2, found 1); INFO field at 1:17365 .. INFO tag [max_aaf_all=0.3841] expected different number of values (expected 2, found 1),INFO tag [af_adj_exac_afr=; 0.1603] expected different number of values (expected 2, found 1),INFO tag [af_exac_all=0.2553] expected different number of values (expect; ed 2, found 1),INFO tag [af_adj_exac_nfe=0.2715] expected different number of values (expected 2, found 1),INFO tag [af_adj_exac_sas=0.2883; ] expected different number of values (expected 2, found 1),INFO tag [af_adj_exac_oth=0.2581] expected different number of values (expected; 2, found 1),INFO tag [af_adj_exac_eas=0.3841] expected different number of values (expected 2, found 1),INFO tag [af_adj_exac_amr=0.221] e; xpected different number of values (expected 2, found 1),INFO tag [af_adj_exac_fin=0.2245] expected different number of values (expected 2,; found 1); INFO field at 1:17373 .. INFO tag [af_adj_exac_amr=0.028] expected different number ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407476343:53,validat,validator,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407476343,3,['validat'],['validator']
Security,"Hi,. Using GATK mutect2's wdl file on Terra (version 21 on agora) I keep getting the same error:; ""pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket"" . Here is part of the stacktrace : ; ```; 20:59:48.744 INFO Mutect2 - Inflater: IntelInflater; 20:59:48.744 INFO Mutect2 - GCS max retries/reopens: 20; 20:59:48.744 INFO Mutect2 - Requester pays: enabled. Billed to: broad-firecloud-ccle; 20:59:48.744 INFO Mutect2 - Initializing engine; 20:59:54.630 INFO FeatureManager - Using codec VCFCodec to read file gs://depmapomicsdata/1000g_pon.hg38.vcf.gz; 20:59:55.629 INFO Mutect2 - Shutting down engine; [October 4, 2021 8:59:55 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=876609536; code: 403; message: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; reason: forbidden; location: null; retryable: false; com.google.cloud.storage.StorageException: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:229); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:406); at com.google.cloud.storage.StorageImpl$4.call(StorageImpl.java:217); ...; ```. This happens while it runs the command:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m\ ; -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\ ; -I gs://cclebams/hg38_wes/CDS-00rz9N.hg38.bam -tumor BC1_HAEMATOPOIETIC_AND_LYMPHOID_TISSUE --germline-resource gs://gcp-public-data--gnomad/release/3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7492:204,access,access,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492,1,['access'],['access']
Security,"Hi,; I update GATK today.; After 158 minutes variant calling on the same bam files, I have another issue :. ```; [3 décembre 2019 13:57:42 CET] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 158.34 minutes.; Runtime.totalMemory()=28647096320; Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded; 	at java.util.LinkedHashMap$LinkedKeySet.iterator(LinkedHashMap.java:543); 	at java.util.HashSet.iterator(HashSet.java:173); 	at java.util.AbstractCollection.toArray(AbstractCollection.java:137); 	at java.util.LinkedList.addAll(LinkedList.java:408); 	at java.util.LinkedList.addAll(LinkedList.java:387); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.graphs.BaseGraph$BaseGraphIterator.next(BaseGraph.java:774); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.graphs.BaseGraph$BaseGraphIterator.next(BaseGraph.java:723); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.graphs.BaseGraph.removePathsNotConnectedToRef(BaseGraph.java:505); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.getAssemblyResult(ReadThreadingAssembler.java:514); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.createGraph(ReadThreadingAssembler.java:492); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assemble(ReadThreadingAssembler.java:401); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.runLocalAssembly(ReadThreadingAssembler.java:148); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:290); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:224); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:320); 	at org.broadinstitute.hellben",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-561188674:447,Hash,HashSet,447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-561188674,2,['Hash'],['HashSet']
Security,Hm - I don't think we can take that last change. Theres not much use in validating args after they've been used by the constructors. Let me see if there is an alternative.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-827854652:72,validat,validating,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-827854652,1,['validat'],['validating']
Security,"Hmm, I don't have access to dsde-docs. I thought by default haplotypeCaller doesn't use supplementary reads in GATK3? If I'm wrong about that assumption, then I don't see any reason why it would be different in GATK4.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2043#issuecomment-235071588:18,access,access,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2043#issuecomment-235071588,1,['access'],['access']
Security,"Hmm, I started taking a stab at the LL score implementation, but I think it's going to complicate the code quite a bit and add some branching options to the tool interfaces. Compounding this with a change in the use of ""truth"" and ""validation"" terminology, I fear that the resulting differences from the legacy strategy might be a bit much for users to digest!. So I'd want to better understand the cost/benefit before we proceed. How critical is automatic tuning of the hard threshold? And what's the relative importance to method changes that increase AUC (i.e., as opposed to figuring out where on the curve to hard threshold)? Is there a clear path forward for evaluating such a tuning process? @meganshand would be glad to chat more!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1065524909:232,validat,validation,232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1065524909,2,['validat'],['validation']
Security,"Hmm, as we discussed, I'm not sure this will be so straightforward, given that we only have easy access to scores for positive truth---and hence, no false positives, which precludes calculation of precision and F1. I *think* we could pass a VCF for a sample with gold-standard positives and negatives and use the existing code for extracting labels, but this will require a bit of engineering and be more trouble than it's worth. There are other options---see https://ir.cwi.nl/pub/30479, for example. We might want to experiment with the LL score discussed there (see https://www.aaai.org/Papers/ICML/2003/ICML03-060.pdf for the original paper---although note that despite the paper's high citation count, I'm not sure what the canonical name for this metric actually is, but it doesn't appear to be ""LL score""---perhaps someone else knows or has better Google-fu and can figure it out) before moving on to their methods for estimating F1. Doing a literature search for other discussions of optimizing F1 or other metrics in the context of positive-unlabeled learning might be worthwhile, but I think most methods will probably involve some sort of estimation of the base rate in unlabeled data. I think we may have to add some mechanism for holding out a validation set during training if we want to automatically tune thresholds in a rigorous fashion. Shouldn't be too bad---we can just have the training tool randomly mask out a set of the truth and pass the mask to the scoring tool (or maybe just determine the threshold in the training tool, if we are running in positive/negative mode and have access to unlabeled data)---but does add a couple of parameters to the tool interfaces. This also adds additional dependence on the quality of the truth resources. I think an implicit assumption in any use of the truth---even just thresholding/calibrating by sensitivity---is that it is a random sample; however, I'm not sure how true this is in actual use. For example, in malaria, it looks like we",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241:97,access,access,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241,2,['access'],['access']
Security,"Hmmm, rather than making assumptions, the aligner could expose its SW parameters and then it comes down to a line of arithmetic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5466#issuecomment-443324822:56,expose,expose,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5466#issuecomment-443324822,1,['expose'],['expose']
Security,Hmn. This is failing with 403 unauthorized errors. Seems like something about authentication changed.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6042#issuecomment-511979513:78,authenticat,authentication,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6042#issuecomment-511979513,1,['authenticat'],['authentication']
Security,How do I access that? I thought that the GATK resources were located here: https://console.cloud.google.com/storage/browser/genomics-public-data/resources/broad/hg38/v0/. Is there a reason this is not in the GATK resource bundle?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5821#issuecomment-481902753:9,access,access,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5821#issuecomment-481902753,1,['access'],['access']
Security,"How to reproduce:. Modify the code in ```AlignmentIntervalUnitTest.testConstructionFromSAMRecord``` to perform a validation of the read returned by ```applyAlignment```:. ```; final SAMRecord samRecord = BwaMemAlignmentUtils.applyAlignment(""whatever"", SVDiscoveryTestDataProvider.makeDummySequence(expectedContigLength, (byte)'A'), null, null, bwaMemAlignment, refNames, hg19Header, false, false);; if (samRecord.isValid() != null) {; throw new IllegalStateException(samRecord.isValid().stream().map(s -> s.getMessage()).collect(Collectors.joining("", "")));; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3459#issuecomment-323218384:113,validat,validation,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3459#issuecomment-323218384,1,['validat'],['validation']
Security,"Huh, I wonder why gradle doesn't like a lustre system. I didn't now that was an issue. Git-lfs is an annoyance to install if you don't have access to a package manager. I *think* it can be installed without sudo but I get why that's a pain. I've spent a non-zero amount of time fighting with git-lfs installation before. Thank you for elaborating!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1042057009:140,access,access,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1042057009,1,['access'],['access']
Security,"I added a test according to #4642 , but can't reproduce the error. The user also noted that the error message was badly formed, which is true because it ended with a colon. Now it looks like:; ""Input src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleBad.vcf fails strict validation of type CHR_COUNTS: the Allele Count (AC) tag is incorrect for the record at position 1:985447, 1 vs. 2""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6076:263,Validat,ValidateVariants,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6076,3,"['Validat', 'validat']","['ValidateVariants', 'validation', 'validationExampleBad']"
Security,"I added one unrelated bugfix. FuncotatorUtils.createReferenceSnippet tries to expand the reference window. When doing this, it should never allow a start less than 1. The last commit addresses that. . Note: I did not see an easy way for createReferenceSnippet() to identify the length of the contig (such as access to the SequenceDictionary), but it would in theory be useful to also check contig size and not exceed it. @droazen or @jonn-smith: it would be helpful if you could approve the test run. ```; 22 Jun 2023 14:54:27,152 DEBUG: 	java.lang.IllegalArgumentException: Invalid interval. Contig:MT start:0 end:20; 22 Jun 2023 14:54:27,154 DEBUG: 		at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804); 22 Jun 2023 14:54:27,155 DEBUG: 		at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); 22 Jun 2023 14:54:27,156 DEBUG: 		at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35); 22 Jun 2023 14:54:27,158 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.createReferenceSnippet(FuncotatorUtils.java:1461); 22 Jun 2023 14:54:27,159 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createIgrFuncotation(GencodeFuncotationFactory.java:2481); 22 Jun 2023 14:54:27,160 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createIgrFuncotations(GencodeFuncotationFactory.java:2407); 22 Jun 2023 14:54:27,162 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createDefaultFuncotationsOnVariant(GencodeFuncotationFactory.java:499); 22 Jun 2023 14:54:27,163 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:217); 22 Jun 2023 14:54:27,164 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.create",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8363#issuecomment-1603412226:308,access,access,308,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8363#issuecomment-1603412226,3,"['access', 'validat']","['access', 'validateArg', 'validatePositions']"
Security,"I addressed partially your comments (and fixed a compilation error due to the tests using the previous arguments). One of the major points of discussion are the following:. * `Collection` instead of `List`: I think that the first is more flexible, because a client maybe wants to have a `LinkedHashSet` as the argument to avoid repetition of the same filter. I agree that the abstract class should discourage not honoring the user order.; * Access to methods/fields: I think that the plugin could be used outside GATK in a different way by extending it. I explained some of my usage cases in one of the comments in the code, but just by overriding a simple method the whole plugin could be used very nicely in some of them. I would prefer to do that than copy your code and re-implement the bits that I would like to change. Back to you for your ideas on this, @cmnbroad!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208:441,Access,Access,441,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208,1,['Access'],['Access']
Security,I agree that it is a good security measure to use fixed signed dependencies for repeatable builds. GATK depends on gradle 3.1.: [download](https://services.gradle.org/distributions/gradle-3.1-bin.zip) [shaw256](https://services.gradle.org/distributions/gradle-3.1-bin.zip.sha256),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5483#issuecomment-444208372:26,secur,security,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5483#issuecomment-444208372,1,['secur'],['security']
Security,"I agree with everything @magicDGS said. In general, the native GATK tool benefits from all of the automatic framework management and I/O support (like honoring md5, lenient args, etc.). #2234 also has the validation checking mentioned, and has more tests. (It is true that when #2223 goes in, we'll have to override the default sequence dictionary validation behavior, maybe via disableSequenceDictionaryValidation). So I think we should take the native one, but if we do choose to keep this one for any reason, then I'll still want to do a line-by-line CR before we merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2232#issuecomment-257306629:205,validat,validation,205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2232#issuecomment-257306629,2,['validat'],['validation']
Security,I also can't access those files. Who can grant access to them?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2658#issuecomment-299493615:13,access,access,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2658#issuecomment-299493615,2,['access'],['access']
Security,"I am aware that those methods should be definetively implemented in the abstract class - but it could be recommended *NOT TO OVERRIDE* unless you know what you are doing. I know the problems of having them exposed, but also I can see some potential to be accessible from an implemented walker when the user knows what is doing...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4964#issuecomment-404134765:206,expose,exposed,206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4964#issuecomment-404134765,2,"['access', 'expose']","['accessible', 'exposed']"
Security,I am getting the following exception when I set `--minimum-mapping-quality` to 60 (but not 50). . ```console; $ gatk --version; ...; The Genome Analysis Toolkit (GATK) v4.2.0.0; HTSJDK Version: 2.24.0; Picard Version: 2.25.0; ```. ```console; $ gatk HaplotypeCaller \; -I in.bam \; -L chr7:145945238-145945238 \; -stand-call-conf 0 \; --disable-optimizations \; --force-active -O out.vcf \; --reference /path/to/ucsc.hg19.fasta \; --minimum-mapping-quality 60;; ...; java.lang.IllegalStateException: There is no variation present.; 	at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyRegionTrimmer$Result.getVariantRegion(AssemblyRegionTrimmer.java:108); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:595); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:273); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. <details>; <summary>test.sam</summary>. ```; @HD	VN:1.6	SO:coordinate; @SQ	SN:chr1	LN:249250621; @SQ	SN:chr2	LN:243199373; @SQ	SN:chr3	LN:198022430; @SQ	SN:chr4	LN:191154276; @SQ	SN:chr5	LN:18,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7123:578,validat,validate,578,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7123,1,['validat'],['validate']
Security,"I am going to be able to identify samples for a new panel of normals this week, after which generating and validating the panel will take another few days.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1289162161:107,validat,validating,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1289162161,1,['validat'],['validating']
Security,"I am having a similar issue with GATK `4.1.4.1` that persists after updating to `4.2.0.0`:; ```; 00:33:06.768 INFO BaseRecalibrationEngine - The covariates being used here:; 00:33:06.768 INFO BaseRecalibrationEngine - ReadGroupCovariate; 00:33:06.768 INFO BaseRecalibrationEngine - QualityScoreCovariate; 00:33:06.768 INFO BaseRecalibrationEngine - ContextCovariate; 00:33:06.768 INFO BaseRecalibrationEngine - CycleCovariate; 21/03/28 00:33:07 INFO BlockManagerInfo: Removed broadcast_0_piece0 on fend04.cluster:42128 in memory (size: 35.5 KB, free: 5.2 GB); 21/03/28 00:33:14 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1); java.lang.IllegalArgumentException: Table1 1,3 not equal to 2,3; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.combineTables(RecalUtils.java:560); at org.broadinstitute.hellbender.utils.recalibration.RecalibrationTables.combine(RecalibrationTables.java:144); at org.broadinstitute.hellbender.utils.recalibration.RecalibrationTables.inPlaceCombine(RecalibrationTables.java:178); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction2$1.apply(JavaPairRDD.scala:1037); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); at scala.collection.Iterator$class.foreach(Iterator.scala:891); at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1334); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190); at org.apa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5854#issuecomment-808817724:748,validat,validateArg,748,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854#issuecomment-808817724,1,['validat'],['validateArg']
Security,"I am looking at using GATK and first checked at the docker image using **_docker pull broadinstitute/gatk_**. this container image has 1460 vulnerabilities and a lot of them are critical. ; <img width=""1737"" alt=""Screenshot 2023-02-21 212830"" src=""https://user-images.githubusercontent.com/4427764/220508376-aeead13b-999b-4cfd-a7d6-295241df532a.png"">. Then I decided not to use this image and instead create my own image and just deploy the released version 4.2.6.1 from here (https://github.com/broadinstitute/gatk/releases/download/4.2.6.1/gatk-4.2.6.1.zip). Even this has many vulnerabilities include things stemming from log4j 1.2.17. These have been fixed by log4j team years back in version 2.17.1 onwards. I am really stunned that a popular library like gatk is not keeping up with basic security fixes. <img width=""854"" alt=""Screenshot 2023-02-21 212751"" src=""https://user-images.githubusercontent.com/4427764/220508300-7bfe331d-8286-4950-a6dc-e1f5f97c65d0.png"">. the latest version of docker desktop has integrated image scanning and can very easily highlight the issues listed above. Can we start addressing these issues sooner than later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215:795,secur,security,795,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215,1,['secur'],['security']
Security,"I am running GenotypeGVCFs using a single, combine GVCF produced from CombineGVCFs in GATK4. The file contains 44 individuals yet I am receiving the warning 'InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples'. Should I be concerned by this? I can validate that my input file contains more than 10 samples by the headers it contains. . This is the code I am running, . java -jar ../../programs/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar GenotypeGVCFs -R revisedAssemblyUnmasked.fa --variant subset_of_pop.vcf -O genotype_subset.vcf. Thank you, . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6238:283,validat,validate,283,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6238,1,['validat'],['validate']
Security,"I am still receiving security warnings about GATK 4.4.0.0:. Detected by File Paths: gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Detected by Library: pkg:java/log4j:log4j; CPE: cpe:/a:apache:log4j:1.2.17; Version End of Life Date: August 4th, 2015 at 7:00 PM",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1513816621:21,secur,security,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1513816621,1,['secur'],['security']
Security,"I appreciate the desire for minimal changes, but I would point out that tying the VariantContext to the source FeatureInput is likely to be a somewhat common need for MultiVariantWalkers. I realize you have a lot of existing example that dont need this capability. While I started this using VariantEval/VariantQC, I more recently tried to port CombineVariants to GATK4 and hit a similar roadblock. I have one or two other lab-specific walkers that would benefit from using the iteration pattern of MultiVariantWalkerGroupedOnStart, but also need some ability to retain the FeatureInput->VariantContext link. I believe GATK3 retained this. It would be nice to at least make this a capability available to all MultiVariantWalkerGroupedOnStart walkers. My suggestion above about making a FeatureInputAwareVariantContext wasnt necessarily meant to be introduced into every class. Would something conceptually like this pass GATK if I could introduce it more surgically, perhaps injected into MultiVariantDataSource alone?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-823594870:975,inject,injected,975,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-823594870,1,['inject'],['injected']
Security,"I can access the artefactory web site. I tried again, and the build worked! Must have been a transient issue. Thanks for checking!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292596069:6,access,access,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292596069,1,['access'],['access']
Security,"I can confirm it was due to ""gs://gcp-public-data--gnomad"" not giving the correct authorization.. I had to copy the file in my own workspace. . It seems pretty problematic as it is the recommended file to run the workflow with...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7492#issuecomment-934925641:82,authoriz,authorization,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492#issuecomment-934925641,1,['authoriz'],['authorization']
Security,"I checked only the tools that are marked `no` in the `correct category in gatk --list` in the 0107Check_category_&_doc tab of https://docs.google.com/spreadsheets/d/19SvP6DHyXewm8Cd47WsM3NUku_czP2rkh4L_6fd-Nac/edit?usp=sharing. ## The following eight tools need fixing:. tool | category philosophically ok? | 2nd check, correct category in gatkDoc? | categorization fixed by https://github.com/broadinstitute/gatk/pull/4094?; -- | -- | -- | --; IndexFeatureFile | NO | y | no; still in Variant Manipulation but should be in Other.; CreateHadoopBamSplittingIndex | NO | y | no; still in Other but no longer with ConvertHeaderlessHadoopBamShardToBam; VariantAnnotator | y | DOES NOT SHOW UP | no; does not show up; FixCallsetSampleOrdering | MISSING | no | no; does not show up; DepthOfCoverage | y | DOES NOT SHOW UP | no; DiagnoseTargets | y | DOES NOT SHOW UP | no; GatherTranches | y | y | DOES NOT SHOW UP IN GATKDOC; shows up in correct category in gatk --list; ValidateBasicSomaticShortMutations | MISSING | no | DOES NOT SHOW UP IN GATKDOC; shows up in correct category in gatk --list. ## The following tools appear fixed by this PR:. tool | category philosophically ok? | 2nd check, correct category in gatkDoc? | categorization fixed by https://github.com/broadinstitute/gatk/pull/4094?; -- | -- | -- | --; CollectBaseDistributionByCycleSpark | y | y | y; ASEReadCounter | y | y | y; CountBases | y | y | y; CountBasesSpark | y | y | y; CountReads | y | y | y; CountReadsSpark | y | y | y; PileupSpark | y | y | y; CollectInsertSizeMetricsSpark | y | y | y; CollectMultipleMetricsSpark | y | y | y; CollectQualityYieldMetricsSpark | y | y | y; CompareBaseQualities | y | y | y; EstimateLibraryComplexityGATK | y | y | y; FilterLongReadAlignmentsSAMSpark | y | y | y; FlagStat | y | y | y; FlagStatSpark | y | y | y; MeanQualityByCycleSpark | y | y | y; QualityScoreDistributionSpark | y | y | y; GatherBQSRReports | y | y | y; ApplyBQSR | y | y | y; ApplyBQSRSpark | y | y | y; BaseRecalibrato",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4094#issuecomment-356110640:966,Validat,ValidateBasicSomaticShortMutations,966,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4094#issuecomment-356110640,1,['Validat'],['ValidateBasicSomaticShortMutations']
Security,I checked that it runs now with picard 2.0.1 and GATK3.5 (validation fails though),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1921:58,validat,validation,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1921,1,['validat'],['validation']
Security,I cleaned up the mutect2 wdl and added multi-sample support. I also optimized resource usage and exposed the memory parameters: https://github.com/phylyc/gatk4-somatic-snvs-indels,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7532#issuecomment-1125321665:97,expose,exposed,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532#issuecomment-1125321665,1,['expose'],['exposed']
Security,"I created a branch in gatk-protected to address this issue. @lbergelson, @droazen -- could I get push access to the gatk-protected repo so I can push and submit a PR? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2717#issuecomment-302442385:102,access,access,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2717#issuecomment-302442385,1,['access'],['access']
Security,"I created a minimal branch to clean up the way we were passing around credentials. We create a GCSOptions class instead of a DataflowPipelineOptions when we create the pipeline and pass in secrets in at the point instead at the ReadSources level. ReadSources now takes a pipeline instead of the secrets file location. This isn't a long term solution. We should switch the code to get rid of the GenomicsSecret and instead use the more general secret. I think much of the secets factory junk can go away now (they dated from a time when the Dataflow API wasn't built out much. All tests passed locally. Oddly, I now am sometimes getting a dialog about DSDE needing access to basic information about my Google account, not sure source of the issue (maybe the secret I grabbed?), if it's repeatable, or blocking. I recommend the reviewer patch my branch and test locally.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/513:664,access,access,664,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/513,1,['access'],['access']
Security,I currently have a feature driven walker in gatk-protected that is slower than a read-walker equivalent due to the repetitive Read iterator re-instantiation when accessing overlapping reads using the ReadContext. . Ideally the engine (FeatureManager?) would try to reuse open read iterators instead of creating them for each feature/interval.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1246:162,access,accessing,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1246,1,['access'],['accessing']
Security,"I didn't expect Hellbender to crash for this command:. `$ ./hb PrintReads -I CEUTrio.HiSeq.WGS.b37.NA12878.bam -O 4m.bam -L 20:1000000-4000000`; (...); [August 17, 2015 2:04:43 PM PDT] org.broadinstitute.hellbender.tools.PrintReads done. Elapsed time: 0.48 minutes.; Runtime.totalMemory()=412090368; java.lang.IllegalArgumentException: end must be >= start. start:2801961 end:2801960; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:33); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:45); at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$19(ReadWalker.java:64); at org.broadinstitute.hellbender.engine.ReadWalker$$Lambda$50/1094674892.accept(Unknown Source); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:63); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:370); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:97); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:150); at org.broadinstitute.hellb",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/828:439,validat,validatePositions,439,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/828,1,['validat'],['validatePositions']
Security,"I didn't realize before that there is no ""include"" (opt-in) validation type arg, only ""exclude"". So I'm not sure what the purpose of having ""ALL"" is in the first place, if the only thing you can usefully do with it is exclude it. I think the best longer term fix would be to add an ""--validation-type-to-include"" arg, and have it default to the everything except for IDs, and then construct the actual types based on merging include/exclude args. But thats a bigger change then just fixing the current (silent do-nothing) default behavior, and requires more error checking for conflicting args. Lets start with changing it so that in the default (no args) case, we log a warning message saying that IDs will be left out since no IDS were provided, and proceed with the remaining validations. Then if we want to get more ambitious we can talk about making the bigger change. Also, as part of the initial fix, it might be a good idea to change `calculateValidationTypesToApply` so that it doesn't modify the `excludeTypes` list directly, since this is the list provided by the user, and instead uses it's own temporary list.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5862#issuecomment-498685279:60,validat,validation,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862#issuecomment-498685279,3,['validat'],"['validation', 'validation-type-to-include', 'validations']"
Security,"I discovered that one of the 345 input gvcfs failed VCF validation. When I removed that file and reran with no other changes, I did not get the ""terminate called without an active exception"" error. However, ImportGvcfs still fails; the failure seems to occur immediately after GenomicsDBImport logs success in importing all batches, in each shard. From all the Cromwell logs it looks like everything is working, but the top level workflow execution fails. I've been trying various configurations of memory, scatter count, and #nodes, so I don't have those log files around still. I can rerun with -DGATK_STACKTRACE_ON_USER_EXCEPTION=true and see if I get anything useful.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1295310651:56,validat,validation,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1295310651,1,['validat'],['validation']
Security,I don't know if it's the same issue but we have recently started seeing random 403 errors running dataproc jobs that appear to be internal to GCS dataproc services:. ```; ERROR: (gcloud.dataproc.jobs.submit.spark) HTTPError 403: cwhelan@broadinstitute.org does not have storage.objects.get access to dataproc-ed605f51-8ceb-44f7-b48c-a87bc588c1a6-us/google-cloud-dataproc-metainfo/dbfb2df5-060a-425e-ac31-77484354f264/jobs/0a5c53e9-f935-48f8-a39e-8a46d20b5ec9/driveroutput.000000010.; ```. For us the job keeps running on the dataproc cluster but the error crashes the client program that submitted the job.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3735#issuecomment-339034362:290,access,access,290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3735#issuecomment-339034362,1,['access'],['access']
Security,I don't think so -- using the API key introduces all sorts of security issues with sanitizing command lines. I think we just want to rely on the default Google credentials.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2402#issuecomment-288549958:62,secur,security,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2402#issuecomment-288549958,2,"['sanitiz', 'secur']","['sanitizing', 'security']"
Security,"I don't think that hiding/disable arguments would work in every case: sometimes, an argument shouldn't be exposed but still available to set programmatically, or maybe just reduce visibility making it `@Hidden` and/or `@Advance`. What is the problem of making an interface for the top-level argument to the GATK? Changing the interface or the `CommadnLineProgram` has the same effect, but the API user can still behave the same as before. It is much more extensible and downstream-friendly. What's about making the `CLPConfigurationArgumentCollection` an interface always returning defaults to be able to change it in a proper way? The cycle of development of a new argument will be: 1) add a new method to the interface with a default returning what will be expected from the previous behaviour, 2) add and return by the argument in the GATK implementation, 3) use the getter in the CLP for perform the operation. This only adds the first point, and operating in 3 classes instead of 3. For API user it is really easy to maintain the previous behavior when upgrading the dependency by just using their own implementation of the class, or include the top-level new arguments by using the GATK implementation. It is much more flexible and extensible (I always think about GATK also as a library). In addition, I think that this approach is also important for evolving GATK. For example, if a new top-level argument is tagged as experimental (still not supported but requested in Barclay), removing it would allow to keep the interface (no version bump) the same and final users can still operate with the experimental argument. The same applies to the `GATKTool` base class (https://github.com/broadinstitute/gatk/issues/4341), and for downstream projects the aim should be to be able to extend safely the `CommandLineProgram` directly to implement their own toolkit using the powerful GATK framework.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-366185003:106,expose,exposed,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-366185003,1,['expose'],['exposed']
Security,"I don't think that's the elusive bug we're looking for, but a bit more argument validation certainly cannot hurt. The gradle files look like they're auto-generated, I certainly didn't change them myself. Let me know if you think we should delete them instead.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2643#issuecomment-298450921:80,validat,validation,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2643#issuecomment-298450921,1,['validat'],['validation']
Security,"I don't think we've made any guarantees about the thread safety of Funcotator or the associated datasource classes. . Also, this account seems to be a bot and I can't access its listed home page…. I can audit the class at some point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7376#issuecomment-891860172:167,access,access,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7376#issuecomment-891860172,2,"['access', 'audit']","['access', 'audit']"
Security,"I found they were incompatible. It seems that the interface isn't match. The error log looks like below. Exception in thread ""main"" java.lang.NoSuchMethodError: scala.collection.Seq.aggregate(Ljava/lang/Object;Lscala/Function2;Lscala/Function2;)Ljava/lang/Object;; at org.bdgenomics.adam.models.NonoverlappingRegions.mergeRegions(NonoverlappingRegions.scala:75); at org.bdgenomics.adam.models.NonoverlappingRegions.<init>(NonoverlappingRegions.scala:55); at org.bdgenomics.adam.models.NonoverlappingRegions$.apply(NonoverlappingRegions.scala:169); at org.bdgenomics.adam.util.TwoBitRecord$.apply(TwoBitFile.scala:193); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.bdgenomics.adam.util.TwoBitFile.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:311); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2073:1089,Hash,HashMap,1089,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073,1,['Hash'],['HashMap']
Security,"I get tarballs from github, and then download dependencies and generate the intermediate tarball internally. We never clone git repositories as you might think due to security issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6395#issuecomment-584408522:167,secur,security,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6395#issuecomment-584408522,1,['secur'],['security']
Security,I got another report of something similar in the non-spark HaplotypeCaller; stack trace below. ````; java.lang.IllegalStateException: Duplicate key [B@42515a2f; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculationEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngine.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngine.java:187); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:520); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:239); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:244); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:217); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:779); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(Com,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018#issuecomment-310805959:251,Hash,HashMap,251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018#issuecomment-310805959,2,['Hash'],['HashMap']
Security,"I got it to work by using the runtime switch --disable-sequence-dictionary-validation . . If that is not used it crashes. . . Docker commandline. . /gatk Funcotator --disable-sequence-dictionary-validation \. -R mydata/refs/Homo_sapiens_assembly19.fasta \. -V mydata/P50513_mutect2_filtered.vcf \. -O mydata/P50513_mutect2_funcotator.maf \. --output-file-format MAF \. --data-sources-path mydata/dataSourcesFolder/funcotator_dataSources.v1.6.20190124s/ --ref-version hg19. . . . From: Louis Bergelson <notifications@github.com> ; Sent: Wednesday, October 30, 2019 10:26 AM; To: broadinstitute/gatk <gatk@noreply.github.com>; Cc: rdbremel <rdbremel017@gmail.com>; Mention <mention@noreply.github.com>; Subject: Re: [broadinstitute/gatk] Funcotator shuts down (#6182). . @rdbremel <https://github.com/rdbremel> This got missed in the churn of issues. Does this happen repeatedly or is it a 1 time occurrence? We've seen similar issues in the past and tried to wrap them all in layers of retries, but sometimes things slip through. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub <https://github.com/broadinstitute/gatk/issues/6182?email_source=notifications&email_token=ANCR2VB4ZCHMAJUHBKE2SP3QRGRQFA5CNFSM4I2MRFQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECUTZZI#issuecomment-547962085> , or unsubscribe <https://github.com/notifications/unsubscribe-auth/ANCR2VHRV5JESZYAYX55YHTQRGRQFANCNFSM4I2MRFQA> . <https://github.com/notifications/beacon/ANCR2VAS2WE5TDCUC6G5LETQRGRQFA5CNFSM4I2MRFQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECUTZZI.gif>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548102382:75,validat,validation,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548102382,2,['validat'],['validation']
Security,"I guess we need to do some work on the doclet code, abstract out example code and use templates to transform it into the appropriate format/syntax depending what project is generating the documentation. Alternatively and only if documentation html is well formed (xhtml like) then in theory we could use XSLT transformation style sheets to convert embedded code example encoded with xml/xhtml into the concrete syntax. Most major browsers support XSLT. EDIT: The XSLT solution won't probably work since even if we try to change the output from html to xhtml, the fact that we are injecting the javadoc's html would probably break the xhtml.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349724551:580,inject,injecting,580,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349724551,1,['inject'],['injecting']
Security,"I have a run going now. On Jan 4, 2018 15:32, ""samuelklee"" <notifications@github.com> wrote:. > Placeholders for now. We can tweak the actual values once @LeeTL1220; > <https://github.com/leetl1220> checks effect on validation.; >; > Closes #4032 <https://github.com/broadinstitute/gatk/issues/4032>.; > ------------------------------; > You can view, comment on, or merge this pull request online at:; >; > https://github.com/broadinstitute/gatk/pull/4046; > Commit Summary; >; > - Changed default values for ModelSegments segmentation parameters.; >; > File Changes; >; > - *M* src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > ModelSegments.java; > <https://github.com/broadinstitute/gatk/pull/4046/files#diff-0> (6); >; > Patch Links:; >; > - https://github.com/broadinstitute/gatk/pull/4046.patch; > - https://github.com/broadinstitute/gatk/pull/4046.diff; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4046>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk8coObtbYN125S1_BMBx1VnnmbF4ks5tHTVzgaJpZM4RTh_B>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4046#issuecomment-355405908:216,validat,validation,216,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4046#issuecomment-355405908,1,['validat'],['validation']
Security,"I have been able to get the connector working on GCP VMs where I have manually authenticated locally with my own account. I have not successfully gotten it working on a cromwell VM or ortherwise using manually supplied keyfiles. Anecdotal evidence, but its worth mentioning that both: `fs.gs.impl`; `fs.AbstractFileSystem.gs.impl`; seem to be optional for getting a run to work. It seems to have defaulted to the right things in the trials I've tested (though thats not to say the default will always work). I have put in a question on the issue tracker asking about available authentication inside a pipelines API VM.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5996#issuecomment-500846568:79,authenticat,authenticated,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5996#issuecomment-500846568,2,['authenticat'],"['authenticated', 'authentication']"
Security,"I have four phased variants in close proximity that have the following pattern:. ```; chrA 10 ... GT:PS 0|1:1; chrA 20 ... GT:PS 0|1:2; chrA 30 ... GT:PS 0|1:1; chrA 40 ... GT:PS 0|1:2; ```. These four variants are wholly contained in a single set of reads. There are of course other reads that partially span them. The first variant is a deletion, while the remaining three are SNVs.; Examining the reads, there are two haplotypes since:; 1. Alternate for the 1st and 3rd read; 2. Alternate for the 2nd and 4th read. I would have expected them all to have the same phase set (`PS`) value. I have a test case I can share privately (let me know a good email to send it to confidentially).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6845:671,confidential,confidentially,671,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6845,1,['confidential'],['confidentially']
Security,"I have just experienced the same problem of stack overflow with my first attempt to use gatk HaplotypeCallerSpark with the option --spark-master local[*]. My GATK version is 4.1.2.0,that was installed via bioconda. ; Should I wait for the corrected version or is there a way to circumvent the problem with extra install or by using options like --java-options '-XssOptimalValue'?; When is the corrected version expected? Is Q2 (end of June?) still an option? Will it be readily on bioconda then?; Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5869#issuecomment-499414227:362,Xss,XssOptimalValue,362,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869#issuecomment-499414227,1,['Xss'],['XssOptimalValue']
Security,"I have noticed that when running spark tools (e.g. CountReadsSpark or MarkDuplicatesSpark) that running with an input in the form ""CountReadsSpark -I gs://my-bucket-dir/my-file.bam."" The tool crashes with the following unhelpful stacktraces:. ```; java.io.IOException: Error getting access token from metadata server at: http://metadata/computeMetadata/v1/instance/service-accounts/default/token; 	at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:208); 	at com.google.cloud.hadoop.util.CredentialConfiguration.getCredential(CredentialConfiguration.java:70); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1825); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1012); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:975); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:500); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:469); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1084); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1072); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.SparkContext.withScope(SparkContext.scala:679); 	at org.ap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4369:283,access,access,283,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369,1,['access'],['access']
Security,"I have tested this with a fresh `gcloud` client and have not been able to reproduce the error. I did find an article from someone else who got the `400: invalid_grant` error: https://blog.timekit.io/google-oauth-invalid-grant-nightmare-and-how-to-fix-it-9f4efaf1da35. The long and short of it is that it's an authentication issue. Can you verify that the authentication you're using on the terminal is valid? That is, can you get at other public resources on gcloud?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-725097056:309,authenticat,authentication,309,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-725097056,2,['authenticat'],['authentication']
Security,"I have three main reasons to propose to move the arguments in CLP to an argument collection that is configurable by downstream tools/projects:. 1. Support hiding some arguments for downstream projects. For example, I do not want to support a config file by the user, but rather decide the settings for the framework and expose only some configuration.; 1. Set custom defaults for some downstream tools (including GATK). For example, a concrete tool might want to force the temp directory to be specified to avoid failures due to no space (and specify that in the documentation).; 1. Support old-style arguments (not kebab-case) for downstream projects that rely on the current argument definitions. I am specially affected by this one, because updating GATK to the 4.0.0 release of January will be a breaking change that will cause some nightmares for my users - and I don't want to do a major version bump yet (I have to re-work a bit my own framework before it). Thus, the first commit of this PR holds the proposal for the new argument collection. As I know that the team is also trying to normalize arguments and documentation, I included two more commits to help with the task (they can be removed if you think that it is better after the argument collection):; * Use `java.nio.Path` for temp directories (to support temp directories in HDFS, for example); * Change arguments moved to the collection to kebab-case (to help with #3853)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998:320,expose,expose,320,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998,1,['expose'],['expose']
Security,"I have to deal with this component recently and I found the design rather awkward.... In general between GATK and htsjdk we don't seem to have a proper support for managing and querying Supplementary alignment information from read alignment records:. 1. Querying: implemented in htsjdk consists in forging artificial SAMRecords that contain only the alignment info in the SA tag element... It seems to me that it makes more sense to create class to hold this information alone (e.g. ReadAlignmentInfo or ReadAlignment); SATagBuilder already has defined a private inner class with that in mind ""SARead"" so why not flesh it out and make it public. 2. Writing: currently SATagBuilder gets attached to a read, parsing its current SA attribute content into SARead instances. It provides the possibility adding additional SAM record one by one or clearing the list. ... then it actually updates the SA attribute on the original read when a method (setTag) is explicitly called.; I don't see the need to attach the SATag Builder to a read... it could perfectly be free standing; the same builder could be re-apply to several reads for that matter and I don't see any gain in hiding the read SA tag setting process,... even if typically this builder output would go to the ""SA"" tag, perhaps at some point we would like to also write SA coordinate list somewhere else, some other tag name or perhaps an error message... why impose this single purpose limitation?; I suggest to drop the notion of a builder for a more general custom ReadAlignmentInfo (or whatever name) list. Such list could be making reference to a dictionary to validate its elements, prevent duplicates, keep the primary SA in the first position... etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3324:1622,validat,validate,1622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3324,1,['validat'],['validate']
Security,I just added you so you should have access now.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2043#issuecomment-235077255:36,access,access,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2043#issuecomment-235077255,1,['access'],['access']
Security,I noticed some classes that were unused in hellbender.; This exposed some others that were only referenced by unused classes. Made slight cosmetic modification to OpticalDuplicateFinder as well,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/780:61,expose,exposed,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/780,1,['expose'],['exposed']
Security,"I noticed that `VcfUtils.getSortedSampleSet` takes a `GenotypeMergeType`. The only case it looks at is `UNIQIFY`. You would expect that calling `getSortedSampleSet(someHeaderWithDuplicateSamples, GenotypeMergeType.REQUIRE_UNIQUE)` should throw, but instead it silently continues. . ex, the following test passes just fine:; ```; @Test; public void testGetSortedSampleSet(){; final HashMap<String, VCFHeader> headers = new HashMap<>();; headers.put(""track1"", new VCFHeader(Collections.emptySet(), Sets.newSet(""sample1"")));; headers.put(""track2"", new VCFHeader(Collections.emptySet(), Sets.newSet(""sample1"")));. final SortedSet<String> sortedSampleSet = VcfUtils.getSortedSampleSet(headers, GATKVariantContextUtils.GenotypeMergeType.REQUIRE_UNIQUE);; }; ```. The method is also lacking any tests or javadoc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3744:381,Hash,HashMap,381,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3744,2,['Hash'],['HashMap']
Security,"I noticed that the PairHMM implementation argument is hidden in `LikelihoodEngineArgumentCollection` for some reason. Shouldn't it be exposed as an advanced argument people can choose what pair hmm they want?. It's also present in the `UnifiedArgumentCollection`, but it's never used from there. ```; /**; * The PairHMM implementation to use for genotype likelihood calculations. The various implementations balance a tradeoff of accuracy and runtime.; */; @Hidden; @Argument(fullName = ""pair_hmm_implementation"", shortName = ""pairHMM"", doc = ""The PairHMM implementation to use for genotype likelihood calculations"", optional = true); public PairHMM.Implementation pairHMM = PairHMM.Implementation.FASTEST_AVAILABLE;; ```. It seems like we should remove it from the `UnifiedArgumentCollection` and make it either a normal argument or an advanced argument in the `LikelihoodEngineArgumentCollection`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3553:134,expose,exposed,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3553,1,['expose'],['exposed']
Security,"I prepared a clean Bam file following GATK Best Practice and used GATK4 HaplotypeCaller to create a gvcf with ploidy1 option:. '''; gatk-4.0.2.1/gatk HaplotypeCaller --native-pair-hmm-threads 24 -I KU_filtered_sorted_mdup.bam -O HC.KU.raw.snps.indels.g.vcf -R ref.fasta -ploidy 1 --emit-ref-confidence GVCF; '''. When I validated the gvcf, ValidateVariants threw errors at the end:. '''; <br />11:27:55.681 INFO ProgressMeter - Traversal complete. Processed 124689522 total variants in 3.8 minutes.; 11:27:55.681 INFO ValidateVariants - Shutting down engine; [April 10, 2018 11:27:55 AM JST] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 3.82 minutes.; Runtime.totalMemory()=4682940416; java.lang.IllegalArgumentException: Illegal character in path at index 15:HC.KU.raw.snps.indels.g.vcf; at java.net.URI.create(URI.java:852); at org.broadinstitute.hellbender.engine.FeatureInput.makeIntoAbsolutePath(FeatureInput.java:242); at org.broadinstitute.hellbender.engine.FeatureInput.toString(FeatureInput.java:314); at java.util.Formatter$FormatSpecifier.printString(Formatter.java:2886); at java.util.Formatter$FormatSpecifier.print(Formatter.java:2763); at java.util.Formatter.format(Formatter.java:2520); at java.util.Formatter.format(Formatter.java:2455); at java.lang.String.format(String.java:2940); at org.broadinstitute.hellbender.engine.FeatureDataSource.close(FeatureDataSource.java:589); at org.broadinstitute.hellbender.engine.FeatureManager.lambda$close$9(FeatureManager.java:505); at java.util.LinkedHashMap$LinkedValues.forEach(LinkedHashMap.java:608); at org.broadinstitute.hellbender.engine.FeatureManager.close(FeatureManager.java:505); at org.broadinstitute.hellbender.engine.GATKTool.onShutdown(GATKTool.java:857); at org.broadinstitute.hellbender.engine.VariantWalker.onShutdown(VariantWalker.java:95); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4657:320,validat,validated,320,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4657,4,"['Validat', 'validat']","['ValidateVariants', 'validated']"
Security,"I ran GATK 4.1.0.0 Mutect2 on a small (~1Mb) targeted panel. I am using a normal control that is not the same individual (basically to exclude technical artifacts), so I do expect to see more variants than with a proper matched normal. I was getting around 100-300 variants per sample with GATK 4.0.6.0. I am still roughly in the same range for some samples GATK 4.1.0.0, but I am getting 0 for some. The problem seems to be at the FilterMutectCalls stage where I am seeing the following error:; ```; [March 19, 2019 10:43:17 PM EDT] org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=8851030016; java.lang.IllegalArgumentException: errorRate must be good probability but got NaN; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:227); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:211); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.applyContaminationFilter(Mutect2FilteringEngine.java:79); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.calculateFilters(Mutect2FilteringEngine.java:518); at org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls.firstPassApply(FilterMutectCalls.java:130); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.lambda$traverseVariants$0(TwoPassVariantWalker.java:76); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$For",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5821:801,validat,validateArg,801,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5821,1,['validat'],['validateArg']
Security,"I ran across a weird case the other day that I wanted to document here.; ` final Set<VCFHeaderLine> headerInfo = new HashSet<>();; headerInfo.addAll(getHeaderForVariants().getMetaDataInInputOrder());; headerInfo.add(GATKVCFHeaderLines.getFilterLine(GATKVCFConstants.LOW_HET_FILTER_NAME));; vcfWriter = createVCFWriter(new File(outputVcf));; vcfWriter.writeHeader(new VCFHeader(headerInfo));; `; Setting up the header this way end up with no genotypes in the output vcf. This is because the sample line is not included in the headerInfo, it is maintained as a separate field in the VCFHeader object. To resolve this issue the last line needed to be:; ` vcfWriter.writeHeader(new VCFHeader(headerInfo, getHeaderForVariants().getSampleNamesInOrder()));; `",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6048#issuecomment-582452360:117,Hash,HashSet,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6048#issuecomment-582452360,1,['Hash'],['HashSet']
Security,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests [failed](https://travis-ci.org/broadinstitute/gatk/builds/422089722), including a few of the Funcotator ones:. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance. These may be test artifacts, but we should audit the Funcotator cache access patterns and see if this is actually causing thrashing that affects performance. Since the FeatureCache caching strategy assumes queries will be forward-only, it might be an indication that Funcotator performance could be improved by either turning off caching or using an alternative cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5143:666,audit,audit,666,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5143,2,"['access', 'audit']","['access', 'audit']"
Security,I recommend we error out when provided with `--validation-type-to-exclude ALL`. It doesn't make sense - why call the validator if you're not going to validate?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5862#issuecomment-485714312:47,validat,validation-type-to-exclude,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862#issuecomment-485714312,3,['validat'],"['validate', 'validation-type-to-exclude', 'validator']"
Security,"I reproduced various out of memory errors in a Linux VM with 4G of RAM, both with the `IntelInflaterDeflaterIntegrationTest` enabled and disabled. Most resulted in the kernel killing the Java process, like this one (from `dmesg`):; ```; [38425.759992] Out of memory: Kill process 10295 (java) score 747 or sacrifice child; [38425.759998] Killed process 10295 (java) total-vm:7885212kB, anon-rss:3250892kB, file-rss:0kB; ```. Some were caught by the JVM, like this one:; ```; #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 90177536 bytes for committing reserved memory.; # Possible reasons:; # The system is out of physical RAM or swap space; # In 32 bit mode, the process size limit was hit; # Possible solutions:; # Reduce memory load on the system; # Increase physical memory or swap space; # Check if swap backing store is full; # Use 64 bit Java on a 64 bit OS; # Decrease Java heap size (-Xmx/-Xms); # Decrease number of Java threads; # Decrease Java thread stack sizes (-Xss); # Set larger code cache with -XX:ReservedCodeCacheSize=; # This output file may be truncated or incomplete.; #; # Out of Memory Error (os_linux.cpp:2627), pid=20484, tid=139679452493568; #; # JRE version: Java(TM) SE Runtime Environment (8.0_72-b15) (build 1.8.0_72-b15); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.72-b15 mixed mode linux-amd64 compressed oops); # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; ```. Here's my theory of what's happening. The `maxHeapSize` for test JVMs is set to 4G in `build.gradle`:; ```; maxHeapSize = ""4G""; ```. A 4G max heap size is too high for systems with 4G of RAM, because the Java heap grows until the system runs out of memory. If we decrease `maxHeapSize`, the GC should prevent the Java heap from growing too large, with the trade-off of more GC calls. I changed the `maxHeapSize` to `2G` a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2490#issuecomment-288423316:1059,Xss,Xss,1059,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2490#issuecomment-288423316,1,['Xss'],['Xss']
Security,"I reran the workflow, this time allocating 200GB RAM to tmp in the slurm job, everything else exactly the same, and got the ""terminate called without an active exception"" failure again, so that error was not due to the gvcf that failed VCF validation as it was not included. This time, shard 9 succeeded in ImportGvcfs and also successfully completed GenotypeGVCFs. I have attached the top level stdout and stderr logs for the slurm job, and the stdout.background and stderr.background logs from shard 3 of ImportGvcfs. No java error log was present on any of the ImportGvcfs shards' execution directories, and all except shard 9 had rc=250.; [ImportGvcfsWithTmpError.tar.gz](https://github.com/broadinstitute/gatk/files/9911311/ImportGvcfsWithTmpError.tar.gz)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1298661994:240,validat,validation,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1298661994,1,['validat'],['validation']
Security,"I see that this is occurring in the mitochondrial chromosome. The model was trained mostly with the autosomes and so scores on the mitochondrial DNA have not been extensively validated. That said, this looks like a bug and we hope to have a fix in soon.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387737569:175,validat,validated,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387737569,1,['validat'],['validated']
Security,"I see the exception on most chromosomes, here's the one for chr1:. java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:79293873 end:79293872; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:49); at org.broadinstitute.hellbender.engine.AssemblyRegion.add(AssemblyRegion.java:335); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.fillNextAssemblyRegionWithReads(AssemblyRegionIterator.java:230); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:194); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:135); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:34); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:290); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:271); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:275)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4120#issuecomment-356718095:207,validat,validateArg,207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4120#issuecomment-356718095,2,['validat'],"['validateArg', 'validatePositions']"
Security,"I see, thanks for pointing this out. We need to run; ```; git archive --format tar.gz --prefix gatk-{hash} -o gatk-{hash}.tar.gz {hash}; ```; to retrieve and archive the files.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6395#issuecomment-584477699:101,hash,hash,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6395#issuecomment-584477699,3,['hash'],['hash']
Security,"I set TEST_TYPE to ""all"" and was able to run this test without failure. The command I used is:; ```; ./gradlew test --tests org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile; ```; I ran it 10 times and got the same result every time:; `BUILD SUCCESSFUL`. It looks like this was a transient problem: either the internet connection was stalled or the authentication server was down temporarily. As this happens at the very beginning of the execution, it's probably not a very big deal: the user can just retry. Incidentally, PR #2506 that is under review lengthens the connection timeouts, if I am not mistaken. This will probably make the problem less likely to reoccur.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2514#issuecomment-288852260:376,authenticat,authentication,376,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514#issuecomment-288852260,1,['authenticat'],['authentication']
Security,"I spoke too soon. No matter how I define the type, whether String or File, when I run womtools validate I get the error:. ```; ERROR: Value for this attribute is expected to be a string:. bam: {; ```. I added the following parameter_meta field to the task:. ![screenshot 2018-11-08 18 02 03](https://user-images.githubusercontent.com/11543866/48232693-7569ff00-e380-11e8-9dad-2eed3ca68118.png). How to correct this @cjllanwarne? Here's the relevant WDL: https://github.com/broadinstitute/gatk/blob/4.0.11.0/scripts/cnv_wdl/cnv_common_tasks.wdl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437188313:95,validat,validate,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437188313,1,['validat'],['validate']
Security,"I suppressed the warnings we were getting. If we can't fix them lets at least not see them.; It seemed like it was ok to suppress the serialization warnings rather than provide a UUID, since java will fill one in for us. We can add a hash value instead if that's better.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/84:234,hash,hash,234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/84,1,['hash'],['hash']
Security,"I think even more important is that they share the reference genome that they are using. Get Outlook for Android<https://aka.ms/AAb9ysg>; ________________________________; From: Gökalp Çelik ***@***.***>; Sent: Wednesday, April 24, 2024 12:28:39 AM; To: broadinstitute/gatk ***@***.***>; Cc: Ilya Soifer ***@***.***>; Assign ***@***.***>; Subject: Re: [broadinstitute/gatk] Prevent users enabling annotations with mismatching data type (flow etc) (Issue #8788). Assigned #8788<https://github.com/broadinstitute/gatk/issues/8788> to @ilyasoifer<https://github.com/ilyasoifer>. —; Reply to this email directly, view it on GitHub<https://github.com/broadinstitute/gatk/issues/8788#event-12581899218>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AGDPRCP66IKPOBF2GPENP6LY63HAPAVCNFSM6AAAAABGTGMBPWVHI2DSMVQWIX3LMV45UABCJFZXG5LFIV3GK3TUJZXXI2LGNFRWC5DJN5XDWMJSGU4DCOBZHEZDCOA>.; You are receiving this because you were assigned.Message ID: ***@***.***>. ________________________________. CONFIDENTIALITY NOTICE: This message (including any attachments) should be presumed to contain confidential, proprietary, privileged and/or private information. Information contained in this message is intended only for the recipient(s) named above. Any disclosure, reproduction, distribution or other use of this message or any attachments by any unauthorized individual or entity is strictly prohibited. If you have received this message in error, please notify the sender immediately, and delete the message and any attachments. Learn more about Ultima Genomics’ Privacy Policy<https://www.ultimagenomics.com/privacy-policy>.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8788#issuecomment-2074020625:1003,CONFIDENTIAL,CONFIDENTIALITY,1003,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8788#issuecomment-2074020625,2,"['CONFIDENTIAL', 'confidential']","['CONFIDENTIALITY', 'confidential']"
Security,"I think for SQ you could limit the lines you write out to the contigs that are covered in your intervals list, ignore the rest. So you can access that info as soon as you've parsed the command line. Or if you're running without an intervals list you could supply a seq dict file through a separate arg. But the former seems safer. . For other modes: EXTREME would hardcode what we consider required. Then you could potentially do ARBITRARY to allow passing strings for specific attributes that you want to drop. . None of these would have to depend on what's in the calls, I think.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2233#issuecomment-266059687:139,access,access,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2233#issuecomment-266059687,1,['access'],['access']
Security,"I think it triggers in certain situations where a firewall is blocking the connection. If the internet is simply unreachable it doesn't happen, so I don't know what the exact error case is. It happened consistently for people inside Intel's firewall or vpn. . An option to disable gcs support isn't a bad idea, it's kind of a hack though, it would be better if we could understand and avoid triggering the problem. If we could only initialize GCS support when we are sure that we actually are accessing files from google that could be a useful, but it doesn't seem like there's any single point we can plug into to detect that, it would have to be spread over everything that uses paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427432141:50,firewall,firewall,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427432141,6,"['access', 'firewall']","['accessing', 'firewall']"
Security,"I think that each tool should either emit proper error messages, or deal with the data it's given. currently BQSR emits a cryptic error message so I think that this PR is an improvement regardless of whether the pipeline is sanitized. . That said, I think that it might be a good idea to make ValidateSamFile break on non-ACGTN bases.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6625#issuecomment-642719900:224,sanitiz,sanitized,224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6625#issuecomment-642719900,2,"['Validat', 'sanitiz']","['ValidateSamFile', 'sanitized']"
Security,"I think that it is necessary to have a way for downstream projects to override some of the top-level arguments in the base CLP class. For example, the config file is for documentation purposes, but I don't want to expose users to that argument because I will set the defaults programmatically. Another example is the GCS retries, which might not be useful for a software that is not planning to support GCS even if it is already implemented (or does not want to expose). As a downstream developer, for me it is important to being able to configure arguments and expose/hide them to my final users; with the current implementation, my main issue is to have an argument that are irrelevant for the toolkit user and that I get questions about why and how to use them (the most clear example, the config file). If the main problem is to change an interface, a default value for new methods can be added to keep the same behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-361876183:214,expose,expose,214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-361876183,6,['expose'],['expose']
Security,"I think that it would be fine to have both legacy and modern styles in the; examples, but I think having gatk in the picard docs will not pass review. On Tue, Dec 5, 2017 at 10:41 PM, sooheelee <notifications@github.com> wrote:. > It would be nice to have a few Picard tools that we feature in BPWs (e.g.; > MarkDuplicates) show both syntaxes:; >; > 1. java -jar picard.jar ValidateSamFile I=reads.bam MODE=SUMMARY; > 2. gatk ValidateSamFile -I reads.bam -MODE=SUMMARY; >; > I assume MODE gets one dash and not two, but really I don't know. So; > having such examples scattered throughout the tool chest for most often; > used tools would be great.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349522774>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0ht41UsGHu_2TgrbKKNJwDepEdMZks5s9gz4gaJpZM4QitCF>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349642831:374,Validat,ValidateSamFile,374,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349642831,2,['Validat'],['ValidateSamFile']
Security,"I think that this is a nice feature (at least for me) and not a bug. For example, if in GATK someone runs a tool with `-RF read_filters.args`, then the pipeline cannot be reproduced in a different dataset unless the file is accessible. I can understand that it could be also nice to preserve the `-RF read_filters.args` to be able to modify the file an re-run the tool with different parameters, but for me the purpose of storing the command line in the header or other places is keep track of the exact params that I used: if a file is modified, then it is impossible to trace the params. For input files, this is expected (if the input has changed, it is expected that the result change), but for arguments it shouldn't be the case (independently of the file changing, the tool was running with exactly that parameters). I vote for solve this in Barclay in a configurable way, to allow users to decide which kind of verbosity of the command line they want (I definitely prefer to expand as currently).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3797#issuecomment-342798092:224,access,accessible,224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3797#issuecomment-342798092,1,['access'],['accessible']
Security,"I think the issue might be that you need a ` -- ` between the gatk options on the left, and the spark specific options on the right. This is a confusing artifact of how our arg parsing works and the fact that the gatk-launch script needs a way of finding the spark options but doesn't have access to our java parser. (we're planning on fixing that in the near future, but no good time line) . Could you try:; ```; /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input hdfs://spark01:7222/user/axverdier/data/710-PE-G1.bam --output hdfs://spark01:7222/user/axverdier/testOutGATK_CountReadsSpark --javaOptions -Dmapr.library.flatclass -- --sparkRunner SPARK --sparkMaster yarn --deploy-mode cluster; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350064538:290,access,access,290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350064538,1,['access'],['access']
Security,I think we need someone with admin access to do this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/740:35,access,access,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/740,1,['access'],['access']
Security,I took over #5367 but since I don't have access to your fork @magicDGS I have created a second branch with my changes. Closes #4860,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5655:41,access,access,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5655,2,['access'],['access']
Security,"I tried clearing my caches and rebuilding, but I resolve everything. I noticed that our artifactory website looks much different today than it did yesterday. I wonder if it was down temporarily for an update. Maybe try again now? Unless they put it behind the firewall which would be a disaster... can you access https://artifactory.broadinstitute.org/?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641:260,firewall,firewall,260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641,4,"['access', 'firewall']","['access', 'firewall']"
Security,"I tried this branch out and got the dreaded 404 error, unfortunately:. ```; $ ./gatk-launch CountReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -- --sparkRunner GCS --cluster droazen-test-cluster --executor-cores 2 --num-executors 2; Using GATK jar /Users/droazen/src/hellbender/build/libs/gatk-package-4.beta.6-54-g0ee99da-SNAPSHOT-spark.jar; jar caching is disabled because GATK_GCS_STAGING is not set. please set GATK_GCS_STAGING to a bucket you have write access too in order to enable jar caching; add the following line to you .bashrc or equivalent startup script. export GATK_GCS_STAGING=gs://<my_bucket>/. Replacing spark-submit style args with dataproc style args. --cluster droazen-test-cluster --executor-cores 2 --num-executors 2 -> --cluster droazen-test-cluster --properties spark.driver.userClassPathFirst=true,spark.io.compression.codec=lzf,spark.driver.maxResultSize=0,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.kryoserializer.buffer.max=512m,spark.yarn.executor.memoryOverhead=600,spark.executor.cores=2,spark.executor.instances=2. Running:; gcloud dataproc jobs submit spark --cluster droazen-test-cluster --properties spark.driver.userClassPathFirst=true,spark.io.compression.codec=lzf,spark.driver.maxResultSize=0,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994:504,access,access,504,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994,1,['access'],['access']
Security,"I was able to download the gnomAD VCFs with a gsutil cp command within a; couple hours of the remote funcotator failures. --; Alan Hoyle - alan@alanhoyle.com - alanhoyle.com; ------------------------------; *From:* Jonn Smith <notifications@github.com>; *Sent:* Tuesday, November 10, 2020 9:59:06 PM; *To:* broadinstitute/gatk <gatk@noreply.github.com>; *Cc:* Alan Hoyle <alan@alanhoyle.com>; Mention <mention@noreply.github.com>; *Subject:* Re: [broadinstitute/gatk] Funcotator with gnomAD enabled crashes; with Bad Request (#6926). I have tested this with a fresh gcloud client and have not been able to; reproduce the error. I did find an article from someone else who got the 400: invalid_grant; error:; https://blog.timekit.io/google-oauth-invalid-grant-nightmare-and-how-to-fix-it-9f4efaf1da35; <https://www.google.com/url?q=https://blog.timekit.io/google-oauth-invalid-grant-nightmare-and-how-to-fix-it-9f4efaf1da35&source=gmail-imap&ust=1605668351000000&usg=AOvVaw03ZXI9QiPy1AFI5zfsFIjB>. The long and short of it is that it's an authentication issue. Can you; verify that the authentication you're using on the terminal is valid? That; is, can you get at other public resources on gcloud?. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub; <https://www.google.com/url?q=https://github.com/broadinstitute/gatk/issues/6926%23issuecomment-725097056&source=gmail-imap&ust=1605668351000000&usg=AOvVaw0sZboplCCqclv3xBdQk3Fb>,; or unsubscribe; <https://www.google.com/url?q=https://github.com/notifications/unsubscribe-auth/AACGX433BU42UPHZTKQLTBTSPH4XVANCNFSM4TD2FDGA&source=gmail-imap&ust=1605668351000000&usg=AOvVaw0n415Vb2d-0gOnEk9wramu>; .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-728261716:1038,authenticat,authentication,1038,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-728261716,2,['authenticat'],['authentication']
Security,I wasn't able to reproduce this exact issue. However at least I can make the error message a bit more user-friendly (submitting #2417 for review). Crucially this will now give the name of the file we're having issues with (thus removing the uncertainty about whether it's the data or the index file we cannot access).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281515286:309,access,access,309,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281515286,2,['access'],['access']
Security,"I would like to keep in some of my tools the read group arguments in sync with the `AddOrReplaceReadGroup` in picard, but currently there is no way of access them. This is a very simple and trivial patch to extract the short/long names to a static String variable to be able to use them. In addition, I refactored the variable names to the camel-case java convention.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2260:151,access,access,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2260,1,['access'],['access']
Security,"I would like to specify what passing a `ReadFilter` to some of my tools means, so maybe passing an `ArgumentCollection` will be simpler than this one, I agree. Although #2085 may solve the issue regarding the `ReadTransformer`/`ReadFilter` ordering, I would like to have in the plugin a way to specify different parameters (maybe some of then hidden before expose to users or advanced in the case of disabling). I will open a new PR for that change, but I will really appreciate if I can get something like that in this and other plugins (if implemented).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-275082983:357,expose,expose,357,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-275082983,2,['expose'],['expose']
Security,"I'd also be hesitant to break the previous expectation that IntervalArgumentCollection contains a non-empty list of intervals. If I understand correctly (and apologies if not, I'm glancing at the repo between paternity-leave duties and am quite sleep deprived!), all calling code would have to add an explicit check that the new option isn't enabled or risk failing ungracefully downstream. For CNV code, this might be as simple as changing the validation method `CopyNumberArgumentValidationUtils.validateIntervalArgumentCollection`, but I wouldn't generally expect it to be so straightforward to add such checks throughout the codebase. I also agree with @lbergelson that the expected behavior might not be immediately clear and that perhaps this could be addressed in the scattering step---seems like shards could just be limited to regions that cover the resource at the outset. Consider also an older comment at https://github.com/broadinstitute/gatk/pull/5392#issuecomment-435588845 about whether or not we should just use the equivalent Picard tool (horrible glob aside).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6209#issuecomment-540740687:445,validat,validation,445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6209#issuecomment-540740687,4,['validat'],"['validateIntervalArgumentCollection', 'validation']"
Security,I'll add: I suspect that this could get done pretty quickly/painlessly if @lbergelson (our travis expert) and yourself (who has set up tests like this before many times) got together in a room and hashed it out. Recommend setting up a meeting with @lbergelson at a time that's convenient in the next week or two. I'll post command lines here by early next week.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2298#issuecomment-287478818:197,hash,hashed,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2298#issuecomment-287478818,1,['hash'],['hashed']
Security,I'm actually a little surprised that it's not emitting a 1/* at the second location (despite that perhaps not being compatible with the spec; I agree with @bhandsaker 's view of what `*` should mean) given the changes I made in https://github.com/broadinstitute/gatk/pull/4963 to support spanning deletions. Those changes were not made with MNPs in mind but reading the code I'm surprised the `*` allele is not injected into the variant context. What version of HaplotypeCaller are you using @tfenne?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5523#issuecomment-447369490:411,inject,injected,411,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5523#issuecomment-447369490,1,['inject'],['injected']
Security,"I'm also seeing this more often during the Docker build, not sure if it is related:. ````; Step 5/27 : RUN /gatk/gradlew clean compileTestJava installAll localJar createPythonPackageArchive -Drelease=$DRELEASE; ---> Running in d08cd7336c45; Downloading https://services.gradle.org/distributions/gradle-3.1-bin.zip; .......................................; Exception in thread ""main"" javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.SSLSocketImpl.checkEOF(SSLSocketImpl.java:1541); 	at sun.security.ssl.AppInputStream.available(AppInputStream.java:60); 	at java.io.BufferedInputStream.available(BufferedInputStream.java:410); 	at sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at org.gradle.wrapper.Download.downloadInternal(Download.java:77); 	at org.gradle.wrapper.Download.download(Download.java:44); 	at org.gradle.wrapper.Install$1.call(Install.java:61); 	at org.gradle.wrapper.Install$1.call(Install.java:48); 	at org.gradle.wrapper.ExclusiveFileAccessManager.access(ExclusiveFileAccessManager.java:69); 	at org.gradle.wrapper.Install.createDist(Install.java:48); 	at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:107); 	at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401:521,secur,security,521,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401,2,['secur'],['security']
Security,"I'm getting the same issue on GATK 4.1.9.0 FilterAlignmentArtifacts. This bug has been present for 1 year. Has this been fixed?; Note: There is no work-around because FilterAlignmentArtifacts does not have a --smith-waterman option. Here is my error:; ```; 20:12:42.724 WARN FilterAlignmentArtifacts - . [1m[31m !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![0m. 20:12:42.725 INFO FilterAlignmentArtifacts - Initializing engine; 20:12:48.403 INFO FeatureManager - Using codec VCFCodec to read file gs://fc-secure-024a1aae-a4f9-4025-aa93-f759f93a8203/50383670-4607-4e59-9bfc-4db970980f0e/Mutect2/773a91ea-25be-4d49-b97c-16527076250c/call-Filter/cacheCopy/TN-20-36-filtered.vcf; 20:12:50.117 INFO FilterAlignmentArtifacts - Done initializing engine; 20:12:51.042 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 20:12:51.099 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 20:12:51.100 INFO IntelPairHmm - Available threads: 14; 20:12:51.100 INFO IntelPairHmm - Requested threads: 4; 20:12:51.100 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 20:12:51.100 INFO ProgressMeter - Starting traversal; 20:12:51.100 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 20:20:25.766 INFO ProgressMeter - chr3:104142090 7.6 1000 132.0; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007efc9818177e, pid=24, tid=0x00007f13b3c76700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # C [libgkl_smithwaterman18",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098:682,secur,secure-,682,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098,1,['secur'],['secure-']
Security,"I'm glad it's working now, but a PS since you asked about `-independent-mates`: several months ago we made Mutect2 force paired reads to share the latent random variable indicating which haplotype they are derived from in the somatic genotyping model. This is correct because paired reads come from the same molecule of DNA. `-independent-mates` disables this and tells Mutect2 to forget about pairing. We only created the option because some synthetic validation data is generated by spiking in variation without regard to pairing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6230#issuecomment-596165833:453,validat,validation,453,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6230#issuecomment-596165833,1,['validat'],['validation']
Security,I'm informed that I have access to the broad-firecloud-dsde billing account and it appears whatever error we encountered the other day was something transient.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437409513:25,access,access,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437409513,1,['access'],['access']
Security,I'm kind of glad that 4.1 exposed this because previously unmatched pairs were silently giving wildly-inflated contamination estimates.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5880#issuecomment-483083398:26,expose,exposed,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5880#issuecomment-483083398,1,['expose'],['exposed']
Security,"I'm looking into migrating custom GATK3 variant Info/GenotypeAnnotations to GATK4. The annotate() method in GATK3 was passed a sizable amount of context. This is greatly reduced in GATK4. I understand a desire to simplify, such as not passing the Walker. FeatureContext in particular would be helpful, is there another way to access that from VariantAnnotations?. Stepping back: the one scenario I want to support is to annotate genotype concordance between the input VCF and a reference VCF. In our GATK3 implementation, the user supplied that VCF on the command line when executing VariantAnnotator. This plugin used GATK3's walker.getResourceRodBindings(), which seems analogous to GATK4 FeatureContext, to find that binding. It then queries that VCF to find any VariantContext from the current site. . I realize this is raising a couple issues: a) access FeatureContext from within annotate(), , b) efficiently query VariantContext from another resource, and c) plugin that would ideally provide its own command-line argument. . Are there any existing GATK annotations or other plugins that deal with these issues?. Thanks in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6930:326,access,access,326,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6930,2,['access'],['access']
Security,"I'm new at this, so I may be missing something, but it looks to me like there is an issue with the conversion code in GenomicsConverter.makeSAMRecord() in com.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:823,validat,validation,823,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033,1,['validat'],['validation']
Security,I'm not sure I have access to that bucket -- which project is it associated with?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2658#issuecomment-299480244:20,access,access,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2658#issuecomment-299480244,1,['access'],['access']
Security,"I'm not sure at this point. ; Is it possible the issue would also occur if the bam passes ValidateSamFile, and the intervals file is sorted, but the 2 have different contig orderings?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6065#issuecomment-591651451:90,Validat,ValidateSamFile,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6065#issuecomment-591651451,1,['Validat'],['ValidateSamFile']
Security,"I'm not sure exactly what's happening but I suspect it has something to do with the way the files are mounted. My guess is that there is some sort of transient interruption happening in the connection between the EC2 instance and the file server, and it's causing an error in gatk. When reading from a local file GATK does not expect any errors since errors in local files are usually fatal problems caused by a broken disk. Its probably some sort of bug in amazon's fuse implementation which isn't properly hiding network problems from the software. . I expect that your output is truncated at the point the error occured, and you probably need to rerun those shards. Instead of mounting them with amazon's fuse, you could try to either copy the files to a local disc, or access them using an NIO filesystem plugins like this plugin https://github.com/awslabs/aws-java-nio-spi-for-s3 or as signed URLs using https://github.com/broadinstitute/http-nio/ (included in gatk 4.6).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8735#issuecomment-2214915942:773,access,access,773,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735#issuecomment-2214915942,1,['access'],['access']
Security,"I'm not sure what went wrong exactly, I might have messed up something when I did the travis encrypt command. I've deleted the previous one, regenerated a new-new key, and reencrypted it. Hopefully this time it will work.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5308#issuecomment-430319851:93,encrypt,encrypt,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5308#issuecomment-430319851,1,['encrypt'],['encrypt']
Security,"I'm pretty sure this is a hadoop-bam issue, but I'm finding that any BAM produced by bwa (VN 0.7.16a-r1181) will not load in Spark. The BAM loads successfully in ValidateSamFile (although it throws errors because there are no RGs). Running it through AddOrReplaceReadGroups makes the error go away. Attempting to load from local disk gives the following error:. `htsjdk.samtools.SAMFormatException: Does not seem like a BAM file; 	at org.seqdoop.hadoop_bam.BAMSplitGuesser.<init>(BAMSplitGuesser.java:88); 	at org.seqdoop.hadoop_bam.BAMInputFormat.addProbabilisticSplits(BAMInputFormat.java:228); 	at org.seqdoop.hadoop_bam.BAMInputFormat.getSplits(BAMInputFormat.java:155); 	at org.seqdoop.hadoop_bam.AnySAMInputFormat.getSplits(AnySAMInputFormat.java:252); 	at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:121); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); 	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); 	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); 	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); 	at scala.Option.getOrE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3488:162,Validat,ValidateSamFile,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3488,1,['Validat'],['ValidateSamFile']
Security,"I'm trying to run Mutect2 in tumor-only mode, for a small panel, and I get this errors at the FilterMutectCalls step. ```bash; [July 26, 2019 9:34:50 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=2129657856; java.lang.IllegalArgumentException: errorRate must be good probability but got NaN; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:225); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:209); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.lambda$applyFiltersAndAccumulateOutputStats$13(Mutect2FilteringEngine.java:176); at java.util.Optional.ifPresent(Optional.java:159); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.applyFiltersAndAccumulateOutputStats(Mutect2FilteringEngine.java:174); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:142); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6058:435,validat,validateArg,435,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6058,1,['validat'],['validateArg']
Security,"I'm using GATK 4.2.1.0-0 tool `Mutect2` to call mutations in a mitochondrion genome, and later processing the VCFs with `FilterMutectCalls` enabling as well the mitochondria mode (`--mitochondria-mode true`). For some reason, this results in **some** of the VCFs to return the following error:. > java.lang.IllegalArgumentException: log10p: Log10-probability must be 0 or less; > 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); > 	at org.broadinstitute.hellbender.utils.MathUtils.log10BinomialProbability(MathUtils.java:646); > 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:639); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.lambda$calculateQuantileBackgroundResponsibilities$10(SomaticClusteringModel.java:271); > 	at org.broadinstitute.hellbender.utils.MathUtils.applyToArray(MathUtils.java:1035); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.calculateQuantileBackgroundResponsibilities(SomaticClusteringModel.java:271); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.initializeClusters(SomaticClusteringModel.java:165); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:325); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:153); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.afterNthPass(FilterMutectCalls.java:165); > 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:44); > 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8455:426,validat,validateArg,426,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8455,1,['validat'],['validateArg']
Security,"I'm working on an imputation pipeline right now, and the contigs in the returned VCF header don't contain lengths. This fix to UpdateVCFSequenceDictionary allows me to force an update to the VCF's sequence dictionary so I have a valid VCF I can use with the rest of our tools when both --replace and --disable-sequence-dictionary-validation are set to true.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6140:330,validat,validation,330,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6140,1,['validat'],['validation']
Security,"IET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Defaults.BUFFER_SIZE : 131072; 17:43:53.162 INFO ValidateVariants - Defaults.COMPRESSION_LEVEL : 1; 17:43:53.162 INFO ValidateVariants - Defaults.CREATE_INDEX : false; 17:43:53.163 INFO ValidateVariants - Defaults.CREATE_MD5 : false; 17:43:53.163 INFO ValidateVariants - Defaults.CUSTOM_READER_FACTORY :; 17:43:53.163 INFO ValidateVariants - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 17:43:53.163 INFO ValidateVariants - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:43:53.163 INFO ValidateVariants - Defaults.REFERENCE_FASTA : null; 17:43:53.163 INFO ValidateVariants - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:43:53.163 INFO ValidateVariants - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:43:53.163 INFO ValidateVariants - Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:43:53.163 INFO ValidateVariants - Deflater IntelDeflater; 17:43:53.163 INFO ValidateVariants - Inflater IntelInflater; 17:43:53.163 INFO ValidateVariants - Initializing engine; 17:43:53.270 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.287 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.291 WARN IndexUtils - Feature file ""/Users/louisb/Workspace/gatk/src/test/resour",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2509:2664,Validat,ValidateVariants,2664,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509,1,['Validat'],['ValidateVariants']
Security,"INDEL informative reads based on the reference confidence model"">; ##FORMAT=<ID=MB,Number=4,Type=Integer,Description=""Per-sample component statistics to detect mate bias"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=PRI,Number=G,Type=Float,Description=""Phred-scaled prior probabilities for genotypes"">; ##FORMAT=<ID=PS,Number=1,Type=Integer,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias"">; ##FORMAT=<ID=SPL,Number=.,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for SNPs based on the reference confidence model"">; ##FORMAT=<ID=SQ,Number=A,Type=Float,Description=""Somatic quality"">; ##DRAGENCommandLine=<ID=HashTableBuild,Version=""SW: 01.003.044.3.8.4, HashTableVersion: 8"",CommandLineOptions=""/opt/edico/bin/dragen --build-hash-table true --enable-cnv true --ht-alt-aware-validate true; ##DRAGENCommandLine=<ID=dragen,Version=""SW: 05.021.609.3.9.5, HW: 05.021.609"",Date=""Wed Feb 09 21:30:31 UTC 2022"",CommandLineOptions=""--bam-input s3://cromwell-dragen-us-east-1/samples/validacaoDRAGEN/ba; ```. About chrM, yeah... all of them have this info... like below. ```; grep ""^chrM"" <name>.hard-filtered.gvcf | head ; chrM	1	.	G	<NON_REF>	.	weak_evidence	END=1	GT:AD:DP:SQ:MIN_DP	0/0:112,1579:1691:0:1691; chrM	2	.	A	<NON_REF>	.	PASS	END=72	GT:AD:DP:SQ:MIN_DP	0/0:2504,2:2506:99:1712; chrM	73	.	A	G,<NON_REF>	.	PASS	DP=2017;MQ=208.10;FractionInformativeReads=0.948	GT:SQ:AD:AF:F1R2:F2R1:DP:SB:MB	1/1:98.13,0.00:0,1912,0:1.000,0.000:0,980,0:0,932,0:1912:0,0,756,1156:0,0,932,980; ```. As I replied to you, in another section,",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7797#issuecomment-1112612397:2359,Hash,HashTableBuild,2359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7797#issuecomment-1112612397,1,['Hash'],['HashTableBuild']
Security,"INDEX false --CREATE_MD5_FILE false --help false --version false --verbosity INFO --QUIET false; [March 9, 2017 7:03:42 PM EST] Executing as gspowley@dna on Linux 3.10.0-514.10.2.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_111-b14; Version: Version:4.alpha.2-170-g8d06823-SNAPSHOT; 19:03:42.998 INFO ValidateSamFile - Defaults.BUFFER_SIZE : 131072; 19:03:42.999 INFO ValidateSamFile - Defaults.COMPRESSION_LEVEL : 1; 19:03:42.999 INFO ValidateSamFile - Defaults.CREATE_INDEX : false; 19:03:42.999 INFO ValidateSamFile - Defaults.CREATE_MD5 : false; 19:03:42.999 INFO ValidateSamFile - Defaults.CUSTOM_READER_FACTORY : ; 19:03:42.999 INFO ValidateSamFile - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 19:03:42.999 INFO ValidateSamFile - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 19:03:42.999 INFO ValidateSamFile - Defaults.REFERENCE_FASTA : null; 19:03:43.000 INFO ValidateSamFile - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_CRAM_REF_DOWNLOAD : false; 19:03:43.000 INFO ValidateSamFile - Deflater JdkDeflater; 19:03:43.000 INFO ValidateSamFile - Inflater JdkInflater; 19:03:43.000 INFO ValidateSamFile - Initializing engine; 19:03:43.000 INFO ValidateSamFile - Done initializing engine; ERROR: Record 9762, Read name 20GAVAAXX100126:7:2:8126:115177, bin field of BAM record does not equal value computed based on alignment start and end, and length of sequence to which read is aligned; ERROR: Record 24466, Read name 20FUKAAXX100202:7:46:13035:77621, bin field of BAM record does not equal value computed based on alignment start and end, and length of sequence to which read is aligned; ERROR: Record 97940, Read name 20FUKAAXX100202:5:7:21464:86",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-285513571:1561,Validat,ValidateSamFile,1561,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-285513571,1,['Validat'],['ValidateSamFile']
Security,"Ideally we'd minimize changes to core engine classes, and instead create new specialized subclasses (like a subclass of MultiVariantWalkerGroupedOnStart) that implement the desired behavior. The main thing is that we need something that only affects `VariantEval` - anything that affects other tools would run into the same issues as #4571 - specifically that tests that use VariantContext equality to validate results will start to fail because of the presence of source name in the actual values.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-823578133:402,validat,validate,402,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-823578133,1,['validat'],['validate']
Security,"If --gcs-project-for-requester-pays is not specified, gatk should use the current billing project to access requester pays buckets.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6669:101,access,access,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6669,1,['access'],['access']
Security,"If I clone GATK with the ssh URL (`git@github.com:broadinstitute/gatk.git`), and then run a `docker build` command from the root of that clone, I get ssh authentication errors at the `git lfs pull` step:. ```; Step 9/36 : RUN git lfs pull; ---> Running in 1f415556efd2; Git LFS: (0 of 104 files) 0 B / 1.28 GB ; batch request: Host key verification failed.: exit status 255; batch request: Host key verification failed.: exit status 255; error: failed to fetch some objects from 'https://github.com/broadinstitute/gatk.git/info/lfs'; The command '/bin/sh -c git lfs pull' returned a non-zero code: 2; ```. If I do the same thing from a GATK clone created using the https URL (`https://github.com/broadinstitute/gatk.git`), I get no lfs error. This also raises the larger question of whether we are authenticating with github before doing `git lfs pull` during the docker build, as I believe that the quotas for unauthenticated `git lfs` operations are much smaller than for authenticated operations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7077:154,authenticat,authentication,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7077,3,['authenticat'],"['authenticated', 'authenticating', 'authentication']"
Security,"If I understand what you're proposing, the tool code would have to use `instanceof` checks and typecasts to take advantage of this feature (unless `FeatureInputAwareVariantContext` was exposed in the `apply` method). Other reviewers may feel differently, but using the source field (which I don't view as ""repurposing"" since I think its aligned with the intent of that field) seems cleaner. Up to you if you want to submit a PR with your proposed change to see if its viable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-824047890:185,expose,exposed,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-824047890,1,['expose'],['exposed']
Security,"If a tool exists and is runnable, but is not documented, it should be accessible via tab-completion. Otherwise people can't depend on tab-completion to give a complete list of all tools that can be run.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3596#issuecomment-330936329:70,access,accessible,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3596#issuecomment-330936329,1,['access'],['accessible']
Security,"If one of the block compressed VCFs in the list is empty (i.e. it does have proper header lines but there are no variant records, which is perfectly valid) then the tool fails with an IllegalStateException:. java.lang.IllegalStateException: Could not read available bytes from BlockCompressedInputStream.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:697); at org.broadinstitute.hellbender.tools.GatherVcfs.gatherWithBlockCopying(GatherVcfs.java:354)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3218:351,validat,validate,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3218,1,['validat'],['validate']
Security,"If only some of the accesses failed, then it's likely what we're seeing here is GCS refusing to serve requests because it feels it's getting too many. How many machines are trying to access the file? How many threads per machine? What storage class is the bucket?. The lower storage classes support fewer parallel accesses, and NIO's prefetching (if enabled) results in two threads per reader.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5631#issuecomment-459842021:20,access,accesses,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631#issuecomment-459842021,3,['access'],"['access', 'accesses']"
Security,"If the sample file is created by extracting a table from BQ, the file might be in a bucket that only the service account can access. Add an option for using the service account to pull the file.; Also, expose the service account input at the workflow (not the task) level",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7299:125,access,access,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7299,2,"['access', 'expose']","['access', 'expose']"
Security,"If we like this -- we also need to . - [x] build a jar; - [x] update the WDL to use this tool (and the Jar); - [ ] Put the BED files someplace public/widely accessible (likely just the 1kb version); - [x] Run an E2E on QuickStart, merge the VCFs and compare (and see no differences); - [x] If we want to validate evenness we need to run with a lot of shards and enough data that they are interesting. Maybe Stroke 10k; - [x] In parallel if we could turn some of the above script into integration tests that would be awesome",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7643#issuecomment-1017113500:157,access,accessible,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7643#issuecomment-1017113500,2,"['access', 'validat']","['accessible', 'validate']"
Security,"If you ask HaplotypeCallerSpark for a gvcf.gz it outputs a base pair resolution GVCF with no blocking. This is due to confusion in hadoop-bam / VariantSparkSink. It works fine if you write an uncompressed g.vcf. This is due to a conditional statement in `KeyIgnoringVCFOutputFormat.getRecordWriter(askAttemptContext ctx)`. ```; 		if (!isCompressed) {; 			return getRecordWriter(ctx, file);; 		} else {; 			FileSystem fs = file.getFileSystem(conf);; 			return getRecordWriter(ctx, codec.createOutputStream(fs.create(file)));; 		}; ```. The two branches call two different overloads of `getRecordWriter`. ```; getRecordWriter(TaskAttemptContext ctx, Path out). getRecordWriter(TaskAttemptContext ctx, OutputStream outputStream); ```. The first is public, and overriden to provide GVCF writers in our code, the second is private and doesn't know about our GVCF writer. We could override `getRecordWriter(ctx)` but we need access to a constructor for `VCFRecordWriter` that takes a stream and propagates the ctx which doesn't exist.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4274:919,access,access,919,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4274,1,['access'],['access']
Security,"If you would like the GenomicsDB for chromosome `CM031199.1` (which, by the way, was created with GATK 4.2.4.1) that I used in the above two examples, for your own debugging purposes, you can download it as a .tar archive from:. [https://drive.google.com/file/d/1LzZCkWfmNb8IcZpdreaNIxtJ8GQQ-b7g/view?usp=sharing](https://drive.google.com/file/d/1LzZCkWfmNb8IcZpdreaNIxtJ8GQQ-b7g/view?usp=sharing). It is 385 Mb, and it has an SHA1 hash (from the Unix `shasum` utility) of `d330a28120713fb05c95d1bf54342944f5d741c9`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014196799:432,hash,hash,432,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014196799,1,['hash'],['hash']
Security,"Implement -L system, enable access to it for tools that request it",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4:28,access,access,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4,1,['access'],['access']
Security,"Implement -L system, enable access to it for tools that request it (define how they 'request it' - maybe by implementing an interface or calling a function or overriding some generic hook - part of this issue is to design it).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4:28,access,access,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4,1,['access'],['access']
Security,"Implements allele collapsing for ""breakend replacement"" BND alleles, as described in section 5.4 of the [VCFv4.2 spec](https://samtools.github.io/hts-specs/VCFv4.2.pdf). Also:; - Validates symbolic alt allele for non-BND SV classes when attempting to collapse multiple alt alleles.; - Greatly improves unit test coverage for `CanonicalSVCollapser`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8408:179,Validat,Validates,179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8408,1,['Validat'],['Validates']
Security,"Implements tool for clustering SVs, built on top of the clustering engine code refined recently in #7243. In addition to a few bug fixes, updates also include:. - `PloidyTable` class, which ingests and serves as a simple data class for a tsv of per-sample contig ploidies. This was necessary for inferring genotypes when input vcfs contain non-matching sample and variant records.; - Modified `SVClusterEngine` to render sorted output.; - Improved code for SV record collapsing (see the `CanonicalSVCollapser`), particularly for CNVs. Genotype collapsing now infers allele phasing in certain unambiguous cases, in particular for DUPs and multi-allelic CNVs. Testing for this has been cleaned up and augmented with further cases to validate this functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7541:731,validat,validate,731,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7541,1,['validat'],['validate']
Security,Improve error message for no-access and disabled-account cases,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2417:29,access,access,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2417,1,['access'],['access']
Security,Improve error message in spark tools when trying to access a local file from other nodes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1417:52,access,access,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1417,1,['access'],['access']
Security,"In ADAM, we have well maintained Hadoop InputFormats for both normal and interleaved FASTQ. Additionally, we wrap these InputFormats in Spark-friendly APIs (exposed in Scala, Java, Python, and R) that add validation and standard transformations. GATK already has a dependency on ADAM, so...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4612#issuecomment-405114805:157,expose,exposed,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4612#issuecomment-405114805,2,"['expose', 'validat']","['exposed', 'validation']"
Security,"In IntervalUtils, when Picard intervals are parsed and checked for validity, (line 359 `glParser.isValidGenomeLoc(interval.getContig(), interval.getStart(), interval.getEnd(), true)`), if the contig doesn't match the supplied reference (via -R) then the error produced is `has an invalid interval`. The interval is perfectly valid, especially since the Picard interval_list has a corresponding sequence dictionary. I'm not sure if the preferred behavior here is to validate against the interval_list seqdict and then note that the -R reference doesn't match or to error because the -R ref doesn't match. Maybe if the tool requiresReference() and the -R doesn't match throw an error?. I encountered this in the context of a tool similar to SplitIntervals, which requires a reference even if a Picard interval_list is provided. I see that this is a TODO in GATKTool::getBestAvailableSequenceDictionary.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5410:465,validat,validate,465,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5410,1,['validat'],['validate']
Security,"In Mutect2 and HaplotypeCaller, we force-call alleles by injecting them into the ref haplotype, then threading these constructed haplotypes into the assembly graph with a large edge weight. There are several drawbacks to this approach:. * The strange edge weights interfere with the `AdaptiveChainPruner`.; * The large edge weights may not be large enough to avoid pruning when depth is extremely high.; * The alleles may be lost if assembly fails.; * If the alleles actually exist but are in phase with another variant we end up putting an enormous amount of weight on a false haplotype. We can get around these issue with the following method:. * assemble haplotypes without regard to the force-called alleles.; * if an allele is present in these haplotypes, do nothing further.; * otherwise, add a haplotype in which the allele is injected into the reference haplotype. @LeeTL1220 I prototyped this and it seems to resolve the missed forced alleles that Ziao found. @ldgauthier Can you think of any objections to making this change in HaplotypeCaller?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5857:57,inject,injecting,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5857,2,['inject'],"['injected', 'injecting']"
Security,"In PathSeqPipelineSpark, the reads are repartitioned to ~5k per partition (by default) just prior to the pathogen BWA alignment step (to ensure an even distribution of work). Currently, some samples with a lot of non-host reads cause 10,000's of sharded BAMs to be written at the end of the pipeline. This PR reduces the number of partitions in the read RDD just before writing to disk in the PathSeqPipelineSpark tool. It exposes a command-line option for the number of reads per partition, with a default value that results in a much more reasonable number of sharded BAMs in even the worst cases.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3545:423,expose,exposes,423,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3545,1,['expose'],['exposes']
Security,"In doing continued profiling of the HaplotypeCaller GVCF mode I have observed that somewhere in the range of 12% of our overall runtime (after i've made my other optimizations) is spent in `VariantContextBuilder.make()` upon further investigation I have noticed that we are currently building a VariantContext object for each pileup in `ReferenceConfidenceModel.calculateReferenceConfidence()`. This means that we are building a unique VariantContext object for essentially every spot on the genome. VariantContext object building represents a significant overhead in terms of validation and construction and memory usage. I suspect that if we were to create some reduced object without as much overhead we could save ourselves a lot of trouble time and memory merging these things. Unfortunately I think the merging of these context objects happens in the GVCF writer which means it won't be a trivial change to make to the engine. Perhaps it is worth investigating what can be done to this code, as it represents another size-able chunk of speedup if we can squash it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5618:577,validat,validation,577,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5618,1,['validat'],['validation']
Security,"In fact, setting the deploy-mode works with manual jobs as we get logs in our Hadoop monitor ( the tool to monitor the jobs on the spark cluster ) and directly on our console if deploy-mode is not set / set to client. Both `--deploy-mode` and `--conf 'spark.submit.deployMode=cluster'`. But with GATK, logs appear directly on my console and not in the Hadoop monitor even if we set with `--conf 'spark.submit.deployMode=cluster`. The other methods `--deploy-mode` and `-- --deploy-mode` having the said problems.; About the `-- --deploy-mode` and the JNI linkage error, I'm currently checking this.; All our Spark nodes have access to the mapr libraries from `/opt/mapr/...`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350676916:625,access,access,625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350676916,1,['access'],['access']
Security,"In gatk3 we had mechanism for multithreading in gatk, but they made the tools very complicated and didn't provide enough speed up to be worthwhile in most cases. In gatk 4 we made the decision to write the tools as single threaded. We have a separate implementation of some tools using spark which is a parallelization library. . If you want to parallelize the HaplotypeCaller you can shard the input and run multiple copies of the tool on different input shards. That's how we do it in production. Alternatively you can try out the beta tool HaplotypeCallerSpark which is natively parallel but is still being developed. It hasn't been validated and may not produce results that are as accurate as the regular HaplotypeCaller.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6117#issuecomment-524554040:636,validat,validated,636,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6117#issuecomment-524554040,1,['validat'],['validated']
Security,In light of the recent #7357 and #7358 it has become clear that we are blind changes that cause the logging outputs for GATK to become unusable because we are spitting endless warnings to stdout. I think we should change our integration tests to capture the log output for each of our tests and assert that none of them balloon beyond some reasonable threshold that would capture these problems (perhaps a megabyte but it would take a little bit of sleuthing to be sure). . I would think the best place would be to add a capture into `CommandLineProgramTest.runCommandLine()` that instead of using the current behavior `injectDefaultVerbosity()` we instead leave the logging output as the default and capture it somewhere explicit where we can make assertions about the size of the outputs. Possibly we could create a dummy logging level that just saves and counts the outputs so we can make assertions about the logs. Ideally this should apply to every tool simultaneously since it would be too patchwork to simply add logging output tests for enough of the tools to protect us manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7368:620,inject,injectDefaultVerbosity,620,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7368,1,['inject'],['injectDefaultVerbosity']
Security,"In looking at his further, the container header contains a stream offset, and each slice header also contains a global record counter. Both of these need to be updated. Its not clear if its possible to repair these without re-encoding the entire container stream, but if so that should probably be done in a method exposed by htsjdk.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2201#issuecomment-324756574:315,expose,exposed,315,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2201#issuecomment-324756574,2,['expose'],['exposed']
Security,"In my case, as an API user, my main usage of GATK is for traverse `GATKRead` and `VariantContext`, so I would like to have in `GATKTool` a simpler way of access to the `FeatureInput<VariantContext>` instead of getting them from `FeatureManager features`. It will be useful in the `VariantWalker` as a step to issue #692, to get all the variants provided by the user in the same walker. My idea is modify the `GATKTool` to include:; - A `public abstract boolean requiresVariant()`, which will be used to determine if we should detach or not all the variants inputs from the `FeatureManager features`.; - A `private void initializeVariants()`, which will implement a way to extract the `FeatureInput<VariantContext>` from `features` and initialize a `FeatureManager variants` or a extended class which includes only `VariantContext` inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1710:154,access,access,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1710,1,['access'],['access']
Security,"In particular add output GATKTool.getDefaultToolVCFHeaderLines to the VCF header, and rewrite the integration test for GenerateVCFFromPosteriors so that it validates the equivalence of variant context records, instead of file equivalency",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4267:156,validat,validates,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4267,1,['validat'],['validates']
Security,"In particular, we are a little lax on sequence-dictionary validation in the CNV pipelines. However, it might be that this is a necessary evil---it seems sequence dictionaries are somewhat inconsistent even in datasets such as TCGA.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3864:58,validat,validation,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3864,1,['validat'],['validation']
Security,"In the VAT validation, give clearer error msg about which clinvar classification values are missing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7939:11,validat,validation,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7939,1,['validat'],['validation']
Security,"In the process of designing correctness tests for `MarkDuplicatesSpark`, @davidadamsphd has come up with a potential set of optimizations to `MarkDuplicatesSpark` that have the potential to improve performance by an order of magnitude. The task here is to meet with @davidadamsphd, get access to and understand his optimizations, and port them to the main `MarkDuplicatesSpark` tool (along with any other optimizations you feel are appropriate).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1100:286,access,access,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1100,1,['access'],['access']
Security,In the work on #7295 it became clear that there are a lot of overlapping overloads of the `createGenomeLoc()` method that has already caused some confusion since some overloads will skip the reference validation step. Somebody should audit all of the uses of `GenomeLocParser` and evaluate where validation is and isn't appropriate (possibly if you want an unvalidated genomeLoc use a SimpleInterval?) and wire them accordingly.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7300:201,validat,validation,201,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7300,3,"['audit', 'validat']","['audit', 'validation']"
Security,"In this branch are a number of improvements and changes that form the baseline for the current ongoing evaluation of the DRAGEN/GATK pipeline. This represents the joint work of both msyelf and @vruano. The major improvements in this branch are as follows:; - `EstimateDragstrModelParameters` tool for estimating the per-sample/per-STRType errors for use in the HMM gap open/gap close penalties as well as the necessary changes to the PairHMM loading code in order to adjust the model appropriately.; - Support for using the DragstrParams and flat SNP priors to compute genotype posteriors and the support for using them in the selection of genotypes as well as for computing the QUAL score. ; - Base Quality Dropout (BQD) model which penalizes variants with low average base quality scores among genotyped reads and reads that were otherwise excluded from the genotyper. A number of additional arguments to expose internal behaviors in the readThreadingAssembler and HaplotypeCaller have been made in order to support threading more lowBQ reads through to the genotyper. ; - Foreign Read Detection (FRD) model which uses an adjusted mapping quality score as well as read strandedness information to penalize reads that are likely to have originated from somewhere else on the genome. A number of additional arguments and behaviors have been exposed in order to preserve lower mapping quality reads in the HaplotypeCaller in service.; - Dynamic Read Disqualification, allows for longer/lower base quality reads to be less likely to be rejected by eliminating the hard cap on quality scores and further adjusting the limit based on the average base quality for bases in the read. . Design decisions that I would direct the reviewers attention to as they correspond to potentially dangerous/controversial changes:; - Because FRD/BQD require low quality ends to be included in the models for genotyping, I have added the option to softclipLowQualityEnds (as opposed to their current treatment which involv",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6634:907,expose,expose,907,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6634,1,['expose'],['expose']
Security,"In validateVariants tool, made the default case behave so that it does the validations that can be done, and issues warning messages for the validations that cannot be done (ie, required external files)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5984:3,validat,validateVariants,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5984,3,['validat'],"['validateVariants', 'validations']"
Security,"In working with the SampleDBBuilder code I have noticed that there is an argument for validationStrictness which purports to assert that there is a >1:1 mach between the discovered samples in the pedigree file and those in the underlying variantDataSources according to the code on line 83. Unfortunately, as it stands there is no way to input `samplesFromDataSources` into the builder, so these assertions are skipped. There are tests for validation but these only apply to asserting that there are no name collisions between the samples added as pedigree files, which appears to be different.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3949:86,validat,validationStrictness,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3949,2,['validat'],"['validation', 'validationStrictness']"
Security,"Includes latest Gencode and an implicit fix for #6564. Had to make some code changes for latest liftover Gencode data(v34 -> hg19). . The associated DS test release correctly annotates data on hg19 and hg38. Left to do:. - [x] Update data sources downloader.; - [x] Update data source version validation code. Code updates:; - Now both hg19 and hg38 have the contig names translated to `chr__`; - Added 'lncRNA' to GeneTranscriptType.; - Added ""TAGENE"" gene tag.; - Added the MANE_SELECT tag to FeatureTag.; - Added the STOP_CODON_READTHROUGH tag to FeatureTag.; - Updated the GTF versions that are parseable.; - Fixed a parsing error with new versions of gencode and the remap; positions (for liftover files).; - Added test for indexing new lifted over gencode GTF.; - Added Gencode_34 entries to MAF output map.; - Minor changes to FuncotatorIntegrationTest.java for code syntax.; - Pointed data source downloader at new data sources URL.; - Minor updates to workflows to point at new data sources. Script updates:; - Updated retrieval scripts for dbSNP and Gencode.; - Added required field to gencode config file generation.; - Now gencode retrieval script enforces double hash comments at; top of gencode GTF files. Bug Fixes:; Removing erroneous trailing tab in MAF file output. - Fixes #6693",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6660:293,validat,validation,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6660,2,"['hash', 'validat']","['hash', 'validation']"
Security,Index files should have an integrity check built-in,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5571:27,integrity,integrity,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5571,1,['integrity'],['integrity']
Security,Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/a795190c-dcc2-40a7-bfcc-84fa6a4ea0dc); Two failed on ValidateVDS (or rather something upstream). I *don't* think this is an effect of this PR.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8807:149,Validat,ValidateVDS,149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8807,1,['Validat'],['ValidateVDS']
Security,"Interesting, it's definitely possible it's coming from one of the other buckets. I don't think we have fine grained control over WHICH bucket we attempt to read requester pays status from, so it's possible if it's enabled it's necessary to have that permission on every bucket. It's annoying that the error message doesn't say which reader is performing the access. Is there a longer stack trace available?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7492#issuecomment-934908586:358,access,access,358,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492#issuecomment-934908586,1,['access'],['access']
Security,"Interesting, when I run locally I get:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.funcotator.FuncotatorIntegrationTest > nonTrivialLargeDataValidationTest[3](/Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/funcotator/validationTestData/regressionTestHg19Large.vcf, /Users/louisb/Workspace/gatk/src/test/resources/large/Homo_sapiens_assembly19.fasta.gz, hg19, /Users/louisb/Workspace/gatk/src/test/resources/large/funcotator/funcotator_dataSources/, /Users/louisb/Workspace/gatk/src/test/resources/org/broadinstitute/hellbender/tools/funcotator/validationTestData/regressionTestHg19Large_expected.vcf) FAILED; java.lang.UnsatisfiedLinkError: 'void org.sqlite.core.NativeDB._open_utf8(byte[], int)'; at org.sqlite.core.NativeDB._open_utf8(Native Method); at org.sqlite.core.NativeDB._open(NativeDB.java:71); at org.sqlite.core.DB.open(DB.java:174); at org.sqlite.core.CoreConnection.open(CoreConnection.java:220); at org.sqlite.core.CoreConnection.<init>(CoreConnection.java:76); at org.sqlite.jdbc3.JDBC3Connection.<init>(JDBC3Connection.java:25); at org.sqlite.jdbc4.JDBC4Connection.<init>(JDBC4Connection.java:24); at org.sqlite.SQLiteConnection.<init>(SQLiteConnection.java:45); at org.sqlite.JDBC.createConnection(JDBC.java:114); at org.sqlite.JDBC.connect(JDBC.java:88); at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677); at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189); at org.broadinstitute.hellbender.tools.funcotator.dataSources.cosmic.CosmicFuncotationFactory.<init>(CosmicFuncotationFactory.java:161); at org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils.createCosmicDataSource(DataSourceUtils.java:469); at org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils.createDataSourceFuncotationFactoriesForDataSources(DataSourceUtils.java:289); at org.broadinstitute.hellbender.tools.funcotator.Funcotator.onTraversalStart(Funcotator.java",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532715444:281,validat,validationTestData,281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532715444,2,['validat'],['validationTestData']
Security,"Introducing the IntervalLocusIterator which will traverse every locus in intervals, regardless of coverage. Minor changes. Removed imports. AlignmentContextLocusIterator first cut. Still needs unit tests. Putting in the walker. Still needs unit tests. Adding tests (and fixes) so that we can get AlignmentContexts. Adding tests (and fixes) so that we can get AlignmentContexts. Working tests. Beginning migration to a LocusWalker change rather than a separate walker. Merging the emit empty loci into locus walker. Still need warnings and validation of parameters. Next step is the LocusWalker testing. Simple test of the new LocusWalker when it emit empty loci. Addressing PR requests and added ShardedIntervalIterator to save RAM on big intervals. Addressing the rest of the PR comments. Rolling back to int from long. Addressing second round of PR comments. Wrapped LIBS in a factory so that we can encapsulate the retrieval of the best alignment context iterator. Spark empty loci traversal being supported. Rebasing based off of the other emit loci branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2731:539,validat,validation,539,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2731,1,['validat'],['validation']
Security,Investigate whether our default validation stringency for reads should be STRICT rather than SILENT,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1457:32,validat,validation,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1457,1,['validat'],['validation']
Security,Is there any update for this issue? I'm asking as AWS moved to NIO v2 late 2018 and htslib supports direct S3 access.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3708#issuecomment-628392999:110,access,access,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3708#issuecomment-628392999,1,['access'],['access']
Security,"It happened multiple times over the course of a couple days. Since I; downloaded the full gnomad exome data locally, I haven't tested again. --; - Alan Hoyle - alan@alanhoyle.com - http://www.alanhoyle.com/ -. On Mon, Nov 9, 2020 at 2:23 PM droazen <notifications@github.com> wrote:. > @alanhoyle <https://github.com/alanhoyle> Can you tell us whether the 400; > Bad Request error is repeatable -- did you see it more than once?; > Oftentimes when accessing cloud data we encounter transient errors like; > this that go away on their own.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/6926#issuecomment-724225557>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AACGX43OY2CFF5KFZXZOH4LSPA6T3ANCNFSM4TD2FDGA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-724267156:448,access,accessing,448,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-724267156,1,['access'],['accessing']
Security,"It looks like picard metrics record at least one more field when the metrics get reported, namely the count of supplementary reads seen. The metrics code should be audited to ensure it matches with picard over the same files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4777:164,audit,audited,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4777,1,['audit'],['audited']
Security,"It looks like the noGenotypes.vcf in GATK3 was derived from the v37 ref with decoy. In GATK4, the sequence dictionary validation requirement is that the input and reference have a common subset of contigs, so given your description of the difference I would expect it to pass that test. Feel free to reopen if you think I've missed something.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4453#issuecomment-410724975:118,validat,validation,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4453#issuecomment-410724975,1,['validat'],['validation']
Security,It looks like we need to update mockito. ; https://storage.googleapis.com/hellbender-test-logs/build_reports/master_27538.13/tests/test/index.html. ```. java.lang.IllegalArgumentException: Unknown Java version: 11; 	at net.bytebuddy.ClassFileVersion.ofJavaVersion(ClassFileVersion.java:135); 	at net.bytebuddy.ClassFileVersion$VersionLocator$ForJava9CapableVm.locate(ClassFileVersion.java:357); 	at net.bytebuddy.ClassFileVersion.ofThisVm(ClassFileVersion.java:147); 	at net.bytebuddy.dynamic.loading.ClassInjector$UsingReflection$Dispatcher$CreationAction.run(ClassInjector.java:301); 	at net.bytebuddy.dynamic.loading.ClassInjector$UsingReflection$Dispatcher$CreationAction.run(ClassInjector.java:290); 	at java.base/java.security.AccessController.doPrivileged(Native Method); 	at net.bytebuddy.dynamic.loading.ClassInjector$UsingReflection.<clinit>(ClassInjector.java:70); 	at net.bytebuddy.dynamic.loading.ClassLoadingStrategy$Default$InjectionDispatcher.load(ClassLoadingStrategy.java:184); 	at net.bytebuddy.dynamic.TypeResolutionStrategy$Passive.initialize(TypeResolutionStrategy.java:79); 	at net.bytebuddy.dynamic.DynamicType$Default$Unloaded.load(DynamicType.java:4456); 	at org.mockito.internal.creation.bytebuddy.SubclassBytecodeGenerator.mockClass(SubclassBytecodeGenerator.java:115); 	at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator$1.call(TypeCachingBytecodeGenerator.java:37); 	at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator$1.call(TypeCachingBytecodeGenerator.java:34); 	at net.bytebuddy.TypeCache.findOrInsert(TypeCache.java:138); 	at net.bytebuddy.TypeCache$WithInlineExpunction.findOrInsert(TypeCache.java:346); 	at net.bytebuddy.TypeCache.findOrInsert(TypeCache.java:161); 	at net.bytebuddy.TypeCache$WithInlineExpunction.findOrInsert(TypeCache.java:355); 	at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator.mockClass(TypeCachingBytecodeGenerator.java:32); 	at org.mockito.internal.creation.bytebuddy.SubclassB,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532377836:724,secur,security,724,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532377836,3,"['Access', 'Inject', 'secur']","['AccessController', 'InjectionDispatcher', 'security']"
Security,"It seems a lot of users use bcftools on their VCFs, and it sometimes converts floats to integers. For example MQ=31.0 to MQ=31. This change causes GATK tools to error. Is it possible to relax this validation?. ----; User Report; ----. Hi,. Every time I had this message, this was due to bcftools which can change some float values to an integer representation : (e.g : before bcftools : MQ=31.0; after bcftools : MQ=31). . The fact that GATK is very strict on that subject (40.0 is considered as a float while 40 is not) have some advantages and some drawbacks. I hope this problem will be resolved in GATK4 because bcftools is really useful and widely used when dealing with vcf files. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/43270#Comment_43270",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3734:197,validat,validation,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3734,1,['validat'],['validation']
Security,"It seems like travis isn't really testing this properly, it's hard to prove that it's not using the default credentials. Would it make sense to do the following?; 1. create a new gcloud project; 2. create a private file in a bucket only accessible by that account ; 3. add a service account json for that account to travis, and use it in this test.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2879#issuecomment-330339002:237,access,accessible,237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2879#issuecomment-330339002,1,['access'],['accessible']
Security,"It turns out we were wrong and there was already a knob exposed for this. I gave the user some directions on increasing the correct buffer size, and they have accepted that answer. . We can probably go ahead and close this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6150#issuecomment-532297743:56,expose,exposed,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6150#issuecomment-532297743,1,['expose'],['exposed']
Security,It was noticed while doing #8351 that the `GencodeFunctotation.equals()` method has the following line in it; ``` ; if (geneTranscriptType != that.geneTranscriptType) return false; ; ```. Unfortunately the geneTranscriptType is stored as a Sting and thus this should NOT be expected to succeed in almost any case. As it stands fixing this innocuous oversight seems to break several of the combinatorial funcotator tests and an integration test. Somebody should fix this behavior (easy) and validate that the test changes are within tolerable levels (hard).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8385:490,validat,validate,490,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8385,1,['validat'],['validate']
Security,It would be better if in addition to / instead of the getter/setter scheme that exists with current `Funcotation` classes we used a `HashMap` to be able to directly get each field by name. This would allow for more programmatic manipulation of `Funcotation` objects.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3919:133,Hash,HashMap,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3919,1,['Hash'],['HashMap']
Security,"It would be nice to have a StrandedInterval class, and then the distal targets could be stored as one of these in the various BreakpointEvidence subclasses that have distal targets, and the PairedStrandedIntervals class could just have two of them. This would also let you eliminate the methods hasDistalTargets and getDistalTargetsUpstreamOfBreakpoints -- getDistalTargets would do the job of all three.; I assume that PairedStrandedIntervals shouldn't ever have null intervals for source or target, so you could just validate in the constructor and then you wouldn't have to test nullness in equals and hashCode.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-328935711:519,validat,validate,519,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-328935711,2,"['hash', 'validat']","['hashCode', 'validate']"
Security,"It would be nice to have a few Picard tools that we feature in BPWs (e.g. MarkDuplicates) show both syntaxes:. 1. java -jar picard.jar ValidateSamFile I=reads.bam MODE=SUMMARY; 2. gatk ValidateSamFile -I reads.bam -MODE SUMMARY. I assume MODE gets one dash and not two, but really I don't know. So having such examples scattered throughout the tool chest for most often used tools would be great. The second example could be prefaced with, e.g.; > Picard tools are callable from GATK4. The equivalent command to the above invoked via the GATK launch script would be:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349522774:135,Validat,ValidateSamFile,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349522774,2,['Validat'],['ValidateSamFile']
Security,"It's a feature, we're so committed to open source that we don't allow it to be made private through encryption.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4718#issuecomment-385800585:100,encrypt,encryption,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4718#issuecomment-385800585,1,['encrypt'],['encryption']
Security,"It's hard to say what is going on here exactly...but a couple of clarifying questions here. You stated that importing to EBS was roughly 2x faster than Lustre (~1hr vs ~2hrs). Is that right? Are the vcfs on Lustre in both cases?. We're not sure exactly what is going on here...generally we do see some slowdown on shared filesystems compared to just private mounted disks. Do you have access to any CloudWatch metrics or the like? I'm not entirely sure what to look for there, but looking at IOPs over time, or transactions per second, read/write transaction size, etc might help show something interesting.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1044815768:385,access,access,385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1044815768,1,['access'],['access']
Security,"It's not clear to me that we want these tools in Gatk4. We deliberately didn't port them because we felt they were unnecessary going forward. . I understand that there are some legitimate use cases that require them: ex low coverage naive variant calling from high ploidy pools which haplotype caller would do poorly on. (Also, do we know that haplotype caller doesn't do well on those sorts of things? Maybe we should consider modifications there if it doesn't?) I'm not sure that supporting that use case is worth the added complexity of maintaining and supporting these tools. Especially since we don't provide a pileup based variant caller as part of gatk4... . @vdauwera Can you comment? . @sooheelee I'm not sure I agree with you that supporting this for mutect 1 is useful. ; A) We don't want to support the use of mutect 1 anymore and would like to encourage people to switch to mutect 2 which I think we now believe is a better variant caller for both snps and indels. ; B) Mutect 1 users are already using gatk3, so they have access to these tools already. Mutect 1 also requires co-cleaning which I believe is a different but related tool to indel realignment. . For the variant review issue, we have thoughts on implementing a much better solution for variant review by creating an assembly plugin for igv.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-308451988:1036,access,access,1036,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-308451988,2,['access'],['access']
Security,"It's true that our measurements have shown some improvement even in a random access case. Surely we should be able to fabricate a more extreme case where prefetching doesn't help, so it still makes sense to offer a choice.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5882#issuecomment-482689496:77,access,access,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5882#issuecomment-482689496,1,['access'],['access']
Security,It's weird that it worked before though if roles aren't set up right. It seems like security issues shouldn't be solved by asking people to upgrade their client software so that it can deny them permission.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330940018:84,secur,security,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330940018,1,['secur'],['security']
Security,Iterator.java:151); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:15); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); ```. I ran the same command again from my computer (not in the cloud) still using the NIO paths and it ran successfully. I've also seen it run successfully when running the same pipeline in the cloud. The only thing I think I've changed is the disk size I'm asking for. I'm in the process of validating the input bam right now.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316:7467,validat,validating,7467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316,1,['validat'],['validating']
Security,Its use in `ValidateBasicSomaticShortMutations` seems limited to the integration test. Can I rewrite the test to do without `AnnotatedInterval` and call it a day?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3884#issuecomment-526876913:12,Validat,ValidateBasicSomaticShortMutations,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3884#issuecomment-526876913,1,['Validat'],['ValidateBasicSomaticShortMutations']
Security,"I’ll take a look at the somatic tests. They should be OK, probably just something related to kebab case changes. EDIT: Or hmm, maybe they weren't passing before. Something to do with annotated-interval validation, I think. I think the WDL tests should be using the Docker, which has g++. Travis machines might be slower?. Integration tests will need to be in the python test group. Take a look at the python tests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-350160424:202,validat,validation,202,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-350160424,1,['validat'],['validation']
Security,"I’m sorry I wasn’t really keeping a good trail of the errors and it was a few weeks ago and I have been doing other stuff. . . I have a notation in my log about the run time switch and I seem to have gotten the notion about using it from reading stuff in one of the tutorials or on the forum. I gotta admit I am clueless about what the dictionary validation is actually doing so it wouldn’t have been anything that I conjured up on my own. . . From: Louis Bergelson <notifications@github.com> ; Sent: Wednesday, October 30, 2019 4:13 PM; To: broadinstitute/gatk <gatk@noreply.github.com>; Cc: rdbremel <rdbremel017@gmail.com>; Mention <mention@noreply.github.com>; Subject: Re: [broadinstitute/gatk] Funcotator shuts down (#6182). . Can you point out where in the log you see that? I'm looking at it but I don't see anything about memory in the log you provided. (Except the Runtime.totalMemory()=4523032576 which is just standard output spam from gatk when it shutsdown) Sequence dictionary validation usually happens first, it's strange that a failure in the middle of a run would be effected by it. I'm no very curious what weird thing is happening that's causing this... —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub <https://github.com/broadinstitute/gatk/issues/6182?email_source=notifications&email_token=ANCR2VFFO5775FSQO6EHFX3QRH2HHA5CNFSM4I2MRFQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECVZCVA#issuecomment-548114772> , or unsubscribe <https://github.com/notifications/unsubscribe-auth/ANCR2VA3C2XW4BZEI5YNGITQRH2HHANCNFSM4I2MRFQA> . <https://github.com/notifications/beacon/ANCR2VFOYRDVUGP66IIIVHDQRH2HHA5CNFSM4I2MRFQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECVZCVA.gif>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548119939:347,validat,validation,347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548119939,2,['validat'],['validation']
Security,Jc validate variants fixes issue5862,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5984:3,validat,validate,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5984,1,['validat'],['validate']
Security,JlbmRlci90b29scy9zcGFyay9BcHBseUJRU1JTcGFyay5qYXZh) | `50% <0%> (-50%)` | `3% <0%> (ø)` | |; | [...ender/tools/spark/pipelines/BQSRPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4087/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvQlFTUlBpcGVsaW5lU3BhcmsuamF2YQ==) | `55% <0%> (-45%)` | `5% <0%> (-3%)` | |; | [...ute/hellbender/tools/walkers/UnmarkDuplicates.java](https://codecov.io/gh/broadinstitute/gatk/pull/4087/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL1VubWFya0R1cGxpY2F0ZXMuamF2YQ==) | `45% <0%> (-45%)` | `5% <0%> (ø)` | |; | [...itute/hellbender/tools/walkers/bqsr/ApplyBQSR.java](https://codecov.io/gh/broadinstitute/gatk/pull/4087/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Jxc3IvQXBwbHlCUVNSLmphdmE=) | `52.381% <0%> (-39.286%)` | `5% <0%> (-1%)` | |; | [.../tools/walkers/validation/CountFalsePositives.java](https://codecov.io/gh/broadinstitute/gatk/pull/4087/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQ291bnRGYWxzZVBvc2l0aXZlcy5qYXZh) | `56.863% <0%> (-36.686%)` | `4% <0%> (-3%)` | |; | [...ender/engine/MultiVariantWalkerGroupedOnStart.java](https://codecov.io/gh/broadinstitute/gatk/pull/4087/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvTXVsdGlWYXJpYW50V2Fsa2VyR3JvdXBlZE9uU3RhcnQuamF2YQ==) | `62% <0%> (-34.875%)` | `14% <0%> (-6%)` | |; | [...nder/tools/spark/pipelines/ReadsPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4087/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUmVhZHNQaXBlbGluZVNwYXJrLmphdmE=) | `54.545% <0%> (-34.816%)` | `10% <0%> (-2%)` | |; | [...lbender/tools/spark/pipelines/CountReadsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pul,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4087#issuecomment-356051662:2695,validat,validation,2695,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4087#issuecomment-356051662,1,['validat'],['validation']
Security,"Just it case, I want to add that ""origin/master)"" kind of outcome is just a possibility ... sometimes you may get a line like ```* (HEAD detached at 18ed19)``` and then it would be ```18ed19)```. So please refrain of trying to handle the detached ```origin/whatever``` case and rather provide a solution that works with any ""garbage"" that the ```git batch --contains HASH``` command may come out with. I would say that unless the output lines is a standard looking branch name (e.g perl regex: ```/\s*([a-zA-Z_0-9]*)\s*/```) then just omit this part of the name or fill it with something constant like a ""UNKNOWN"" or ""DETACHED"" or ""HEAD"" or ""SNAPSHOT""....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3642#issuecomment-333904259:367,HASH,HASH,367,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3642#issuecomment-333904259,1,['HASH'],['HASH']
Security,"Just noting, as we discussed, this could allow us to condense coverage files, etc. We'd want to make sure that bins are always validated and that we don't introduce use cases that corrupt the bins (e.g., filtering bins). Also not really sure how hard indexing would be. Should be in conjunction with #4717. We are probably OK shuffling around relatively large interval and count files for the time being, but we can adjust priority when it makes sense (also depending on where htsjdk development is on these issues).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5702#issuecomment-466203330:127,validat,validated,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5702#issuecomment-466203330,1,['validat'],['validated']
Security,"Just some thoughts for @samuelklee. For example, CollectFragmentCounts produces the following hybrid-type `@RG` line:. ![screenshot 2018-02-22 15 05 48](https://user-images.githubusercontent.com/11543866/36908820-66b90938-1e0a-11e8-8830-793ff3f71e96.png). ```; @RG ID:GATKCopyNumber SM:HCC1143_tumor; ```; Official format specifications are at https://samtools.github.io/hts-specs/. Let me briefly describe the #choices. ---; If we are to follow conventions used in the alignment world (SAM specs, for interval lists), then... We note data transformations using `@PG` program groups. These can be added successively to the same data file, given unique `@PG ID` fields, and collectively these lines showcase the history of data transformations for a dataset. The `@RG` group is reserved for lane level data and yes, does unify based on the sample or library. ---; If we examine VCFs, the convention is to use `#` hashtags to denote header rows (VCF specs). Double hashtags `##` denote all metadata lines and a single hashtag `#` denotes the line with the column labels. Here are some select rows from an M2 VCF header:; ```; ##fileformat=VCFv4.2; ##FILTER=<ID=artifact_in_normal,Description=""artifact_in_normal"">; ...; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ...; ##GATKCommandLine=<ID=FilterMutectCalls,CommandLine=""FilterMutectCalls...; ...; ##GATKCommandLine=<ID=Mutect2,CommandLine=""Mutect2 --tumor-sample HCC1143_tumor ...; ...; ##INFO=<ID=TLOD,Number=A,Type=Float,Description=""Tumor LOD score"">; ##Mutect Version=2.1-beta; ##command=FilterByOrientationBias --output hcc1143_T_clean-filtered.vcf...; ...; ##contig=<ID=chr1,length=248956422>; ##contig=<ID=chr2,length=242193529>; ...; ##contig=<ID=HLA-DRB1*16:02:01,length=11005>; ##filtering_status=These calls have been filtered by FilterMutectCalls to label false positives with a list of failed filters and true positives with PASS.; ##normal_sample=HCC1143_normal; #",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4481:912,hash,hashtags,912,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4481,1,['hash'],['hashtags']
Security,"Just the last commit. As requested by @LeeTL1220 to enable Intel to access task-level parameters for subworkflows. Note that this adds an unnecessary amount of boilerplate and will quickly become untenable if there are many identically named parameters. As discussed in #3980, this sort of thing really should be handled by Cromwell, otherwise there is not much benefit to using optional parameters. I've filed #4287 to revert when we can. Closes #3980.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4288:68,access,access,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4288,1,['access'],['access']
Security,"Just to add - GenomicsDB relies on the filesystem for integrity checks just like any other file on the filesystem. We could add integrity checks at a micro chunk-level, but that would be at the expense of performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-713055866:54,integrity,integrity,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-713055866,2,['integrity'],['integrity']
Security,"Just to make sure I understand the issue---will this cause technical problems in the Firecloud environment, or is it more of a style issue?. If the latter, one reason I prefer the use of optional file inputs to trigger tool-level ""modes"" when possible is that it propagates more naturally from the tool level. For example, let's consider a tool that can operate in either tumor-only or matched-pair mode. It is natural at the tool level to make the tumor a required input and the normal optional. The other options are quite awkward: 1) make both inputs required and switch between using the normal or not with a flag (in which case it is very easy for the user to shoot themselves in the foot if they forget to set the flag right, and we'd have to pass a dummy normal every time we want to run tumor only if we don't actually have a pair), 2) leave the normal as optional but add a flag anyway, which would be redundant and require an additional validation (i.e., if the flag is set to matched mode but we don't have a normal, we should fail early), or 3) write separate tools for each mode with the corresponding required inputs. If we accept that optional file input is the way to handle such a scenario at the tool level but not at the workflow level, then we will simply run into the same problems at the workflow level. I'm sure there are more complex scenarios when triggering on file presence/absence doesn't uniquely specify a workflow, in which case flags are a must. But for simple scenarios, I'm not sure why we shouldn't take advantage of the ability to specify optional file inputs in WDL (actually, I'm not sure how else we are supposed to use them?). However, if this is a problem for Firecloud, then I'd like to understand why---and what possible solutions there might be.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3657#issuecomment-334046444:947,validat,validation,947,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3657#issuecomment-334046444,2,['validat'],['validation']
Security,"K Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:33:37.513 INFO Mutect2 - Deflater: IntelDeflater; 10:33:37.513 INFO Mutect2 - Inflater: IntelInflater; 10:33:37.514 INFO Mutect2 - GCS max retries/reopens: 20; 10:33:37.514 INFO Mutect2 - Requester pays: disabled; 10:33:37.514 INFO Mutect2 - Initializing engine; 10:33:37.874 INFO Mutect2 - Shutting down engine; [August 28, 2019 at 10:33:37 AM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.05 minutes.; Runtime.totalMemory()=161480704; java.lang.NullPointerException; at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getContigNames(SequenceDictionaryUtils.java:463); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getCommonContigsByName(SequenceDictionaryUtils.java:457); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.compareDictionaries(SequenceDictionaryUtils.java:234); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:150); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:98); at org.broadinstitute.hellbender.engine.GATKTool.validateSequenceDictionaries(GATKTool.java:769); at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:711); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:161); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291). This Issue was generated from your [forums] ; [forums",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6142:3382,validat,validateDictionaries,3382,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6142,1,['validat'],['validateDictionaries']
Security,"KTool.java:525); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:728); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:79); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; GET https://storage.googleapis.com/storage/v1/b/fc-secure-bd7b8bc9-f665-4269-997e-5a402088a369/o?maxResults=1&prefix=5c2db926-3b1c-479c-9ed3-a99ce518de91/omics_mutect2/60955825-7723-4bc9-8202-bdd9975bb5c0/call-mutect2/Mutect2/7d737efc-c8be-4a6d-8803-4f786129521a/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list.idx/&projection=full&userProject; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""User project specified in the request is invalid."",; ""reason"" : ""invalid""; } ],; ""message"" : ""User project specified in the request is invalid.""; }; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428); 	at com.google.api.client.http.HttpRequest.e",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7716:4064,secur,secure-,4064,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7716,1,['secur'],['secure-']
Security,"Keeping the dockstore as is for now because I may want to run this on a few shards from the 30k while it's still in review. This pr adds a fair amount of work to the bcftools task (ExtractAnAcAfFromVCF) and adds a significant number of columns to the schema: the sample count for all of the samples, as well as for each subpopulation. Note that AC_hemi will be added in a follow on pr; Note that additional validation tests will be added in a follow on pr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7456:407,validat,validation,407,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7456,1,['validat'],['validation']
Security,Leave validation cluster running at end [VS-901],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8729:6,validat,validation,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8729,1,['validat'],['validation']
Security,"Legacy pipeline (note, the following should only be done after final ModelSegments PR is in):; - [x] Delete prototype tools. (#3887) (SL, PR issued by 12/1); - ~~Add deprecated/legacy tag to legacy pipeline tools. (SL, PR issued by 12/1 EDIT: need further input from @vdauwera )~~; - ~~Update docs/arguments (w/ Comms, see #3853). This will follow deletion of prototype tools. (all, PR issued by 12/15)~~; - ~~(Reach) Collect all legacy code in a new package.~~; - [x] Delete old pipelines. (SL, #3935 awaiting review). ModelSegments pipeline:; - [x] Review and merge denoising PR (#3820).; - [x] Add WDL changes from @LeeTL1220, @meganshand, and @jsotobroad to dev branch. (Note that we exposed PreprocessIntervals.bin_length in these WDLs; I'm assuming that https://github.com/broadinstitute/cromwell/issues/2912 will allow this to be specified via the json, so I reverted this change.); - [x] Make simple improvements to ReCapSeg caller (#3825).; - [x] Review and merge modeling/WDL PR. (#3913 awaiting review. Note that this PR also deletes the old germline WDL.); - ~~Write MultidimensionalKernelSegmenterUnitTest.~~ (SL, punting, filed #3916); - ~~Write ModelSegmentsIntegrationTest.~~ (SL, punting, filed #3916); - [x] Preliminary PCAWG or HCC1143 purity evaluation. (@LeeTL1220) (LL, should be done in time for @vdauwera to present at Broad retreat); - [x] Update docs/arguments (w/ Comms, see #3853). This will follow deletion of prototype tools. (PR #4010 awaiting review.); - [x] Add SM tag and sequence dictionary headers to all appropriate files and sort accordingly. (SL, #3914 awaiting review); - [x] Update tutorial data. (@MartonKN); - [ ] (Reach) Add VCF output.; - [ ] (Reach) Add PG tags to all files.; - [ ] (Reach) Replace ReCapSeg caller with improved version. (@MartonKN). gCNV pipeline:; - [x] Review and merge Python code (#3838). (MB and SL, PR #3925 awaiting review.); - [x] CLI for ploidy determination (cohort). (@samuelklee); - [x] CLI for ploidy determination (case). (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3826:688,expose,exposed,688,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3826,1,['expose'],['exposed']
Security,"Let me know if you want more methods exposed, or need any help in general.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2072#issuecomment-236905536:37,expose,exposed,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2072#issuecomment-236905536,1,['expose'],['exposed']
Security,"Let's create a mock up of a possible future configuration setup using the Owner library (https://github.com/lviggiano/owner). For the mock up, I recommend we have two configuration files, one containing system properties and the other containing a few general engine settings. . We can select a few system properties from `gatk-launch` for inclusion in the system properties config file (eg., `samjdk.compression_level`, `samjdk.use_async_io_read_samtools`, etc.). . For the engine settings file, I recommend including `codecPackages` (a `List<String>` currently hardcoded in `FeatureManager.CODEC_PACKAGES`), `cloudPrefetchBuffer`/`cloudIndexPrefetchBuffer` (int values) from `GATKTool`, and `createOutputBamIndex` (boolean), also from `GATKTool`. As part of this, we'll have to prove that we can inject the system properties sufficiently early on that libraries such as htsjdk will pick them up.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3126:798,inject,inject,798,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126,1,['inject'],['inject']
Security,"Let's discuss further before you get too far along. The design of the Collections code was intended to ensure that very strict file formats are adhered to within the CNV pipeline. Making it more flexible to accommodate TSVs with arbitrary column headers, relax requirements for sequence dictionaries, etc. undermines that goal. There are also two other issues to consider:. 1) It looks like @jonn-smith has also been putting considerable effort into building a TSV framework for Funcotator. Perhaps CombineSegmentBreakpoints should consider using that framework instead, if it is more appropriate. We can also discuss bringing the CNV pipeline over into that framework, but this should definitely wait until after release. The end goal is for CNV team to spend as little time as possible writing or maintaining any code related to TSV parsing. 2) @mbabadi has put together some python evaluation code for the new gCNV, which makes use of the IntervalTree python package and PyVCF to accomplish some things that are very similar to what CombineSegmentBreakpoints is doing. Perhaps we could implement a similar approach purely in Java by making use of the IntervalTree implementation in htsjdk. I think for now we should treat CombineSegmentBreakpoints as a one-off tool to be used for internal validations. After release, we should design a more generic evaluation tool. This tool could take as input multiple collections of annotated locatables, with a few rigidly defined formats allowed (e.g., VCF, CNV Collection TSVs, perhaps some TSVs from other tools, etc.), with one designated as ground truth. The regions for evaluation could also be specified via -L (since it is possible this might not completely specified by the ground-truth collection). The appropriate intersections and lookups could then be performed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3995#issuecomment-352860616:1293,validat,validations,1293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3995#issuecomment-352860616,1,['validat'],['validations']
Security,"Let's talk about groupReadPairs, pairedReads, and unpairedReads. The groupBy method called by groupReadPairs is very expensive both in time and in memory. It's a full hash shuffle of GATKReads (time expensive), that results in a gazillion 1- and 2-element Lists (memory expensive). So you certainly don't want to do it twice. But the way pairedReads and unpairedReads is set up, you *will* do it twice if you want to process both paired and unpaired reads. (And even if you aren't, someone else might try to use this code to do so.). So my first suggestion is that you remove the call to groupReadPairs from pairedReads and unpairedReads, and let a user groupReadPairs once, and reuse the resulting JavaPairRDD to process paired and unpaired reads. My second suggestion is quite a bit more complicated, but I think it would result in far better performance. I'll sketch it out here, and then I can explain it further in person, if it's a direction you'd like to pursue. The first step is to create a JavaRDD<GATKRead> in which all pairs sharing a template name are in the same partition (but without grouping them). To do that, you temporarily boost the input JavaRDD into a JavaPairRDD<String,GATKRead> by extracting the read name as a key. Then you repartition (to do the shuffle). Then you map back to an ordinary JavaRDD<GATKRead> by keeping just the value. (Note: if the BAM has queryname sort order, you can just skip this step entirely.); Now you can do a mapPartition operation to filter for paired or unpaired reads: Iterate over the reads in the partition, and keep a hash map of [name -> read] of reads that have not yet found mates. To filter for paired reads, whenever you find the name of the current read already in the table, just emit the current read and the read in the map as a pair, and delete the read from the map (you're done with that name -- this keeps the table smaller). To filter for unpaired reads, just delete any map entry that you successfully look up, and insert any ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2664#issuecomment-299955039:167,hash,hash,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2664#issuecomment-299955039,1,['hash'],['hash']
Security,"Likely an analogous problem to #1417. ```; ./gatk-launch MarkDuplicatesSpark -I file:///home/unix/louisb/flag_stat.bam -O file:///home/unix/louisb/testoutput.bam -- --sparkRunner SPARK --sparkMaster yarn-client; ```. results in:. ```; java.lang.IllegalArgumentException: Wrong FS: file:/home/unix/louisb/testoutput.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:654); at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193); at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:105); at org.apache.hadoop.hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:645); at org.apache.hadoop.hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:641); at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:641); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.deleteHadoopFile(ReadsSparkSink.java:200); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:191); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:106); at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:94); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1451:581,access,access,581,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1451,1,['access'],['access']
Security,List data access patterns in Picard/GATK/Foghorn,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1:10,access,access,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1,1,['access'],['access']
Security,Long file names prevent cloning GATK on system with encrypted /home,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4718:52,encrypt,encrypted,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4718,1,['encrypt'],['encrypted']
Security,"Lots of the test input VCFs (and some expected test VCFs) are invalid: GQs that don't match their PLs (which should get fixed by the time I'm done with #3404 ), the wrong number of PLs for the alleles in the VC (src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/SelectVariants/vcfexample.loseAlleleInSelection.vcf) and probably more issues too. It's hard to be confident our output VCFs are correct when the expected behavior is sometimes wrong. Ideally we should run GATK ValidateVariants and/or vcftools validate on all the test VCFs (input and expected) and ensure files are valid where weren't not testing format/parsing issues.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3407:499,Validat,ValidateVariants,499,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3407,2,"['Validat', 'validat']","['ValidateVariants', 'validate']"
Security,M2 VCFs do not validate,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3296:15,validat,validate,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3296,1,['validat'],['validate']
Security,"MIGRATED FROM GATK3. @ldgauthier commented on [Thu Jul 16 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053). Currently ValidateVariants relies on genotypes to transitively check that each alt allele occurs in at least one sample and that the AC adds up. However, this can fail on sites-only files because there are no genotypes. We should use the definition of the info annotations in the header to check how many entries each should have.; ### Outline; - Add a new validation type for info-field counts to enum and to switch statement; - Grab info headers from input VCF with something like GATKVCFUtils.getVCFHeadersFromRods(getToolkit(), variantCollection.variants.getName()) and VCFHeader::getInfoHeaderLines; - In the map() function, for each info header line, call on each VCFInfoHeaderLine getCount(vc) to get the expected number of info annotation entries; - Compare the expected number with a count based on vc.getAttribute(currentVCFinfoHeaderLine.getID()), which will require some additional parsing because it returns an Object; - (Bonus points if you use the isFixedCount() and getCount() functions on the VCF info header line to simplify annotations that aren't according to the number of alt alleles); ### Test data. /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; Should fail AC/AF validation at ; `1 768589 . A C,G 76 PASS AC=1;AF=0.00047;AN=2120`; See results using:. ```; use VCFtools; vcf-validator /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; ```. which outputs:; `INFO field at 1:768589 .. INFO tag [AC=1] expected different number of values (expected 2, found 1),INFO tag [AF=0.00047] expected different number of values (expected 2, found 1)`; ### Notes. Currently, all the validation modes call out to HTSJDK. Do we want to put the new functionality there as well?. ---. @yfarjoun commented on [Thu Jul 16 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122130280). I think that it is very a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2507:135,Validat,ValidateVariants,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507,2,"['Validat', 'validat']","['ValidateVariants', 'validation']"
Security,"MProxy: Connecting to ResourceManager at mg/10.131.101.159:8032; 17/10/13 18:11:36 INFO yarn.Client: Requesting a new application from cluster with 3 NodeManagers; 17/10/13 18:11:36 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (164726 MB per container); 17/10/13 18:11:36 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead; 17/10/13 18:11:36 INFO yarn.Client: Setting up container launch context for our AM; 17/10/13 18:11:36 INFO yarn.Client: Setting up the launch environment for our AM container; 17/10/13 18:11:36 INFO yarn.Client: Preparing resources for our AM container; 17/10/13 18:11:37 INFO yarn.Client: Uploading resource file:/tmp/hdfs/spark-c7e5eece-205e-4bce-a69b-4168c9b79045/__spark_conf__2918234914787361986.zip -> hdfs://mg:8020/user/hdfs/.sparkStaging/application_1507856833944_0003/__spark_conf__.zip; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); groups with view permissions: Set(); users with modify permissions: Set(hdfs); groups with modify permissions: Set(); 17/10/13 18:11:37 INFO yarn.Client: Submitting application application_1507856833944_0003 to ResourceManager; 17/10/13 18:11:37 INFO impl.YarnClientImpl: Submitted application application_1507856833944_0003; 17/10/13 18:11:37 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1507856833944_0003 and attemptId None; 17/10/13 18:11:38 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:38 INFO y",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:10558,Secur,SecurityManager,10558,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['Secur'],['SecurityManager']
Security,Make GCS authentication work without using dataflow options classes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/963:9,authenticat,authentication,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/963,1,['authenticat'],['authentication']
Security,Make PathSeq test BAMs pass ValidateSamFile,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3204:28,Validat,ValidateSamFile,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3204,2,['Validat'],['ValidateSamFile']
Security,Make a bunch of in silico contaminated samples and validate contamination tool,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3256:51,validat,validate,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3256,1,['validat'],['validate']
Security,Make sequence dictionary validation a bit less strict,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/877:25,validat,validation,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/877,1,['validat'],['validation']
Security,Make sequence dictionary validation in BQSR dataflow work when the reference is in a bucket,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/737:25,validat,validation,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/737,1,['validat'],['validation']
Security,Make some noise when VDS validation succeeds,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8155:25,validat,validation,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8155,1,['validat'],['validation']
Security,"Makes CreateVariantIngestFiles robust to partially or fully loaded samples. Commit 21828af8f5a925cc331dce6093c0d510042d7b64 is what I actually propose to merge, while commit de673204183a4c45059dc9ea4e05868e2ea6ae59 randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7843:224,inject,injects,224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7843,1,['inject'],['injects']
Security,"Makes `CreateVariantIngestFiles` robust to partially or fully loaded samples. Commit a8dc5ea89653a7f94588aa040b49d0264d17f72d is what I actually propose to merge, while commit 118a44604343e8f77d53bcc6545b2360fefbe1cc randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7831:226,inject,injects,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7831,1,['inject'],['injects']
Security,"Managers; 17/10/13 18:11:36 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (164726 MB per container); 17/10/13 18:11:36 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead; 17/10/13 18:11:36 INFO yarn.Client: Setting up container launch context for our AM; 17/10/13 18:11:36 INFO yarn.Client: Setting up the launch environment for our AM container; 17/10/13 18:11:36 INFO yarn.Client: Preparing resources for our AM container; 17/10/13 18:11:37 INFO yarn.Client: Uploading resource file:/tmp/hdfs/spark-c7e5eece-205e-4bce-a69b-4168c9b79045/__spark_conf__2918234914787361986.zip -> hdfs://mg:8020/user/hdfs/.sparkStaging/application_1507856833944_0003/__spark_conf__.zip; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); groups with view permissions: Set(); users with modify permissions: Set(hdfs); groups with modify permissions: Set(); 17/10/13 18:11:37 INFO yarn.Client: Submitting application application_1507856833944_0003 to ResourceManager; 17/10/13 18:11:37 INFO impl.YarnClientImpl: Submitted application application_1507856833944_0003; 17/10/13 18:11:37 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1507856833944_0003 and attemptId None; 17/10/13 18:11:38 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:38 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: root.users.hdfs; 	 start",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:10710,Secur,SecurityManager,10710,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['Secur'],['SecurityManager']
Security,"MannWhitneyU was re-written from scratch in 2016 in GATK3,; but these changes never got ported to GATK4. This new version; produces significantly different results from the version; currently in GATK4, resulting in VERY different values for the; RankSumTest annotations in HaplotypeCaller output. @meganshand informs me that the updated GATK3 version has been; validated in R, and has much better tests than the old version. This is a straightforward port of that version with minimal changes:. -Merged ""MWUnitTest"" and ""RankSumUnitTest"" from GATK3 into a single; test class MannWhitneyUUnitTest; -Ported MathUtils.binomialCoefficient() and wrote new test for it; -Updated RankSumTest class and tests as appropriate. I've confirmed that with this change, the RankSum annotations produced; by the GATK4 HaplotypeCaller closely match those produced by the GATK3; HaplotypeCaller. Resolves #2604",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2605:361,validat,validated,361,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2605,1,['validat'],['validated']
Security,Many times the question comes up of whether variants are lost in HaplotypeCaller and Mutect2 because they are not assembled. It seems like an easy and scalable way to answer this would be to emit an optional sites-only vcf of all variants in the `EventMap` before genotyping. That way we could do internal validations about assembly much faster than currently. and user questions in this vein would not require the IDE or looking at bamouts in IGV. I envision something like this:; ```; gatk Mutect2 -I tumor.bam -O out.vcf --assembled-variants assembled.vcf; gatk SelectVariants truth.vcf --discordance assembled.vcf -O assembly-false-negatives.vcf; ```. @ldgauthier @yfarjoun what do you think about this?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5426:306,validat,validations,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5426,1,['validat'],['validations']
Security,"Maybe I misunderstand the underlying model, but if some Pedigree annotations only need to know which samples are founders (ExcessHet ?) , and some need to know the full relationships (PossibleDeNovo), then I'm suggesting we change the class hierarchy to reflect that:. PedigreeAnnotation; |--TrioAnnotation; |----PossibleDeNovo; |--ExcessHet (assuming ExcessHet only needs founders...); ... Then the plugin could deterministically validate whether the user has provided sufficient args for the set of requested annotations; and if so, propagate them accordingly. A TrioAnnotation could only be populated (from the command line at least) from a file, whereas the others could be populated from either a file or just a set of IDs. I think it would simplify the annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-463372550:431,validat,validate,431,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-463372550,2,['validat'],['validate']
Security,Merge adjacent blocks when validating GVCFs so we use less memory and dont fail when not using an interval argument,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3445:27,validat,validating,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3445,1,['validat'],['validating']
Security,"Merging this according to discussion with @LeeTL1220. We still need to investigate some possible regressions in the CRSP validation, but we should be good for prerelease on the CNV somatic side. There is one last branch (#4061) on the germline side, but I think we didn't plan on having gCNV ready for prerelease.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4046#issuecomment-355665197:121,validat,validation,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4046#issuecomment-355665197,1,['validat'],['validation']
Security,"Meta-comments for reviewers: the new program groups in this PR are based on @sooheelee 's spreadsheet, including some that are placeholders for things that will soon live in Picard, but aren't accessible from there yet. I've left the old program groups intact because they're still being referenced by tools. As the doc PRs are merged in, eventually these will be left dangling with no references, and then we'll remove them. In the meantime we'll have some redundancies (ReadProgramGroup will be replaced by ReadDataProgramGroup, or whatever we settle on).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3924#issuecomment-349722389:193,access,accessible,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3924#issuecomment-349722389,1,['access'],['accessible']
Security,"Minimally, VariantWalkerSpark, but we should also audit the other classes included in https://github.com/broadinstitute/gatk/pull/2256/files to see if readFilters are properly propagated.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2338:50,audit,audit,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2338,1,['audit'],['audit']
Security,"MisencodedBaseQualityReads Fix Illumina base quality scores in a SAM/BAM/CRAM file; FlagStat A reimplementation of the 'samtools flagstat' subcommand; GatherBQSRReports Gathers scattered BQSR recalibration reports into a single file; GatherBamFiles Concatenates one or more BAM files together as efficiently as possible; LeftAlignIndels Left-aligns indels from reads in a SAM/BAM/CRAM file; MarkDuplicates Examines aligned records in the supplied SAM/BAM/CRAM file to locate duplicate molecules.; MergeBamAlignment Merges alignment data from a SAM/BAM with data in an unmapped SAM/BAM/CRAM file; MergeSamFiles Merges multiple SAM/BAM files into one file; PrintReads Print reads in the SAM/BAM/CRAM file; ReorderSam Reorders reads in a SAM/BAM file to match ordering in reference; ReplaceSamHeader Replace the SAMFileHeader in a SAM/BAM file with the given header; RevertBaseQualityScores Revert Quality Scores in a SAM/BAM/CRAM file; RevertOriginalBaseQualitiesAndAddMateCigar Reverts the original base qualities and adds the mate cigar tag to read-group BAMs; RevertSam Reverts SAM/BAM files to a previous state; SamFormatConverter Convert a SAM/BAM/CRAM file to a SAM/BAM/CRAM file; SamToFastq Converts a SAM/BAM file into a FASTQ; SortSam Sorts a SAM/BAM/CRAM file; SplitNCigarReads Split Reads with N in Cigar; SplitReads Outputs reads from a SAM/BAM/CRAM by read group, sample and library name; UnmarkDuplicates Unmark duplicates in a SAM/BAM/CRAM file; ValidateSamFile Validates a SAM/BAM/CRAM file. --------------------------------------------------------------------------------------; Spark Validation tools: Tools written in Spark to compare aspects of two different files; CompareBaseQualitiesSpark Diff qs of the BAMs; CompareDuplicatesSpark Compares two BAMs for duplicates. --------------------------------------------------------------------------------------; Spark pipelines: Pipelines that combine tools and use Apache Spark for scaling out (experimental); BQSRPipelineSpark Both st",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1669:6290,Validat,ValidateSamFile,6290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1669,2,['Validat'],"['ValidateSamFile', 'Validates']"
Security,"Model (BGMM) backend vs. python sklearn IsolationForest backend; (BGMM tests to be added once PR for the backend goes in.); - [x] Tool-level docs. Minor TODOs:. - [x] Parameter-level docs.; - [x] Parameter/mode validation.; - [x] Refactor main code block for model training; it's a bit monolithic and procedural now.; - [x] Decide on behavior for ill-behaved annotations. E.g., all missing, zero variance. Future work:. - [ ] We could allow subsetting of annotations here, which might allow for easier treatment of ill-behaved annotations. However, I'd say enabling workflows where the set of annotations is fixed is the priority.; - [ ] We could do positive-unlabeled training more rigorously or iteratively. Right now, we essentially do a single iteration to determine negative data. This could perhaps be preceded by a round of refactoring to clean up model training and make it less procedural.; - [ ] Automatic threshold tuning could be built into the tool, see #7711. We'd probably have to introduce a ""validation"" label. Perhaps it makes sense to keep this sort of thing at the workflow level?; - [ ] In the positive-negative framework enforced by the Java code in this tool, a ""model"" is anything that assigns a score, we fit two models to different subsets of the data, and then take the difference of the two scores. While the python backend does give some freedom to specify a model, future developers may want to go beyond the framework itself. For example, more traditional classification frameworks, etc. could be explored. As an intermediate step, one could perhaps use the positive/negative scores from the current framework in a more sophisticated way (e.g., using them as features), rather than just taking their difference. This sort of future work could be developed completely independently of the codebase associated with the current training tool (or done externally in python), but should still be able to make use of the extract and score tools, since the contracts should be",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948369:1589,validat,validation,1589,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948369,1,['validat'],['validation']
Security,More fix enum hashCode,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4623:14,hash,hashCode,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4623,1,['hash'],['hashCode']
Security,"Most methods in `FuncotatorUtils` take a `SequenceComparison` object as input. This should be changed to either:. 1. Take the base fields as arguments (i.e. whatever is accessed in the `SequenceComparison` object). or . 2. For each method taking a `SequenceComparison`, create a `canCall` method that takes a `SequenceComparison` and returns a boolean - whether, based on the assigned fields in the `SequenceComparison` object, you can call the method itself.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3959:169,access,accessed,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3959,1,['access'],['accessed']
Security,"Most spark tools use the one in GATKSparkTool, but some have some special requirements that make it not work for them. They have to specify different sequence dictionaries or something like that in a way that isn't exposed. Maybe something could be refactored there, but they needed manually adjusting to match the new behavior because of their special handling of the writing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6458#issuecomment-594167389:215,expose,exposed,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6458#issuecomment-594167389,1,['expose'],['exposed']
Security,Moved validation test data out of large files area.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5381:6,validat,validation,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5381,1,['validat'],['validation']
Security,"Moving to [GenomicsDB 1.4.1 ](https://github.com/GenomicsDB/GenomicsDB/releases/tag/v1.4.1)release will allow for the direct use of the native GCS C++ client instead of the GCS Cloud Connector via HDFS. The GCS Cloud Connector can still be used with GenomicsDB via the `--genomicsdb-use-gcs-hdfs-connector` option. Using the native client with gcs allows for GenomicsDB to use the standard paradigms to help with authentication, retries with exponential backoff, configuring credentials, etc. The defaults are all hardcoded to match what is in gatk at present. It also helps with performance issues with gcs, see #7070. This version also contains fixes for #7089, although it will require additional support from gatk(will be part of a separate PR).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7224:413,authenticat,authentication,413,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7224,1,['authenticat'],['authentication']
Security,Multi-input Picard tools and metrics should perform sequence dictionary validation on their inputs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1272:72,validat,validation,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1272,1,['validat'],['validation']
Security,MultiVariantWalker sequence dictionary cross-validation is miserable,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6589:45,validat,validation,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6589,1,['validat'],['validation']
Security,"Multiple causes can cause closed connections when reading from GCS, almost all of which are outside of our control. This will never be ""completely fixed"" in the sense that even if the code is perfect it's completely possible to send too many requests to GCS, and it'll respond by closing connections. The main factors that I know of are:. - number of concurrent accesses to the GCS bucket in question; - number of concurrent accesses to the GCP project in question; - storage class of the GCS bucket in question (the more expensive ones have more replicas, thus can handle a higher load). If you're running into those difficulties I would suggest trying to reduce the load (reduce the number of concurrent workers or threads) and making sure it's not a single-region storage bucket. If that fails, perhaps try using a different bucket that no one else is also using (to reduce other sources of load). If I understand correctly that you didn't change the version you're using but are suddenly seeing more issues than before, then perhaps the cause is a server-side change from GCS (outside of our control), a change in configuration (are you reading from a bucket of a different class from before), or perhaps just an increase of other activity on the same bucket/project. The current code is very persistent in its retries: as you can see from the messages it spent a whole half hour waiting. If it's an overload situation then you may get better performance by reducing the worker count (as they will have to retry less).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5631#issuecomment-526270716:362,access,accesses,362,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631#issuecomment-526270716,2,['access'],['accesses']
Security,"Mutect2.wdl: ""pet-@.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7492:74,access,access,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492,1,['access'],['access']
Security,My branch only validates the lengths of some annotations of type list -- I didn't add any checks with respect to the spec.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6762#issuecomment-705185296:15,validat,validates,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6762#issuecomment-705185296,1,['validat'],['validates']
Security,My hypothesis is that the many accesses exhausts something along the path and that something as a result rejects all connections. Perhaps to serve a client they need a file handle and there's a limit of < 1e6 handles? . If we really need to open this many perhaps we should try to put together a minimal program that reproduces the problem and then submit that to cloud support.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308586876:31,access,accesses,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308586876,1,['access'],['accesses']
Security,"My problem is fixed with the first commit, because if `customCommandLineValidation()` throws an `UserException.CommandLineException` it is catched and printed (otherwise, it is ignore in `Main`, except for the returning status). Anyway, I added a commit with the implementation of the void method for all the tools. I guess that it is not a good idea after all, because it could help to print several errors. My first commit deal with the two situations, printing one/several errors if the `String[]` is not null, and if the validation throws an error catching it and handle in the same way as the ones in `CommandLineProgram`. I will rather go for the first and changing the printing of the errors in the array to the same place as the catched exceptions, and decorate it in the same way for not confusing the software user. What do you thing, @lbergelson?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2226#issuecomment-255856456:525,validat,validation,525,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2226#issuecomment-255856456,1,['validat'],['validation']
Security,"My recollection is that this is a use case we never put any priority on so there's no test in the GATK suite for access to private files. There should be, of course. The feature is there and (at least locally) it worked when I tried it. NIO does not use the API_KEY, it uses default credentials. Those are environment variables that are set by the `gcloud` command or pre-set for you in the case of virtual machines on Google. There are two cases: local execution and Spark. . I just tested local execution and it worked fine for me:. ```; $ ./gatk-launch PrintReads -L Broad.human.exome.b37.small.interval_list -I gs://jpmartin-private/bench/WGS-G94982-NA12878.bam -O t_gcs.bam; ```. this command worked even though (unless I'm mistaken) neither the bucket nor the file are public. One challenge however is that the way to set default credentials has changed recently. Calling `gcloud auth login` used to be enough but now we have to call (IIRC) `gcloud auth application-default login`. For Spark, the default credentials are set as whoever owns the dataproc environment that's used to run the show. So it should be set so it has access to the buckets necessary. NIO has mechanisms for accessing buckets that belong to someone other than who is running the Spark job, but they are not hooked into GATK yet.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277832658:113,access,access,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277832658,3,['access'],"['access', 'accessing']"
Security,"My suspicion was wrong. We also include a safety check which cause us to correctly reject most accidental matches. If we detect 2 chromosomes with the same name but different lengths we fail even if we detect otherwise matching chromosomes. I've run all the dictionaries I could find in the gatk bundle against each and only b37 and b37_decoy are compatible with each other which is the desired behavior I believe. | | hg18 | hg19 | b37 | b37_decoy | hg38 |; | -- |------|-----|------|-----------|-------|; | hg18 | ✅ | | | | |; | hg19 | | ✅ | | | |; | b37 | | | ✅ | ✅ | |; | b37_decoy | | | ✅ | ✅ | |; | hg38 | | | | | ✅ |. ```; @DataProvider; public Iterator<Object[]> getComparisons(){; final ArrayList<Object[]> comparisons = new ArrayList<>();; final List<String> dicts = Arrays.asList(""Homo_sapiens_assembly18.dict"",; ""ucsc.hg19.dict"",; ""human_b36_both.dict"",; ""human_g1k_v37.dict"",; ""human_g1k_v37_decoy.dict"",; ""Homo_sapiens_assembly38.dict"");; for( String left : dicts) {; for (String right: dicts){; Path leftDict =Paths.get(""/Users/louisb/Downloads/dicts"", left);; Path rightDict = Paths.get(""/Users/louisb/Downloads/dicts"", right);. comparisons.add( new Object[] {leftDict, rightDict});; }; }; return comparisons.iterator();; }. @Test(dataProvider = ""getComparisons""); public void testSequenceDictionariesAgainstEachother(Path left, Path right){; String leftName = left.getFileName().toString();; String rightName = right.getFileName().toString();; SequenceDictionaryUtils.validateDictionaries(leftName,; SAMSequenceDictionaryExtractor.extractDictionary(left),; rightName,; SAMSequenceDictionaryExtractor.extractDictionary(right));; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3754#issuecomment-494924193:1485,validat,validateDictionaries,1485,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3754#issuecomment-494924193,1,['validat'],['validateDictionaries']
Security,"NIO uses the gcloud system authentication, so we shouldn't need this anymore.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2402:27,authenticat,authentication,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2402,1,['authenticat'],['authentication']
Security,"Need some guidance here. The CompareSAMs tool was not propagating the validation stringency. I have a fix for that, but that alone doesn't fix the compareBAMFiles test in BaseRecalibrationIntegrationTest.java, since that uses SamAssertionUtils.assertSamsEqual, which also doesn't propagate (or accept) a validation stringency. Changing SamAssertionUtils to use either SILENT or LENIENT does fix the integration test, and all the other tests pass, but it seems like a relaxing of the stringency, and I'm not sure it should be necessary to the BQSR test. If relaxing the stringency for BQSR test _IS_ the right path, one possibility is to add a new method to SamAssertionUtils that accepts a validation stringency argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266:70,validat,validation,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/419#issuecomment-109796266,3,['validat'],['validation']
Security,"New version allows restricting which users can trigger carrot jobs based on their access to the repository. In this case, I've set it to restrict to only users who have at least write access.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6986:82,access,access,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6986,2,['access'],['access']
Security,"No problem – I can see how this would be challenging to review – maybe it’s not even practical – if you decide it’s best just not to use it that’s OK - working on these issues has been a valuable learning experience for me. . I sent an archive with the new validations and the diffs between the old and new bams to akiezun. Although in many cases the files shared types of errors, I had to look at each file individually to take into account the particular errors in each file and how to fix them without (to the best of my knowledge) interfering with the purpose of the test. I did write a python script to use where necessary for converting multiple unpaired reads in a file to single reads, and I used bash scripts to call the picard tools to convert multiple files at a time from bam to sam for editing and then back again after they were modified. In some cases I had to modify the values in test output files to match the values produced by the test using the modified bams/sams, or just capture the new output files and use them to replace the old with the new. In two cases where file format errors appeared to be necessary but the filename did not indicate this, I renamed the files to make this clear. From: Louis Bergelson ; Sent: Thursday, August 20, 2015 2:13 PM; To: broadinstitute/hellbender ; Cc: nenewell ; Subject: Re: [hellbender] Issue 569 - bam and sam file cleanup. (#809). @nenewell Sorry this has been sitting. We've been trying to figure out how to review this one. Could you describe how you made the changes? Did you script it or go through by hand and modify them all?. —; Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051:257,validat,validations,257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051,1,['validat'],['validations']
Security,No validation here. I was satisfied with the validation from the Palantir report and using this as a robustness test to show that GATK4 HC isn't going to fall over. I have a matched list of GVCFs here: /humgen/gsa-hpprojects/dev/gauthier/scratch/newQualHC/check.list @skwalker could you adapt your analysis to run with this list? I'll need to give you a different jar for the GenotypeGVCFs step on my GVCFs since the annotation format is outdated.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380822981:3,validat,validation,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380822981,2,['validat'],['validation']
Security,"Normally one provides passing workflow runs with a PR. For the integration run [that is here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ab86fb6d-c5d6-48b6-8322-923af691751c). There's also a ""real"" run taking place using this branch [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/db59d5b8-e2ac-4619-9563-aa5631bf053c). However for testing correctness of these changes with respect to the requester pays flag, my pet ""does not have serviceusage.services.use access to the Google Cloud project"". I therefore present instead a [run with my changes](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9e712055-f466-4929-b6eb-5306f3cde1a0) that fails in exactly the same way as a [run without my changes](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/185506f5-9dc1-4c02-997d-6fe3f5695259).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8552:500,access,access,500,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8552,1,['access'],['access']
Security,"Not in general, because prefetching isn't always desirable. By its nature it's designed for long sequential reads rather than random accesses. In GATK though there is prefetcher code in a FeatureDataSource constructor, so anyone who uses that one will get the prefetching.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5882#issuecomment-482660586:133,access,accesses,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5882#issuecomment-482660586,1,['access'],['accesses']
Security,"Not yet. I am curious about the mathematical details of it. How does it distinguish no dominant clone from contamination or too many infiltrating immune cells? In the meantime, I wrote a small R script. Example output below. This is a sun-exposed skin cancer, hence the enormous mutation burden in whole genome sequencing data. ![image](https://user-images.githubusercontent.com/631218/87098600-08acb380-c28b-11ea-8876-9d9817ab13d7.png)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-656391166:239,expose,exposed,239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-656391166,1,['expose'],['exposed']
Security,"Note #4439, which concerns a Picard tool that might also need options for interval merging exposed. Just something to be aware of---I'm guessing that it's probably a bit ambitious to have identical options for all interval inputs to both Picard and GATK tools?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4341#issuecomment-367810395:91,expose,exposed,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4341#issuecomment-367810395,1,['expose'],['exposed']
Security,"Note: the test failures seem to have a lot of docker ""toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading"" and are probably not due to these changes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7219#issuecomment-824459892:140,authenticat,authenticating,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7219#issuecomment-824459892,1,['authenticat'],['authenticating']
Security,"Notes from @kvg :. Manuscript on LdBG details: https://academic.oup.com/bioinformatics/article/34/15/2556/4938484. Reference implementation of the assembler in C: https://github.com/mcveanlab/mccortex; mcveanlab/mccortex. My Java library for accessing and manipulating LdBGs and associated formats: https://github.com/mcveanlab/CortexJDK. A useful starting point is the tests I've written to produce Figure 1 (the one with the pentagram cycle) from the manuscript. Without links: https://github.com/mcveanlab/CortexJDK/blob/4ba64ee268314729c871916dfc9ee7c9c422c5cb/public/java/tests/uk/ac/ox/well/cortexjdk/utils/traversal/TraversalEngineTest.java#L210. With links: https://github.com/mcveanlab/CortexJDK/blob/4ba64ee268314729c871916dfc9ee7c9c422c5cb/public/java/tests/uk/ac/ox/well/cortexjdk/utils/traversal/TraversalEngineTest.java#L229. (Oh and FYI, there's one place where my implementation of the read threading currently doesn't match the McCortex C reference implementation - the handling of sequencing errors and replacing the errorful kmers with kmers from the graph. It's an easy thing to add; I just hadn't gotten around to it because I didn't have the need to do that alignment myself given that all my graphs and links were constructed with McCortex anyway.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5828#issuecomment-475750843:242,access,accessing,242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5828#issuecomment-475750843,1,['access'],['accessing']
Security,Now can specify a master sequence dictionary that preempts all other; dictionaries that are found (in GATKTool.getBestAvailableSequenceDictionary). Added in associated validity checks with new dictionary in; GATKTool.validateSequenceDictionaries. Fixes #2410,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3058:217,validat,validateSequenceDictionaries,217,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3058,1,['validat'],['validateSequenceDictionaries']
Security,"Now that sequence dictionary validation is in, we can re-enable this test,; which was previously failing with a java.lang.OutOfMemoryError due to lack; of upfront validation of the reads vs. reference sequence dictionaries.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/668:29,validat,validation,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/668,2,['validat'],['validation']
Security,"Now that there's more rigorous sequence dictionary validation a bunch of dictionaries don't jive with the reference, especially files of the form src/test/resources/org/broadinstitute/hellbender/tools/copynumber/gcnv-postprocess/shard_0-calls/interval_list.tsv",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6957:51,validat,validation,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6957,1,['validat'],['validation']
Security,Now the validation test data sets are in the normal git file repository.; This allows them to be visually inspected for differences when they have; changed (during a code review). Fixes #5379,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5381:8,validat,validation,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5381,1,['validat'],['validation']
Security,"O ""$i"".snp.vcf.gz; gatk VariantFiltration -R $reference -V $path_BQSR/bqsr1.snp.vcf.gz -filter ""QD < 2.0"" --filter-name ""QD2"" -filter ""FS > 60.0"" --filter-name ""FS60"" -filter ""MQ < 40.0"" --filter-name ""MQ40"" -filter ""MQRankSum < -12.5"" --filter-name ""MQRankSum-12.5"" -filter ""ReadPosRankSum < -8.0"" --filter-name ""ReadPosRankSum-8.0"" -O $""i"".filtered.snp.vcf.gz; gatk SelectVariants -R $reference -V $""i"".filtered.snp.vcf.gz --exclude-filtered -O ""$i"".select.filtered.snp.vcf.gz; #INDEL; gatk SelectVariants -R $reference -V ""$i"".vcf.gz -select-type INDEL -O ""$i"".indel.vcf.gz; gatk VariantFiltration -R $reference -V ""$i"".indel.vcf.gz -filter ""QD < 2.0"" --filter-name ""QD2"" -filter ""FS > 200.0"" --filter-name ""FS200"" -filter ""ReadPosRankSum < -20.0"" --filter-name ""ReadPosRankSum-20"" -O ""$i"".filtered.indel.vcf.gz; gatk SelectVariants -R $reference -V ""$i"".filtered.indel.vcf.gz --exclude-filtered -O ""$i"".selected.filtered.indel.vcf.gz. gatk BaseRecalibrator -R $reference -I $i_bam -O grp1 --use-original-qualities --known-sites ""$i"".select.filtered.snp.vcf.gz --known-sites ""$i"".selected.filtered.indel.vcf.gz; gatk ApplyBQSR -R $reference -I $i_bam -O ""$i"".sorted.dedup.BQSR1.bam -bqsr grp1 --static-quantized-quals 10 --static-quantized-quals 20 --static-quantized-quals 30 --add-output-sam-program-record --create-output-bam-md5 --use-original-qualities; gatk ValidateSamFile -I ""$i"".sorted.dedup.BQSR.bam -O ""$i""_validateSamFile_of_bqsr_bam_file.out; samtools index ""$i"".sorted.dedup.BQSR.bam; done; gatk --java-options ""-Xmx30G"" HaplotypeCaller -R $reference -I sample1.sorted.dedup.BQSR.bam sample2.sorted.dedup.BQSR.bam sample3.sorted.dedup.BQSR.bam -O sample1_sample2_sample4.g.vcf.gz --tmp-dir tmp -ERC GVCF; ```. Secondly, how do I set the ploidy parameter for different samples in the population-based snp-calling?; Finally, for each taxa, there are some samples with relatively high sequence depth (> 10x). Is there any better choices for the snp-calling pipeline ??. Sincerely.; Jing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8414:2613,Validat,ValidateSamFile,2613,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8414,1,['Validat'],['ValidateSamFile']
Security,"O field at 1:768589 .. INFO tag [AC=1] expected different number of values (expected 2, found 1),INFO tag [AF=0.00047] expected different number of values (expected 2, found 1)`; ### Notes. Currently, all the validation modes call out to HTSJDK. Do we want to put the new functionality there as well?. ---. @yfarjoun commented on [Thu Jul 16 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122130280). I think that it is very appropriate to validate in htsjdk. On Thu, Jul 16, 2015 at 4:05 PM, ldgauthier notifications@github.com; wrote:. > Currently ValidateVariants relies on genotypes to transitively check that; > each alt allele occurs in at least one sample and that the AC adds up.; > However, this can fail on sites-only files because there are no genotypes.; > We should use the definition of the info annotations in the header to check; > how many entries each should have.; > Outline; > - Add a new validation type for info-field counts to enum and to; > switch statement; > - Grab info headers from input VCF with something like; > GATKVCFUtils.getVCFHeadersFromRods(getToolkit(),; > variantCollection.variants.getName()) and VCFHeader::getInfoHeaderLines; > - In the map() function, for each info header line, call on each; > VCFInfoHeaderLine getCount(vc) to get the expected number of info; > annotation entries; > - Compare the expected number with a count based on; > vc.getAttribute(currentVCFinfoHeaderLine.getID()), which will require some; > additional parsing because it returns an Object; > - (Bonus points if you use the isFixedCount() and getCount() functions; > on the VCF info header line to simplify annotations that aren't according; > to the number of alt alleles); > ; > Test data; > ; > /humgen/gsa-hpprojects/dev/gauthier/scratch/supportingMultiA.vcf; > Should fail AC/AF validation at; > 1 768589 . A C,G 76 PASS AC=1;AF=0.00047;AN=2120; > See results using:; > ; > use VCFtools; > vcf-validator /humgen/gsa-hpprojects/dev/gauthier/scra",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2507:2484,validat,validation,2484,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507,1,['validat'],['validation']
Security,"O yarn.Client: Preparing resources for our AM container; 20/10/22 12:02:26 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; 20/10/22 12:02:29 INFO yarn.Client: Uploading resource file:/tmp/spark-28ab5ef4-82d1-425e-879f-5056e9b51e43/__spark_libs__7655440475844189559.zip -> hdfs://192.168.0.104:9000/user/jacky/.sparkStaging/application_1603353714322_0004/__spark_libs__7655440475844189559.zip; 20/10/22 12:02:31 INFO yarn.Client: Uploading resource file:/home/jacky/Exec/gatk/build/libs/gatk-spark.jar -> hdfs://192.168.0.104:9000/user/jacky/.sparkStaging/application_1603353714322_0004/gatk-spark.jar; 20/10/22 12:02:33 INFO yarn.Client: Uploading resource file:/tmp/spark-28ab5ef4-82d1-425e-879f-5056e9b51e43/__spark_conf__3248804172036151699.zip -> hdfs://192.168.0.104:9000/user/jacky/.sparkStaging/application_1603353714322_0004/__spark_conf__.zip; 20/10/22 12:02:33 INFO spark.SecurityManager: Changing view acls to: jacky; 20/10/22 12:02:33 INFO spark.SecurityManager: Changing modify acls to: jacky; 20/10/22 12:02:33 INFO spark.SecurityManager: Changing view acls groups to: ; 20/10/22 12:02:33 INFO spark.SecurityManager: Changing modify acls groups to: ; 20/10/22 12:02:33 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jacky); groups with view permissions: Set(); users with modify permissions: Set(jacky); groups with modify permissions: Set(); 20/10/22 12:02:33 INFO yarn.Client: Submitting application application_1603353714322_0004 to ResourceManager; 20/10/22 12:02:33 INFO impl.YarnClientImpl: Submitted application application_1603353714322_0004; 20/10/22 12:02:34 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:34 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: default; 	 start ti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6906:3249,Secur,SecurityManager,3249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6906,1,['Secur'],['SecurityManager']
Security,"ODE VERBOSE --MAX_OUTPUT 100 --IGNORE_WARNINGS false --VALIDATE_INDEX true --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 1 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --help false --version false --verbosity INFO --QUIET false; [March 9, 2017 7:03:42 PM EST] Executing as gspowley@dna on Linux 3.10.0-514.10.2.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_111-b14; Version: Version:4.alpha.2-170-g8d06823-SNAPSHOT; 19:03:42.998 INFO ValidateSamFile - Defaults.BUFFER_SIZE : 131072; 19:03:42.999 INFO ValidateSamFile - Defaults.COMPRESSION_LEVEL : 1; 19:03:42.999 INFO ValidateSamFile - Defaults.CREATE_INDEX : false; 19:03:42.999 INFO ValidateSamFile - Defaults.CREATE_MD5 : false; 19:03:42.999 INFO ValidateSamFile - Defaults.CUSTOM_READER_FACTORY : ; 19:03:42.999 INFO ValidateSamFile - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 19:03:42.999 INFO ValidateSamFile - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 19:03:42.999 INFO ValidateSamFile - Defaults.REFERENCE_FASTA : null; 19:03:43.000 INFO ValidateSamFile - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:03:43.000 INFO ValidateSamFile - Defaults.USE_CRAM_REF_DOWNLOAD : false; 19:03:43.000 INFO ValidateSamFile - Deflater JdkDeflater; 19:03:43.000 INFO ValidateSamFile - Inflater JdkInflater; 19:03:43.000 INFO ValidateSamFile - Initializing engine; 19:03:43.000 INFO ValidateSamFile - Done initializing engine; ERROR: Record 9762, Read name 20GAVAAXX100126:7:2:8126:115177, bin field of BAM record does not equal value computed based on alignment start and end, and length of sequence to which read is aligned; ERROR: Record 24466, Read name 20FUKAA",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-285513571:1338,Validat,ValidateSamFile,1338,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2423#issuecomment-285513571,1,['Validat'],['ValidateSamFile']
Security,"OK - PedigreeValidationType is now set in the constructor and is final. This does not separate the two intertwined codepaths around PedigreeFile vs. FounderIds, but that was a pre-existing problem. It doesnt doesnt change the pre-existing weirdness around the timing of setting pedigreeFile and/or founderIds within GATKAnnotationPluginDescriptor, where PedigreeAnnotation gets special treatment. I dont think this makes that situation any worse. if you still have concerns on this proposal, I actually think I could make our code work if you simply exposed a protected getPedigreeFile() method on PedigreeAnnotation. I can make the SampleDB instance in my code without needed to share code here. It seemed useful to expose some of that code to avoid duplication, but if it's going to over-complicate we can remove it. Also: that one test failure seems potentially unrelated (https://travis-ci.com/github/broadinstitute/gatk/jobs/510624560)? A compile issue with javadoc?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-853986169:550,expose,exposed,550,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-853986169,4,['expose'],"['expose', 'exposed']"
Security,"OK, thanks @droazen. I'll go ahead and expose all of them in a branch for now. For my own education, perhaps @jamesemery or @vruano can comment---does turning on DRAGEN sidestep all or some subset of the code paths where the above 3 sets of parameters come into play?. For what it's worth, now that I'm looking at short variants in malaria as a ""novice"" HC/M2 user, the command line options are quite daunting! Many of them are not well documented and it's not always clear which options might interact with each other. Perhaps once the evaluations are in place, it might be worth doing some model ablation to see if we can come up with a more minimal set of options (including the consolidation of the parameters under discussion, if possible).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705096593:39,expose,expose,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705096593,2,['expose'],['expose']
Security,"OK, that's reasonable. I'll dig into the other test changes. I can answer a few:. - Regarding passing the VariantWalker: I agree that's not an improvement by itself, but I would argue it's not that much different than it was. My plan is to pass a VariantEvalContext object, which would obscure any need to have knowledge of the walker. In an attempt to keep this PR simpler, I didnt complete that work. I do expect to make a second PR in relatively short order, once we get this resolved. - With respect to testEvalTrackWithoutGenotypesWithSampleFields and the different reference: I think the issue is that the old version (master GATK branch) didnt validate as strictly. When switching to MultiVariantWalkerGroupedOnStart, the reference is required, and the tool will error if the contigs dont match. VariantEval on the master branch didnt really need the reference for anything, and was apparently more permissive if it didnt line up. It probably preferentially grabbed the dictionary from the VCF header. I will look into those other questions",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-744698072:651,validat,validate,651,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-744698072,2,['validat'],['validate']
Security,"OK. Passes checks. @droazen One final look? I couldn't figure out how to use runCommandLine, because I needed access to my walker instance. Is this OK, or is there a better way to do it?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7695#issuecomment-1089021503:110,access,access,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7695#issuecomment-1089021503,1,['access'],['access']
Security,"OK. StratManager is about 80% wired to do the merging this would require. It would require me to implement some existing methods, like VariantEvaluator.combine(), but the basic idea exists today in StratificationManager.combineStrats(). I'd probably propose to expose this via VariantEvalEngine by adding saveToDisk(File targetFile), and restore(List<File> serializedStratManagers) methods. I'd propose to keep interaction with this process protected and only expose the save/restore methods. Alternately, I could expose save, restore (single-object) and combine methods. Regarding actually saving the state of StratificationManager to disk: some form of Jackson-based serialization would probably be easiest. My initial thought would be to make a new class specifically for serializing: SerializedStratificationManager. This would cache the relevant information and be the object that is serialized/deserialized. I would add a new constructor to StratificationManager that accepts this object and restores the state within StratificationManager. I think this could be all be kept internal to GATK. Does that broad outline seem like a concept you might accept in GATK?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-755351932:261,expose,expose,261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-755351932,3,['expose'],['expose']
Security,"Oh I'm so sorry about that. I ran ValidateVariants on a few samples (running the whole batch now). This is the error I get when I don't specify the --validate-GVCF or --validation-type-to-exclude ALLELES . > A USER ERROR has occurred: Input renamed_seq1trimq10_LHA_AS02_1.raw_variants.g.vcf fails strict validation of type ALL: one or more of the ALT allele(s) for the record at position DS235882:56737 are not observed at all in the sample genotypes. The position in the gvcf. > DS235882 56737 . A T,<NON_REF> 0 . BaseQRankSum=-3.172;DP=29;ExcessHet=3.0103;MLEAC=0,0;MLEAF=0.00,0.00;MQRankSum=-1.507;RAW_MQandDP=97098,29;Re; adPosRankSum=-0.312 GT:AD:DP:GQ:PL:SB 0/0:27,2,0:29:67:0,67,1051,81,1056,1070:13,14,1,1. When I exclude the Alleles, I get no errors for the samples I checked thus far. This error cannot be causing the initial problem because I got the same error (in different position) on some gvcfs that are currently working fine with CombineGVCFs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6913#issuecomment-716710858:34,Validat,ValidateVariants,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6913#issuecomment-716710858,4,"['Validat', 'validat']","['ValidateVariants', 'validate-GVCF', 'validation', 'validation-type-to-exclude']"
Security,"Oh, I hadn't noticed that there was a compilation warning causing the test to fail. ```; /gatk/src/test/java/org/broadinstitute/hellbender/MainTest.java:55: warning: [serial] serializable class ExitNotAllowedExcepion has no definition of serialVersionUID; private static final class ExitNotAllowedExcepion extends SecurityException {; ^; error: warnings found and -Werror specified; ```. Please fix that also :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4283#issuecomment-361661772:314,Secur,SecurityException,314,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4283#issuecomment-361661772,1,['Secur'],['SecurityException']
Security,"Oh, good point. Maybe we should add a GATKRProtectedRegistrator that first applies the GATKRegistrator and then does additional gatk-protected specific registrations? We'll have to add a way to inject the right one though as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2337#issuecomment-272483079:194,inject,inject,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2337#issuecomment-272483079,1,['inject'],['inject']
Security,"Oh, that's right, I'd forgotten about the SGA license issue. Since we're; about to move to fermi-lite (hopefully), let's just hold off on checking in; the initialization script until that's done, keeping it in the known bucket; location. On Wed, Mar 8, 2017 at 11:37 AM, Steve Huang <notifications@github.com>; wrote:. > @cwhelan <https://github.com/cwhelan> I was actually debating with myself; > about whether to include the initialization script here, as it was living; > in the bucket referred to in the creation script.; > So we could do this:; > always store the initialization script locally with the creation script; > instead of referring to a script living remotely, and makes that a required; > argument. The good: this makes it easier to track changes; The bad:; > initialization script must be removed from the bucket to avoid tracking; > possible different versions.; >; > A non-technical issue: we are ""delivering"" SGA in the initialization; > script, if that comes in to this repo, legal might have a problem with it.; > On the other hand, it the initialization script lives in a place only we; > can access, we are ""installing SGA for our own use"", which is not a problem; > with the GPL license.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/2435#issuecomment-285093289>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AArTZZPv4WyEYz-yYaZZIIjH8LBMOhZ4ks5rjtlCgaJpZM4MTqFc>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2435#issuecomment-285105258:1117,access,access,1117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2435#issuecomment-285105258,1,['access'],['access']
Security,"Ok -- caveat for all -- objects in bucket that are accessed via simple API Key need to have: User:allUsers:reader ACL permissions. if you need more complex access control, we'll have to support the ""secretFile"" attribute in `gcloud dataproc` -- not just the apiKey.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1609#issuecomment-228425738:51,access,accessed,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1609#issuecomment-228425738,2,['access'],"['access', 'accessed']"
Security,"Ok @jean-philippe-martin, I have an updated patch that seems to resolve the 503 errors! It's here: https://github.com/droazen/google-cloud-java/tree/dr_retry_CloudStorageReadChannel_fetchSize. Will you have time before you leave on vacation to open a PR against google-cloud-java? If not, let me know and we'll try to sort out our CLA issues and PR it ourselves. I didn't have time to write unit tests, unfortunately, though we're running it now with 1000 concurrent jobs each accessing 11,000 files and not seeing any errors.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-315447319:477,access,accessing,477,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-315447319,1,['access'],['accessing']
Security,"Ok, thanks for validating that. I'll make a PR for this change (the way those unit tests are written is a little sketchy so I may fix that at the same time). I forgot about the spark failures - can you post the log output for those failures as well ?. Also, to answer your original question, you should generally always be able to work directly from head of master and all tests should pass, though sometimes things like this can slip through. Also, to answer your original question",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5511#issuecomment-446964064:15,validat,validating,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5511#issuecomment-446964064,1,['validat'],['validating']
Security,"Okay, I have a new panel for hg38 here: gs://broad-dsde-methods-davidben/mutect2-2023-panel-of-normals/mutect2-hg38-pon.vcf.gz. It has all the variants of the old panel, plus more that arose in more recent versions of Mutect2. It is also generally somewhat more conservative, with a greater bias toward precision than the previous one. This panel is intended to be used at your own risk. I can vouch that it doesn't wreck the results of our own validations but I do not have time to vet it thoroughly enough to put it in the best practices google bucket. Likewise, I cannot promise that it will improve specificity in any particular set of samples. Within several months I hope we are all running the next version of Mutect and never need to see a panel of normals again.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1430315034:445,validat,validations,445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1430315034,1,['validat'],['validations']
Security,"Once again I've managed to convince David R. to let me merge with some tech debt as follows:; - [ ] Add to GnarlyGenotyper an integration test like testRawAndFinalizedAlleleSpecificAnnotationsThoroughly() for GGVCFs; - [ ] Add a direct unit test for makeReducedAnnotationString() if you exposed it as package-accessible; - [ ] ~Break out finalized key definition, promote getKeyNames and getRawKeyNames to default methods in ReducibleAnnotation interface~; - [ ] One last `ann.getRawKeyNames().get(0)` in GnarlyGenotyperEngine",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6203:287,expose,exposed,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6203,2,"['access', 'expose']","['accessible', 'exposed']"
Security,"Once https://github.com/broadinstitute/gatk/pull/3620/ is in, we should be able to remove the download of picard.jar from .travis.yml, and change the M2 WDL to no longer depend having access to it. Workflow calls to picard tools can be replaced with calls to the same tools in GATK, although the argument syntax will have to change from picard style to Barclay style (""I=..."" to ""-I ..."").",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3625:184,access,access,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3625,1,['access'],['access']
Security,"Once https://github.com/samtools/htsjdk/pull/327 is merged into htsjdk and propagates to hellbender, let's audit our Spark tools to ensure that we are always serializing headerless SAMRecords",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1072:107,audit,audit,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1072,1,['audit'],['audit']
Security,"One more thing: I'm also wondering if it would be possible to get a quick, preliminary evaluation of such a process without actually doing the work of adding it into the training tool. It's probably possible to do a slightly more ""manual"" validation split (say, using one or a few chromosomes), run the score tool on that validation set, use some external code to calculate the desired threshold from the resulting scores, and then use that threshold going forward. Actually, now that I've written it out, that sounds a lot cleaner and more flexible! Let me try to hack together the corresponding workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1065543611:239,validat,validation,239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1065543611,2,['validat'],['validation']
Security,One of the issues:. [Utils] [ERROR] [Error] java.lang.IllegalArgumentException: Invalid interval. Contig:1 start:350001 end:300000; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.&lt;init&gt;(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.tools.copynumber.utils.annotatedinterval.AnnotatedIntervalUtilsUnitTest.provideMergeByAnnotation(AnnotatedIntervalUtilsUnitTest.java:215); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:55); 	at org.testng.internal.MethodInvocationHelper.invokeMethodNoCheckedException(MethodInvocationHelper.java:45); 	at org.testng.internal.MethodInvocationHelper.invokeDataProvider(MethodInvocationHelper.java:115); 	at org.testng.internal.Parameters.handleParameters(Parameters.java:509); 	at org.testng.internal.Invoker.handleParameters(Invoker.java:1308); 	at org.testng.internal.Invoker.createParameters(Invoker.java:1036); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1126); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5339#issuecomment-431874410:178,validat,validateArg,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5339#issuecomment-431874410,2,['validat'],"['validateArg', 'validatePositions']"
Security,"One part of this ticket is done: https://github.com/broadinstitute/gatk/pull/4964 added accessors that allow direct descendants of `GATKTool` to directly access engine datasources, while still forbidding direct access for tools that extend a Walker base class (except for Walker types living in the engine package, which still have access).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4341#issuecomment-483829878:88,access,accessors,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4341#issuecomment-483829878,4,['access'],"['access', 'accessors']"
Security,"Otherwise, when json is refreshed, contents of the file are different, hash of the file is different, and call-caching will not register a match, despite the same ""account"" being used. - changed input type from `File` to `String`; - changed the name to make it more obvious/clear. Closes https://github.com/broadinstitute/dsp-spec-ops/issues/327",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7347:71,hash,hash,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7347,1,['hash'],['hash']
Security,"Our R dependency is primarily for producing plots. It could be possible to create plots using javascript instead. Javascript plots have several potential advantages but also several major downsides. The biggest and most obvious drawback is that we don't have any code to produce them yet, and they are likely harder to generate and experiment with than R scripts. . The advantage would be that we could avoid requiring an R installation to run hellbender scripts, we could potentially also include interactive plotting or other neat tricks to make the plots more useful. I see 2 possible routes to replacing Rscripts with javascript. The first would be for tools that require graphs to perform some html generation and produce html reports with embedded javascript. The user could then open these in their browser and view the plots ( much like how our test suite report and jacoco is done). . A different option would be to use javascript plotting libraries directly within the jvm to generate SVG. Java 8 has a new javascript engine which is supposed to be reasonably fast and offers access to java objects from within it. Unfortunately it doesn't offer a full DOM like a browser does, so most existing javascript libraries will fall over. It seems like it would take a lot of hacking to get something like d3 to run directly on the jvm. (someone has done something of the kind here: http://jazdw.net/content/server-side-svg-generation-using-d3js) . Other options would be to use the javafx web panes to display a browser directly, or to plot directly on a canvas. Either of these options seem like they would be painful and awful.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/248:1086,access,access,1086,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/248,1,['access'],['access']
Security,"Out of 11 runs on exactly the same data, FilterByOrientationBias fails 6 times and succeeds 5 times. Assigning @LeeTL1220, given prior interaction with user. - User reports this error in: https://gatkforums.broadinstitute.org/gatk/discussion/comment/40412#Comment_40412; - My recapitulation is in: https://github.com/broadinstitute/dsde-docs/issues/2294. Data is at `/humgen/gsa-scr1/pub/incoming/byoo_FilterByOrientationBias.zip`. Command is:; ```; gatk-launch FilterByOrientationBias \; -A 'G/T' -A 'C/T' \; -V test2.vcf \; -P test2.pre_adapter_detail_metrics \; --output ob_filtered2.vcf; ```. Error message changes between:; ```; java.lang.IllegalStateException: Allele in genotype C* not in the variant context [G*, T]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.tools.exome.orientationbiasvariantfilter.OrientationBiasFilterer.annotateVariantContextsWithFilterResults(OrientationBiasFilterer.java:216); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:168); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:781); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3291:774,validat,validateGenotypes,774,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3291,2,['validat'],"['validate', 'validateGenotypes']"
Security,"Overriding the defaults would get us most of the way there. Right now, we perform the following check on the IAC and fail if the defaults aren't changed to values that the CNV tools require, which is awkward:. ```; /**; * Validate that the interval-argument collection parameters minimally modify the input intervals.; */; public static void validateIntervalArgumentCollection(final IntervalArgumentCollection intervalArgumentCollection) {; Utils.validateArg(intervalArgumentCollection.getIntervalSetRule() == IntervalSetRule.UNION,; ""Interval set rule must be set to UNION."");; Utils.validateArg(intervalArgumentCollection.getIntervalExclusionPadding() == 0,; ""Interval exclusion padding must be set to 0."");; Utils.validateArg(intervalArgumentCollection.getIntervalPadding() == 0,; ""Interval padding must be set to 0."");; Utils.validateArg(intervalArgumentCollection.getIntervalMergingRule() == IntervalMergingRule.OVERLAPPING_ONLY,; ""Interval merging rule must be set to OVERLAPPING_ONLY."");; }; ```. If we override defaults, we'd still perform the check to make sure the user didn't muck with them, but it'd still be nicer than forcing the user to change the original defaults on their own. However, there are still two more awkward points: 1) there is no value for `-interval-set-rule` that does *nothing* to the incoming intervals (you must either union or intersect), and 2) we have to specify our own `padding` argument in `PreprocessIntervals` that is distinct from `-interval-padding`, since we want to implement our own padding. The first can be easily addressed by adding an option to do nothing to the intervals; I'm not so sure what the best way to handle the second would be, so we can punt on it if it'd be more work than it's worth---these are relatively minor pain points, in the end.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4341#issuecomment-363219857:222,Validat,Validate,222,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4341#issuecomment-363219857,6,"['Validat', 'validat']","['Validate', 'validateArg', 'validateIntervalArgumentCollection']"
Security,"Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ad05b4d1-7aed-4482-8b5c-ced7b87d2d37).; Verified that GQ0 dropped in 'hail_lite' run and not in 'hail_vcf' run; (queries of count by state from ref ranges table):. **Hail Lite (Hail path, drop state 0):; state count**; 2 2495387; 3 4773472  ; 4 5959290. **Lite VCF (VCF path, drop_state 40):; state count**; 0 2764630; 2 2495387; 3 4773472. Spun up a notebook and ran the vds_validation.py script on the VDS generated by 'hail_lite'. And it passed:. > 2023-10-04 19:08:01.278 Hail: WARN: cols(): Resulting column table is sorted by 'col_key'.; > To preserve matrix table column order, first unkey columns with 'key_cols_by()'; > checking that:; > * no reference blocks have GQ=0; > * all ref blocks have END after start; > * all ref blocks are max 1000 bases long; > running densify on 200kb region (0 + 1) / 1]; > took 10.9s to densify 0 rows after interval query; > Hail VDS validation successful======================================(1 + 0) / 1]",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8538:981,validat,validation,981,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8538,1,['validat'],['validation']
Security,Passing VAT from VDS run on Quickstart here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/533017b9-2dfb-42ec-83ef-42dfda67f5c1; And a 'passing' (two expected failures) ValidateVat run here:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/e0af0c06-c724-4ae2-b821-6a04b558b21e,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8144#issuecomment-1371268610:208,Validat,ValidateVat,208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8144#issuecomment-1371268610,1,['Validat'],['ValidateVat']
Security,PathSeqPipelineSpark validation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8212:21,validat,validation,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8212,1,['validat'],['validation']
Security,"Per comments on the Slack channel, this can also be used as a component in germline tagging for matched tumor-normal pairs. For the same series above:. Here is the 100% normal run through ModelSegments, which yields 36 segments:; ![N modeled](https://user-images.githubusercontent.com/11076296/76631751-931d1a80-6518-11ea-8c18-2a8f41057ce3.png). Joint segmentation of the 100% normal and the 100% tumor yields 241 segments (up from 130 for the 100% tumor alone, as above). Using this joint segmentation for subsequent ModelSegments runs:. For the 100% normal, this yields 88 segments (up from 36):; ![N-SJS modeled](https://user-images.githubusercontent.com/11076296/76632024-ebecb300-6518-11ea-89ff-109c97970ef0.png). For the 100% tumor, this yields 166 segments (up from 130):; ![T-SJS modeled](https://user-images.githubusercontent.com/11076296/76632125-13dc1680-6519-11ea-9901-0c78809d08ba.png). I haven't performed detailed validations, but some spot checking suggests that this actually mitigate oversegmentation while still increasing sensitivity to shared events. For example, there is a small 13-bin deletion in chr19 that is found when running the 100% normal alone, but gets broken up into two adjacent deletions when running the 100% tumor alone (probably just due to statistical noise in the copy ratios). When running jointly, the deletion does not get broken up. However, as discussed over Slack, we should probably run some scenarios with simulated data to check behavior---for example, how robust is the joint segmentation to some of the samples being noisy/oversegmented?. There are lots of options for restructuring the workflow. We could potentially modify ModelSegments to take in the denoised copy ratios from the normal, when available, and add modeling of the normal and germline tagging to that tool. Or we could break things up into separate tools. @fleharty any opinions?. Note that another benefit of using this joint segmentation for germline tagging is that common breakp",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598764477:929,validat,validations,929,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598764477,1,['validat'],['validations']
Security,"Per discussion with @kgururaj, the proposal is to have GenomicsDB expose/document the existing protocol buffers already used internally, along with protobuf-based constructors for both GenomicsDBImporter and GenomicsDBFeatureReader.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3689#issuecomment-336962583:66,expose,expose,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3689#issuecomment-336962583,1,['expose'],['expose']
Security,"Per discussions with @fleharty, we are looking to significantly revamp the automated somatic CNV evaluations in preparation for benchmarking the TH prototype. The existing evaluations use a few unsupported/experimental tools and idiosyncratic/redundant classes (e.g., the `src/main/java/org/broadinstitute/hellbender/tools/copynumber/utils/annotatedinterval` class this issue concerns), the functionality of which we can hopefully move to python-based validation code. . The aforementioned code was purposefully decoupled from supported CNV code, but since then it has been incorporated into `Funcotator` tools and `ValidateBasicSomaticShortMutations`, at least. @jonn-smith @davidbenjamin can we discuss a plan for cleaning this code up? Would it be easy to use an existing TSV/XSV class to handle the functionality needed for these tools?. @jonn-smith perhaps we should also discuss the plan for future `FuncotateSegments` development/integration with @fleharty.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3884#issuecomment-526226506:452,validat,validation,452,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3884#issuecomment-526226506,2,"['Validat', 'validat']","['ValidateBasicSomaticShortMutations', 'validation']"
Security,Performance issues when accessing Reads in Interval/Feature based walkers.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1246:24,access,accessing,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1246,1,['access'],['accessing']
Security,Picard docs must reflect Picards command line syntax or their syntax as how it gets exposed thru GATK? I.e. ```INPUT=XXX``` or ```-I|--INPUT XXX```?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349489906:84,expose,exposed,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349489906,1,['expose'],['exposed']
Security,Picard tools don't perform validation of the sequence dictionary which will occasionally lead to errors. They should implement the same checking as the rest of our tools,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1272:27,validat,validation,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1272,1,['validat'],['validation']
Security,PileupElement : save memory and time by accessing bases and quals directly without copying.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1794:40,access,accessing,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1794,1,['access'],['accessing']
Security,PileupElement: save memory and time by accessing bases and quals directly without copying. Showed up on profile in HC,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1794:39,access,accessing,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1794,1,['access'],['accessing']
Security,Placeholders for now. We can tweak the actual values once @LeeTL1220 checks effect on validation. Closes #4032.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4046:86,validat,validation,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4046,1,['validat'],['validation']
Security,Please make this option hidden if it's only being kept for testing purposes (and document clearly that that is the case). Users should not have access to options that are not expected to have value.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2231#issuecomment-316842338:144,access,access,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2231#issuecomment-316842338,2,['access'],['access']
Security,"Please update the GitHub description to use https://www.broadinstitute.org/gatk/ which saves one redirect, and is more secure with rogue DNS servers.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3211:119,secur,secure,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3211,1,['secur'],['secure']
Security,Polishing hapmap M2 validation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3171:20,validat,validation,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3171,1,['validat'],['validation']
Security,PoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden; 443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to broad-jg-dev-11k-call-set/JointGenotyping/0cb36821-b8bf-4e6d-a352-07b101f6b7d1/call-ApplyRecalibration/shard-1734/GMKF_Seidman_CHD_WGS_904.filtered.1734.vcf.gz.; 	at shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoogleClientRequest.java:380); 	at shaded.cloud_nio.com.google.api.services.storage.Storage$Objects$Get.executeMedia(Storage.java:6133); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:505); 	... 12 more; ```. This service account does have access to these files because every shard accesses the same files and most of them succeed and when the same task is rerun it does succeed. @snovod should have any other information you may need.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3735:6610,access,access,6610,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3735,2,['access'],"['access', 'accesses']"
Security,"PoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). I don't understand why if the command is the same:; ```; $GATK_PATH BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 or 4000000 \; --input hdfs://namenode:8020/$dir_prepro$ubam \; --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit \; --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img \; --disable-sequence-dictionary-validation true \; --output hdfs://namenode:8020/$dir_prepro$output -- \; --spark-runner SPARK --spark-master spark://$SPARK_MASTER_HOST:7077 \; --driver-memory 20g --executor-cores 4 --executor-memory 8g; ```. Furthermore I have this problem with this version v4.0.4.0-23-g6e1cc8c-SNAPSHOT. > mark duplicate records objects corresponding to read with name, this could be the result of readnames spanning more than one partition; 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$null$0(MarkDuplicatesSpark.java:109); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$mark$62928560$1(MarkDuplicatesSpark.java:109); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:7087,Hash,HashMap,7087,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,1,['Hash'],['HashMap']
Security,Populating the DB SNP validation status field properly,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5046:22,validat,validation,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5046,1,['validat'],['validation']
Security,Porting over gvcf validation option to ValidateVariants,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3331:18,validat,validation,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3331,2,"['Validat', 'validat']","['ValidateVariants', 'validation']"
Security,"PostprocessGermlineCNVCalls performs a check of the denoising/calling hyperparameter configs used to generate the model in GermlineCNVCaller cohort mode against those used to generate the case-mode result passed to PostprocessGermlineCNVCalls. However, although some of these hyperparameters are not exposed in case mode (since they have no effect on the sample-level parameters inferred in case mode, e.g., `psi_t_scale`), their python default values are nevertheless written to the case-mode config. I think that this results in a spurious mismatch between the cohort/case mode configs, which causes PostprocessGermlineCNVCalls to emit the following warnings in case mode when non-default values are used:. ````; WARNING gcnvkernel.postprocess.viterbi_segmentation - Different denoising configuration between model and calls -- proceeding at your own risk!; WARNING gcnvkernel.postprocess.viterbi_segmentation - Different calling configuration between model and calls -- proceeding at your own risk!; ````. I'm pretty sure that inference is actually performed correctly, but we may want to double check and clean up these warnings. We should probably just copy the non-exposed values from the model config on the python side when running GermlineCNVCaller in case mode. Not sure if there's any way to emit sensible warnings on the Java side. These hyperparameters are still exposed to the Java command line in case mode, they just aren't passed on to the python command line. So the user can change their values from their engine defaults without having any effect at all, but this is probably what we want. Perhaps we can document, though.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6994:300,expose,exposed,300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6994,3,['expose'],['exposed']
Security,Prefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.SocketTimeoutException: Read timed out; 	at java.net.SocketInputStream.socketRead0(Native Method); 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); 	at java.net.SocketInputStream.read(SocketInputStream.java:171); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	at sun.security.ssl.InputRecord.read(InputRecord.java:503); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); 	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1569); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1474); 	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetH,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:6717,secur,security,6717,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['secur'],['security']
Security,"Previously, a temporary table is created as part of extract of the VQSR features, and it goes into a separate `temp_tables` dataset in the current project -- that is no longer true, and it now goes into the default dataset as a short living temp table with the task name and a hash. This pr should:. - default to the current dataset (with the VET etc tables) rather than a different dataset. - give a prefix to the temp tables so we know which one came from which step. - temp table TTL---not a changeable option, but default to 24 hours across the board. Still to discuss:; Parameterization of the location (dataset) to create the temp table in (default to the default dataset); manual clean up/TTL is a changeable option and TTL is parametrizable (currently the TTL is a parameter for the prepare step -- but then we set a default as 24 hrs in the WDL) . ![Screen Shot 2022-06-10 at 1 27 58 PM](https://user-images.githubusercontent.com/6863459/173121781-4486c1d1-ef7a-4ab8-aa62-fdc5018fd3b9.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7742:277,hash,hash,277,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7742,1,['hash'],['hash']
Security,Provide a mechanism for dataflow authentication information to be set once and stay set,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/701:33,authenticat,authentication,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/701,1,['authenticat'],['authentication']
Security,Provide a tool for outputting possible pathogen injection site on (human) host,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3458:48,inject,injection,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3458,1,['inject'],['injection']
Security,Providing counts for supporting alt reads in the validation normal.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5062:49,validat,validation,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5062,1,['validat'],['validation']
Security,Python tools need doc and validation that the conda environment is activated,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4127:26,validat,validation,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4127,1,['validat'],['validation']
Security,Qs of reads sharing kmers with putative SV breakpoints for local assembly; FlagStatSpark FlagStat on Spark; MarkDuplicatesSpark MarkDuplicates on Spark; MeanQualityByCycleSpark MeanQualityByCycle on Spark; PrintReadsSpark PrintReads on Spark; QualityScoreDistributionSpark QualityScoreDistribution on Spark; SortReadFileSpark SortSam on Spark (works on SAM/BAM/CRAM). --------------------------------------------------------------------------------------; Spark tools for structural variation analysis: Structural variation analysis tools that use Apache Spark for scaling out (experimental); CollectInsertSizeMetricsSpark Collect Insert Size Distribution on Spark. --------------------------------------------------------------------------------------; VCF: Tools for manipulating variants and associated metadata; CountVariants Count variants in a VCF file; ExampleVariantWalker Example tool that prints variants with optional contextual data; FilterVcf Hard-filters a VCF file; GatherVcfs Gathers multiple VCF files from a scatter operation into a single VCF file; GenotypeConcordance Calculates the concordance between genotype data for two samples in two different VCFs; IndexFeatureFile Creates indices for Feature-containing files (eg VCF and BED files); LiftOverVcf Lifts a VCF between genome builds; MakeSitesOnlyVcf Creates a VCF bereft of genotype information from an input VCF; MergeVcfs Merges multiple VCF files into one VCF file; RenameSampleInVcf Rename a sample within a VCF; SelectVariants Select a subset of variants from a larger callset in a VCF file; SortVcf Sorts one or more VCF files; SplitVcfs Splits an input VCF file into two VCF files; ValidateVariants Validate VCF; VariantFiltration Hard-filter variants VCF (mark them as FILTER); VariantsToTable Extract specific fields from a VCF file to a tab-delimited table; VcfToIntervalList Converts a VCF file to a Picard Interval List. --------------------------------------------------------------------------------------; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1669:9457,Validat,ValidateVariants,9457,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1669,2,['Validat'],"['Validate', 'ValidateVariants']"
Security,"Questions from developers, mainly, about things like ""my IntelliJ project is broken"" and ""GCS authentication not working"" -- you get the picture :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3199#issuecomment-312054764:94,authenticat,authentication,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3199#issuecomment-312054764,1,['authenticat'],['authentication']
Security,R code does not have access to changes in column names/file formats,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2862:21,access,access,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2862,1,['access'],['access']
Security,"READ_PAIR_DUPLICATES READ_PAIR_OPTICAL_DUPLICATES PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; Unknown 0 9951 0 0 0 0 0 0. ## HISTOGRAM java.lang.Integer; duplication_group_count Unknown; 1 9951; ```. MarkDuplicatesSpark; ```; ## htsjdk.samtools.metrics.StringHeader; # MarkDuplicatesSpark --output temp/align/markduplicates/c_lib1.bam --metrics-file stats/align/markduplicates/c_lib1.metrics.txt --input temp/align/bwa_aln/c_lib1_L001.sorted.bam --read-validation-stringency LENIENT --spark-master local[8] --allow-multiple-sort-orders-in-input false --treat-unsorted-as-querygroup-ordered false --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES --do-not-mark-unmapped-mates false --duplicate-tagging-policy DontTag --remove-all-duplicates false --remove-sequencing-duplicates false --read-name-regex <optimized capture of last three ':' separated fields as numeric values> --optical-duplicate-pixel-distance 100 --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --use-nio false --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --create-output-bam-index true --create-output-bam-splitting-index true --splitting-index-granularity 4096 --create-output-variant-index true --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false; ## htsjdk.samtools.metrics.StringHeader; # Started on: March 24, 2021 9:31:36 PM CET. ## METRICS CLASS org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics; LIBRARY UNPAIRED_READS_EXAMINED READ_PAIRS_EXAMINED SECONDARY_OR_SUPPLEMENTARY_RDS UNMAPPED_READS UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES READ_PAIR_OPTICAL_DUPLICATES PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; Unknown Library 0 9998 0 0 0 0 0 0; ```. MarkDuplica",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7161:2227,validat,validation-stringency,2227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7161,2,['validat'],"['validation', 'validation-stringency']"
Security,"RN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; 20/10/22 12:02:29 INFO yarn.Client: Uploading resource file:/tmp/spark-28ab5ef4-82d1-425e-879f-5056e9b51e43/__spark_libs__7655440475844189559.zip -> hdfs://192.168.0.104:9000/user/jacky/.sparkStaging/application_1603353714322_0004/__spark_libs__7655440475844189559.zip; 20/10/22 12:02:31 INFO yarn.Client: Uploading resource file:/home/jacky/Exec/gatk/build/libs/gatk-spark.jar -> hdfs://192.168.0.104:9000/user/jacky/.sparkStaging/application_1603353714322_0004/gatk-spark.jar; 20/10/22 12:02:33 INFO yarn.Client: Uploading resource file:/tmp/spark-28ab5ef4-82d1-425e-879f-5056e9b51e43/__spark_conf__3248804172036151699.zip -> hdfs://192.168.0.104:9000/user/jacky/.sparkStaging/application_1603353714322_0004/__spark_conf__.zip; 20/10/22 12:02:33 INFO spark.SecurityManager: Changing view acls to: jacky; 20/10/22 12:02:33 INFO spark.SecurityManager: Changing modify acls to: jacky; 20/10/22 12:02:33 INFO spark.SecurityManager: Changing view acls groups to: ; 20/10/22 12:02:33 INFO spark.SecurityManager: Changing modify acls groups to: ; 20/10/22 12:02:33 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jacky); groups with view permissions: Set(); users with modify permissions: Set(jacky); groups with modify permissions: Set(); 20/10/22 12:02:33 INFO yarn.Client: Submitting application application_1603353714322_0004 to ResourceManager; 20/10/22 12:02:33 INFO impl.YarnClientImpl: Submitted application application_1603353714322_0004; 20/10/22 12:02:34 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:34 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: default; 	 start time: 1603360953394; 	 final status: UNDEFINED; 	 tracking URL: http://jacky:80",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6906:3325,Secur,SecurityManager,3325,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6906,1,['Secur'],['SecurityManager']
Security,ROC Curve Walker for validation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2723:21,validat,validation,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2723,1,['validat'],['validation']
Security,"R_SAMTOOLS : true; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:53:34.608 INFO ValidateVariants - Deflater: IntelDeflater; 19:53:34.608 INFO ValidateVariants - Inflater: IntelInflater; 19:53:34.608 INFO ValidateVariants - GCS max retries/reopens: 20; 19:53:34.608 INFO ValidateVariants - Requester pays: disabled; 19:53:34.608 INFO ValidateVariants - Initializing engine; 19:53:35.169 INFO FeatureManager - Using codec VCFCodec to read file file://chr1-22.phased.rename.reheader.vcf.gz; 19:53:35.594 INFO ValidateVariants - Done initializing engine; 19:53:35.594 WARN ValidateVariants - IDS validation cannot be done because no DBSNP file was provided; 19:53:35.594 WARN ValidateVariants - Other possible validations will still be performed; 19:53:35.594 INFO ProgressMeter - Starting traversal; 19:53:35.595 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 19:53:35.660 INFO ValidateVariants - Shutting down engine; [October 25, 2020 7:53:35 PM CDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2114453504; java.lang.ArrayIndexOutOfBoundsException: -87; 	at org.broadinstitute.hellbender.utils.BaseUtils.convertIUPACtoN(BaseUtils.java:123); 	at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.getSubsequenceAt(CachingIndexedFastaSequenceFile.java:340); 	at org.broadinstitute.hellbender.engine.ReferenceFileSource.queryAndPrefetch(ReferenceFileSource.java:78); 	at org.broadinstitute.hellbender.engine.ReferenceDataSource.queryAndPrefetch(ReferenceDataSource.java:64); 	at org.broadinstitute.hellbender.engine.ReferenceContext.getBases(ReferenceContext.java:197); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants.apply(ValidateVariants.java:236); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	at org.broadinstitute.h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911:3238,Validat,ValidateVariants,3238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911,1,['Validat'],['ValidateVariants']
Security,Rc vat validation typo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7366:7,validat,validation,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7366,1,['validat'],['validation']
Security,Re-enable BaseRecalibratorDataflowIntegrationTest.testBQSRFailWithIncompatibleReference once sequence dictionary validation is in,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/625:113,validat,validation,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/625,1,['validat'],['validation']
Security,"Read counts at different stages of the PathSeq pipeline are now logged using `MetricsFile`. The filter metrics contains the number of reads remaining and number of reads filtered at each step (after filtering pre-aligned reads, low quality/complexity reads, host reads, and duplicates). The score metrics give number of pathogen-mapped and unmapped reads. These metrics are now validated in the PathSeq integration tests, which have also been refactored to use DataProviders instead of separate functions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3611:378,validat,validated,378,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3611,1,['validat'],['validated']
Security,ReadSparkSink adds a crc checksum file even for local files,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1266:25,checksum,checksum,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1266,1,['checksum'],['checksum']
Security,ReadsDataSource: enable setting validation stringency,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/181:32,validat,validation,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/181,1,['validat'],['validation']
Security,ReadsDataSource: enable setting validation stringency.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/585:32,validat,validation,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/585,1,['validat'],['validation']
Security,ReadsDataSource: expose setting validation stringency at command line,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/611:17,expose,expose,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/611,2,"['expose', 'validat']","['expose', 'validation']"
Security,Recent refactoring seems to have introduced a bug in pileup mode that failed to enforce the limit on the number of haplotypes to be considered. With this patch:. - HaplotypeCaller once again respects the limit on haplotypes before genotyping.; - Changed some `HashSet`s to `LinkedHashSets` to preserve determinism.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8489:260,Hash,HashSet,260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8489,1,['Hash'],['HashSet']
Security,Refactor MT wdl to make validations easier,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5708:24,validat,validations,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5708,1,['validat'],['validations']
Security,"Reference, Feature and whatever context fully by injection.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/242:49,inject,injection,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/242,1,['inject'],['injection']
Security,"Regarding writing the SAM file, I now remember why it was created instead of using existing ones: the SV pipeline needs to be able to write to HDFS, hence cannot rely on a `File` interface&mdash;which the `ReadUtils.createCommonSAMWriter(...)` exposes. ; So the current hand rolled version in `SVFileUtils.getSAMFileWriter()` calls into `BucketUtils.createFile(...)` for that HDFS compatibility, and then makes use of the `SAMFileWriterFactory.makeBAMWriter(final SAMFileHeader header, final boolean presorted, final OutputStream stream)`, unlike `ReadUtils.createCommonSAMWriter(...)` which calls into `SAMFileWriterFactory.makeSAMOrBAMWriter(final SAMFileHeader header, final boolean presorted, final Path outputPath)`. So in short: HDFS compatibility.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3674#issuecomment-339391912:244,expose,exposes,244,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3674#issuecomment-339391912,1,['expose'],['exposes']
Security,Remove the <20 obfuscation for AC; This needs to be removed from the Python (which did the original obfuscation); And the VAT Validation WDL -- which checked on it,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7435:126,Validat,Validation,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7435,1,['Validat'],['Validation']
Security,Removes unnecessary and buggy validation check,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8580:30,validat,validation,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8580,1,['validat'],['validation']
