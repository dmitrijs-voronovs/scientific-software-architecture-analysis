quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Modifiability,"benchmarks were kind of their own thing and a little neglected.; This change moves the benchmarks into the `hail/python` folder and updates them to use pytest with a custom plugin/set of pytest hooks.; Now, benchmarks can be run from the command line like any pytest.; This change removes the `benchmark-hail` (or `hailbench`) utility. Benchmarks are marked by `pytest.mark.benchmark` (via the `@benchmark` decorator).; By convention, benchmarks are python tests whose names are prefixed by `benchmark_` and are located in files with the same prefix.; Nothing enforces this, however, so you could name your benchmarks `test_*` and put them in files named `test_*.py`.; Benchmarks may import and use any test code or utilities defined in `test/`.; The results of each benchmark are outputted as json lines (`.jsonl`) to the file specified by the `--output` pytest arg or stdout. The folder structure should be familiar, resembling our `test/` directory.; I believe this is flexible enough to add `hailtop` benchmarks should we so wish:; ```; pytest.ini - hoisted from `test/` to include benchmark marks; benchmark/; - conftest.py for custom pytest command line args ; - hail/; - confest.py for custom plugin that runs hail benchmarks; - benchmark_*.py hail query benchmark code; - tools/; - shared utilites, including the `@benchmark`; ```; Supporting pytest fixtures required writing a custom plugin to run benchmarks, as using off-the-shelf; solutions like `pytest-benchmark` would forbid method level fixtures like `tmp_path` etc.; The plugin is designed to run ""macro-benchmarks"" (ie long-running tests) and fully supports pytest parameterisation.; For each benchmark, the plugin initialises hail and then repeats (for a number of iterations defined by the pytest mark); acquiring fixtures, timing invocation and tearing-down fixtures, finally stopping hail. It is therefore unsuitable for; microbenchmarks, for which we currenly have none in python. If we add them we'd need to tweak this so supp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14565:979,flexible,flexible,979,https://hail.is,https://github.com/hail-is/hail/pull/14565,1,['flexible'],['flexible']
Modifiability,binomTest refactored to use integer enum instead of string.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3439:10,refactor,refactored,10,https://hail.is,https://github.com/hail-is/hail/pull/3439,1,['refactor'],['refactored']
Modifiability,"ble. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_cer",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:7040,config,config-site,7040,https://hail.is,https://github.com/hail-is/hail/pull/8561,1,['config'],['config-site']
Modifiability,"bot.com/Azure/azure-sdk-for-java/issues/31389"">#31389</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/a651fcc7c0026641717465f8bbae64de808df187""><code>a651fcc</code></a> Doc consistenty review updates (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/31332"">#31332</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/6b1aef8c5c09239d6e4534a116fba195891e3d57""><code>6b1aef8</code></a> target version with fixes (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/31310"">#31310</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/b17fed8df6b512691656673a6c5b7e8033ff31c2""><code>b17fed8</code></a> Prepare release for Schema Registry (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/31375"">#31375</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/1c2a0d90d42432387794daa3c03469fc0fcd1060""><code>1c2a0d9</code></a> [Perf] Call configureClientBuilder() in DataLake and FileShare tests (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/31324"">#31324</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/c85090e334e4ff77a3b3178a57b8b6d24518859d""><code>c85090e</code></a> Create Media Streaming package parser with updated contracts (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/31309"">#31309</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/6f7b2986b03ae7c4ab67f52452c5af7f260a3880""><code>6f7b298</code></a> Add Merge-Branch script eng/scripts (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/31222"">#31222</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/02fdc27bc10fa3e1bda08f3125059e95be8a4bb0""><code>02fdc27</code></a> Release/azure communication common/1.2.2 (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12450:1953,config,configureClientBuilder,1953,https://hail.is,https://github.com/hail-is/hail/pull/12450,1,['config'],['configureClientBuilder']
Modifiability,"bot.com/pytest-dev/pytest/issues/9396"">#9396</a>: Ensure <code>pytest.Config.inifile</code>{.interpreted-text role=&quot;attr&quot;} is available during the <code>pytest_cmdline_main &lt;_pytest.hookspec.pytest_cmdline_main&gt;</code>{.interpreted-text role=&quot;func&quot;} hook (regression during <code>7.0.0rc1</code>).</li>; </ul>; <h2>Improved Documentation</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9404"">#9404</a>: Added extra documentation on alternatives to common misuses of [pytest.warns(None)]{.title-ref} ahead of its deprecation.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9505"">#9505</a>: Clarify where the configuration files are located. To avoid confusions documentation mentions; that configuration file is located in the root of the repository.</li>; </ul>; <h2>Trivial/Internal Changes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9521"">#9521</a>: Add test coverage to assertion rewrite path.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pytest-dev/pytest/commit/3f12087fe0d86a319216653b08b66a96d400bee2""><code>3f12087</code></a> [pre-commit.ci] auto fixes from pre-commit.com hooks</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/bc3021cdfd76507aa3d9e278bd885da9bc1907b2""><code>bc3021c</code></a> Prepare release version 7.0.1</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/591d476f14e3e83d90fbea75d326a93c5e368708""><code>591d476</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9673"">#9673</a> from nicoddemus/backport-9511</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/6ca733e8f19fa5c4271bf3e5bb295c8b62757e4a""><code>6ca733e</code></a> Enable testing with Python 3.11 (<a href=""https://github-redirect.dependabot.com/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11516:3834,rewrite,rewrite,3834,https://hail.is,https://github.com/hail-is/hail/pull/11516,3,['rewrite'],['rewrite']
Modifiability,"by the key is irrelevant. Is this surprising to a user? Suppose they had a text file ordered by `family id, sample id`, they might reasonably expect that `import_table(..., key=['family id'])` does not reorder the rows within a family. Hail doesn't guarantee this even though we do make pains to not reorder the same data imported by `import_table(..., key=[])`. # Ordering and the Optimizer. Currently, the Hail optimizer does not remove a `choose_cols` that precedes a `order_by()`. Nor does it remove an `order_by(x, ...)` that precedes an `order_by()`. It does, however, remove a `key_by(x, ...)` that precedes an `order_by()` or a `key_by()`. ```; In [47]: import hail as hl ; ...: mt = hl.utils.range_matrix_table(3,3) ; ...: mt = mt.key_cols_by().choose_cols([1,2,0]) ; ...: mt.cols().order_by().show() ; ...: ; +---------+; | col_idx |; +---------+; | int32 |; +---------+; | 1 |; | 2 |; | 0 |; +---------+. In [48]: t = hl.utils.range_table(3) ; ...: t = t.order_by(-t.idx) ; ...: t = t.order_by().show() ; +-------+; | idx |; +-------+; | int32 |; +-------+; | 2 |; | 1 |; | 0 |; +-------+. In [49]: t = hl.utils.range_table(3) ; ...: t = t.key_by(x=-t.idx) ; ...: t = t.order_by().show() ; +-------+-------+; | idx | x |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 0 |; | 1 | -1 |; | 2 | -2 |; +-------+-------+. In [50]: t = hl.utils.range_table(3) ; ...: t = t.key_by(x=-t.idx) ; ...: t = t.key_by().show() ; +-------+-------+; | idx | x |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 0 |; | 1 | -1 |; | 2 | -2 |; +-------+-------+; ```. # Dealing With It. In practice, this following will remove the key on a table without changing the ordering imposed by previous order-changing operations. (NB: ""latent"" ordering inherited from a file [see aforementioned family id, sample id example] is not guaranteed to be preserved by this though, in practice, it often is). ```python; def unkey(t):; if len(t.key) != 0:; t = t.order_by(t.key); return t; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6929:6325,inherit,inherited,6325,https://hail.is,https://github.com/hail-is/hail/issues/6929,1,['inherit'],['inherited']
Modifiability,cala); E 	at __C256669Compiled.apply(Emit.scala); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$7(CompileAndEvaluate.scala:74); E 	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:74); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:19); E 	at is.hail.expr.ir.lowering.LowerDistributedSort$.distributedSort(LowerDistributedSort.scala:163); E 	at is.hail.backend.service.ServiceBackend.lowerDistributedSort(ServiceBackend.scala:354); E 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:100); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:23); E 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$4(RewriteBottomUp.scala:26); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:36); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:20); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:157); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LowerAndExecuteSh,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:1597,Rewrite,RewriteBottomUp,1597,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['Rewrite'],['RewriteBottomUp']
Modifiability,"cala:311; 2022-05-14 12:09:11 root: INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 30: Thread-4; 2022-05-14 12:09:11 root: ERROR: HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advan",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:1957,adapt,adapted,1957,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['adapt'],['adapted']
Modifiability,cala:41); 	at scala.collection.Iterator.foreach(Iterator.scala:943); 	at scala.collection.Iterator.foreach$(Iterator.scala:943); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); 	at is.hail.expr.ir.analyses.SemanticHash$.go$1(SemanticHash.scala:41); 	at is.hail.expr.ir.analyses.SemanticHash$.$anonfun$apply$4(SemanticHash.scala:54); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.analyses.SemanticHash$.$anonfun$apply$1(SemanticHash.scala:34); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.analyses.SemanticHash$.apply(SemanticHash.scala:26); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:509); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:546); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:542); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:541); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:62); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$3(SparkBackend.scala:368); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:364); 	at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:541); 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:51); 	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79); 	at sun.net.httpserver.AuthFilter.doFi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13915:6204,adapt,adapted,6204,https://hail.is,https://github.com/hail-is/hail/issues/13915,1,['adapt'],['adapted']
Modifiability,"can return in two possible ways:; - `Missing` - the stream is actually entirely missing; this usually happens if one of the parameters to a stream is missing (e.g. `ArrayRange(0, NA, 1) = NA`); - `State(s0)` - the stream has started; its initial state is `s0`. `step` can return in three possible ways as well:; - `EOS` - we've reached the End Of Stream, there are no more elements left.; - `Skip(s1)` - this iteration didn't produce an element, you must try stepping again with state `s1` (see ""design notes"").; - `Yield(elt, s1)` - the stream computed an element `elt`; the following stream state will be `s1`. **Design Notes**. - The `Skip` return is very useful for simplifying the implementation of `ArrayFilter`. There is basically no nice way to implement filter otherwise without introducing some significant code duplication.; - ~~The stream ""parameter"", as well as the `Empty` return, are not very useful for the basic streams in this PR. However, they simplify the implementation of `ArrayFlatMap` (aka ""composing"" two parameterized streams).~~ NOTE (to Patrick): I decided to abandon the ""empty"" return idea in favor of just providing ""default states"" that always yield empty streams. **Implementation Notes**. - The implementation makes great use of Scala's type system. Most of the streams are implemented first in a very type aware manner, where it is easy to reason about the types of data flowing in and out, before being instantiated with EmitTriplets and Envs which don't hold very much type information. For instance, we have the following helper for `map`:; ```scala; Parameterized[P, A].map(f: A => B): Parameterized[P, B]; ```; The emitter instantiates P = `Any`, A = `EmitTriplet`, B = `EmitTriplet` :/. - Complex streams will have non trivial control flow and jumps. Therefore `init` and `step` both take `JoinPointBuilder`s and return `Code[Ctrl]`, to indicate that they may create join points and do jumps. - I've utilized a cute continuation passing style trick in multiple",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7228:2067,parameteriz,parameterized,2067,https://hail.is,https://github.com/hail-is/hail/pull/7228,1,['parameteriz'],['parameterized']
Modifiability,"can we please get this in? If there's a reason I'm not seeing to be cautious, I want to hear it. But for my own sanity, I'd like another line of defense against needing to rewrite benchmark-on-pipeline again.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7864#issuecomment-583466260:172,rewrite,rewrite,172,https://hail.is,https://github.com/hail-is/hail/pull/7864#issuecomment-583466260,1,['rewrite'],['rewrite']
Modifiability,"cc @iris-garden: You might find this discussion interesting. > Is it kosher to use write_output on the individual items within a ResourceGroup like this?. Yes, that is fine. > The resulting code is IMHO overall less clear than the original version in which the resource group models the relationship between all three filenames. The resource group was intended for sets of files that should be **localized** together. The primary motivation was for PLINK files with `.bed, .bim, and .fam` extensions. This is not quite your use case. > If Hail Batch is a well-rounded orthogonal API, then that code ought to work too. It's a fair criticism that the ResourceGroup API isn't very flexible. We can think about ways to improve it. It's unclear from your code what exactly you want to have happen. Do you want to be able to declare not to localize the tsv only file? In this toy example, a string path is assumed to be `ResourceGroupFile(path, localize=True)`. ```python3; j = b.new_job(…). j.declare_resource_group(counts={; 'tsv': ResourceGroupFile('{root}.counts.tsv', localize=False),; 'tsv.gz': '{root}.counts.tsv.gz',; 'tsv.gz.tbi': '{root}.counts.tsv.gz.tbi',; }). j.command(f""""""; gatk SubCommand … --output {j.tsv_counts}; bgzip {j.tsv_counts}; gatk IndexFeatureFile --input {j.counts['tsv.gz']}; """"""). b.write_output(j.counts, output_dir_path); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13191#issuecomment-1599590271:678,flexible,flexible,678,https://hail.is,https://github.com/hail-is/hail/issues/13191#issuecomment-1599590271,2,['flexible'],['flexible']
Modifiability,"cc @tpoterba . My apologies. I made several changes to lowered logistic regression as well. All the generalized linear model methods share the same fit result. I abstracted this into one datatype at the top of `statgen.py`: `numerical_regression_fit_dtype`. ---. You'll notice I moved the cases such that we check for convergence *before* checking if we are at the maximum iteration. It seemed to me that:; - `max_iter == 0` means do not even attempt to fit.; - `max_iter == 1` means take one gradient step, if you've converged, then return successfully, otherwise fail.; - etc. The `main` branch currently always fails if you set `max_iter == 1`, even if the first step lands on the true maximum likelihood fit. I substantially refactored logistic regression. There were dead code paths (e.g. the covariates array is known to be non-empty). I also found all the function currying and comingling of fitting and testing really confusing. To be fair, the Scala code does this (and its really confusing). I think the current structure is easier to follow:. 1. Fit the null model.; 2. If wald, assume the beta for the genotypes is zero and use the rest of the parameters from the null model fit to compute the score (i.e. the gradient of the likelihood). Recall calculus: gradient near zero => value near the maximum. Return: this is the test.; 3. Otherwise, fit the full model starting at the null fit parameters.; 4. Test the ""goodness"" of this new & full fit. ---. Poisson regression is similar but with a different likelihood function and gradient thereof. Notice that I `key_cols_by()` to indicate to Hail that the order of the cols is irrelevant (the result is a locus-keyed table after all). This is necessary at least until #12753 merges. I think it's generally a good idea though: it indicates to Hail that the ordering of the columns is irrelevant, which is potentially useful information for the optimizer!. ---. Both logistic and Poisson regression can benefit from BLAS3 by running at least t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12793:729,refactor,refactored,729,https://hail.is,https://github.com/hail-is/hail/pull/12793,1,['refactor'],['refactored']
Modifiability,"cc: @akotlar @cseed . I whipped this up today. It modifies the batch api to accept user-configurable cookies and headers. The `BatchClient` looks for a token in `~/.hail/batch/token` or at a user-provided path. There's a whitelist of authorized users. If you can talk directly to batch, you can spoof the `User` header. This makes testing easy. For this approach to be secure, we need to ensure:; - only `gateway` may send HTTP requests to `batch`; - `gateway` does not forward any ""secure"" headers (currently, just `User`)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5605:88,config,configurable,88,https://hail.is,https://github.com/hail-is/hail/pull/5605,1,['config'],['configurable']
Modifiability,"cc: @cseed, @patrick-schultz, @catoverdrive . A `ContextRDD[C, T]` is an `RDD[C => Iterator[T]]` and captures the idea that a computation needs a context. I did not inherit from RDD so that `map` and friends can be implemented as if this was an `RDD[T]`. To access the context, one uses `cmap` and friends. The lifetime of a context corresponds roughly to the dynamic extent of an `RDD` computation happening on a single worker node. In particular, a context always ends before a `shuffle` or other network communication happens. One may explicitly end a `ContextRDD`'s context lifetime by calling `ContextRDD.run` which produces an `RDD[T]`. Generally the partitions of a `ContextRDD[C, T]` need only contain one element, but (I believe) I have written `ContextRDD[C, T]` to also handle many `C => Iterator[T]` functions inside a single partition. Most operations on `RVD` do not use the `crdd` yet. I have a growing local stack of branches in which more of these operations are implemented. I am only holding them back because stacked PRs have proven unwieldy. Finally, I added `RVDContext`, which is not fleshed out. It will need to carry at least a region. We might also want it to carry a random seed. Unless you have severe issues with it, I think it's best to treat it as a placeholder for something that will later adopt its name.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3221:165,inherit,inherit,165,https://hail.is,https://github.com/hail-is/hail/pull/3221,1,['inherit'],['inherit']
Modifiability,"cc: @cseed, this explains some weirdness with docker caches. The Docker docs are [misleading at best wrt `--cache-from`](https://github.com/moby/moby/issues/32612). `--cache-from X` means treat `X`'s layers as a cache source *and do not use the local cache*. This is a crucial misfeature for two reasons. First, you [must explicitly specify every image that may have useful cache layers](https://github.com/moby/moby/issues/33002). Second, you cannot include in the cache untagged local images. The latter is particularly an issue for local developers who might run docker builds that fail half-way through. The first issue is not relevant to us because we don't share many layers between images. The second issue is addressed with Makefile conditions that provide a different experience for local versus CI users. Each CI deploy pushes the `latest` tag so that future builds can use it as a cache.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5016:200,layers,layers,200,https://hail.is,https://github.com/hail-is/hail/pull/5016,3,['layers'],['layers']
Modifiability,"cc: @daniel-goldstein, this is a tricky asyncio situation which you should also keep in mind. OK, there were two problems:. 1. A timeout of 5s appears to be now too short for Google Cloud Storage. I am not sure why but we; timeout substantially more frequently. I have observed this myself on my laptop. Just this; morning I saw it happen to Daniel. 2. When using an `aiohttp.AsyncIterablePayload`, it is *critical* to always check if the coroutine; which actually writes to GCS (which is stashed in the variable `request_task`) is still; alive. In the current `main`, we do not do this which causes hangs (in particular the timeout; exceptions are never thrown ergo we never retry). To understand the second problem, you must first recall how writing works in aiogoogle. There are; two Tasks and an `asyncio.Queue`. The terms ""writer"" and ""reader"" are somewhat confusing, so let's; use left and right. The left Task has the owning reference to both the source ""file"" and the; destination ""file"". In particular, it is the *left* Task which closes both ""files"". Moreover, the; left Task reads chunks from the source file and places those chunks on the `asyncio.Queue`. The; right Task takes chunks off the queue and writes those chunks to the destination file. This situation can go awry in two ways. First, if the right Task encounters any kind of failure, it will stop taking chunks off of the; queue. When the queue (which has a size limit of one) is full, the left Task will hang. The system; is stuck. The left Task will wait forever for the right Task to empty the queue. The second scenario is exactly the same except that the left Task is trying to add the ""stop""; message to the queue rather than a chunk. In either case, it is critical that the left Task waits simultaneously on the queue operation *and*; on the right Task completing. If the right Task has died, no further writes can occur and the left; Task must raise an exception. In the first scenario, we do not observe the right Task'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11830:504,variab,variable,504,https://hail.is,https://github.com/hail-is/hail/pull/11830,1,['variab'],['variable']
Modifiability,"cc: @danking . Here is the first major database migration. The goal is to add all attempt resources into the database for any attempt with batch format version less than 3. The node configuration was the same for all attempts back then with standard instances, 16 cores, and a 100GB boot disk. I figured out what the quantity for each resource should be by looking at how we compute quantities for resources in `batch/batch/resources.py`. I checked the inserted quantities are identical to the attempts that already exist in the database right after we converted the billing over to using resources. Once we have all resources for all attempts, the next step (future PR) is to do a scan and repopulate the new aggregated billing tables **by date**. In this PR, I don't try and add the usage to the existing `aggregated_*_resources` tables. I did this to cut down on time and space since we're eventually going to deprecate those tables anyways. Because I don't touch those tables, we don't need to worry about modifying the client code and how the current billing information is calculated. How this migration works is there are 5 phases:; 1. Compute the expected number of attempts to process for format version < 3. ; 2. Divide the search space into chunks of size 100 attempts (empirically determined this was the best chunk size) and randomize the order of the chunks.; 3. Serially process 5000 chunks with only 10 out of the 100 records as a ""burn in period"" to avoid the birthday problem when trying to insert records in parallel.; 4. In parallel, process all the chunks with 10 way parallelism (empirically determined to max CPU for a 4 core db instance); 5. Do an audit of the results to make sure the attempt resources now has the correct number of rows and the billing is within $0.001 per job with the old way and new way of computing the billing. The tolerance of $0.001 was empirically determined. At a threshold of $0.0001, 33/30,000,000 attempts failed. I think this is good enough as t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11990:182,config,configuration,182,https://hail.is,https://github.com/hail-is/hail/pull/11990,1,['config'],['configuration']
Modifiability,"cc: @tpoterba ; ![Screen Shot 2020-03-30 at 6 53 49 PM](https://user-images.githubusercontent.com/106194/77969485-cd780d00-72b7-11ea-96fc-4f529297c830.png). Since my mind was already thinking about CI to address other issues, I thought it prudent to implement this longstanding request. The key idea is that each batch now has a target sha, source sha, and an attempt id. When we refresh state from batch, we look for the oldest batch with the newest attempt_id. We increment attempt_id whenever we `_start_build` on a batch. If the source sha changes, we set the attempt id to zero. It will be incremented to one the next time we build. In this manner, we prevent CI from refreshing from batch and blowing away a retried batch even though it has a higher batch id (and we always prefer lower ids). Retry is implemented in the standard a PR is noted as needing a retry and the watched branch is informed that its state has changed. The heal method of a PR checks for the retry flag, cancels any existing batch, and triggers a new build. Miscellaneous changes/fixes:; - a developers only endpoint to force CI to update right now (convenient for dev deploy where the GitHub triggers are not configured); - don't try to clean up a database that wasn't created; - correct URL for CI in a couple places",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8398:1189,config,configured,1189,https://hail.is,https://github.com/hail-is/hail/pull/8398,1,['config'],['configured']
Modifiability,centralize serializable and broadcasted Hadoop configuration,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5422:47,config,configuration,47,https://hail.is,https://github.com/hail-is/hail/pull/5422,1,['config'],['configuration']
Modifiability,"ch bucket to pull the VEP cache data from. In more recent versions (tested with 0.2.130), this `VEP_REPLICATE` variable has been changed to `VEP_REPLICATE=australia-southeast1`, however the Australian bucket containing the VEP cache data is still `aus-sydney`, meaning that the VEP data is not copied into the dataproc cluster, and when trying to run VEP I get the error `No cache found for homo_sapiens, version 95`. ### Version. 0.2.130. ### Relevant log output. ```shell; FatalError: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:1467,Plugin,Plugins,1467,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,['Plugin'],['Plugins']
Modifiability,"ch-pods""; Object: &{map[""kind"":""ServiceAccount"" ""metadata"":map[""name"":""batch-svc"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""apiVersion"":""v1""]}; from server for: ""deployment.yaml"": serviceaccounts ""batch-svc"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get serviceaccounts in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""rbac.authorization.k8s.io/v1, Resource=roles"", GroupVersionKind: ""rbac.authorization.k8s.io/v1, Kind=Role""; Name: ""batch-pods-admin"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""rbac.authorization.k8s.io/v1"" ""kind"":""Role"" ""metadata"":map[""name"":""batch-pods-admin"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""rules"":[map[""apiGroups"":[""""] ""resources"":[""pods""] ""verbs"":[""get"" ""list"" ""watch"" ""create"" ""update"" ""patch"" ""delete""]] map[""apiGroups"":[""""] ""resources"":[""pods/log""] ""verbs"":[""get""]]]]}; from server for: ""deployment.yaml"": roles.rbac.authorization.k8s.io ""batch-pods-admin"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get roles.rbac.authorization.k8s.io in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""rbac.authorization.k8s.io/v1, Resource=rolebindings"", GroupVersionKind: ""rbac.authorization.k8s.io/v1, Kind=RoleBinding""; Name: ""batch-pods-admin-binding"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""rbac.authorization.k8s.io/v1"" ""kind"":""RoleBinding"" ""metadata"":map[""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""name"":""batch-pods-admin-binding"" ""namespace"":""batch-pods""] ""roleRef"":map[""apiGroup"":"""" ""kind"":""Role"" ""name"":""batch-pods-admin""] ""subjects"":[map[""kind"":""ServiceAccount"" ""name"":""ba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:1743,config,configuration,1743,https://hail.is,https://github.com/hail-is/hail/issues/4609,1,['config'],['configuration']
Modifiability,check that nginx config is valid,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4429:17,config,config,17,https://hail.is,https://github.com/hail-is/hail/pull/4429,1,['config'],['config']
Modifiability,checker configs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4558:8,config,configs,8,https://hail.is,https://github.com/hail-is/hail/pull/4558,1,['config'],['configs']
Modifiability,"ci failure:; ```; hail version: 0.2.109-c163bcb21073; Error summary: AssertionError: assertion failed; self = <test.hail.linalg.test_linalg.Tests testMethod=test_tree_matmul>. @fails_service_backend(); @fails_local_backend(); def test_tree_matmul(self):; nm = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]); m = BlockMatrix.from_numpy(nm, block_size=2); nrow = np.array([[7.0, 8.0, 9.0]]); row = BlockMatrix.from_numpy(nrow, block_size=2); ; > with BatchedAsserts() as b:. test/hail/linalg/test_linalg.py:612: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/hail/linalg/test_linalg.py:96: in __exit__; vals.extend(list(hl.eval(tuple([all_bms[k] for k in bm_keys[batch_start:batch_start + batch_size]])))); <decorator-gen-692>:2: in eval; ???; hail/typecheck/check.py:577: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:191: in eval; return eval_timed(expression)[0]; <decorator-gen-690>:2: in eval_timed; ???; hail/typecheck/check.py:577: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:161: in eval_timed; return Env.backend().execute(MakeTuple([ir]), timed=True)[0]; hail/backend/py4j_backend.py:82: in execute; raise e.maybe_user_error(ir) from None; hail/backend/py4j_backend.py:76: in execute; result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); ../../.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321: in __call__; return_value = get_return_value(; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . args = ('xro549', <py4j.clientserver.JavaClient object at 0x7fd0d58f6fb0>, 'o1', 'executeEncode'); kwargs = {}; pyspark = <module 'pyspark' from '/home/edmund/.local/src/hail/.venv/lib/python3.10/site-packages/pyspark/__init__.py'>; s = 'java.lang.AssertionError: assertion failed', tpl = JavaObject id=o550; deepest = 'AssertionError: assertion failed'; full = 'java.lang.AssertionError: asserti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229:646,extend,extend,646,https://hail.is,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229,1,['extend'],['extend']
Modifiability,"ck trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19) (my-first-hail-cluster-w-0.c.open-targets-eu-dev.internal executor 1): java.lang.NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; 	at __C144Compiled.applyregion0_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682:6919,adapt,adapted,6919,https://hail.is,https://github.com/hail-is/hail/issues/10682,1,['adapt'],['adapted']
Modifiability,cleaned/killed a bad import and an unused variable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3098:42,variab,variable,42,https://hail.is,https://github.com/hail-is/hail/pull/3098,1,['variab'],['variable']
Modifiability,"closes #7357. * implements `.tail` function on tables:. ```python; def tail(self, n) -> 'Table':; """"""Subset table to last `n` rows. Examples; --------; Subset to the last three rows:. >>> table_result = table1.tail(3); >>> table_result.count(); 3. Notes; -----. The number of partitions in the new table is equal to the number of; partitions containing the last `n` rows. Parameters; ----------; n : int; Number of rows to include. Returns; -------; :class:`.Table`; Table including the last `n` rows.; """""". return Table(TableTail(self._tir, n)); ```. * refactored some of the logic in `TableHead`, because a lot of the behavior is the same. * specifically, moved some partition-counts calculations to `is.hail.utils.PartitionCounts`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7386:554,refactor,refactored,554,https://hail.is,https://github.com/hail-is/hail/pull/7386,1,['refactor'],['refactored']
Modifiability,"cloud-GRCh37.properties /nirvana. chmod -R 777 /nirvana. apt-get -y install curl libunwind8 gettext; curl -sSL -o dotnet.tar.gz https://go.microsoft.com/fwlink/?linkid=843453; mkdir -p /opt/dotnet && sudo tar zxf dotnet.tar.gz -C /opt/dotnet; ln -s /opt/dotnet/dotnet /usr/local/bin; ```. The properties file `nirvana-cloud-GRCh37.properties` points Nirvana to these local resources:; ```; hail.nirvana.location = /nirvana/netcoreapp1.1/Nirvana.dll; hail.nirvana.cache = /nirvana/Data/Cache/GRCh37/Ensembl84; hail.nirvana.reference = /nirvana/Data/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.supplementaryAnnotationDirectory = /nirvana/Data/SupplementaryDatabase/GRCh37; ```. I started a cluster with the init script and ran Nirvana on all of `profile225.vcf`, and later exported results for just a region bounding the gene CABIN1:; ```; from hail import *; hc = (HailContext()). (hc; .import_vcf(path='gs://jbloom/profile225.vcf.bgz'); .filter_multi(); .nirvana(block_size=10000, config='/nirvana/nirvana-cloud-GRCh37.properties'); .variants_table(); .filter(expr='v.start > 24430000 && v.start < 24580000'); .export(output='gs://jbloom/nirvana_cabin1.tsv')); ```. The top-level categories show reasonable-looking variation, except for ""clinvar"" and ""genes"" which are all `null` valued. Comparing a few variants in [gnomad](http://gnomad.broadinstitute.org/gene/ENSG00000099991), the annotations line up nicely. Here's an example of a common missense variant:; ```; 22:24468386:G:A	{""rsid"":null,""qual"":38350.97,""filters"":null,""info"":{""AC"":[306],""AF"":[0.061],""AN"":5018,""BaseQRankSum"":26.807,""ClippingRankSum"":-0.538,""DP"":22432,""DS"":null,""FS"":1.203,""HaplotypeScore"":null,""InbreedingCoeff"":0.0335,""MLEAC"":[309],""MLEAF"":[0.062],""MQ"":59.13,""MQ0"":0,""MQRankSum"":16.406,""QD"":14.9,""ReadPosRankSum"":-0.637,""set"":null},""nirvana"":{""chromosome"":""22"",""refAllele"":""G"",""position"":24468386,""altAlleles"":[""A""],""cytogeneticBand"":""22q11.23"",""filters"":null,""variants"":[{""altAllele"":""A"",""refAllele"":""G"",""chro",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2377#issuecomment-340889701:2075,config,config,2075,https://hail.is,https://github.com/hail-is/hail/pull/2377#issuecomment-340889701,1,['config'],['config']
Modifiability,"com/gaborbernat""><code>@​gaborbernat</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/pull/232"">tox-dev/sphinx-autodoc-typehints#232</a></li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.18.1...1.18.2"">https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.18.1...1.18.2</a></p>; <h2>1.18.1</h2>; <p>No release notes provided.</p>; <h2>1.18.0</h2>; <p>No release notes provided.</p>; <h2>1.17.1</h2>; <p>No release notes provided.</p>; <h2>typehints_use_rtype support and handle TypeError</h2>; <p>No release notes provided.</p>; <h2>1.16.0</h2>; <p>No release notes provided.</p>; <h2>1.15.3</h2>; <p>No release notes provided.</p>; <h2>1.15.2</h2>; <p>No release notes provided.</p>; <h2>1.15.1</h2>; <p>No release notes provided.</p>; <h2>1.15.0</h2>; <p>No release notes provided.</p>; <h2>1.14.1</h2>; <p>No release notes provided.</p>; <h2>Added document_defaults config option</h2>; <p>No release notes provided.</p>; <h2>Fix NewType is inserting a reference as first argument</h2>; <p>No release notes provided.</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/blob/main/CHANGELOG.md"">sphinx-autodoc-typehints's changelog</a>.</em></p>; <blockquote>; <h2>1.18.3</h2>; <ul>; <li>Support and require <code>nptyping&gt;=2.1.2</code></li>; </ul>; <h2>1.18.2</h2>; <ul>; <li>Support and require <code>nptyping&gt;=2.1.1</code></li>; </ul>; <h2>1.18.1</h2>; <ul>; <li>Fix mocked module import not working when used as guarded import</li>; </ul>; <h2>1.18.0</h2>; <ul>; <li>Support and require <code>nptyping&gt;=2</code></li>; <li>Handle <code>UnionType</code></li>; </ul>; <h2>1.17.1</h2>; <ul>; <li>Mark it as requiring <code>nptyping&lt;2</code></li>; </ul>; <h2>1.17.0</h2>; <ul>; <li>Add <code>typehints_use_rtype</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11909:2183,config,config,2183,https://hail.is,https://github.com/hail-is/hail/pull/11909,1,['config'],['config']
Modifiability,"com/gaborbernat""><code>@​gaborbernat</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/pull/232"">tox-dev/sphinx-autodoc-typehints#232</a></li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.18.1...1.18.2"">https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.18.1...1.18.2</a></p>; <h2>1.18.1</h2>; <p>No release notes provided.</p>; <h2>1.18.0</h2>; <p>No release notes provided.</p>; <h2>1.17.1</h2>; <p>No release notes provided.</p>; <h2>typehints_use_rtype support and handle TypeError</h2>; <p>No release notes provided.</p>; <h2>1.16.0</h2>; <p>No release notes provided.</p>; <h2>1.15.3</h2>; <p>No release notes provided.</p>; <h2>1.15.2</h2>; <p>No release notes provided.</p>; <h2>1.15.1</h2>; <p>No release notes provided.</p>; <h2>1.15.0</h2>; <p>No release notes provided.</p>; <h2>1.14.1</h2>; <p>No release notes provided.</p>; <h2>Added document_defaults config option</h2>; <p>No release notes provided.</p>; <h2>Fix NewType is inserting a reference as first argument</h2>; <p>No release notes provided.</p>; <h2>Python 3.10 support and PEP-563, drop 3.6</h2>; <p>No release notes provided.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/blob/main/CHANGELOG.md"">sphinx-autodoc-typehints's changelog</a>.</em></p>; <blockquote>; <h2>1.18.2</h2>; <ul>; <li>Support and require <code>nptyping&gt;=2.1.1</code></li>; </ul>; <h2>1.18.1</h2>; <ul>; <li>Fix mocked module import not working when used as guarded import</li>; </ul>; <h2>1.18.0</h2>; <ul>; <li>Support and require <code>nptyping&gt;=2</code></li>; <li>Handle <code>UnionType</code></li>; </ul>; <h2>1.17.1</h2>; <ul>; <li>Mark it as requiring <code>nptyping&lt;2</code></li>; </ul>; <h2>1.17.0</h2>; <ul>; <li>Add <code>typehints_use_rtype</code> option</li>; <li>Handles <code>TypeError</code> wh",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11893:1680,config,config,1680,https://hail.is,https://github.com/hail-is/hail/pull/11893,1,['config'],['config']
Modifiability,"com/googleapis/java-storage/issues/2197"">#2197</a>) (<a href=""https://github.com/googleapis/java-storage/commit/26552f4b78f77d90df4e3dfb829c3f9c092fc817"">26552f4</a>)</li>; <li>Update dependency info.picocli:picocli to v4.7.4 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2177"">#2177</a>) (<a href=""https://github.com/googleapis/java-storage/commit/0c908147375fe58ac280179f5fba10bdd3886003"">0c90814</a>)</li>; <li>Update dependency info.picocli:picocli to v4.7.5 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2183"">#2183</a>) (<a href=""https://github.com/googleapis/java-storage/commit/f2448615ded6d9f43344bf1b9cda7ae3b191223b"">f244861</a>)</li>; <li>Update dependency net.jqwik:jqwik to v1.8.0 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2187"">#2187</a>) (<a href=""https://github.com/googleapis/java-storage/commit/aedbd6a811c4fcfedff68d7d46bb68e93bf9eeee"">aedbd6a</a>)</li>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.26 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2196"">#2196</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4f8bb658e9ff3cba5e745acae13ec4094a1a48d5"">4f8bb65</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.26.0...v2.26.1"">2.26.1</a> (2023-08-14)</h2>; <h3>Bug Fixes</h3>; <ul>; <li>Make use of ImmutableMap.Builder#buildOrThrow graceful (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2159"">#2159</a>) (<a href=""https://github.com/googleapis/java-storage/commit/e9746f856e9204c1c0ec62f19e6f71ff8a0b9750"">e9746f8</a>)</li>; <li>Update gRPC writeAndClose to only set finish_write on the last message (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2163"">#2163</a>) (<a href=""https://github.com/googleapis/java-storage/commit/95df758d6753005226556177e68a3e9c630c789b"">95df758</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency org.graalvm.buildt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13624:10554,plugin,plugin,10554,https://hail.is,https://github.com/hail-is/hail/pull/13624,1,['plugin'],['plugin']
Modifiability,"com/googleapis/java-storage/issues/2197"">#2197</a>) (<a href=""https://github.com/googleapis/java-storage/commit/26552f4b78f77d90df4e3dfb829c3f9c092fc817"">26552f4</a>)</li>; <li>Update dependency info.picocli:picocli to v4.7.4 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2177"">#2177</a>) (<a href=""https://github.com/googleapis/java-storage/commit/0c908147375fe58ac280179f5fba10bdd3886003"">0c90814</a>)</li>; <li>Update dependency info.picocli:picocli to v4.7.5 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2183"">#2183</a>) (<a href=""https://github.com/googleapis/java-storage/commit/f2448615ded6d9f43344bf1b9cda7ae3b191223b"">f244861</a>)</li>; <li>Update dependency net.jqwik:jqwik to v1.8.0 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2187"">#2187</a>) (<a href=""https://github.com/googleapis/java-storage/commit/aedbd6a811c4fcfedff68d7d46bb68e93bf9eeee"">aedbd6a</a>)</li>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.26 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2196"">#2196</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4f8bb658e9ff3cba5e745acae13ec4094a1a48d5"">4f8bb65</a>)</li>; </ul>; <h2>v2.26.1</h2>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.26.0...v2.26.1"">2.26.1</a> (2023-08-14)</h2>; <h3>Bug Fixes</h3>; <ul>; <li>Make use of ImmutableMap.Builder#buildOrThrow graceful (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2159"">#2159</a>) (<a href=""https://github.com/googleapis/java-storage/commit/e9746f856e9204c1c0ec62f19e6f71ff8a0b9750"">e9746f8</a>)</li>; <li>Update gRPC writeAndClose to only set finish_write on the last message (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2163"">#2163</a>) (<a href=""https://github.com/googleapis/java-storage/commit/95df758d6753005226556177e68a3e9c630c789b"">95df758</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13624:4592,plugin,plugin,4592,https://hail.is,https://github.com/hail-is/hail/pull/13624,1,['plugin'],['plugin']
Modifiability,"com/prometheus/client_python/issues/730"">#730</a>; [ENHANCEMENT] Begin to add type hints to functions. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/705"">#705</a>; [ENHANCEMENT] Improved go-to-declaration behavior for editors. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/747"">#747</a>; [BUGFIX] Remove trailing slashes from pushgateway URLS. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/722"">#722</a>; [BUGFIX] Catch non-integer bucket/count values. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/726"">#726</a></p>; <h2>0.12.0 / 2021-10-29</h2>; <p>[FEATURE] Exemplar support (excludes multiprocess) <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/669"">#669</a>; [ENHANCEMENT] Add support for Python 3.10 <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/706"">#706</a>; [ENHANCEMENT] Restricted Registry will handle metrics added after restricting <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/675"">#675</a>, <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/680"">#680</a><br />; [ENHANCEMENT] Raise a more helpful error if a metric is not observable <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/666"">#666</a>; [BUGFIX] Fix instance_ip_grouping_key not working on MacOS <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/687"">#687</a>; [BUGFIX] Fix assertion error from favicion.ico with Python 2.7 <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/715"">#715</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/prometheus/client_python/commit/a234283a853238dc73fa22651532590330fd72a1""><code>a234283</code></a> Release 0.13.1</li>; <li><a href=""https://github.com/prometheus/client_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11515:2184,ENHANCE,ENHANCEMENT,2184,https://hail.is,https://github.com/hail-is/hail/pull/11515,1,['ENHANCE'],['ENHANCEMENT']
Modifiability,"com/sveltejs/svelte/issues/7440"">#7440</a>)</li>; <li>Fix handling of void tags in <code>&lt;svelte:element&gt;</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7449"">#7449</a>)</li>; <li>Fix handling of boolean attributes in <code>&lt;svelte:element&gt;</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7478"">#7478</a>)</li>; <li>Add special style scoping handling of <code>[open]</code> selectors on <code>&lt;dialog&gt;</code> elements (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7494"">#7495</a>)</li>; </ul>; <h2>3.47.0</h2>; <ul>; <li>Add support for dynamic elements through <code>&lt;svelte:element&gt;</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/2324"">#2324</a>)</li>; <li>Miscellaneous variable context fixes in <code>{@const}</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/pull/7222"">#7222</a>)</li>; <li>Fix <code>{#key}</code> block not being reactive when the key variable is not otherwise used (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7408"">#7408</a>)</li>; <li>Add <code>Symbol</code> as a known global (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7418"">#7418</a>)</li>; </ul>; <h2>3.46.6</h2>; <ul>; <li>Actually include action TypeScript interface in published package (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/pull/7407"">#7407</a>)</li>; </ul>; <h2>3.46.5</h2>; <ul>; <li>Add TypeScript interfaces for typing actions (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/6538"">#6538</a>)</li>; <li>Do not generate <code>unused-export-let</code> warning inside <code>&lt;script context=&quot;module&quot;&gt;</code> blocks (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7055"">#7055</a>)</li>; <li>Do not collapse whitespace-only CSS vars (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/71",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12032:4289,variab,variable,4289,https://hail.is,https://github.com/hail-is/hail/pull/12032,3,['variab'],['variable']
Modifiability,configure apache webserver to allow navbar to be loaded from github,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1024:0,config,configure,0,https://hail.is,https://github.com/hail-is/hail/issues/1024,1,['config'],['configure']
Modifiability,"cool, I'll proably rewrite the tests using that. the option for 1 already exists in `LoadMatrix.apply`; I just haven't exposed it to HailContext because there's a lot of stuff there already and I couldn't figure out what the name of the flag should be (currently, it's hasRowKeyLabel but I don't feel like that's super descriptive) but I could do that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2440#issuecomment-345032622:19,rewrite,rewrite,19,https://hail.is,https://github.com/hail-is/hail/pull/2440#issuecomment-345032622,1,['rewrite'],['rewrite']
Modifiability,"could name your benchmarks `test_*` and put them in files named `test_*.py`.; Benchmarks may import and use any test code or utilities defined in `test/`.; The results of each benchmark are outputted as json lines (`.jsonl`) to the file specified by the `--output` pytest arg or stdout. The folder structure should be familiar, resembling our `test/` directory.; I believe this is flexible enough to add `hailtop` benchmarks should we so wish:; ```; pytest.ini - hoisted from `test/` to include benchmark marks; benchmark/; - conftest.py for custom pytest command line args ; - hail/; - confest.py for custom plugin that runs hail benchmarks; - benchmark_*.py hail query benchmark code; - tools/; - shared utilites, including the `@benchmark`; ```; Supporting pytest fixtures required writing a custom plugin to run benchmarks, as using off-the-shelf; solutions like `pytest-benchmark` would forbid method level fixtures like `tmp_path` etc.; The plugin is designed to run ""macro-benchmarks"" (ie long-running tests) and fully supports pytest parameterisation.; For each benchmark, the plugin initialises hail and then repeats (for a number of iterations defined by the pytest mark); acquiring fixtures, timing invocation and tearing-down fixtures, finally stopping hail. It is therefore unsuitable for; microbenchmarks, for which we currenly have none in python. If we add them we'd need to tweak this so support them.; Perhaps an inner loop or something. The process of submitting benchmarks to batch is greatly simplified as the old `Makefile` infrastructure for ; building wheels and docker images etc has been replaced with the script `benchmark_in_batch.py`.; Benchmark images are now based off the `hail-dev` image built in CI (or via the `hail-dev-image` make target). ; Furthermore, you can control the number of ""replicate"" jobs created for each benchmark at the benchmark level using; the `@benchmark(batch_jobs=N)` decotator. Limitations/shortcomings:; - Output is currently jsonl only. So",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14565:1545,plugin,plugin,1545,https://hail.is,https://github.com/hail-is/hail/pull/14565,1,['plugin'],['plugin']
Modifiability,could parameterize this to not run if running in no-test mode:; https://github.com/hail-is/hail/blob/836baa3604ab05fb6a603cfce80c94e55e988a43/hail/python/hail/docs/Makefile#L53,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6996#issuecomment-528542699:6,parameteriz,parameterize,6,https://hail.is,https://github.com/hail-is/hail/pull/6996#issuecomment-528542699,1,['parameteriz'],['parameterize']
Modifiability,"cs detected.; 2018-06-26 01:49:09 Hail: INFO: interval filter loaded 27 of 586 partitions; ```. I'll kill what I have and run a single regression, since this will take a long time.; ```; 2018-07-18 15:39:30 Hail: INFO: Number of BGEN files parsed: 1; 2018-07-18 15:39:30 Hail: INFO: Number of samples in BGEN files: 487409; 2018-07-18 15:39:30 Hail: INFO: Number of variants across all BGEN files: 1255683; 2018-07-18 15:40:37 Hail: INFO: Coerced almost-sorted dataset; 2018-07-18 15:40:39 Hail: INFO: interval filter loaded 5 of 293 partitions; 2018-07-18 15:43:13 Hail: WARN: 126215 of 487409 samples have a missing phenotype or covariate.; 2018-07-18 15:43:13 Hail: INFO: linear_regression: running on 361194 samples for 110 response variables y,; with input variable x, intercept, and 25 additional covariates...; 2018-07-18 15:44:06 Hail: WARN: 132571 of 487409 samples have a missing phenotype or covariate.; 2018-07-18 15:44:06 Hail: INFO: linear_regression: running on 354838 samples for 1 response variable y,; with input variable x, intercept, and 25 additional covariates...; 2018-07-18 15:44:59 Hail: WARN: 132781 of 487409 samples have a missing phenotype or covariate.; 2018-07-18 15:44:59 Hail: INFO: linear_regression: running on 354628 samples for 1 response variable y,; with input variable x, intercept, and 25 additional covariates...; 2018-07-18 15:45:42 Hail: WARN: 133165 of 487409 samples have a missing phenotype or covariate.; 2018-07-18 15:45:42 Hail: INFO: linear_regression: running on 354244 samples for 1 response variable y,; with input variable x, intercept, and 25 additional covariates...; 2018-07-18 15:46:57 Hail: WARN: 132601 of 487409 samples have a missing phenotype or covariate.; 2018-07-18 15:46:57 Hail: INFO: linear_regression: running on 354808 samples for 1 response variable y,; with input variable x, intercept, and 25 additional covariates...; ```. Also, the default parallelism in 0.1 was better for the same interval: 27 partitions to 5 partitions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3945#issuecomment-405979965:1645,variab,variable,1645,https://hail.is,https://github.com/hail-is/hail/pull/3945#issuecomment-405979965,6,['variab'],['variable']
Modifiability,"ct.dependabot.com/PyCQA/pylint/issues/5452"">#5452</a></p>; </li>; <li>; <p>Fix false negative for <code>consider-iterating-dictionary</code> during membership checks encapsulated in iterables; or <code>not in</code> checks</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5323"">#5323</a></p>; </li>; <li>; <p><code>unused-import</code> now check all ancestors for typing guards</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5316"">#5316</a></p>; </li>; </ul>; <h1>What's New in Pylint 2.12.1?</h1>; <p>Release date: 2021-11-25</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/PyCQA/pylint/commit/eec287fae66f8fc514d5daa9caee46fd0e0cb6d9""><code>eec287f</code></a> Bump pylint to 2.12.2, update changelog</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/7def5278afc86224a98cc9d1706fbd9523ddda1b""><code>7def527</code></a> Add Copyrite configuration for Yu Shao</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/608ed329aaee9e457ac51347699d4892d29df802""><code>608ed32</code></a> Require <code>\</code> for asterisks in Sphinx-style parameter docstrings (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5464"">#5464</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/f89a3374ec7d49d2a984c90530758a506eaa4384""><code>f89a337</code></a> Deprecate <code>is_inside_lambda</code> from utils (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5447"">#5447</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/765a0b74bc5f2cface4595661f8832a3aebc68ba""><code>765a0b7</code></a> Add endLine and endColumn keys to JSONReporter (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5456"">#5456</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/28a33ef874cd63b92a32208e844b97f0c6a2f082""><code>28a33ef</code></a> Update outdated class ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11461:5982,config,configuration,5982,https://hail.is,https://github.com/hail-is/hail/pull/11461,2,['config'],['configuration']
Modifiability,"ct.dependabot.com/prometheus/client_python/issues/718"">#718</a>; [FEATURE] Support adding labels when using <code>.time()</code> <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/730"">#730</a>; [ENHANCEMENT] Begin to add type hints to functions. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/705"">#705</a>; [ENHANCEMENT] Improved go-to-declaration behavior for editors. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/747"">#747</a>; [BUGFIX] Remove trailing slashes from pushgateway URLS. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/722"">#722</a>; [BUGFIX] Catch non-integer bucket/count values. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/726"">#726</a></p>; <h2>0.12.0 / 2021-10-29</h2>; <p>[FEATURE] Exemplar support (excludes multiprocess) <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/669"">#669</a>; [ENHANCEMENT] Add support for Python 3.10 <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/706"">#706</a>; [ENHANCEMENT] Restricted Registry will handle metrics added after restricting <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/675"">#675</a>, <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/680"">#680</a><br />; [ENHANCEMENT] Raise a more helpful error if a metric is not observable <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/666"">#666</a>; [BUGFIX] Fix instance_ip_grouping_key not working on MacOS <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/687"">#687</a>; [BUGFIX] Fix assertion error from favicion.ico with Python 2.7 <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/715"">#715</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11515:2047,ENHANCE,ENHANCEMENT,2047,https://hail.is,https://github.com/hail-is/hail/pull/11515,1,['ENHANCE'],['ENHANCEMENT']
Modifiability,ctIntervalFilters$.openInterval(ExtractIntervalFilters.scala:94); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:205); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:201); 	at scala.Option.flatMap(Option.scala:171); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:201); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:151); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractPartitionFilters(ExtractIntervalFilters.scala:249); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:3779,Rewrite,RewriteBottomUp,3779,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Rewrite'],['RewriteBottomUp']
Modifiability,ctTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$cl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:2434,Rewrite,RewriteBottomUp,2434,https://hail.is,https://github.com/hail-is/hail/issues/9128,2,['Rewrite'],['RewriteBottomUp']
Modifiability,ctTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:3914,Rewrite,RewriteBottomUp,3914,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"current issues: ; - `copyFrom` on this aggregator assumes that the seqOp has not been called on the data we're copying, just the initOp. This is somewhat annoying to fix because we would need to refactor `copyFrom` to take the AggregatorState rather than address, since the region is necessary to look up the java object.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7557#issuecomment-555494694:195,refactor,refactor,195,https://hail.is,https://github.com/hail-is/hail/pull/7557#issuecomment-555494694,1,['refactor'],['refactor']
Modifiability,cute$1(EvalRelationalLets.scala:10); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.lower$1(EvalRelationalLets.scala:18); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.apply(EvalRelationalLets.scala:32); E 	at is.hail.expr.ir.lowering.EvalRelationalLetsPass.transform(LoweringPass.scala:157); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.EvalRelationalLetsPass.apply(LoweringPass.scala:151); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); E 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); E 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); E 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:150); E 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:189); E 	at is.hail.backend.local.LocalBackend.$anonfun$executeToEncoded$1(LocalBackend.scala:209); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); E 	at is.hail.backend.ExecuteContext$.scoped(ExecuteCon,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:12504,adapt,adapted,12504,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['adapt'],['adapted']
Modifiability,cute$1(EvalRelationalLets.scala:10); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.lower$1(EvalRelationalLets.scala:18); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.apply(EvalRelationalLets.scala:37); E 	at is.hail.expr.ir.lowering.EvalRelationalLetsPass.transform(LoweringPass.scala:147); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.EvalRelationalLetsPass.apply(LoweringPass.scala:141); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); E 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); E 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); E 	at is.hail.backend.service.ServiceBackend.execute(ServiceBackend.scala:310); E 	at is.hail.backend.service.ServiceBackend.execute(ServiceBackend.scala:346); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$executeOneCommand$12(ServiceBackend.scala:698); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.withIRFunctionsReadFromInput(ServiceBackend.scala:801); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$executeOneCommand$11(ServiceBackend.scala:696); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$executeOneCommand$2(ServiceBackend.scala:654); E 	at is.hail.backend.ExecuteContext$.$anonfu,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:4209,adapt,adapted,4209,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['adapt'],['adapted']
Modifiability,cute$1(EvalRelationalLets.scala:12); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.lower$1(EvalRelationalLets.scala:18); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.apply(EvalRelationalLets.scala:37); E 	at is.hail.expr.ir.lowering.EvalRelationalLetsPass.transform(LoweringPass.scala:147); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.EvalRelationalLetsPass.apply(LoweringPass.scala:141); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); E 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); E 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); E 	at is.hail.backend.service.ServiceBackend.execute(ServiceBackend.scala:313); E 	at is.hail.backend.service.ServiceBackend.execute(ServiceBackend.scala:349); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$executeOneCommand$12(ServiceBackend.scala:702); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.withIRFunctionsReadFromInput(ServiceBackend.scala:805); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$executeOneCommand$11(ServiceBackend.scala:700); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$executeOneCommand$2(ServiceBackend.scala:658); E 	at is.hail.backend.ExecuteContext$.$anonfu,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13074:3923,adapt,adapted,3923,https://hail.is,https://github.com/hail-is/hail/issues/13074,2,['adapt'],['adapted']
Modifiability,cuteContext.close(ExecuteContext.scala:148); E 	at is.hail.utils.package$.using(package.scala:660); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); E 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$executeOneCommand$1(ServiceBackend.scala:555); E 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); E 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.withExecuteContext$1(ServiceBackend.scala:535); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:602); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$7(ServiceBackend.scala:433); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$7$adapted(ServiceBackend.scala:432); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6(ServiceBackend.scala:432); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:77); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5(ServiceBackend.scala:432); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5$adapted(ServiceBackend.scala:430); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:430); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:77); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:430); E 	at is.hail.backend.service.Main$.main(Main.scala:33); E 	at is.h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:2670,adapt,adapted,2670,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['adapt'],['adapted']
Modifiability,"d I got a new error. Seem to be a problem with the profile having too small a starting maxPartition size and openCost size. I'm uncertain how to change these parameters even after extensive googling. Any Ideas? Thank you!. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 64, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o18.apply.; : is.hail.utils.package$FatalException: Found problems with SparkContext configuration:; Invalid config parameter 'spark.sql.files.openCostInBytes=': too small. Found 0, require at least 50G; Invalid config parameter 'spark.sql.files.maxPartitionBytes=': too small. Found 0, require at least 50G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:5); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:104); 	at is.hail.HailContext$.apply(HailContext.scala:162); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978:1052,config,config,1052,https://hail.is,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978,1,['config'],['config']
Modifiability,"d to hail version. JAR and ZIP:; ```; gs://hail-common/builds/0.2/jars/hail-0.2-3b1cb0772301-Spark-2.2.0.jar; gs://hail-common/builds/0.2/python/hail-0.2-3b1cb0772301.zip; ```. In Google Chrome we see 404s for; ```; GET http://localhost:8123/spark/api/v1/applications; ```; which happened repeatedly if you try to evaluate a cell. On the leader node of the spark cluster, `journalctl -u jupyter` shows:; ```; -- Logs begin at Fri 2019-03-01 19:54:49 UTC, end at Fri 2019-03-01 20:11:51 UTC. --; Mar 01 19:59:03 dk-m systemd[1]: Started Jupyter Notebook.; Mar 01 19:59:04 dk-m python[5149]: [I 19:59:04.630 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.796 NotebookApp] All authentication is disabled. Anyone who can connect to this server will be able to run code.; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.802 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.803 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /usr/local/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /opt/conda/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /root/.jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.904 NotebookApp] Error loading server extension jupyter_spark; Mar 01 19:59:04 dk-m python[5149]: Traceback (most recent call last):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/notebook/notebookapp.py"", line 1575, i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5505:975,config,configuration,975,https://hail.is,https://github.com/hail-is/hail/issues/5505,1,['config'],['configuration']
Modifiability,"d%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ajtpio+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​jtpio</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Akrassowski+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​krassowski</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ameeseeksmachine+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​meeseeksmachine</code></a></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/notebook/blob/@jupyter-notebook/tree@7.0.7/CHANGELOG.md"">notebook's changelog</a>.</em></p>; <blockquote>; <h2>7.0.7</h2>; <p>(<a href=""https://github.com/jupyter/notebook/compare/@jupyter-notebook/application-extension@7.0.6...089c78c48fd00b2b0d2f33e4463eb42018e86803"">Full Changelog</a>)</p>; <h3>Enhancements made</h3>; <ul>; <li>Update to JupyterLab 4.0.11 <a href=""https://redirect.github.com/jupyter/notebook/pull/7215"">#7215</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Update ruff config and typing <a href=""https://redirect.github.com/jupyter/notebook/pull/7145"">#7145</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Clean up lint handling <a href=""https://redirect.github.com/jupyter/notebook/pull/7142"">#7142</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Adopt ruff format <a href=""https://redirect.github.com/jupyter/notebook/pull/7132"">#7132</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>[7.0.x] Install stable JupyterLab 4.0 in the releaser hook <a href=""https://redirect.github.com/jupyter/notebook/pull/7183"">#7183</a> (<a href=""https://github.com/jtpio""><code>@​j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14182:3318,Enhance,Enhancements,3318,https://hail.is,https://github.com/hail-is/hail/pull/14182,1,['Enhance'],['Enhancements']
Modifiability,"d. In the case of Caitlin's PRS scoring method, that's about 1% or less of the original variant set. It's a bit janky. The file paths need to be the fully qualified ones that are seen by the hadoop reader. So `file:/full/path/to/file.bgen` or `gs://full/path/to/bgen.file`, which is annoying. I don't have any better way to generically uniquely identify files though. ---; ### Calc Depth Bug. I also had to fix a bug in the indices. Neither my `OnDiskBTreeIndexToValue` nor the existing `IndexBTree` correctly calculated the sizes of the given trees. Recall that a b-tree is a series of layers. Layer 0 is at most `branchingFactor` in size. Layer i is at most `branchingFactor ^ (i+1)` in size. The total size of the b-tree is the sum of the layer sizes. Here's a few max sizes for a branchingFactor of 1024:. - 1 layer tree: 1024; - 2 layer tree: 1024^2 + 1024; - 3 layer tree: 1024^3 + 1024^2 + 1024. If you look carefully at the old `calcDepth` method, it incorrectly concludes that fully populated 3 layer trees have four layers because they have more than 1024^3 total (internal+leaf) elements. This issue rears it's head on an exponentially small number of trees (at depth `i`, the number of leaf elements must lie in `[1024^i-1024^(i-1), 1024^i]`. This discrepancy is what lead to my confusion for the last few days. It shows up quite quickly with very small branching factors (e.g. 3) but with a large branching factor (the default of 1024 and what all the tests were written against) it's fairly rare. ---; ### Summary of Changes. - add `_variants_per_file` which is a map from absolute file paths to lists of variants (identified by their in-file index) to keep; - a test for `_variants_per_file`; - a fixed `calcDepth` which is now used by both index classes; - a set of tests for `calcDepth`; - some clean up in `BgenBlockReader`: use `private[this]` for things that are truly private fields (otherwise they're accessed through `invokevirtual`) and use `using` to manage resources; - teach",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3813:1421,layers,layers,1421,https://hail.is,https://github.com/hail-is/hail/pull/3813,1,['layers'],['layers']
Modifiability,"d/.local/src/hail/hail/python/hail/expr/expressions/expression_typecheck.py"", line 80, in check; raise TypecheckFailure from e; hail.typecheck.check.TypecheckFailure. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/home/edmund/.pyenv/versions/3.8.16/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/home/edmund/.pyenv/versions/3.8.16/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py"", line 39, in <module>; cli.main(); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 430, in main; run(); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 284, in run_file; runpy.run_path(target, run_name=""__main__""); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 321, in run_path; return _run_module_code(code, init_globals, run_name,; File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 135, in _run_module_code; _run_code(code, mod_globals, init_globals,; File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 124, in _run_code; exec(code, run_globals); File ""/home/edmund/.local/src/hail/test.py"", line 10, in <module>; expr = hl.any(lambda x:; File ""/home/edmund/.local/src/hail/hail/python/hail/expr/functions.py"", line 3530, in any; collection = arg_check(ar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:3427,adapt,adapter,3427,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['adapt'],['adapter']
Modifiability,"d/Decoder.d -MT build/Decoder.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ApproximateQuantiles_test.cpp -MG -M -MF build/ApproximateQuantiles_test.d -MT build/ApproximateQuantiles_test.o; g++ -o build/NativeBoot.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux -MD -MF build/NativeBoot.d -MT build/NativeBoot.o -c NativeBoot.cpp; g++ -fvisibility=default -rdynamic -shared -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux build/NativeBoot.o -o lib/linux-x86-64/libboot.so; g++ -o build/davies.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux -MD -MF build/davies.d -MT build/davies.o -c davies.cpp; davies.cpp: In member function ‘real DaviesAlgo::qf(real*, real*, int*, int, real, real, int, real, real*, int*)’:; davies.cpp:241:8: error: variable ‘qfval’ might be clobbered by ‘longjmp’ or ‘vfork’ [-Werror=clobbered]; real qfval;; ^; cc1plus: all warnings being treated as errors; make: *** [build/davies.o] Error 1; Makefile:63: recipe for target 'build/davies.o' failed; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5659:7552,variab,variable,7552,https://hail.is,https://github.com/hail-is/hail/issues/5659,1,['variab'],['variable']
Modifiability,"d0eb6c2af0e3e98350e24047c4df7d5b8aad89a""><code>fd0eb6c</code></a> Bump pylint to 2.13.0, update changelog</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/1c509edc4ee2dbf1bbe8822e91e0b7df02ce463d""><code>1c509ed</code></a> [cleanup] Remove unused code in pylint.checker.base following refactor</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/1e7d3fa6219934028d2539ad290fe16ce8ea78e2""><code>1e7d3fa</code></a> [refactor] Create a file for the BasicChecker in pylint.checker.base</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/c0b8b32592f8d5d34ff37250adbda6b65269a0af""><code>c0b8b32</code></a> [refactor] Create a file for the BasicErrorChecker in pylint.checker.base</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/3f11fe629a7b89d2a3b92dce09ac5818f3904cee""><code>3f11fe6</code></a> [refactor] Create a package for the NameChecker in pylint.checker.base</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/6940715ba15f81fbd7d9e8685c0a714a8b612f24""><code>6940715</code></a> [refactor] Create a file for the DocstringChecker in pylint.checker.base</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/84d22cf24202bf6006fc179541e1853d145d33e0""><code>84d22cf</code></a> [refactor] Create a file for the PassChecker in pylint.checker.base</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/977b08d160e81aaecebf871d2b8ba2f9a96ef9d6""><code>977b08d</code></a> [refactor] Create files for comparison checker in pylint.checker.base</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/ddfca0ca884d677e4eb0e6f53553b16e7a503157""><code>ddfca0c</code></a> [refactor] Create a file for _BasicChecker in pylint.checkers</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/be4699399904654ef4107a228817b4ef176d8999""><code>be46993</code></a> [refactor] Create a package in order to be able to burst base.py</li>; <li>Additional commits viewable in <a href=""https://github.com/PyCQA/pylint/compare/v2.12.2...v2.13.0"">compare view</a></li>; </ul>; </detail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11674:4481,refactor,refactor,4481,https://hail.is,https://github.com/hail-is/hail/pull/11674,1,['refactor'],['refactor']
Modifiability,"dDisk"": {; ""diskEncryptionSet"": null,; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/disks/batch-worker-pr-11144-default-nbthv8fduvd6-highmem-13t6m-os"",; ""resourceGroup"": ""dgoldste"",; ""storageAccountType"": ""Standard_LRS""; },; ""name"": ""batch-worker-pr-11144-default-nbthv8fduvd6-highmem-13t6m-os"",; ""osType"": ""Linux"",; ""vhd"": null,; ""writeAcceleratorEnabled"": null; }; },; ""tags"": {; ""batch-worker"": ""1"",; ""namespace"": ""pr-11144-default-nbthv8fduvd6""; },; ""type"": ""Microsoft.Compute/virtualMachines"",; ""userData"": null,; ""virtualMachineScaleSet"": null,; ""vmId"": ""2612958c-ef4e-4678-8482-29726290ae20"",; ""zones"": null; },; {; ""additionalCapabilities"": null,; ""applicationProfile"": null,; ""availabilitySet"": null,; ""billingProfile"": {; ""maxPrice"": -1.0; },; ""capacityReservation"": null,; ""diagnosticsProfile"": null,; ""evictionPolicy"": ""Delete"",; ""extendedLocation"": null,; ""extensionsTimeBudget"": null,; ""hardwareProfile"": {; ""vmSize"": ""Standard_D16d_v4"",; ""vmSizeProperties"": null; },; ""host"": null,; ""hostGroup"": null,; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/virtualMachines/batch-worker-pr-11144-default-nbthv8fduvd6-standard-i4sun"",; ""identity"": {; ""principalId"": null,; ""tenantId"": null,; ""type"": ""UserAssigned"",; ""userAssignedIdentities"": {; ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.ManagedIdentity/userAssignedIdentities/batch-worker"": {; ""clientId"": ""890af904-42f1-4136-810a-c52f4e132c6b"",; ""principalId"": ""b952a3bb-1091-4f11-803b-9d5199219a27""; }; }; },; ""instanceView"": null,; ""licenseType"": null,; ""location"": ""eastus"",; ""name"": ""batch-worker-pr-11144-default-nbthv8fduvd6-standard-i4sun"",; ""networkProfile"": {; ""networkApiVersion"": null,; ""networkInterfaceConfigurations"": null,; ""networkInterfaces"": [; {; ""deleteOption"": ""Delete"",; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resou",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686:10077,extend,extendedLocation,10077,https://hail.is,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686,1,['extend'],['extendedLocation']
Modifiability,dEvaluate.scala:60); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$2$adapted(CompileAndEvaluate.scala:58); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:144); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:144); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:58); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:17); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:17); 	at is.hail.expr.ir.TableWriter.apply(TableWriter.scala:51); 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:921); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:66); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:20); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:11771,rewrite,rewrite,11771,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['rewrite'],['rewrite']
Modifiability,"dPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.130-bea04d9c79b5; Error summary: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:20686,Plugin,Plugins,20686,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,"d` an `OrderedRVD`, possibly with some non-empty key. This is consistent with the rule that the `rvd` must always have a stronger/longer key than the `TableType`.; * **small tweaks** - Now I start working through the `TableIR` nodes, rewriting them to remove explicit uses of `UnpartitionedRVD`. The general plan is to sandwich the rvd logic between `toOrderedRVD` and `toOldStyleRVD`. The first takes an `UnpartitionedRVD` to an `OrderedRVD` with empty key (and leaves `OrderedRVD`s alone), and the second takes an `OrderedRVD` to an `UnpartitionedRVD` if its key was empty, and leaves it alone otherwise. Once they're all rewritten this way, I redefine `toOldStyleRVD` to always return `OrderedRVD`, and `UnpartitionedRVD` is no longer used.; * **remove `TableUnkey`** - With `UnpartitionedRVD` going away, `TableUnkey` is no longer necessary, it's equivalent to keying by an empty key.; * **small tweaks** - these next two rewrite more `TableIR` nodes; * **Merge master** - the big one; * **tweak MatrixColsTable** - 1) Optimize `coerce` by checking if the requested key is empty, avoiding a scan in that case. 2) Optimize `sortedColsValue` by checking if the column key is empty, avoiding the sort in that case. 3) Simplify `colsRVD`, removing the case on the type of the `RVD`, just calling `coerce` and letting the previous optimizations avoid unnecessary work.; * **`distinctByKey` fix** - While looking over `TableIR` implementations, I noticed a bug in `distinctByKey`: you need to be sure no key is split across multiple partitions. To be sure the empty key edge case still works, I added a test to check that `strictify` on an empty-key partitioner will always collapse everything to one partition.; * **Flipped switch** - redifines `toOldStyleRVD` to just return the `OrderedRVD` unchanged, and asserts that `TableValue.rvd` is always an `OrderedRVD`.; * **rest of the `TableIR` tweaks** - added a factory method `OrderedRVD.unkeyed` to replace `UnpartitionedRVD.apply`.; * the rest are si",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4319:1736,rewrite,rewrite,1736,https://hail.is,https://github.com/hail-is/hail/pull/4319,1,['rewrite'],['rewrite']
Modifiability,dcast(BroadcastManager.scala:75); at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1539); at is.hail.backend.spark.SparkBackend.broadcast(SparkBackend.scala:411); at is.hail.io.plink.MatrixPLINKReader.executeGeneric(LoadPlink.scala:390); at is.hail.io.plink.MatrixPLINKReader.lower(LoadPlink.scala:561); at is.hail.expr.ir.TableReader.lower(TableIR.scala:663); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1062); at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:728); at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:1021); at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:27); at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:11); at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:91); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:27); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); at scala.collection.mutable.ResizableArray.foreach(ResizableA,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:5452,rewrite,rewrite,5452,https://hail.is,https://github.com/hail-is/hail/issues/14168,3,['rewrite'],['rewrite']
Modifiability,"dce09ac5818f3904cee""><code>3f11fe6</code></a> [refactor] Create a package for the NameChecker in pylint.checker.base</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/6940715ba15f81fbd7d9e8685c0a714a8b612f24""><code>6940715</code></a> [refactor] Create a file for the DocstringChecker in pylint.checker.base</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/84d22cf24202bf6006fc179541e1853d145d33e0""><code>84d22cf</code></a> [refactor] Create a file for the PassChecker in pylint.checker.base</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/977b08d160e81aaecebf871d2b8ba2f9a96ef9d6""><code>977b08d</code></a> [refactor] Create files for comparison checker in pylint.checker.base</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/ddfca0ca884d677e4eb0e6f53553b16e7a503157""><code>ddfca0c</code></a> [refactor] Create a file for _BasicChecker in pylint.checkers</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/be4699399904654ef4107a228817b4ef176d8999""><code>be46993</code></a> [refactor] Create a package in order to be able to burst base.py</li>; <li>Additional commits viewable in <a href=""https://github.com/PyCQA/pylint/compare/v2.12.2...v2.13.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pylint&package-manager=pip&previous-version=2.12.2&new-version=2.13.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11674:5254,refactor,refactor,5254,https://hail.is,https://github.com/hail-is/hail/pull/11674,1,['refactor'],['refactor']
Modifiability,dd.RDD.$anonfun$collect$2(RDD.scala:1021); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2276); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2861); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2792); 	at org.apache.spark.u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936:5266,adapt,adapted,5266,https://hail.is,https://github.com/hail-is/hail/issues/12936,1,['adapt'],['adapted']
Modifiability,dding 'hailtop/aiotools/fs/fs.py'; adding 'hailtop/aiotools/fs/stream.py'; adding 'hailtop/auth/__init__.py'; adding 'hailtop/auth/auth.py'; adding 'hailtop/auth/sql_config.py'; adding 'hailtop/auth/tokens.py'; adding 'hailtop/batch/__init__.py'; adding 'hailtop/batch/backend.py'; adding 'hailtop/batch/batch.py'; adding 'hailtop/batch/batch_pool_executor.py'; adding 'hailtop/batch/conftest.py'; adding 'hailtop/batch/docker.py'; adding 'hailtop/batch/exceptions.py'; adding 'hailtop/batch/globals.py'; adding 'hailtop/batch/hail_genetics_images.py'; adding 'hailtop/batch/job.py'; adding 'hailtop/batch/resource.py'; adding 'hailtop/batch/utils.py'; adding 'hailtop/batch_client/__init__.py'; adding 'hailtop/batch_client/aioclient.py'; adding 'hailtop/batch_client/client.py'; adding 'hailtop/batch_client/globals.py'; adding 'hailtop/batch_client/parse.py'; adding 'hailtop/cleanup_gcr/__init__.py'; adding 'hailtop/cleanup_gcr/__main__.py'; adding 'hailtop/config/__init__.py'; adding 'hailtop/config/deploy_config.py'; adding 'hailtop/config/user_config.py'; adding 'hailtop/fs/__init__.py'; adding 'hailtop/fs/fs.py'; adding 'hailtop/fs/fs_utils.py'; adding 'hailtop/fs/router_fs.py'; adding 'hailtop/fs/stat_result.py'; adding 'hailtop/hailctl/__init__.py'; adding 'hailtop/hailctl/__main__.py'; adding 'hailtop/hailctl/deploy.yaml'; adding 'hailtop/hailctl/describe.py'; adding 'hailtop/hailctl/auth/__init__.py'; adding 'hailtop/hailctl/auth/cli.py'; adding 'hailtop/hailctl/auth/create_user.py'; adding 'hailtop/hailctl/auth/delete_user.py'; adding 'hailtop/hailctl/auth/login.py'; adding 'hailtop/hailctl/batch/__init__.py'; adding 'hailtop/hailctl/batch/batch_cli_utils.py'; adding 'hailtop/hailctl/batch/cli.py'; adding 'hailtop/hailctl/batch/list_batches.py'; adding 'hailtop/hailctl/batch/submit.py'; adding 'hailtop/hailctl/batch/billing/__init__.py'; adding 'hailtop/hailctl/batch/billing/cli.py'; adding 'hailtop/hailctl/config/__init__.py'; adding 'hailtop/hailctl/config/cli.py',MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13445:10845,config,config,10845,https://hail.is,https://github.com/hail-is/hail/issues/13445,1,['config'],['config']
Modifiability,"de it with claims:</p>; <pre><code>claims_options = {; 'iss': {'essential': True, 'values': ['required']}; }; jwt.decode(token, key, claims_options=claims_options); </code></pre>; <p>It didn't raise an error before this fix.</p>; <h2>Version 0.15.3</h2>; <p>Fixed <code>.authorize_access_token</code> for OAuth 1.0 services, via <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/308"">lepture/authlib#308</a></p>; <h2>Version 0.15.2</h2>; <p>Fixed httpx authentication bug via <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/283"">#283</a></p>; <h2>Version 0.15.1</h2>; <p>Backward compitable fix for using JWKs in JWT, via <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/280"">#280</a>.</p>; <h2>Version 0.15</h2>; <p>This is the last release before v1.0. In this release, we added more RFCs; implementations and did some refactors for JOSE:</p>; <ul>; <li>RFC8037: CFRG Elliptic Curve Diffie-Hellman (ECDH) and Signatures in JSON Object Signing and Encryption (JOSE)</li>; <li>RFC7638: JSON Web Key (JWK) Thumbprint</li>; </ul>; <p>We also fixed bugs for integrations:</p>; <ul>; <li>Fixed support for HTTPX&gt;=0.14.3</li>; <li>Added OAuth clients of HTTPX back via <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/270"">#270</a></li>; <li>Fixed parallel token refreshes for HTTPX async OAuth 2 client</li>; <li>Raise OAuthError when callback contains errors via <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/275"">#275</a></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/lepture/authlib/blob/master/docs/changelog.rst"">authlib's changelog</a>.</em></p>; <blockquote>; <h2>Version 0.15.5</h2>; <p><strong>Released on Oct 18, 2021.</strong></p>; <ul>; <li>Make Authlib compatible with latest httpx</li>; <li>Make Authlib compatible with latest werkzeug</li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11483:1544,refactor,refactors,1544,https://hail.is,https://github.com/hail-is/hail/pull/11483,1,['refactor'],['refactors']
Modifiability,decSpec.scala:57); E 	at is.hail.io.AbstractTypedCodecSpec.decodeArrays$(CodecSpec.scala:54); E 	at is.hail.io.TypedCodecSpec.decodeArrays(TypedCodecSpec.scala:19); E 	at is.hail.expr.ir.Interpret$.$anonfun$run$1(Interpret.scala:80); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:79); E 	at is.hail.expr.ir.Interpret$.interpret$1(Interpret.scala:67); E 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:110); E 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:58); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$foldConstants$1(FoldConstants.scala:47); E 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); E 	at is.hail.expr.ir.FoldConstants$.foldConstants(FoldConstants.scala:13); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$apply$1(FoldConstants.scala:10); E 	at is.hail.backend.ExecuteContext$.$anonfun$scopedNewRegion$1(ExecuteContext.scala:86); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.backend.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:83); E 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:9); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$4(Optimize.scala:22); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$1(Optimize.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.runOpt$1(Optimize.scala:15); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$2(Optimize.scala:22); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.utils.Execut,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:6473,Rewrite,RewriteBottomUp,6473,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Rewrite'],['RewriteBottomUp']
Modifiability,delint batch2; fix worker boot disk config; internal-gateway: propagate X-Forwarded-Proto for blog,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7492:36,config,config,36,https://hail.is,https://github.com/hail-is/hail/pull/7492,1,['config'],['config']
Modifiability,der$.scopedCode(EmitCodeBuilder.scala:23); 	at is.hail.expr.ir.EmitMethodBuilder.emitWithBuilder(EmitClassBuilder.scala:1009); 	at is.hail.expr.ir.WrappedEmitMethodBuilder.emitWithBuilder(EmitClassBuilder.scala:1062); 	at is.hail.expr.ir.WrappedEmitMethodBuilder.emitWithBuilder$(EmitClassBuilder.scala:1062); 	at is.hail.expr.ir.EmitFunctionBuilder.emitWithBuilder(EmitClassBuilder.scala:1078); 	at is.hail.expr.ir.Emit.$anonfun$emitI$238(Emit.scala:2361); 	at is.hail.expr.ir.IEmitCodeGen.map(Emit.scala:336); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:2341); 	at is.hail.expr.ir.Emit.emitI$3(Emit.scala:2555); 	at is.hail.expr.ir.Emit.$anonfun$emit$22(Emit.scala:2638); 	at is.hail.expr.ir.EmitCode$.fromI(Emit.scala:445); 	at is.hail.expr.ir.Emit.emit(Emit.scala:2637); 	at is.hail.expr.ir.Emit.emit$1(Emit.scala:621); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:657); 	at is.hail.expr.ir.Emit.$anonfun$emitVoidInSeparateMethod$1(Emit.scala:579); 	at is.hail.expr.ir.Emit.$anonfun$emitVoidInSeparateMethod$1$adapted(Emit.scala:577); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:1011); 	at is.hail.expr.ir.Emit.emitVoidInSeparateMethod(Emit.scala:577); 	at is.hail.expr.ir.Emit.emitInSeparateMethod(Emit.scala:601); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:793); 	at is.hail.expr.ir.Emit.emitI$1(Emit.scala:630); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$26(Emit.scala:748); 	at is.hail.expr.ir.RelationalWriter.writeMetadata(TableWriter.scala:463); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:748); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3(Emit.scala:70); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3$adapted(Emit.scala:68); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWit,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533:7819,adapt,adapted,7819,https://hail.is,https://github.com/hail-is/hail/issues/12533,1,['adapt'],['adapted']
Modifiability,ding 'hailtop/auth/auth.py'; adding 'hailtop/auth/flow.py'; adding 'hailtop/auth/sql_config.py'; adding 'hailtop/auth/tokens.py'; adding 'hailtop/batch/__init__.py'; adding 'hailtop/batch/backend.py'; adding 'hailtop/batch/batch.py'; adding 'hailtop/batch/batch_pool_executor.py'; adding 'hailtop/batch/conftest.py'; adding 'hailtop/batch/docker.py'; adding 'hailtop/batch/exceptions.py'; adding 'hailtop/batch/globals.py'; adding 'hailtop/batch/hail_genetics_images.py'; adding 'hailtop/batch/job.py'; adding 'hailtop/batch/resource.py'; adding 'hailtop/batch/utils.py'; adding 'hailtop/batch_client/__init__.py'; adding 'hailtop/batch_client/aioclient.py'; adding 'hailtop/batch_client/client.py'; adding 'hailtop/batch_client/globals.py'; adding 'hailtop/batch_client/parse.py'; adding 'hailtop/batch_client/types.py'; adding 'hailtop/cleanup_gcr/__init__.py'; adding 'hailtop/cleanup_gcr/__main__.py'; adding 'hailtop/config/__init__.py'; adding 'hailtop/config/deploy_config.py'; adding 'hailtop/config/user_config.py'; adding 'hailtop/config/variables.py'; adding 'hailtop/fs/__init__.py'; adding 'hailtop/fs/fs.py'; adding 'hailtop/fs/fs_utils.py'; adding 'hailtop/fs/router_fs.py'; adding 'hailtop/fs/stat_result.py'; adding 'hailtop/hailctl/__init__.py'; adding 'hailtop/hailctl/__main__.py'; adding 'hailtop/hailctl/deploy.yaml'; adding 'hailtop/hailctl/describe.py'; adding 'hailtop/hailctl/auth/__init__.py'; adding 'hailtop/hailctl/auth/cli.py'; adding 'hailtop/hailctl/auth/create_user.py'; adding 'hailtop/hailctl/auth/delete_user.py'; adding 'hailtop/hailctl/auth/login.py'; adding 'hailtop/hailctl/batch/__init__.py'; adding 'hailtop/hailctl/batch/batch_cli_utils.py'; adding 'hailtop/hailctl/batch/cli.py'; adding 'hailtop/hailctl/batch/initialize.py'; adding 'hailtop/hailctl/batch/list_batches.py'; adding 'hailtop/hailctl/batch/submit.py'; adding 'hailtop/hailctl/batch/utils.py'; adding 'hailtop/hailctl/batch/billing/__init__.py'; adding 'hailtop/hailctl/batch/billing/cli.py';,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:26432,config,config,26432,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['config'],['config']
Modifiability,"dleware type alias <code>aiohttp.typedefs.Middleware</code>.</p>; <p>(<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/5898"">#5898</a>)</p>; </li>; <li>; <p>Exported <code>HTTPMove</code> which can be used to catch any redirection request; that has a location -- :user:<code>dreamsorcerer</code>.</p>; <p>(<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/6594"">#6594</a>)</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/blob/master/CHANGES.rst"">aiohttp's changelog</a>.</em></p>; <blockquote>; <h1>3.9.0 (2023-11-18)</h1>; <h2>Features</h2>; <ul>; <li>; <p>Introduced <code>AppKey</code> for static typing support of <code>Application</code> storage.; See <a href=""https://docs.aiohttp.org/en/stable/web_advanced.html#application-s-config"">https://docs.aiohttp.org/en/stable/web_advanced.html#application-s-config</a></p>; <p><code>[#5864](https://github.com/aio-libs/aiohttp/issues/5864) &lt;https://github.com/aio-libs/aiohttp/issues/5864&gt;</code>_</p>; </li>; <li>; <p>Added a graceful shutdown period which allows pending tasks to complete before the application's cleanup is called.; The period can be adjusted with the <code>shutdown_timeout</code> parameter. -- by :user:<code>Dreamsorcerer</code>.; See <a href=""https://docs.aiohttp.org/en/latest/web_advanced.html#graceful-shutdown"">https://docs.aiohttp.org/en/latest/web_advanced.html#graceful-shutdown</a></p>; <p><code>[#7188](https://github.com/aio-libs/aiohttp/issues/7188) &lt;https://github.com/aio-libs/aiohttp/issues/7188&gt;</code>_</p>; </li>; <li>; <p>Added <code>handler_cancellation &lt;https://docs.aiohttp.org/en/stable/web_advanced.html#web-handler-cancellation&gt;</code>_ parameter to cancel web handler on client disconnection. -- by :user:<code>mosquito</code>; This (optionally) reintroduces a feature removed in a previous release.; Recom",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14027:3764,config,config,3764,https://hail.is,https://github.com/hail-is/hail/pull/14027,6,['config'],['config']
Modifiability,do we need loop?. don't we just need another parameterized aggregator?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7173#issuecomment-570312503:45,parameteriz,parameterized,45,https://hail.is,https://github.com/hail-is/hail/issues/7173#issuecomment-570312503,1,['parameteriz'],['parameterized']
Modifiability,docker_http_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/client/__pycache__/docker_creds_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/client/__pycache__/docker_name_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_puller_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_pusher_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_appender_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_appender_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_puller_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_pusher_.cpython-39.pyc; /usr/local/share/google/dataproc/npd-config/docker-monitor-counter.json; /usr/local/share/google/dataproc/npd-config/docker-monitor.json; /usr/local/share/google/dataproc/npd-config/health-checker-docker.json; /usr/local/share/google/dataproc/npd-config/docker-monitor-filelog.json; /usr/local/share/google/dataproc/bdutil/fluentd/container_logging/plugin/test/Dockerfile; /usr/local/share/google/dataproc/bdutil/components/initialize/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/post-install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/activate/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/shared/docker.sh; /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/configure_docker.sh; /run/docker.sock; /tmp/dataproc/uninstall/docker-ce; /tmp/dataproc/components/uninstall/docker-ce.running; /tmp/dataproc/components/uninstall/docker-ce.done; /tmp/dataproc/components/pre-uninstall/docker-ce.run,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:11781,config,config,11781,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['config'],['config']
Modifiability,"docker_prefix is not exactly the ""registry name"" in azure's definition, but it is `<registry_name>.azurecr.io` which `az acr login` accepts alternatively to just the registry name. Didn't seem worth adding a mostly redundant config field.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11301:225,config,config,225,https://hail.is,https://github.com/hail-is/hail/pull/11301,1,['config'],['config']
Modifiability,"doesn't include the ability to use user-provided expr to generate double matrixes for PCA. @cseed @tpoterba I think you had feelings on whether or not PCA should keep the current behavior where it returns an annotated vsm. . Currently, I've kept VariantSampleMatrix.pca as something that annotates and returns a vsm (using annotateVariantsTable for loadings), and then there's another method (VariantSampleMatrix.pcaResults) that just returns a tuple of (scores, loadings, eigenvalues). I could refactor them such that pca() => pcaAsAnnotations() (or something) and pcaResults() => pca(), and updating the python interface to match?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2432:495,refactor,refactor,495,https://hail.is,https://github.com/hail-is/hail/pull/2432,1,['refactor'],['refactor']
Modifiability,"doop.mapred.DirectFileOutputCommitter not found'; full = 'java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.map...mmand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). '. @decorator; def handle_py4j(func, *args, **kwargs):; try:; r = func(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); E FatalError: ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E; E Java stack trace:; E java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2227); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFun",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:3781,Config,Configuration,3781,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['Config'],['Configuration']
Modifiability,"e HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) serve",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:3795,config,configuration,3795,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['config'],['configuration']
Modifiability,"e Method); 	at java.io.FileOutputStream.open(FileOutputStream.java:270); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:213); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:133); 	at org.apache.log4j.FileAppender.setFile(FileAppender.java:294); 	at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165); 	at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104); 	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842); 	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768); 	at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648); 	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514); 	at org.apache.log4j.PropertyConfigurator.configure(PropertyConfigurator.java:440); 	at is.hail.HailContext$.configureLogging(HailContext.scala:132); 	at is.hail.HailContext$.apply(HailContext.scala:159); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-289>"", line 2, in __init__; File ""/opt/Software/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198:2302,config,configure,2302,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198,1,['config'],['configure']
Modifiability,"e all async fixtures and tests; automatically by asyncio; provide <em>strict</em> mode if a test suite; should work with different async frameworks simultaneously, e.g.; <code>asyncio</code> and <code>trio</code>.</li>; </ul>; <h1>Installation</h1>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pytest-dev/pytest-asyncio/blob/master/CHANGELOG.rst"">pytest-asyncio's changelog</a>.</em></p>; <blockquote>; <h1>0.20.1 (22-10-21)</h1>; <ul>; <li>Fixes an issue that warned about using an old version of pytest, even though the most recent version was installed. <code>[#430](https://github.com/pytest-dev/pytest-asyncio/issues/430) &lt;https://github.com/pytest-dev/pytest-asyncio/issues/430&gt;</code>_</li>; </ul>; <h1>0.20.0 (22-10-21)</h1>; <ul>; <li>BREAKING: Removed <em>legacy</em> mode. If you're upgrading from v0.19 and you haven't configured <code>asyncio_mode = legacy</code>, you can upgrade without taking any additional action. If you're upgrading from an earlier version or you have explicitly enabled <em>legacy</em> mode, you need to switch to <em>auto</em> or <em>strict</em> mode before upgrading to this version.</li>; <li>Deprecate use of pytest v6.</li>; <li>Fixed an issue which prevented fixture setup from being cached. <code>[#404](https://github.com/pytest-dev/pytest-asyncio/issues/404) &lt;https://github.com/pytest-dev/pytest-asyncio/pull/404&gt;</code>_</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/c8d017407d39dd81d6864fa9a58ba1240d54be9f""><code>c8d0174</code></a> fix: Do not warn about outdated pytest version when pytest&gt;=7 is installed. (...</li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/6450ddbe974f5359d56317ba8bdda8b2ab48655a""><code>6450ddb</code></a> Prepare release of v0.20.0. (<a href=""https://github-redir",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12390:3214,config,configured,3214,https://hail.is,https://github.com/hail-is/hail/pull/12390,1,['config'],['configured']
Modifiability,"e handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not cancelled when its parents are cancelled.; - [ ] add namespace dependencies. CI allocates anonymous namespaces as requested by the build process. All `exec`s are, by default, run in an anonymous namespace. CI adds a finalizer job that deletes namespaces when all relevant `exec`s are finished; - [ ] add image dependencies. CI can create a batch job that builds a docker image with an anonymous name and pushes it to the project's GCR. CI adds a finalizer job that deletes the image when all relevant `exec`s are finished.; - [ ] batch and notebook are parameterized by their worker namespace so they can use the namespaces described above; - [ ] hail's build steps are parameterized in a way that permits them to use a jar not built locally on this machine (hopefully the Make PR makes this easy, otherwise we have to fool gradle into not rebuilding the jar). To reliably handle clean up, we *must* persist batch jobs, so I think that should be either higher priority or at least happening in parallel to the above (i.e. two developers working in parallel). - [ ] persist batch jobs in a durable store with all of the fields in the beginning of `Job.__init__`. When batch starts up, before serving any requests, it restores its state from the durable store and then refreshes from k8s. The k8s label `hail.is/batch-instance` is retired. Instead, pods have `hail.is/batch-version` which is a monotonically increasing natural number. It is only incremented if batch is backwards incompatible with the pod specs. Probably batch should destroy any pods that are alive from an out-of-date version of batch.; - [ ] persist CI information in a durable store [this needs more detail]; - [ ] How do we configure the database? How do we create new tables? Should this be done in the applications themselves or during deployment?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5193:3694,config,configure,3694,https://hail.is,https://github.com/hail-is/hail/issues/5193,1,['config'],['configure']
Modifiability,"e in one <code>read</code> call. :issue:<code>2558</code></li>; <li>A cookie header that starts with <code>=</code> is treated as an empty key and discarded,; rather than stripping the leading <code>==</code>.</li>; <li>Specify a maximum number of multipart parts, default 1000, after which a; <code>RequestEntityTooLarge</code> exception is raised on parsing. This mitigates a DoS; attack where a larger number of form/file parts would result in disproportionate; resource use.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/werkzeug/commit/22a254fca2ad0130adbbcbd11d3de51bcb04a08b""><code>22a254f</code></a> release version 2.2.3</li>; <li><a href=""https://github.com/pallets/werkzeug/commit/517cac5a804e8c4dc4ed038bb20dacd038e7a9f1""><code>517cac5</code></a> Merge pull request from GHSA-xg9f-g7g7-2323</li>; <li><a href=""https://github.com/pallets/werkzeug/commit/babc8d9e8c9fa995ef26050698bc9b5a92803664""><code>babc8d9</code></a> rewrite docs about request data limits</li>; <li><a href=""https://github.com/pallets/werkzeug/commit/09449ee77934a0c883f5959785864ecae6aaa2c9""><code>09449ee</code></a> clean up docs</li>; <li><a href=""https://github.com/pallets/werkzeug/commit/fe899d0cdf767a7289a8bf746b7f72c2907a1b4b""><code>fe899d0</code></a> limit the maximum number of multipart form parts</li>; <li><a href=""https://github.com/pallets/werkzeug/commit/cf275f42acad1b5950c50ffe8ef58fe62cdce028""><code>cf275f4</code></a> Merge pull request from GHSA-px8h-6qxv-m22q</li>; <li><a href=""https://github.com/pallets/werkzeug/commit/8c2b4b82d0cade0d37e6a88e2cd2413878e8ebd4""><code>8c2b4b8</code></a> don't strip leading = when parsing cookie</li>; <li><a href=""https://github.com/pallets/werkzeug/commit/7c7ce5cb73f3f7d3b9c09340e4f322aeb583dbc5""><code>7c7ce5c</code></a> [pre-commit.ci] pre-commit autoupdate (<a href=""https://github-redirect.dependabot.com/pallets/werkzeug/issues/2585"">#2585</a>)</li>; <li><a href=""https://g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12703:3450,rewrite,rewrite,3450,https://hail.is,https://github.com/hail-is/hail/pull/12703,1,['rewrite'],['rewrite']
Modifiability,"e key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key. - Create a test query-gsa-key in test and dev namespaces. - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`. - The `query` user was missing from bootstrap-create-accounts. - `hail-ubuntu-stmp` was missing from `docker/Makefile`'s `clean` rule. - Only apply nest_asyncio when an event loop is already started.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10314:3533,config,config,3533,https://hail.is,https://github.com/hail-is/hail/pull/10314,1,['config'],['config']
Modifiability,"e requester pays bucket in the region specified by the user, available regions are `'us'` and `'eu'`. . `db = hl.experimental.DB(region='us')`; `mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics')`. Entries in the `annotation_db.json` file were modified to the following format:. ```; ""dataset_name"": { ""description"": ""some description here"",; ""key_properties"": [],; ""url"": ""https://www.someurlhere.com"",; ""versions"": [{""url"": {""eu"": ""gs://hail-datasets-eu/dataset"",; ""us"": ""gs://hail-datasets-us/dataset""},; ""version"": ""one_version""},; {""url"": {""eu"": ""gs://hail-datasets-eu/dataset"",; ""us"": ""gs://hail-datasets-us/dataset""},; ""version"": ""another_version""}]}; ```. The `annotation_db.json` file is now used by the `load_dataset()` function in `datasets.py` as well, any dataset in the JSON file should now be able to be loaded this way. Made changes to the following:; - `DB` class now requires a `region` parameter.; - `Dataset.from_name_and_json()` has had a `custom_config` parameter added that indicates whether or not the user has supplied their own `config` or `url`. `Dataset.from_name_and_json()` now calls `DatasetVersion.get_region()` method to retrieve the dataset from the bucket in the selected region if `custom_config` is `False`. ; - The `DatasetVersion.get_region()` method takes the dataset `name`, a list of `DatasetVersion` objects, and a `region`, and returns a list of the versions that are available for that region. This method calls the instance method `in_region()` to check if the dataset is available in the requested region.; - If `in_region()` determines the desired region is not available for some dataset that otherwise is available in another region, it will raise a warning. If user still tries to call `db.annotate_rows_db()` using a dataset unavailable in their region, then it will get caught by the `_check_availability` instance method in the `DB` class and raise a `ValueError`.; - Started to add documentation to the classes and methods, still a work in progr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9496:1156,config,config,1156,https://hail.is,https://github.com/hail-is/hail/pull/9496,1,['config'],['config']
Modifiability,"e seen here: https://ci.hail.is/batches/6627486/jobs/105; ```; During handling of the above exception, another exception occurred:. @skip_unless_service_backend(); def test_tiny_driver_has_tiny_memory():; try:; hl.utils.range_table(100_000_000, 50).to_pandas(); except Exception as exc:; # Sometimes the JVM properly OOMs, sometimes it just dies.; > assert (; 'java.lang.OutOfMemoryError: Java heap space' in exc.args[0] or; 'batch.worker.jvm_entryway_protocol.EndOfStream' in exc.args[0]; ); E assert ('java.lang.OutOfMemoryError: Java heap space' in {'batch_status': {'attributes': {'name': 'test_tiny_driver_has_tiny_memory'}, 'billing_project': 'test', 'closed': True, 'complete': True, ...}, 'job_status': {'attributes': {'name': 'driver'}, 'batch_id': 6627669, 'billing_project': 'test', 'cost': 0.0015413897092729028, ...}, 'log': {'main': ""2022-11-15 20:30:18.004 Tokens: INFO: tokens found for namespaces {default}\n2022-11-15 20:30:18.004 tls: INFO: ssl config file found at /batch/2bbb233e4e3c4a96bbffb515019daac9/secrets/ssl-config/ssl-config.json\n2022-11-15 20:30:18.006 GoogleStorageFS$: INFO: Initializing google storage client from service account key\n2022-11-15 20:30:18.114 root: INFO: RegionPool: initialized for thread 8: pool-1-thread-1\n2022-11-15 20:30:18.114 ServiceBackend$: INFO: executing: cEPZ5IV9gUtSnCiAiHXOPs None\n2022-11-15 20:30:18.127 root: INFO: optimize optimize: darrayLowerer, initial IR: before: IR size 17: \n(Let __rng_state\n (RNGStateLiteral (0 0 0 0))\n (MakeTuple (0)\n (TableAggregate\n (TableMapRows\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (InsertFields\n (SelectFields () (SelectFields (idx) (Ref row)))\n None\n (idx (GetField idx (Ref row)))))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row)))))))))\n2022-11-15 20:30:18.146 root: INFO: optimize optimize: darrayLowerer, initial IR: after: IR size 8:\n(MakeTuple (0)\n (TableAggregate\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (MakeStruct\n (idx\n ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:992,config,config,992,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,3,['config'],['config']
Modifiability,"e should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`.; - The `query` user was missing from bootstrap-create-accounts.; - `hail-ubuntu-stmp` was missing from `docker/Makefile`'s `clean` rule; - Use a dummy `WorkerBackend` when we're on the worker. The worker isn't allowed to call these; methods anyway, that would amount to Hail Queries inside of Hail Queries, madness!; - New transient error in Java.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:2170,config,config,2170,https://hail.is,https://github.com/hail-is/hail/pull/10279,1,['config'],['config']
Modifiability,"e to stage failure: Task 8 in stage 1.0 failed 20 times, most recent failure: Lost task 8.19 in stage 1.0 (TID 2899) (hail-test-w-1.australia-southeast1-a.c.pb-dev-312200.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:4427,Plugin,Plugins,4427,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,"e with <code>python -Wd</code> and check for <code>DeprecationWarning</code> s).; Our development attention will now shift to bug-fix releases on the; 1.10.x branch, and on adding new features on the main branch.</p>; <p>This release requires Python <code>3.8+</code> and NumPy <code>1.19.5</code> or greater.</p>; <p>For running on PyPy, PyPy3 <code>6.0+</code> is required.</p>; <h1>Highlights of this release</h1>; <ul>; <li>A new dedicated datasets submodule (<code>scipy.datasets</code>) has been added, and is; now preferred over usage of <code>scipy.misc</code> for dataset retrieval.</li>; <li>A new <code>scipy.interpolate.make_smoothing_spline</code> function was added. This; function constructs a smoothing cubic spline from noisy data, using the; generalized cross-validation (GCV) criterion to find the tradeoff between; smoothness and proximity to data points.</li>; <li><code>scipy.stats</code> has three new distributions, two new hypothesis tests, three; new sample statistics, a class for greater control over calculations; involving covariance matrices, and many other enhancements.</li>; </ul>; <h1>New features</h1>; <h1><code>scipy.datasets</code> introduction</h1>; <ul>; <li>A new dedicated <code>datasets</code> submodule has been added. The submodules; is meant for datasets that are relevant to other SciPy submodules ands; content (tutorials, examples, tests), as well as contain a curated; set of datasets that are of wider interest. As of this release, all; the datasets from <code>scipy.misc</code> have been added to <code>scipy.datasets</code>; (and deprecated in <code>scipy.misc</code>).</li>; <li>The submodule is based on <a href=""https://www.fatiando.org/pooch/latest/"">Pooch</a>; (a new optional dependency for SciPy), a Python package to simplify fetching; data files. This move will, in a subsequent release, facilitate SciPy; to trim down the sdist/wheel sizes, by decoupling the data files and; moving them out of the SciPy repository, hosting them externa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13227:1900,enhance,enhancements,1900,https://hail.is,https://github.com/hail-is/hail/pull/13227,1,['enhance'],['enhancements']
Modifiability,"e-based.html) based on information from the [TeamCity wiki](https://confluence.jetbrains.com/pages/viewpage.action?pageId=74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadMo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1371,config,configure,1371,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170,1,['config'],['configure']
Modifiability,"e.apis:google-api-services-storage to v1-rev...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/5e76f1963db18c9d081133755b5572e186cd1b34""><code>5e76f19</code></a> chore: Update the Java code generator (gapic-generator-java) to 2.25.0 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2198"">#2198</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/26552f4b78f77d90df4e3dfb829c3f9c092fc817""><code>26552f4</code></a> deps: update dependency com.google.cloud:google-cloud-shared-dependencies to ...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/bffb397730d39f4c1c9f8fa80e316a26c39534ce""><code>bffb397</code></a> chore: add SyncingFileChannel (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2157"">#2157</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/4f8bb658e9ff3cba5e745acae13ec4094a1a48d5""><code>4f8bb65</code></a> deps: update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.26...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/67badabaa5126ec6d011879c3983e6b69880c900""><code>67badab</code></a> chore(benchmarking): Remove default for temp directory and read from java.io....</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/45e66e89373ef016eff9b7deb30dbdfa818770d2""><code>45e66e8</code></a> deps: update actions/checkout action to v4 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2190"">#2190</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/5c048c499eef224dade8f4409dfae732cb5a7017""><code>5c048c4</code></a> deps: update actions/checkout action to v4 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2189"">#2189</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/googleapis/java-storage/compare/v2.17.1...v2.27.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-bad",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13624:14287,plugin,plugin,14287,https://hail.is,https://github.com/hail-is/hail/pull/13624,1,['plugin'],['plugin']
Modifiability,e.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.126-ee77707f4fab; Error summary: HailException: cannot set missing field for required type +PFloat64; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:20176,adapt,adapted,20176,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['adapt'],['adapted']
Modifiability,"e.select', value_struct=hl.struct(**row)); 867 ; 868 @typecheck_method(exprs=oneof(str, Expression)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . ~/projects/hail/python/hail/table.py in _process_joins(self, *exprs); 1463 def broadcast_f(left, data, jt):; 1464 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1465 return process_joins(self, exprs, broadcast_f); 1466 ; 1467 def cache(self):. ~/projects/hail/python/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 365 all_uids.extend(list(t)); 366 data = hail.Struct(**{b.uid: b.value for b in broadcasts}); --> 367 data_json = t._to_json(data); 368 left = broadcast_f(left, data_json, t._jtype); 369 . ~/projects/hail/python/hail/expr/types.py in _to_json(self, x); 176 def _to_json(self, x):; 177 converted = self._convert_to_json_na(x); --> 178 return json.dumps(converted); 179 ; 180 def _convert_to_json_na(self, x):. ~/anaconda2/envs/hail/lib/python3.6/json/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw); 229 cls is None and indent is None and separators is None and; 230 default is None and not sort_keys and not kw):; --> 231 return _default_encoder.encode(obj); 232 if cls is None:; 233 cls = JSONEncoder. ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in encode(self, o); 197 # exceptions aren't as detailed. The list call should be roughly; 198 # equivalent to the PySequence_Fast that ''.join() would do.; --> 199 chunks = self.iterencode(o, _on",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3708:3341,extend,extend,3341,https://hail.is,https://github.com/hail-is/hail/issues/3708,1,['extend'],['extend']
Modifiability,"e/userFiles-7053b92c-9117-46b6-8c52-752fee2701e9/fetchFileTemp5171194947676284646.tmp; 2018-10-09 14:46:41 Executor: INFO: Adding file:/private/var/folders/w4/9k0my8pd6113d61pq05fvqlr0000gn/T/spark-02128b51-f37e-4798-84bb-d3e3819e51be/userFiles-7053b92c-9117-46b6-8c52-752fee2701e9/sparklyr-2.2-2.11.jar to class loader; 2018-10-09 14:46:41 CodeGenerator: INFO: Code generated in 142.013327 ms; 2018-10-09 14:46:41 Executor: INFO: Finished task 0.0 in stage 0.0 (TID 0). 972 bytes result sent to driver; 2018-10-09 14:46:41 TaskSetManager: INFO: Finished task 0.0 in stage 0.0 (TID 0) in 406 ms on localhost (executor driver) (1/1); 2018-10-09 14:46:41 TaskSchedulerImpl: INFO: Removed TaskSet 0.0, whose tasks have all completed, from pool ; 2018-10-09 14:46:41 DAGScheduler: INFO: ResultStage 0 (collect at utils.scala:44) finished in 0.420 s; 2018-10-09 14:46:41 DAGScheduler: INFO: Job 0 finished: collect at utils.scala:44, took 0.669318 s; 2018-10-09 14:46:41 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:41 SparkSqlParser: INFO: Parsing command: table7e606a8b83f4; 2018-10-09 14:46:41 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:41 SparkSqlParser: INFO: Parsing command: CACHE TABLE `table7e606a8b83f4`; 2018-10-09 14:46:41 SparkSqlParser: INFO: Parsing command: `table7e606a8b83f4`; 2018-10-09 14:46:41 CodeGenerator: INFO: Code generated in 17.141549 ms; 2018-10-09 14:46:41 CodeGenerator: INFO: Code generated in 10.417049 ms; 2018-10-09 14:46:41 SparkContext: INFO: Starting job: sql at NativeMethodAccessorImpl.java:0; 2018-10-09 14:46:41 DAGScheduler: INFO: Registering RDD 12 (sql at NativeMethodAccessorImpl.java:0); 2018-10-09 14:46:41 DAGScheduler: INFO: Got job 1 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions; 2018-10-09 14:46:41 DAGScheduler: INFO: Final stage: ResultStage 2 (sql at NativeMethodAccessorImpl.java:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:35506,config,configuration,35506,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['config'],['configuration']
Modifiability,"eRef"":map[""apiGroup"":"""" ""kind"":""Role"" ""name"":""batch-pods-admin""] ""subjects"":[map[""kind"":""ServiceAccount"" ""name"":""batch-svc"" ""namespace"":""default""]]]}; from server for: ""deployment.yaml"": rolebindings.rbac.authorization.k8s.io ""batch-pods-admin-binding"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get rolebindings.rbac.authorization.k8s.io in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""hail.is/sha"":""1c6dbf20333a"" ""app"":""batch""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""1c6dbf20333a""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:4b4139c73fe9be3bee6c2895aa74059e157eb861d2bdac7d2304ba44b5421f88"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""labels"":map[""app"":""batch""] ""name"":""batch""] ""spec""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:3665,config,configuration,3665,https://hail.is,https://github.com/hail-is/hail/issues/4609,1,['config'],['configuration']
Modifiability,"e_pool.py code has been split amongst zone_monitor.py, pool.py, gce.py, create_instance.py, and instance_collection.py. The scheduler code is now in PoolScheduler in pool.py. The SQL code has vectorized user_resources by instance_collection as well as batch_cancellable_resources and batches_staging. There are also two new tables: inst_colls and pools. Each job and instance must belong to an instance collection noted by the field `inst_coll`. The job_update trigger had to be updated to insert into user_resources to the correct pool. The cancel_batch and close_batch functions changed to vectorize by instance collection. I deleted the global `ready_cores` table. The front end code does not change except looking for a `worker_type` field in the resources field of the job spec (default if undefined is standard). I added a PoolSelector class which is overkill for now, but will be used in the future for more complicated scenarios. There was an issue with our existing code for converting between memory in bytes to memory in MB in the worker_config.py code for the `resources()` function. For the highcpu case, it is impossible for the memory in bytes to be divisible by 1024**2. The utils.py code now rounds up bytes using math.ceil. The hailtop.batch library adds a `worker_type` method on Job. I didn't change the interface significantly at this time as I think this is fine for now. More significant changes will come when we change how cpu and memory and storage are interpreted by the worker. I added 3 new tests: we spin up a highmem and highcpu machine in test_batch.py. In the hailtop.batch tests, I check the `worker_type` interface. The driver UI page has changed such that there's a list of pools with links to a pools.html page, which gives information on the instances in the pool, the configuration updates, and the current user resources for that pool. Things to double check:; - SQL code especially cancel_batch where I had to use temporary variables and recompute_incremental",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9832:2301,config,configuration,2301,https://hail.is,https://github.com/hail-is/hail/pull/9832,2,"['config', 'variab']","['configuration', 'variables']"
Modifiability,"eadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/batch/server.py"", line 417, in run_forever; target(*args, **kwargs); File ""/batch/server.py"", line 441, in kube_event_loop; requests.post('http://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-10-23 03:58:08,124 | server.py | run_forever:416 | run_forever: run target kube_event_loop; INFO | 2018-10-23 03:59:30,729 | server.py | mark_complete:175 | wrote log for job 153 to logs/job-153.log; INFO | 2018-10-23 03:59:30,730 | server.py | set_state:141 | job 153 changed state: Created -> Complete; INFO | 2018-10-23 03:59:30,730 | server.py | mark_complete:184 | job 153 complete, exit_code 2; INFO | 2018-10-23 03:59:30,737 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,806 | _internal.py | _log:88 | 10.56.142.33 - - [23/Oct/2018 03:59:30] ""GET /jobs HTTP/1.1"" 200 -; INFO | 2018-10-23 03:59:30,815 | _internal.py | _",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:1717,adapt,adapter,1717,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038,1,['adapt'],['adapter']
Modifiability,"ebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services-and-pods; apiGroup: """"; ---; ```. ### Results of test runs. Before:. ```sh; kubectl apply -f k8s-config.yaml; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. namespace/batch-pods unchanged; ...; The RoleBinding ""notebook-create-services-and-pods"" is invalid: roleRef: Invalid value: rbac.RoleRef{APIGroup:""rbac.authorization.k8s.io"", Kind:""Role"", Name:""create-services""}: cannot change roleRef; make: *** [k8s-config] Error 1; ```. After:; ```sh; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. ...; role.rbac.authorization.k8s.io/create-services-and-pods unchanged; rolebinding.rbac.authorization.k8s.io/notebook-create-services-and-pods configured; role.rbac.authorization.k8s.io/read-get-user-secret unchanged; rolebinding.rbac.authorization.k8s.io/notebook-read-get-users-secret configured; ```. I think the error just reflects my not having hail-vdc-staging permissions, that is unaffected by this PR. cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5746:2845,config,config,2845,https://hail.is,https://github.com/hail-is/hail/pull/5746,3,['config'],"['config', 'configured']"
Modifiability,"eck, the busybox shell is annoying under featured. Can you add an environment variable instead, maybe KSSH_SHELL?. ```; kubectl -n ${2:-default} exec -it ""$(kfind1 ""$1"" ""$2"")"" ${3:+--container=""$3""} -- ${KSSH_SHELL:-/bin/sh}; ```; That way I can override it by default to give me a good shell.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10951#issuecomment-936649070:78,variab,variable,78,https://hail.is,https://github.com/hail-is/hail/pull/10951#issuecomment-936649070,1,['variab'],['variable']
Modifiability,"ecovery-cluster</code>: [<code>botocore</code>] This release adds a new API option to enable overriding safety rules to allow routing control state updates.</li>; <li>api-change:<code>amplifyuibuilder</code>: [<code>botocore</code>] We are adding the ability to configure workflows and actions for components.</li>; <li>api-change:<code>athena</code>: [<code>botocore</code>] This release adds support for updating an existing named query.</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] This release adds support for new AMI property 'lastLaunchedTime'</li>; <li>api-change:<code>servicecatalog-appregistry</code>: [<code>botocore</code>] AppRegistry is deprecating Application and Attribute-Group Name update feature. In this release, we are marking the name attributes for Update APIs as deprecated to give a heads up to our customers.</li>; </ul>; <h1>1.21.8</h1>; <ul>; <li>api-change:<code>elasticache</code>: [<code>botocore</code>] Doc only update for ElastiCache</li>; <li>api-change:<code>panorama</code>: [<code>botocore</code>] Added NTP server configuration parameter to ProvisionDevice operation. Added alternate software fields to DescribeDevice response</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/boto3/commit/261b0f2ffe079b6940d683657fcad358195f882e""><code>261b0f2</code></a> Merge branch 'release-1.21.12'</li>; <li><a href=""https://github.com/boto/boto3/commit/44f4f5ef0b66d1a508685b62388f1e4a7d60dace""><code>44f4f5e</code></a> Bumping version to 1.21.12</li>; <li><a href=""https://github.com/boto/boto3/commit/bb003d02bd7afefede0ab4678abaea99fe1662ce""><code>bb003d0</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/ad9a92e8fe3d5bc90b2980cdb6839c713b56fbda""><code>ad9a92e</code></a> Merge branch 'release-1.21.11'</li>; <li><a href=""https://github.com/boto/boto3/commit/42b3e0d3c1d02",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11486:4758,config,configuration,4758,https://hail.is,https://github.com/hail-is/hail/pull/11486,1,['config'],['configuration']
Modifiability,"ecrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 13m default-scheduler Successfully assigned batch-pods/batch-3-job-41-39d17b to gke-vdc-preemptible-pool-9c7148b2-1f89; Warning FailedCreatePodSandBox 13m kubelet, gke-vdc-preemptible-pool-9c7148b2-1f89 Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container ""99ac9edad98221dbfaf4ab8eb443bc6d3fdc6df84164594469900813652fd913"" network for pod ""batch-3-job-41-39d17b"": NetworkPlugin kubenet failed to set up pod ""batch-3-job-41-39d17b_batch-pods"" network: Error adding container to network: failed to set bridge addr: could not add IP address to ""cbr0"": file exists; ```. ```; $ kubectl -n batch-pods get pods -o yaml batch-3-job-41-39d17b; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-07-12T17:17:15Z""; labels:; app: batch-job; batch_id: ""3""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""41""; task: main; user: ci; uuid: f53f127847864f1cbf7d4bdc911a6646; name: batch-3-job-41-39d17b; namespace: batch-pods; resourceVersion: ""87247110""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-3-job-41-39d17b; uid: e4d87ac3-a4c8-11e9-a4bb-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6625:2283,sandbox,sandbox,2283,https://hail.is,https://github.com/hail-is/hail/issues/6625,2,['sandbox'],['sandbox']
Modifiability,"ect.github.com/aio-libs/aiohttp/issues/5704"">#5704</a>)</p>; </li>; <li>; <p>Added a middleware type alias <code>aiohttp.typedefs.Middleware</code>.</p>; <p>(<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/5898"">#5898</a>)</p>; </li>; <li>; <p>Exported <code>HTTPMove</code> which can be used to catch any redirection request; that has a location -- :user:<code>dreamsorcerer</code>.</p>; <p>(<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/6594"">#6594</a>)</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/blob/master/CHANGES.rst"">aiohttp's changelog</a>.</em></p>; <blockquote>; <h1>3.9.0 (2023-11-18)</h1>; <h2>Features</h2>; <ul>; <li>; <p>Introduced <code>AppKey</code> for static typing support of <code>Application</code> storage.; See <a href=""https://docs.aiohttp.org/en/stable/web_advanced.html#application-s-config"">https://docs.aiohttp.org/en/stable/web_advanced.html#application-s-config</a></p>; <p><code>[#5864](https://github.com/aio-libs/aiohttp/issues/5864) &lt;https://github.com/aio-libs/aiohttp/issues/5864&gt;</code>_</p>; </li>; <li>; <p>Added a graceful shutdown period which allows pending tasks to complete before the application's cleanup is called.; The period can be adjusted with the <code>shutdown_timeout</code> parameter. -- by :user:<code>Dreamsorcerer</code>.; See <a href=""https://docs.aiohttp.org/en/latest/web_advanced.html#graceful-shutdown"">https://docs.aiohttp.org/en/latest/web_advanced.html#graceful-shutdown</a></p>; <p><code>[#7188](https://github.com/aio-libs/aiohttp/issues/7188) &lt;https://github.com/aio-libs/aiohttp/issues/7188&gt;</code>_</p>; </li>; <li>; <p>Added <code>handler_cancellation &lt;https://docs.aiohttp.org/en/stable/web_advanced.html#web-handler-cancellation&gt;</code>_ parameter to cancel web handler on client disconnection. -- by :user:<code>mosqui",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14027:3689,config,config,3689,https://hail.is,https://github.com/hail-is/hail/pull/14027,6,['config'],['config']
Modifiability,ection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); 	at is.hail.asm4s.ClassesBytes.load(ClassBuilder.scala:62); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:715); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:708); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1(Compile.scala:311); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1$adapted(Compile.scala:310); 	at is.hail.expr.ir.lowering.TableStageToRVD$.$anonfun$apply$9(RVDToTableStage.scala:106); 	at is.hail.sparkextras.ContextRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1234); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1233); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:5008,adapt,adapted,5008,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['adapt'],['adapted']
Modifiability,"ed arguments. `-o`, which starts with a dash, is an option. This PR makes the following changes:. - For dataproc commands taking extra gcloud parameters, all parameters after a double-dash (--) are passed to gcloud.; - The actual rule is slightly more complicated, but I think the above rule is the right take away. In detail, extra parameters are passed to gcloud. Unknown options (starting with a dash) before `--` are reported as an error. So arguments (not options) before `--` and all parameters after are passed to gcloud. ; - Short options don't need a `=` when specifying a value. It is now `-p2`, not `-p=2`.; - While I was making breaking changes, I changed `dataproc submit` `--gcloud_configuration` to `--gcloud-configuration`. I am happy to undo this one.; - Group arguments must go before the next command. Write `hailctl dataproc --beta start ...` not `hailctl dataproc start --beta ...`, which is an error since `start` has no option `--beta`. This PR rewrites argument parsing to use click instead of argparse: https://click.palletsprojects.com/en/7.x/. Things you need to know about click:; - A group is a group of commands or subgroups, like `hailctl dataproc`, `hailctl batch`, etc. Groups are defined like this:; ; ```; @hailctl.group(; help=""Manage the Hail Batch service.""); def batch():; pass; ```; - A command in a group is defined like this:. ```; @batch.command(; help=""Get a particular batch's info.""); @click.argument('batch_id', type=int); @click.option('--output-format', '-o',; type=click.Choice(['yaml', 'json']),; default='yaml', show_default=True,; help=""Specify output format"",); def get(batch_id, output_format):; ...; ```. The command decorator replaces the function with one that takes a `List[str]` of command line parameters, parses them, and calls the original function. The option options are pretty self-explanatory. - To access an argument to a group (like `dataproc --beta`) in a (sub)command, use the decorator `click.pass_context` to pass the click cont",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842:1316,rewrite,rewrites,1316,https://hail.is,https://github.com/hail-is/hail/pull/9842,1,['rewrite'],['rewrites']
Modifiability,"ed the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:3299,config,config-http,3299,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['config'],['config-http']
Modifiability,ed.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.Trav,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:2249,Rewrite,RewriteBottomUp,2249,https://hail.is,https://github.com/hail-is/hail/issues/9128,2,['Rewrite'],['RewriteBottomUp']
Modifiability,ed.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.h,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:3729,Rewrite,RewriteBottomUp,3729,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,ed.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(Nativ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:5120,Rewrite,RewriteBottomUp,5120,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Rewrite'],['RewriteBottomUp']
Modifiability,ed.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$an,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:4469,Rewrite,RewriteBottomUp,4469,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"edDisk"": {; ""diskEncryptionSet"": null,; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/disks/batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5-os"",; ""resourceGroup"": ""dgoldste"",; ""storageAccountType"": ""Standard_LRS""; },; ""name"": ""batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5-os"",; ""osType"": ""Linux"",; ""vhd"": null,; ""writeAcceleratorEnabled"": null; }; },; ""tags"": {; ""batch-worker"": ""1"",; ""namespace"": ""pr-11144-default-nbthv8fduvd6""; },; ""type"": ""Microsoft.Compute/virtualMachines"",; ""userData"": null,; ""virtualMachineScaleSet"": null,; ""vmId"": ""4eaaa3a5-69ce-4eb0-ba8c-266bd66c821e"",; ""zones"": null; },; {; ""additionalCapabilities"": null,; ""applicationProfile"": null,; ""availabilitySet"": null,; ""billingProfile"": {; ""maxPrice"": -1.0; },; ""capacityReservation"": null,; ""diagnosticsProfile"": null,; ""evictionPolicy"": ""Delete"",; ""extendedLocation"": null,; ""extensionsTimeBudget"": null,; ""hardwareProfile"": {; ""vmSize"": ""Standard_E4d_v4"",; ""vmSizeProperties"": null; },; ""host"": null,; ""hostGroup"": null,; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/virtualMachines/batch-worker-pr-11144-default-nbthv8fduvd6-highmem-13t6m"",; ""identity"": {; ""principalId"": null,; ""tenantId"": null,; ""type"": ""UserAssigned"",; ""userAssignedIdentities"": {; ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.ManagedIdentity/userAssignedIdentities/batch-worker"": {; ""clientId"": ""890af904-42f1-4136-810a-c52f4e132c6b"",; ""principalId"": ""b952a3bb-1091-4f11-803b-9d5199219a27""; }; }; },; ""instanceView"": null,; ""licenseType"": null,; ""location"": ""eastus"",; ""name"": ""batch-worker-pr-11144-default-nbthv8fduvd6-highmem-13t6m"",; ""networkProfile"": {; ""networkApiVersion"": null,; ""networkInterfaceConfigurations"": null,; ""networkInterfaces"": [; {; ""deleteOption"": ""Delete"",; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resource",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686:5734,extend,extendedLocation,5734,https://hail.is,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686,1,['extend'],['extendedLocation']
Modifiability,"edirect.dependabot.com/kubernetes/kubernetes/pull/108129"">kubernetes/kubernetes#108129</a>, <a href=""https://github.com/ahg-g""><code>@​ahg-g</code></a>)</li>; <li>The AnyVolumeDataSource feature is now beta, and the feature gate is enabled by default. In order to provide user feedback on PVCs with data sources, deployers must install the VolumePopulators CRD and the data-source-validator controller. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108736"">kubernetes/kubernetes#108736</a>, <a href=""https://github.com/bswartz""><code>@​bswartz</code></a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/9ecc1143a8f7b34264b48dea12edd4d66230476f""><code>9ecc114</code></a> [chore] update version</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/ae5fd81a0fbea7d7fdac8d418876acaa288bcc0f""><code>ae5fd81</code></a> fix: config reader handles bool types (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/issues/218"">#218</a>)</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/7bbb327cee0d4ed908a5deb28aaf5a9ccc1f4602""><code>7bbb327</code></a> [chore] update version</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/4d52e04e03c0195eef581919f9efa4b6d0bd35ea""><code>4d52e04</code></a> [chore] update changelog</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/078dde081f8be6b190bbe9271d9f8d3839b617c7""><code>078dde0</code></a> fixed watch.stream bug of not working with apis with follow kwarg (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/issues/216"">#216</a>)</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/3ecb956c6af223b343ea5695d377982e98d4341c""><code>3ecb956</code></a> [chore] update version</li>; <li><a href=""https://github.com/tomplus/kubernetes_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12196:14342,config,config,14342,https://hail.is,https://github.com/hail-is/hail/pull/12196,1,['config'],['config']
Modifiability,"edirect.dependabot.com/samtools/htsjdk/issues/1621"">#1621</a>); 9fd0ecf21 Disable codecov until we can fix the uploader (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1622"">#1622</a>); 347c0ac57 Fix EdgeReadIterator (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1616"">#1616</a>); d15a5bacb Added ULTIMA and ELEMENT as valid value for RG-PL according to SAM spec. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1619"">#1619</a>)</p>; <h2>3.0.0</h2>; <p>Htsjdk 3.0.0: Revenge of the Simple Allele</p>; <p>This is the first htsjdk with a major version increase in a long time. We bumped it to indicate there are some breaking changes that will potentially require downstream code changes. Notably, <code>Allele</code> became an interface instead of a concrete class. <code>SimpleAllele</code> may be used as a replacement if you have classes which previously subclassed allele.</p>; <p>New Plugin Infrastructure:; 6a60de7c2 Move API marker annotations into new annotation package. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1558"">#1558</a>); 7ac95d5f7 Plugin framework and interfaces for versioned file format codecs (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1525"">#1525</a>); d40fe5412 Beta implementation of Bundles. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1546"">#1546</a>)</p>; <p>CRAM; 489c4192d Support CRAM reference regions. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1605"">#1605</a>); 22aec6782 Fix decoding of CRAM Scores read feature during normalization. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1592"">#1592</a>); 6507249a4 Make the CRAM MD5 failure message more user friendly. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1607"">#1607</a>); b5af659e6 Fix restoration of read base feature code. <a href=""https://github-redirect.dependabot.co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12229:1606,Plugin,Plugin,1606,https://hail.is,https://github.com/hail-is/hail/pull/12229,2,['Plugin'],['Plugin']
Modifiability,"edit: ah, I see, you removed the wait, great. Here is a possible alternative solution:. `kubectl -n blog wait --timeout=1h --for=condition=ready pods --selector=app=blog`. Tested with `pods --all`, `pods --selector=app=prometheus`. A difference that is worth being aware of: timeout will apply to each resource matched by the selector. So you'd need to check if the resource specified was a stateful set, and substitute a selector that they would need to pass in the config.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-546465650:467,config,config,467,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546465650,1,['config'],['config']
Modifiability,"een changed to `VEP_REPLICATE=australia-southeast1`, however the Australian bucket containing the VEP cache data is still `aus-sydney`, meaning that the VEP data is not copied into the dataproc cluster, and when trying to run VEP I get the error `No cache found for homo_sapiens, version 95`. ### Version. 0.2.130. ### Relevant log output. ```shell; FatalError: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:1656,Plugin,Plugins,1656,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,"ef=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1160"">#1160</a>)</li>; <li>Enable mypy in CI (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1101"">#1101</a>)</li>; <li>Synchronized reading the responses from a connection (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1106"">#1106</a>)</li>; </ul>; <h2>Fixes</h2>; <ul>; <li>Remove del from Redis (Fixes <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1115"">#1115</a>) (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1227"">#1227</a>)</li>; <li>fix socket.error raises (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1129"">#1129</a>)</li>; <li>Fix buffer is closed error when using PythonParser class (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1213"">#1213</a>)</li>; </ul>; <h2>Version v2.0.0</h2>; <p>Version 2.0 is a complete rewrite of aioredis. Starting with this version, aioredis now follows the API of <a href=""https://github.com/andymccurdy/redis-py"">redis-py</a>, so you can easily adapt synchronous code that uses redis-py for async applications with aioredis-py.</p>; <p><strong>NOTE:</strong> This version is <em>not</em> compatible with earlier versions of aioredis. If you upgrade, you will need to make code changes.</p>; <p>For more details, read our <a href=""https://aioredis.readthedocs.io/en/latest/migration/"">documentation on migrating to version 2.0</a>.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aioredis-py/blob/master/CHANGELOG.md"">aioredis's changelog</a>.</em></p>; <blockquote>; <h2>2.0.1 - (2021-12-20)</h2>; <h3>Features</h3>; <ul>; <li>Added Python 3.10 to CI &amp; Updated the Docs; (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1160"">#1160</a>)</li>; <li>Enable mypy in CI (",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11569:1377,rewrite,rewrite,1377,https://hail.is,https://github.com/hail-is/hail/pull/11569,1,['rewrite'],['rewrite']
Modifiability,"ef=""https://github.com/microsoft/debugpy/commit/6e247fb17bc15488a2cbefb7eed80c87179b147c""><code>6e247fb</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/1009"">#1009</a> from rchiodo/rchiodo/missed_thread_for_self_trace</li>; <li><a href=""https://github.com/microsoft/debugpy/commit/cccfb954bd7321fbb25f234f0c9a2e8372646297""><code>cccfb95</code></a> Missed a thread to allow debugging</li>; <li><a href=""https://github.com/microsoft/debugpy/commit/6c19aba462d2a9b6ea79d5cbce2886bd961823eb""><code>6c19aba</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/1007"">#1007</a> from rchiodo/rchiodo/allow_adapter_debugging</li>; <li><a href=""https://github.com/microsoft/debugpy/commit/10d8839a4cac4bdaf99cfe44ca2ee5479c95c003""><code>10d8839</code></a> Review feedback</li>; <li><a href=""https://github.com/microsoft/debugpy/commit/83ff28006d2f85cfbd03fcdc2650eea6831ab2b1""><code>83ff280</code></a> Add DEBUGPY_TRACE_DEBUGPY variable to allow debugpy to debug itself</li>; <li><a href=""https://github.com/microsoft/debugpy/commit/4f6638b0a6bbdec598987f7c2b62107a57edd6a6""><code>4f6638b</code></a> Fix <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/1001"">#1001</a>: Enable controlling shell expansion via &quot;argsCanBeInterpretedByShell&quot;</li>; <li><a href=""https://github.com/microsoft/debugpy/commit/6b276e339cd850c5f8c93ff4bdbd305dd963d7bb""><code>6b276e3</code></a> Step in/step over support for IPython. Fixes <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/869"">#869</a></li>; <li><a href=""https://github.com/microsoft/debugpy/commit/a294092d9c6d8459126ecb8f537b6012fb7e7d28""><code>a294092</code></a> Properly stop at line 1 in frame eval mode. Fixes <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/995"">#995</a></li>; <li>Additional commits viewable in <a href=""https://github.com/microsoft/debugpy/compare/v1.6.0..",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12103:4383,variab,variable,4383,https://hail.is,https://github.com/hail-is/hail/pull/12103,2,['variab'],['variable']
Modifiability,"ef=""https://redirect.github.com/fonttools/fonttools/issues/3006"">#3006</a>).</li>; <li>[featureVars] Fixed bug in <code>overlayBox</code> (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3003"">#3003</a>, <a href=""https://redirect.github.com/fonttools/fonttools/issues/3005"">#3005</a>).</li>; <li>[glyf] Added experimental support for cubic bezier curves in TrueType glyf table, as outlined in glyf v1 proposal (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2988"">#2988</a>):<br />; <a href=""https://github.com/harfbuzz/boring-expansion-spec/blob/main/glyf1-cubicOutlines.md"">https://github.com/harfbuzz/boring-expansion-spec/blob/main/glyf1-cubicOutlines.md</a></li>; <li>Added new qu2cu module and related qu2cuPen, the reverse of cu2qu for converting TrueType quadratic splines to cubic bezier curves (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2993"">#2993</a>).</li>; <li>[glyf] Added experimental support for reading and writing Variable Composites/Components as defined in glyf v1 spec proposal (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2958"">#2958</a>):<br />; <a href=""https://github.com/harfbuzz/boring-expansion-spec/blob/main/glyf1-varComposites.md"">https://github.com/harfbuzz/boring-expansion-spec/blob/main/glyf1-varComposites.md</a>.</li>; <li>[pens]: Added <code>addVarComponent</code> method to pen protocols' base classes, which pens can implement to handle varcomponents (by default they get decomposed).</li>; <li>[misc.transform] Added DecomposedTransform class which implements an affine transformation with separate translate, rotation, scale, skew, and transformation-center components (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2598"">#2598</a>)</li>; <li>[sbix] Ensure Glyph.referenceGlyphName is set; fixes error after dumping and re-compiling sbix table with 'dupe' glyphs (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2984"">#2984</a>).</li>; <li>[feaLib] ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12910:6175,Variab,Variable,6175,https://hail.is,https://github.com/hail-is/hail/pull/12910,1,['Variab'],['Variable']
Modifiability,"elt`; the following stream state will be `s1`. **Design Notes**. - The `Skip` return is very useful for simplifying the implementation of `ArrayFilter`. There is basically no nice way to implement filter otherwise without introducing some significant code duplication.; - ~~The stream ""parameter"", as well as the `Empty` return, are not very useful for the basic streams in this PR. However, they simplify the implementation of `ArrayFlatMap` (aka ""composing"" two parameterized streams).~~ NOTE (to Patrick): I decided to abandon the ""empty"" return idea in favor of just providing ""default states"" that always yield empty streams. **Implementation Notes**. - The implementation makes great use of Scala's type system. Most of the streams are implemented first in a very type aware manner, where it is easy to reason about the types of data flowing in and out, before being instantiated with EmitTriplets and Envs which don't hold very much type information. For instance, we have the following helper for `map`:; ```scala; Parameterized[P, A].map(f: A => B): Parameterized[P, B]; ```; The emitter instantiates P = `Any`, A = `EmitTriplet`, B = `EmitTriplet` :/. - Complex streams will have non trivial control flow and jumps. Therefore `init` and `step` both take `JoinPointBuilder`s and return `Code[Ctrl]`, to indicate that they may create join points and do jumps. - I've utilized a cute continuation passing style trick in multiple functions. Instead of `init` ""returning"" `Missing`/`Start(s0)` (which is impossible, since these are compile time data structures; they won't exist during JVM runtime), init takes a continuation `k: Init[S] => Code[Ctrl]`, which it must call with one of these values. To use `init`, you can simply provide a pattern matching lambda, which closely resembles the syntax you would normally use to pattern match on a returned value, e.g. (from `contMap`):; ```scala; self.init(mb, jb, param) {; case Missing => k(Missing); case Start(s) => Code(setup, k(Start(s))); };",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7228:2626,Parameteriz,Parameterized,2626,https://hail.is,https://github.com/hail-is/hail/pull/7228,1,['Parameteriz'],['Parameterized']
Modifiability,"em)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:3352,config,configuration,3352,https://hail.is,https://github.com/hail-is/hail/pull/8513,2,['config'],"['configuration', 'configure']"
Modifiability,"emoving batch workers' reliance on the docker daemon and docker in general, in favor of a lower level of abstraction that gives us finer control over resources on the worker like overlays and network namespaces, allowing us to shortcut and pre-configure some of the overhead that goes into running a job. ## What this does differently; Currently, the high-level process for running a job involves communicating with the docker daemon to:; 1. Pull an image for a job; 2. Start a container from that image; 3. Run the container; 4. Delete the container and its associated resources. We offload some of these responsibilities into the worker code and onto [crun](https://github.com/containers/crun), a lightweight low-level runtime with the same API as `runc`, what docker uses to run containers. Once docker has retrieved an image, if we see that the pulled image has a new digest from one we currently have cached on the worker, we extract the image's filesystem into a directory on the worker's disk. We then:. - use `mount` to create an overlay on top of the image that the container will use as its rootfs; - use `xfs_quota` to limit the container's storage in the overlay; - invoke `crun` to run a container with the overlay as its root filesystem and an appropriate network namespace that we set up at worker-start time. Since we control the overlay, we can set the XFS quota before creating the container. So what was separate calls to docker create/start/run/delete is just a single `crun run`. Fewer steps, less back and forth with a single daemon, and pre-configuring the networks gives some sizable performance gains reliable, as well as reliable and consistent performance. ## What this doesn't solve; - Docker is still running the worker container. I don't see any real challenge to this it's just a matter of translating the docker parameters; - Still using docker to pull images and extract filesystems / environment variables from them. I don't have a substitute for this at the moment.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10376:1591,config,configuring,1591,https://hail.is,https://github.com/hail-is/hail/pull/10376,2,"['config', 'variab']","['configuring', 'variables']"
Modifiability,"en deprecated for several years.; :meth:<code>~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey.public_bytes</code>; and; :meth:<code>~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey.from_encoded_point</code>; should be used instead.</li>; <li><strong>BACKWARDS INCOMPATIBLE:</strong> Support for using MD5 or SHA1 in; :class:<code>~cryptography.x509.CertificateBuilder</code>, other X.509 builders, and; PKCS7 has been removed.</li>; <li><strong>BACKWARDS INCOMPATIBLE:</strong> Dropped support for macOS 10.10 and 10.11, macOS; users must upgrade to 10.12 or newer.</li>; <li><strong>ANNOUNCEMENT:</strong> The next version of <code>cryptography</code> (40.0) will change; the way we link OpenSSL. This will only impact users who build; <code>cryptography</code> from source (i.e., not from a <code>wheel</code>), and specify their; own version of OpenSSL. For those users, the <code>CFLAGS</code>, <code>LDFLAGS</code>,; <code>INCLUDE</code>, <code>LIB</code>, and <code>CRYPTOGRAPHY_SUPPRESS_LINK_FLAGS</code> environment; variables will no longer be respected. Instead, users will need to; configure their builds <code>as documented here</code>_.</li>; <li>Added support for; :ref:<code>disabling the legacy provider in OpenSSL 3.0.x&lt;legacy-provider&gt;</code>.</li>; <li>Added support for disabling RSA key validation checks when loading RSA; keys via; :func:<code>~cryptography.hazmat.primitives.serialization.load_pem_private_key</code>,; :func:<code>~cryptography.hazmat.primitives.serialization.load_der_private_key</code>,; and; :meth:<code>~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateNumbers.private_key</code>.; This speeds up key loading but is :term:<code>unsafe</code> if you are loading potentially; attacker supplied keys.</li>; <li>Significantly improved performance for; :class:<code>~cryptography.hazmat.primitives.ciphers.aead.ChaCha20Poly1305</code></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12668:2354,variab,variables,2354,https://hail.is,https://github.com/hail-is/hail/pull/12668,4,['variab'],['variables']
Modifiability,"endabot.com/sveltejs/svelte/pull/7297"">#7297</a>)</li>; <li>Fix value of <code>let:</code> bindings not updating in certain cases (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7440"">#7440</a>)</li>; <li>Fix handling of void tags in <code>&lt;svelte:element&gt;</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7449"">#7449</a>)</li>; <li>Fix handling of boolean attributes in <code>&lt;svelte:element&gt;</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7478"">#7478</a>)</li>; <li>Add special style scoping handling of <code>[open]</code> selectors on <code>&lt;dialog&gt;</code> elements (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7494"">#7495</a>)</li>; </ul>; <h2>3.47.0</h2>; <ul>; <li>Add support for dynamic elements through <code>&lt;svelte:element&gt;</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/2324"">#2324</a>)</li>; <li>Miscellaneous variable context fixes in <code>{@const}</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/pull/7222"">#7222</a>)</li>; <li>Fix <code>{#key}</code> block not being reactive when the key variable is not otherwise used (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7408"">#7408</a>)</li>; <li>Add <code>Symbol</code> as a known global (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7418"">#7418</a>)</li>; </ul>; <h2>3.46.6</h2>; <ul>; <li>Actually include action TypeScript interface in published package (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/pull/7407"">#7407</a>)</li>; </ul>; <h2>3.46.5</h2>; <ul>; <li>Add TypeScript interfaces for typing actions (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/6538"">#6538</a>)</li>; <li>Do not generate <code>unused-export-let</code> warning inside <code>&lt;script context=&quot;module&quot;&gt;</code> blocks (<a href=""https://github-redirect.depe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12032:4082,variab,variable,4082,https://hail.is,https://github.com/hail-is/hail/pull/12032,3,['variab'],['variable']
Modifiability,"endabot.com/tqdm/tqdm/issues/1246"">#1246</a>)</li>; <li>misc tidying &amp; refactoring</li>; <li>misc build/dev framework updates; <ul>; <li>update dependencies</li>; <li>update linters</li>; <li>update docs deployment branches</li>; </ul>; </li>; <li>misc test/ci updates; <ul>; <li>test forks</li>; <li>tidy OS &amp; Python version tests</li>; <li>bump primary python version 3.7 =&gt; 3.8</li>; <li>beta py3.10 testing</li>; <li>fix py2.7 tests</li>; <li>better timeout handling</li>; </ul>; </li>; </ul>; <h2>tqdm v4.62.2 stable</h2>; <ul>; <li>fix notebook memory leak (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1216"">#1216</a>)</li>; <li>fix <code>contrib.concurrent</code> with generators (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1233"">#1233</a> &lt;- <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1231"">#1231</a>)</li>; </ul>; <h2>tqdm v4.62.1 stable</h2>; <ul>; <li><code>contrib.logging</code>: inherit existing handler output stream (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1191"">#1191</a>)</li>; <li>fix <code>PermissionError</code> by using <code>weakref</code> in <code>DisableOnWriteError</code> (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1207"">#1207</a>)</li>; <li>fix <code>contrib.telegram</code> creation rate limit handling (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1223"">#1223</a>, <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1221"">#1221</a> &lt;- <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1220"">#1220</a>, <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1076"">#1076</a>)</li>; <li>tests: fix py27 <code>keras</code> dependencies (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1222"">#1222</a>)</li>; <li>misc tidy: use relative imports (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1222"">#1222</a>)</li>; <li>minor documentation updates (<a ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11587:2384,inherit,inherit,2384,https://hail.is,https://github.com/hail-is/hail/pull/11587,1,['inherit'],['inherit']
Modifiability,"eneric index operations (not sure what to call these). I'll first give some examples using informal notation, then I'll give a proposal for an IR representation. I'll say ""sum"" everywhere, but that could be any aggregation. Let T be a 1-tensor. O represents the output of the operation. Then. * `T(i) -> O()` is the sum of T. * `T(i) -> O(i)` is identity. * `T(i) -> O(i, i)` makes a square matrix whose diagonal is T. * `T(i) -> O(i, j)` and `T(i) -> O(j, i)` broadcast T over a matrix in the two possible directions. Now let T be a 2-tensor. * `T(i, j) -> O(i)` is the vector of row-sums of T. * `T(i, i) -> O(i)` is the diagonal of T. * `T(i, i) -> O()` is the trace of T. * `T(i, j) -> O(j, i)` is transposition. How do we represent matrix multiplication? Let T1 and T2 be 2-tensors. Then letting `T = Out(T1, T2, ""and"").map((x, y) => x * y)`, the matrix product is given by. * `T(i, j, j, k) -> O(i, k)`. In general, an index operation on T requires specifying an output tensor O (including its shape, though you can deduce that in non-broadcasting cases), a set of index variables (eg. ""i, j, k""), and an assignment of a variable to each dimension of T and O. . More abstractly, let DT and DO be the sets of dimensions of T and O. An index operation consists of a set I and two functions DT -> I <- DO, a ""cospan"". These operations compose by cospan composition, which involves a pushout (a disjoint union and a quotient). Let i: DT -> I and o: DO -> I be the two maps assigning index variables. It helps to consider some special cases (compare to the examples above):. * If i is surjective, and o is identity, this is extracting a diagonal from T. * If i is injective, and o is identity, this is a broadcast. * If i is identity, and o is surjective, this embeds T as a diagonal of a higher-dimensional output tensor. * If i is identity, and o is injective, this is a pure aggregation, summing out some dimensions of T. To make composition easy to compute, we could represent an index operation",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772:2995,variab,variables,2995,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772,2,['variab'],['variables']
Modifiability,"ensure we always return to the original working directory. I also fixed the path. - Use pytest [recommended directory structure](https://docs.pytest.org/en/latest/goodpractices.html) when your tests directory is a python module [1]. In particular, we now use:. ```; python/; - setup.py; - src/; - hail/; - __init__.py; - ...; - tests/; - __init__.py; - ...; ```. - Calculate number of cores using python's multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5130:1750,config,configuration,1750,https://hail.is,https://github.com/hail-is/hail/pull/5130,1,['config'],['configuration']
Modifiability,"er --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:2107,Plugin,Plugins,2107,https://hail.is,https://github.com/hail-is/hail/issues/14513,4,['Plugin'],['Plugins']
Modifiability,"er details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/. Using Python version 3.9.18 (main, Oct 25 2023 05:26:35); Spark context Web UI available at http://ip-192-168-125-39.ap-southeast-1.compute.internal:4040; Spark context available as 'sc' (master = yarn, app id = application_1698211907929_0001).; SparkSession available as 'spark'.; >>> import hail as hl; >>> hl.version(); '0.2.124-e739a95489e4'; hl.init(sc); pip-installed Hail requires additional configuration options in Spark referring; to the path to the Hail Python module directory HAIL_DIR,; e.g. /path/to/python/site-packages/hail:; spark.jars=HAIL_DIR/backend/hail-all-spark.jar; spark.driver.extraClassPath=HAIL_DIR/backend/hail-all-spark.jar; spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 3.3.2-amzn-0.1; SparkUI available at http://ip-192-168-110-167.ap-southeast-1.compute.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.124-e739a95489e4; LOGGING: writing to /mnt/tmp/hail/hail/hail-20231025-0729-0.2.124-e739a95489e4.log; >>> mt = hl.balding_nichols_model(n_populations=3, n_samples=500, n_variants=1_000); 2023-10-25 07:29:48.283 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 500 samples, and 1000 variants...; >>> mt.count(); (1000, 500); ```. it seems working in command line using pyspark !. I need to test on jupyter notebook now... FYI the pyspark configs. ```sh ; - Classification: spa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949:2373,config,configuration,2373,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949,1,['config'],['configuration']
Modifiability,"er, actually it's impossible to generate these nodes:. ```; final case class Literal(_typ: Type, value: Annotation) extends IR {; require(!CanEmit(_typ)); require(value != null); }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5141#issuecomment-454422240:116,extend,extends,116,https://hail.is,https://github.com/hail-is/hail/pull/5141#issuecomment-454422240,1,['extend'],['extends']
Modifiability,"er.ServerImpl$Dispatcher.handle(ServerImpl.java:560); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:16017,Plugin,Plugins,16017,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,er.scala:8); at is.hail.expr.ir.lowering.LowerBlockMatrixIR$.apply(LowerBlockMatrixIR.scala:1067); at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:33); at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:11); at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:91); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29); at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.apply(LoweringPass.scala:86); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); at is.hail.backend.service.ServiceBackend.execute(ServiceBackend.scala:345); at is.hail.backend.service.ServiceBackend.execute(ServiceBackend.scala:389); at is.hail.backend.service.ServiceBackendAPI.$anonfun$doAction$3(ServiceBackend.scala:610); at is.hail.backend.service.ServiceBackendAPI.withIRFunctionsReadFromInput(ServiceBackend.scala:655); at is.hail.backend.service.ServiceBackendAPI.$anonfun$doAction$2(ServiceBackend.scala:609); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); at is.hail.utils.package$.using(package.scala:664); at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78); at is.hail.uti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14537:9040,adapt,adapted,9040,https://hail.is,https://github.com/hail-is/hail/issues/14537,1,['adapt'],['adapted']
Modifiability,er://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'WHEEL_FOR_AZURE is unse,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:4614,variab,variables,4614,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['variab'],['variables']
Modifiability,erEventProcessLoop.onReceive(DAGScheduler.scala:2792); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2257); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2289); 	at is.hail.sparkextras.ContextRDD.crunJobWithIndex(ContextRDD.scala:238); 	at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1029); 	at is.hail.rvd.RVD$.makeCoercer(RVD.scala:1104); 	at is.hail.rvd.RVD$.coerce(RVD.scala:1060); 	at is.hail.rvd.RVD.changeKey(RVD.scala:142); 	at is.hail.rvd.RVD.changeKey(RVD.scala:135); 	at is.hail.backend.spark.SparkBackend.lowerDistributedSort(SparkBackend.scala:716); 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:143); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:17); 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:14); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:167); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringP,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:12044,Rewrite,RewriteBottomUp,12044,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Rewrite'],['RewriteBottomUp']
Modifiability,erOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:45); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:600); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:636); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:631); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:630); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:65); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:407); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:393); 	at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:630); 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:88); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpser,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:13488,adapt,adapted,13488,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['adapt'],['adapted']
Modifiability,erfile/dockerfile.js; /opt/conda/miniconda3/pkgs/notebook-6.2.0-py38h578d9bd_0/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/pkgs/notebook-6.2.0-py38h578d9bd_0/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/conda/miniconda3/lib/python3.8/site-packages/nbclassic/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/lib/python3.8/site-packages/nbclassic/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/conda/miniconda3/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/google-fluentd/embedded/lib/ruby/gems/2.7.0/gems/fluent-plugin-kubernetes_metadata_filter-2.5.2/test/cassettes/kubernetes_docker_metadata_dotted_labels.yml; /opt/google-fluentd/embedded/lib/ruby/gems/2.7.0/gems/fluent-plugin-kubernetes_metadata_filter-2.5.2/test/cassettes/kubernetes_docker_metadata_annotations.yml; /usr/share/man/man1/gcloud_artifacts_docker_images_scan.1.gz; /usr/share/man/man1/gcloud_artifacts_docker_images_list-vulnerabilities.1.gz; /usr/share/man/man1/gcloud_beta_artifacts_docker_images_describe.1.gz; /usr/share/man/man1/gcloud_beta_artifacts_docker_images_scan.1.gz; /usr/share/man/man1/gcloud_alpha_auth_configure-docker.1.gz; /usr/share/man/man1/gcloud_beta_artifacts_docker_images.1.gz; /usr/share/man/man1/gcloud_beta_artifacts_docker_images_list.1.gz; /usr/share/man/man1/gcloud_artifacts_docker_images_delete.1.gz; /usr/share/man/man1/gcloud_beta_artifacts_docker_images_delete.1.gz; /usr/share/man/man1/gcloud_alpha_artifacts_docker_images.1.gz; /usr/share/man/man1/gcloud_beta_auth_configure-docker.1.gz; /usr/share/man/man1/gcloud_artifacts_docker_tags_list.1.gz; /usr/share/man/man1/gcloud_beta_artifacts_docker_tags_list.1.gz; /usr/share/man/man1/gcloud_alpha_artifacts_docker.1.gz; /usr/sh,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:2233,plugin,plugin-,2233,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['plugin'],['plugin-']
Modifiability,"erring; to the path to the Hail Python module directory HAIL_DIR,; e.g. /path/to/python/site-packages/hail:; spark.jars=HAIL_DIR/backend/hail-all-spark.jar; spark.driver.extraClassPath=HAIL_DIR/backend/hail-all-spark.jar; spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 3.3.2-amzn-0.1; SparkUI available at http://ip-192-168-110-167.ap-southeast-1.compute.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.124-e739a95489e4; LOGGING: writing to /mnt/tmp/hail/hail/hail-20231025-0729-0.2.124-e739a95489e4.log; >>> mt = hl.balding_nichols_model(n_populations=3, n_samples=500, n_variants=1_000); 2023-10-25 07:29:48.283 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 500 samples, and 1000 variants...; >>> mt.count(); (1000, 500); ```. it seems working in command line using pyspark !. I need to test on jupyter notebook now... FYI the pyspark configs. ```sh ; - Classification: spark-defaults; ConfigurationProperties:; spark.jars: /opt/hail/backend/hail-all-spark.jar; spark.driver.extraClassPath: /opt/hail/backend/hail-all-spark.jar:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar; spark.executor.extraClassPath: /opt/hail/backend/hail-all-spark.jar:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949:3413,Config,ConfigurationProperties,3413,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949,1,['Config'],['ConfigurationProperties']
Modifiability,ers.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileA,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:4598,Rewrite,RewriteBottomUp,4598,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"es added</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10260"">#10260</a>: Enable <code>FORCE_COLOR</code> and <code>NO_COLOR</code> for terminal colouring</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10234"">#10234</a>: autosummary: Add &quot;autosummary&quot; CSS class to summary tables</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10125"">#10125</a>: extlinks: Improve suggestion message for a reference having title</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10112"">#10112</a>: extlinks: Add :confval:<code>extlinks_detect_hardcoded_links</code> to enable; hardcoded links detector feature</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9494"">#9494</a>, <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9456"">#9456</a>: html search: Add a config variable; :confval:<code>html_show_search_summary</code> to enable/disable the search summaries</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9337"">#9337</a>: HTML theme, add option <code>enable_search_shortcuts</code> that enables :kbd:'/' as; a Quick search shortcut and :kbd:<code>Esc</code> shortcut that; removes search highlighting.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10107"">#10107</a>: i18n: Allow to suppress translation warnings by adding <code>#noqa</code>; comment to the tail of each translation message</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10252"">#10252</a>: C++, support attributes on classes, unions, and enums.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10253"">#10253</a>: :rst:dir:<code>pep</code> role now generates URLs based on peps.python.org</li>; </ul>; <h2>Bugs fixed</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11714:2408,config,config,2408,https://hail.is,https://github.com/hail-is/hail/pull/11714,4,"['config', 'variab']","['config', 'variable']"
Modifiability,"es using multiple keys and logical operators.</li>; <li>api-change:<code>lakeformation</code>: [<code>botocore</code>] This release adds a new parameter &quot;Parameters&quot; in the DataLakeSettings.</li>; <li>api-change:<code>managedblockchain</code>: [<code>botocore</code>] Updating the API docs data type: NetworkEthereumAttributes, and the operations DeleteNode, and CreateNode to also include the supported Goerli network.</li>; <li>api-change:<code>proton</code>: [<code>botocore</code>] Add support for CodeBuild Provisioning</li>; <li>api-change:<code>rds</code>: [<code>botocore</code>] This release adds support for restoring an RDS Multi-AZ DB cluster snapshot to a Single-AZ deployment or a Multi-AZ DB instance deployment.</li>; <li>api-change:<code>workdocs</code>: [<code>botocore</code>] Added 2 new document related operations, DeleteDocumentVersion and RestoreDocumentVersions.</li>; <li>api-change:<code>xray</code>: [<code>botocore</code>] This release enhances GetServiceGraph API to support new type of edge to represent links between SQS and Lambda in event-driven applications.</li>; </ul>; <h1>1.26.8</h1>; <ul>; <li>api-change:<code>glue</code>: [<code>botocore</code>] Added links related to enabling job bookmarks.</li>; <li>api-change:<code>iot</code>: [<code>botocore</code>] This release add new api listRelatedResourcesForAuditFinding and new member type IssuerCertificates for Iot device device defender Audit.</li>; <li>api-change:<code>license-manager</code>: [<code>botocore</code>] AWS License Manager now supports onboarded Management Accounts or Delegated Admins to view granted licenses aggregated from all accounts in the organization.</li>; <li>api-change:<code>marketplace-catalog</code>: [<code>botocore</code>] Added three new APIs to support tagging and tag-based authorization: TagResource, UntagResource, and ListTagsForResource. Added optional parameters to the StartChangeSet API to support tagging a resource while making a request to create it.</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12466:1405,enhance,enhances,1405,https://hail.is,https://github.com/hail-is/hail/pull/12466,1,['enhance'],['enhances']
Modifiability,"es who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site mak",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:6546,config,config,6546,https://hail.is,https://github.com/hail-is/hail/pull/8561,1,['config'],['config']
Modifiability,essentially you need to add the following as configuration in Spark:. ```; HAIL_JAR_LOCATION=/path/to/python/site-packages/hail/hail-all-spark.jar; spark.jars=${HAIL_JAR_LOCATION}; spark.driver.extraClassPath=${HAIL_JAR_LOCATION}; spark.executor.extraClassPath=./hail-all-spark.jar; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7080#issuecomment-536711994:45,config,configuration,45,https://hail.is,https://github.com/hail-is/hail/issues/7080#issuecomment-536711994,1,['config'],['configuration']
Modifiability,"et the error `No cache found for homo_sapiens, version 95`. ### Version. 0.2.130. ### Relevant log output. ```shell; FatalError: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:1887,Plugin,Plugins,1887,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,"et.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:544); at sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:509); at java.lang.Thread.run(Thread.java:750); ```. Interesting thing is, when I tried to convert the exactly same data in local computer using singularity instead of docker, it worked. Also, for the the other chromosomes with less variants but same samples, such as chr13, it worked well in Terra. Since we will convert multiple plink files to hailmatrix table using Terra platform in future, I need to figure the problem out. Any advise would be appreciated. ### Version. 0.2.127. ### Relevant log output. ```shell; 2024/01/17 20:20:25 Starting container setup.; 2024/01/17 20:20:26 Done container setup.; 2024/01/17 20:20:27 Starting localization.; 2024/01/17 20:20:34 Localization script execution started...; 2024/01/17 20:20:34 Localizing input gs://fc-5a8938eb-1299-4afc-957f-afb53ef602b9/submissions/e8747e74-47d1-4f52-acfc-1ac7f81d79ba/VUMCBed2HailMatrix/683447d9-9342-4058-bcfc-ba21422d3121/call-Bed2HailMatrix/script -> /cromwell_root/script; 2024/01/17 20:20:36 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bed -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bed; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.fam -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.fam; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bim -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bim; Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.fam...; / [0 files][ 0.0 B/910.3 KiB] / [1 files][910.3 KiB/910.3 KiB] Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.bim...; / [1 files][910.3 KiB/369.7 MiB] - - [1 files][ 51.9 MiB/369.7 MiB] \ | | [1 files][107.6 MiB/369.7 MiB] / - - [1 files][162.3 MiB/369.7 MiB] \ \ [1 files][213.9 MiB/369.7 MiB] | / / [1 files][286.6 MiB/369.7 MiB] - \ \ [1 files][342.1 MiB/369.7 MiB] |; Operation completed over 2 objects/369.7 MiB.; | [2 files][369.7 MiB/369.7 MiB] 2024/01",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:9648,sandbox,sandbox,9648,https://hail.is,https://github.com/hail-is/hail/issues/14168,1,['sandbox'],['sandbox']
Modifiability,"et.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:16095,Plugin,Plugins,16095,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,et.scala:425); E at is.hail.variant.VariantDatasetFunctions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:7636,Config,Configuration,7636,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['Config'],['Configuration']
Modifiability,"etails>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/java-storage/blob/main/CHANGELOG.md"">com.google.cloud:google-cloud-storage's changelog</a>.</em></p>; <blockquote>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.26.0...v2.26.1"">2.26.1</a> (2023-08-14)</h2>; <h3>Bug Fixes</h3>; <ul>; <li>Make use of ImmutableMap.Builder#buildOrThrow graceful (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2159"">#2159</a>) (<a href=""https://github.com/googleapis/java-storage/commit/e9746f856e9204c1c0ec62f19e6f71ff8a0b9750"">e9746f8</a>)</li>; <li>Update gRPC writeAndClose to only set finish_write on the last message (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2163"">#2163</a>) (<a href=""https://github.com/googleapis/java-storage/commit/95df758d6753005226556177e68a3e9c630c789b"">95df758</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.24 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2158"">#2158</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4f5682a4f6d6d5372a2d382ae3e47dace490ca0d"">4f5682a</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.25.0...v2.26.0"">2.26.0</a> (2023-08-03)</h2>; <h3>Features</h3>; <ul>; <li>Implement BufferToDiskThenUpload BlobWriteSessionConfig (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2139"">#2139</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4dad2d5c3a81eda7190ad4f95316471e7fa30f66"">4dad2d5</a>)</li>; <li>Introduce new BlobWriteSession (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2123"">#2123</a>) (<a href=""https://github.com/googleapis/java-storage/commit/e0191b518e50a49fae0691894b50f0c5f33fc6af"">e0191b5</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li><strong>grpc:</strong> Return error if credentials are detected to be null (<a ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13605:5598,plugin,plugin,5598,https://hail.is,https://github.com/hail-is/hail/pull/13605,1,['plugin'],['plugin']
Modifiability,"eted` as it; has very low cardinality. In some forms of this query, the planner tries to use; it to its peril. A problem in query 0 with #14629 (see below) was that fewer filters on batches; made the optimiser consider joins in a suboptimal order - it did a table scan ; on `job_groups` first then sorted the results by to `batches.id DESC` instead; of doing an index scan on `batches` in reverse. Using `STRAIGHT_JOIN`s instead of `INNER JOIN` mades the optimiser start from; `batches` and read its index in reverse before considering other tables in ; subsequent joins. From the [documentation](https://dev.mysql.com/doc/refman/8.4/en/join.html):. > STRAIGHT_JOIN is similar to JOIN, except that the left table is always read; before the right table. This can be used for those (few) cases for which the; join optimizer processes the tables in a suboptimal order. This is advantageous for a couple of reasons:; - We want to list newer batches first; - For this query, the `batches` table has more applicables indexes; - We want the variable to order by to be in the primary key of the first; table so we can read the index in reverse. Before and after timings, collected by running the query 5 times, then using; profiles gathered by MySQL.; ```; +-------+---------------------------------------------------*; | query | description | ; +-------+---------------------------------------------------+; | 0 | All batches accessible to user `ci` |; | 1 | All batches accessible to user `ci` owned by `ci` |; +-------+---------------------------------------------------*. +-------+--------+--------------------------------------------------------+------------+------------+; | query | branch | timings | mean | stdev | ; +-------+--------+--------------------------------------------------------+------------+------------+; | 0 | main | 0.05894400,0.05207850,0.07067875,0.06281800,0.060250 | 0.06095385 | 0.00602255 |; | 1 | main | 14.1106150,12.2619323,13.8442850,12.0749633,14.0297822 | 13.2643156 | 0.9",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14649:1319,variab,variable,1319,https://hail.is,https://github.com/hail-is/hail/pull/14649,1,['variab'],['variable']
Modifiability,"eters have been removed.; <code>download_name</code> replaces <code>attachment_filename</code>, <code>max_age</code>; replaces <code>cache_timeout</code>, and <code>etag</code> replaces <code>add_etags</code>.; Additionally, <code>path</code> replaces <code>filename</code> in; <code>send_from_directory</code>.</li>; <li>The <code>RequestContext.g</code> property returning <code>AppContext.g</code> is; removed.</li>; </ul>; </li>; <li>; <p>Update Werkzeug dependency to &gt;= 2.2.</p>; </li>; <li>; <p>The app and request contexts are managed using Python context vars; directly rather than Werkzeug's <code>LocalStack</code>. This should result; in better performance and memory use. :pr:<code>4682</code></p>; <ul>; <li>Extension maintainers, be aware that <code>_app_ctx_stack.top</code>; and <code>_request_ctx_stack.top</code> are deprecated. Store data on; <code>g</code> instead using a unique prefix, like; <code>g._extension_name_attr</code>.</li>; </ul>; </li>; <li>; <p>The <code>FLASK_ENV</code> environment variable and <code>app.env</code> attribute are; deprecated, removing the distinction between development and debug; mode. Debug mode should be controlled directly using the <code>--debug</code>; option or <code>app.run(debug=True)</code>. :issue:<code>4714</code></p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/flask/commit/a1c478bc93d3dc018a6e7a1ba3cf5409553c9df3""><code>a1c478b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/flask/issues/4755"">#4755</a> from pallets/release-2.2.2</li>; <li><a href=""https://github.com/pallets/flask/commit/43d2fff317aec64a000604a764b8ab2dc751c753""><code>43d2fff</code></a> release version 2.2.2</li>; <li><a href=""https://github.com/pallets/flask/commit/e9af7c23ae19fcc50781b7711f4672c113636892""><code>e9af7c2</code></a> Merge pull request <a href=""https://github-re",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12206:6969,variab,variable,6969,https://hail.is,https://github.com/hail-is/hail/pull/12206,1,['variab'],['variable']
Modifiability,"evan); Fixed: GITHUB-893: TestNG should provide an Api which allow to find all dependent of a specific test (Krishnan Mahadevan); New: Added .yml file extension for yaml suite files, previously only .yaml was allowed for yaml (Steven Jubb); Fixed: GITHUB-141: regular expression in &quot;dependsOnMethods&quot; does not work (Krishnan Mahadevan); Fixed: GITHUB-2770: FileAlreadyExistsException when report is generated (melloware); Fixed: GITHUB-2825: Programmatically Loading TestNG Suite from JAR File Fails to Delete Temporary Copy of Suite File (Steven Jubb); Fixed: GITHUB-2818: Add configuration key for callback discrepancy behavior (Krishnan Mahadevan); Fixed: GITHUB-2819: Ability to retry a data provider in case of failures (Krishnan Mahadevan); Fixed: GITHUB-2308: StringIndexOutOfBoundsException in findClassesInPackage - Surefire/Maven - JDK 11 fails (Krishnan Mahadevan); Fixed: GITHUB:2788: TestResult.isSuccess() is TRUE when test fails due to expectedExceptions (Krishnan Mahadevan); Fixed: GITHUB-2800: Running Test Classes with Inherited <a href=""https://github.com/Factory""><code>@​Factory</code></a> and <a href=""https://github.com/DataProvider""><code>@​DataProvider</code></a> Annotated Non-Static Methods Fail (Krishnan Mahadevan); New: Ability to provide custom error message for assertThrows\expectThrows methods (Anatolii Yuzhakov); Fixed: GITHUB-2780: Use SpotBugs instead of abandoned FindBugs; Fixed: GITHUB-2801: JUnitReportReporter is too slow; Fixed: GITHUB-2807: buildStackTrace should be fail-safe (Sergey Chernov); Fixed: GITHUB-2830: TestHTMLReporter parameter toString should be fail-safe (Sergey Chernov); Fixed: GITHUB-2798: Parallel executions coupled with retry analyzer results in duplicate retry analyzer instances being created (Krishnan Mahadevan)</p>; <p>7.6.1; Fixed: GITHUB-2761: Exception: ERROR java.nio.file.NoSuchFileException: /tmp/testngXmlPathInJar-15086412835569336174 (Krishnan Mahadevan); 7.6.0; Fixed: GITHUB-2741: Show fully qualified name",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:11567,Inherit,Inherited,11567,https://hail.is,https://github.com/hail-is/hail/pull/12665,1,['Inherit'],['Inherited']
Modifiability,"eve breaks the reading process which expects the number of bytes to correspond to the size of the tree)~ Moved to #3750. - ~added `IndexBTree2` which is just an in-memory list of the variant start positions. This is a fair bit of data. Chromosome 1 has about 250 million bases, so in the worst case this is 250 * 8 million bytes = 2 GB. It occurs to me that this is actually way to much data to load on the master node in general (since I just try to open the indexes for every file). I should switch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with fatals, so as not to allocate strings~ Moved to #3771. - ~We no longer copy the genotype data into a buffer in the block reader. This was forcing the `fastKeys` to do an unnecessary data copy~ Moved to #3783 (with some substantial refactoring so it doesn't look much like this PR anymore). - ~I changed the contract on BgenRecord to require that `getValue` is called to ""consume"" the record before the next record is taken~ Irrelevant thanks to #3783 's refactoring. - ~`getValue(null)` just skips bytes (no copy, no decompression)~ Irrelevant thanks to #3783 's refactoring. - ~I added `RegionValueBuilder.unsafeAdvance` which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work.~ Moved to #3773. - ~I use `RegionValueBuilder.unsafeAdvance` to make loading a BGEN without entry fields very fast.~ Rolled into #3783. - ~I fixed `Table.index` to not trigger a partition key info gathering~ Moved to #3774. I had to ship the arrays of filtered variant indices to the workers somehow, so I shipped them as base64 encoded arrays of bytes. It's pretty groady (and that's why I added the commons-codec library). I don't know how else to initializ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3727:2232,refactor,refactoring,2232,https://hail.is,https://github.com/hail-is/hail/pull/3727,1,['refactor'],['refactoring']
Modifiability,export_elasticsearch does not accept config argument,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4063:37,config,config,37,https://hail.is,https://github.com/hail-is/hail/issues/4063,1,['config'],['config']
Modifiability,"ext.autosummary.import_by_name()</code> now raises; <code>ImportExceptionGroup</code> instead of <code>ImportError</code> when it failed to import; target object. Please handle the exception if your extension uses the; function to import Python object. As a workaround, you can disable the; behavior via <code>grouped_exception=False</code> keyword argument until v7.0.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9962"">#9962</a>: texinfo: Customizing styles of emphasized text via <code>@definfoenclose</code>; command was not supported because the command was deprecated since texinfo 6.8</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/2068"">#2068</a>: :confval:<code>intersphinx_disabled_reftypes</code> has changed default value; from an empty list to <code>['std:doc']</code> as avoid too surprising silent; intersphinx resolutions.; To migrate: either add an explicit inventory name to the references; intersphinx should resolve, or explicitly set the value of this configuration; variable to an empty list.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10197"">#10197</a>: html theme: Reduce <code>body_min_width</code> setting in basic theme to 360px</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9999"">#9999</a>: LaTeX: separate terms from their definitions by a CR (refs: <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9985"">#9985</a>)</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10062"">#10062</a>: Change the default language to <code>'en'</code> if any language is not set in; <code>conf.py</code></li>; </ul>; <p>5.0.0 final</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10474"">#10474</a>: :confval:<code>language</code> does not accept <code>None</code> as it value. The default; value of <code>language</code> becomes to <code>'en",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11871:2623,config,configuration,2623,https://hail.is,https://github.com/hail-is/hail/pull/11871,2,"['config', 'variab']","['configuration', 'variable']"
Modifiability,"ext.autosummary.import_by_name()</code> now raises; <code>ImportExceptionGroup</code> instead of <code>ImportError</code> when it failed to import; target object. Please handle the exception if your extension uses the; function to import Python object. As a workaround, you can disable the; behavior via <code>grouped_exception=False</code> keyword argument until v7.0.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9962"">#9962</a>: texinfo: Customizing styles of emphasized text via <code>@definfoenclose</code>; command was not supported because the command was deprecated since texinfo 6.8</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/2068"">#2068</a>: :confval:<code>intersphinx_disabled_reftypes</code> has changed default value; from an empty list to <code>['std:doc']</code> as avoid too surprising silent; intersphinx resolutions.; To migrate: either add an explicit inventory name to the references; intersphinx should resolve, or explicitly set the value of this configuration; variable to an empty list.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10197"">#10197</a>: html theme: Reduce <code>body_min_width</code> setting in basic theme to 360px</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9999"">#9999</a>: LaTeX: separate terms from their definitions by a CR (refs: <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9985"">#9985</a>)</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10062"">#10062</a>: Change the default language to <code>'en'</code> if any language is not set in; <code>conf.py</code></li>; </ul>; <p>5.0.0 final</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10474"">#10474</a>: :confval:<code>language</code> does not accept <code>None</code> as it value. The default</li>; </ul>; <!-- raw HTML omitted -->; </blockquote",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11885:3422,config,configuration,3422,https://hail.is,https://github.com/hail-is/hail/pull/11885,2,"['config', 'variab']","['configuration', 'variable']"
Modifiability,ext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.scala:944); at is.hail.expr.ir.functions.MatrixWriteBlockMatrix.execute(MatrixWriteBlockMatrix.scala:47); at is.hail.expr.ir.functions.WrappedMatrixToValueFunction.execute(RelationalFunctions.scala:88); at is.hail.expr.ir.Interpret$.run(Interpret.scala:735); at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:50); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:45); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20); at is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:14571,rewrite,rewrite,14571,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['rewrite'],['rewrite']
Modifiability,extend FilterVariants to extract an arbitrary subset of positions or variants as listed in a file,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/114:0,extend,extend,0,https://hail.is,https://github.com/hail-is/hail/issues/114,1,['extend'],['extend']
Modifiability,extend `missing` argument in import_table to accept a list of missing values,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5612:0,extend,extend,0,https://hail.is,https://github.com/hail-is/hail/issues/5612,1,['extend'],['extend']
Modifiability,"external_data_disk_size_gb` in the PoolConfig; - Added {Azure,GCP}UserCredentials to the worker to abstract away the names of environment variables and the mount paths of credentials in containers. ## Auth; - Added new fields in the auth database for `azure service principal name` and `azure_credentials_secret_name`; - Made `auth` only create `GSAResource` if CLOUD == 'gcp'. ## Gear; - Added `azure-vm` to the location options for `DeployConfig`. # Assumptions:; - Mapped `{'lowmen': 'F', 'standard': 'D', 'highmem': 'E'}` for machine types in Azure. This corresponds to 2Gi/core, 4Gi/core, and 8Gi/core.; - Spot price is set to -1 for now until we figure out a better billing strategy; - We look for existing network security groups to tell if a VM has been fully cleaned up already in the garbage collection loop. # To-Do:. ## Services. - Use global config and make an `AzureConfig` (@daniel-goldstein not sure if you're already doing this) instead of optional environment variables; - Azure user disks are not implemented; There's a maximum number of disks that can be mounted per machine type with a maximum of 32 along with figuring out the API calls. We'll need a semaphore of some sort.; - No activity logs loop. Not necessary for initial development and preemption billing is not working how intended anyways (will add to the list to fix!). We also don't track vm creation success rates per zone like we do with GCP. It might be good to look for VM deletion events to remove instances that are no longer present and then do a deep delete as then we'll have some redundancy and faster response times.; - Figure out how to do a deep-delete as much as possible for VMs when using the create VM REST API. This is essential for cleaning up resources for idled out workers when the driver is down for a long period of time.; - User billing based on resources used based on the `AzureInstanceConfig`; - Spot billing strategy; - Check network IP settings in the worker; - Add garbage collection CL",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10970:1360,config,config,1360,https://hail.is,https://github.com/hail-is/hail/pull/10970,2,"['config', 'variab']","['config', 'variables']"
Modifiability,"f0d9453391f1f13""><code>213e006</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pycqa/flake8/issues/1653"">#1653</a> from asottile/lower-bound-importlib-metadata</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/e94ee2b5f1801354b940cfe830b9160852915aec""><code>e94ee2b</code></a> require sufficiently new importlib-metadata</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/318a86a4a12d28cdfc41030ead547332d3461f45""><code>318a86a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pycqa/flake8/issues/1646"">#1646</a> from televi/main</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/7b8b374c9bc1a141ca7cf6670c8cee6708398490""><code>7b8b374</code></a> Clarify entry point naming</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/7160561028ee5db09ea58b8555db08c66ee092b3""><code>7160561</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pycqa/flake8/issues/1649"">#1649</a> from PyCQA/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/84d56a8c25106b5e0a41cdf63b5de261f8da5c99""><code>84d56a8</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/ff6569b87db8ae28c41b548071454de620ad14d5""><code>ff6569b</code></a> Release 5.0.3</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/e76b59ae44f46f7958d13b28bd2d7d9bdc0f5962""><code>e76b59a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pycqa/flake8/issues/1648"">#1648</a> from PyCQA/invalid-syntax-partial-parse</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/25e8ff18b30b58f1dabc1d20546ebc20fd775560""><code>25e8ff1</code></a> ignore config files that partially parse as flake8 configs</li>; <li>Additional commits viewable in <a href=""https://github.com/pycqa/flake8/compare/4.0.1...5.0.4"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12105:1347,config,config,1347,https://hail.is,https://github.com/hail-is/hail/pull/12105,1,['config'],['config']
Modifiability,f2e99865&quot;&gt;&lt;code&gt;a1639ef&lt;/code&gt;&lt;/a&gt; Update CHANGES.rst (&lt;a href=&quot;https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/411&quot;&gt;#411&lt;/a&gt;)&lt;/li&gt;; &lt;li&gt;&lt;a href=&quot;https://github.com/pytest-dev/pytest-html/commit/727b305a5707a937b427894360eba11c402b1755&quot;&gt;&lt;code&gt;727b305&lt;/code&gt;&lt;/a&gt; Enable camelcase eslint rule (&lt;a href=&quot;https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/410&quot;&gt;#410&lt;/a&gt;)&lt;/li&gt;; &lt;li&gt;&lt;a href=&quot;https://github.com/pytest-dev/pytest-html/commit/dade11a7a1281ca3060bf1149fdc1d6d0763c97e&quot;&gt;&lt;code&gt;dade11a&lt;/code&gt;&lt;/a&gt; fixed css sort tringles (&lt;a href=&quot;https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/409&quot;&gt;#409&lt;/a&gt;)&lt;/li&gt;; &lt;li&gt;&lt;a href=&quot;https://github.com/pytest-dev/pytest-html/commit/e00532d9c8a598fb848d16b0ce23665789e3517a&quot;&gt;&lt;code&gt;e00532d&lt;/code&gt;&lt;/a&gt; Use scss nesting &amp;amp; variables (&lt;a href=&quot;https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/393&quot;&gt;#393&lt;/a&gt;)&lt;/li&gt;; &lt;li&gt;&lt;a href=&quot;https://github.com/pytest-dev/pytest-html/commit/9bd4907682f10849dde1fe866b5a71402c74e551&quot;&gt;&lt;code&gt;9bd4907&lt;/code&gt;&lt;/a&gt; remove all read the doc documentation from the repo (&lt;a href=&quot;https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/405&quot;&gt;#405&lt;/a&gt;)&lt;/li&gt;; &lt;li&gt;&lt;a href=&quot;https://github.com/pytest-dev/pytest-html/commit/21fafe40b19e7d1c8b4b8bceb7fe1410e2cbdc2a&quot;&gt;&lt;code&gt;21fafe4&lt;/code&gt;&lt;/a&gt; Document how to modify the environment section after tests are finished (&lt;a href=&quot;https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/400&quot;&gt;#400&lt;/a&gt;)&lt;/li&gt;; &lt;li&gt;&lt;a href=&quot;https://github.com/pytest-dev/pytest-html/commit/8b7bdc1fc5f21e946,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11524:14582,variab,variables,14582,https://hail.is,https://github.com/hail-is/hail/pull/11524,1,['variab'],['variables']
Modifiability,"f=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/316"">#316</a>) (<a href=""https://github.com/googleapis/python-logging/commit/5267152574b2ee96eb6f5c536a762f58bd2f886e"">5267152</a>)</li>; <li>support string-encoded json (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/339"">#339</a>) (<a href=""https://github.com/googleapis/python-logging/commit/6fa17735fe3edb45483ec5e3abd1f53c24ffa881"">6fa1773</a>)</li>; <li>trace improvements (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/450"">#450</a>) (<a href=""https://github.com/googleapis/python-logging/commit/e0c5fc02160ae87faf4ba5c2b62be86de6b02cf3"">e0c5fc0</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>allow reading logs from non-project paths (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/444"">#444</a>) (<a href=""https://github.com/googleapis/python-logging/commit/97e32b67603553fe350b6327455fc9f80b8aa6ce"">97e32b6</a>)</li>; <li>api consistency between HTTP and Gapic layers (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/375"">#375</a>) (<a href=""https://github.com/googleapis/python-logging/commit/e1506fa9030776353878048ce562c53bf6ccf7bf"">e1506fa</a>)</li>; </ul>; <h3>Miscellaneous Chores</h3>; <ul>; <li>deprecate AppEngineHandler and ContainerEngineHandler (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/310"">#310</a>) (<a href=""https://github.com/googleapis/python-logging/commit/e3cac888d40bf67af11e57b74615b0c3b8e8aa3e"">e3cac88</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>update usage guide for v3.0.0 (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/456"">#456</a>) (<a href=""https://github.com/googleapis/python-logging/commit/8a67b73cdfcb9da545671be6cf59c724360b1544"">8a67b73</a>)</li>; </ul>; <h2><a href=""https://www.github.com/googleapis/python-logging/compare/v2.6.0...v2.7.0"">2.7.0</a> (2021-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11574:9170,layers,layers,9170,https://hail.is,https://github.com/hail-is/hail/pull/11574,1,['layers'],['layers']
Modifiability,"f=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/316"">#316</a>) (<a href=""https://github.com/googleapis/python-logging/commit/5267152574b2ee96eb6f5c536a762f58bd2f886e"">5267152</a>)</li>; <li>support string-encoded json (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/339"">#339</a>) (<a href=""https://github.com/googleapis/python-logging/commit/6fa17735fe3edb45483ec5e3abd1f53c24ffa881"">6fa1773</a>)</li>; <li>trace improvements (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/450"">#450</a>) (<a href=""https://github.com/googleapis/python-logging/commit/e0c5fc02160ae87faf4ba5c2b62be86de6b02cf3"">e0c5fc0</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>allow reading logs from non-project paths (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/444"">#444</a>) (<a href=""https://github.com/googleapis/python-logging/commit/97e32b67603553fe350b6327455fc9f80b8aa6ce"">97e32b6</a>)</li>; <li>api consistency between HTTP and Gapic layers (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/375"">#375</a>) (<a href=""https://github.com/googleapis/python-logging/commit/e1506fa9030776353878048ce562c53bf6ccf7bf"">e1506fa</a>)</li>; </ul>; <h3>Miscellaneous Chores</h3>; <ul>; <li>deprecate AppEngineHandler and ContainerEngineHandler (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/310"">#310</a>) (<a href=""https://github.com/googleapis/python-logging/commit/e3cac888d40bf67af11e57b74615b0c3b8e8aa3e"">e3cac88</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>update usage guide for v3.0.0 (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/456"">#456</a>) (<a href=""https://github.com/googleapis/python-logging/commit/8a67b73cdfcb9da545671be6cf59c724360b1544"">8a67b73</a>)</li>; </ul>; <h2>v2.7.0</h2>; <h3>Features</h3>; <ul>; <li>add context manager support in client (<a href=""https://g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11574:3737,layers,layers,3737,https://hail.is,https://github.com/hail-is/hail/pull/11574,1,['layers'],['layers']
Modifiability,"f=""https://github-redirect.dependabot.com/prometheus/client_python/issues/747"">#747</a>; [BUGFIX] Remove trailing slashes from pushgateway URLS. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/722"">#722</a>; [BUGFIX] Catch non-integer bucket/count values. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/726"">#726</a></p>; <h2>0.12.0 / 2021-10-29</h2>; <p>[FEATURE] Exemplar support (excludes multiprocess) <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/669"">#669</a>; [ENHANCEMENT] Add support for Python 3.10 <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/706"">#706</a>; [ENHANCEMENT] Restricted Registry will handle metrics added after restricting <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/675"">#675</a>, <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/680"">#680</a><br />; [ENHANCEMENT] Raise a more helpful error if a metric is not observable <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/666"">#666</a>; [BUGFIX] Fix instance_ip_grouping_key not working on MacOS <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/687"">#687</a>; [BUGFIX] Fix assertion error from favicion.ico with Python 2.7 <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/715"">#715</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/prometheus/client_python/commit/a234283a853238dc73fa22651532590330fd72a1""><code>a234283</code></a> Release 0.13.1</li>; <li><a href=""https://github.com/prometheus/client_python/commit/557d123b349f3881cd6475a29ff4c79088a85a26""><code>557d123</code></a> Relax type constraints Timestamp</li>; <li><a href=""https://github.com/prometheus/client_python/commit/b44b63e59b168c6a8498ca31ddcce3ea5e46dcdc""><code>b44b63e</code></a> Declare <code>reg",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11515:2458,ENHANCE,ENHANCEMENT,2458,https://hail.is,https://github.com/hail-is/hail/pull/11515,1,['ENHANCE'],['ENHANCEMENT']
Modifiability,"f=""https://github.com/afrouzMashaykhi""><code>@​afrouzMashaykhi</code></a>) [SIG API Machinery, Cluster Lifecycle, Instrumentation and Node]</li>; <li>Kubernetes is now built with golang 1.15.0-rc.1.; <ul>; <li>The deprecated, legacy behavior of treating the CommonName field on X.509 serving certificates as a host name when no Subject Alternative Names are present is now disabled by default. It can be temporarily re-enabled by adding the value x509ignoreCN=0 to the GODEBUG environment variable. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/93264"">kubernetes/kubernetes#93264</a>, <a href=""https://github.com/justaugustus""><code>@​justaugustus</code></a>) [SIG API Machinery, Auth, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation, Network, Node, Release, Scalability, Storage and Testing]</li>; </ul>; </li>; <li>Promote Immutable Secrets/ConfigMaps feature to Beta and enable the feature by default.; This allows to set <code>Immutable</code> field in Secrets or ConfigMap object to mark their contents as immutable. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/89594"">kubernetes/kubernetes#89594</a>, <a href=""https://github.com/wojtek-t""><code>@​wojtek-t</code></a>) [SIG Apps and Testing]</li>; <li>Remove <code>BindTimeoutSeconds</code> from schedule configuration <code>KubeSchedulerConfiguration</code> (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91580"">kubernetes/kubernetes#91580</a>, <a href=""https://github.com/cofyc""><code>@​cofyc</code></a>) [SIG Scheduling and Testing]</li>; <li>Remove kubescheduler.config.k8s.io/v1alpha1 (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/89298"">kubernetes/kubernetes#89298</a>, <a href=""https://github.com/gavinfish""><code>@​gavinfish</code></a>) [SIG Scheduling]</li>; <li>Reserve plugins that fail to reserve will trigger the unreserve extension point (<a href=""https://github-redirect.dependabot.com/kubernetes/kubern",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:11283,Config,ConfigMap,11283,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['Config'],['ConfigMap']
Modifiability,"f=""https://redirect.github.com/googleapis/java-storage/issues/2196"">#2196</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4f8bb658e9ff3cba5e745acae13ec4094a1a48d5"">4f8bb65</a>)</li>; </ul>; <h2>v2.26.1</h2>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.26.0...v2.26.1"">2.26.1</a> (2023-08-14)</h2>; <h3>Bug Fixes</h3>; <ul>; <li>Make use of ImmutableMap.Builder#buildOrThrow graceful (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2159"">#2159</a>) (<a href=""https://github.com/googleapis/java-storage/commit/e9746f856e9204c1c0ec62f19e6f71ff8a0b9750"">e9746f8</a>)</li>; <li>Update gRPC writeAndClose to only set finish_write on the last message (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2163"">#2163</a>) (<a href=""https://github.com/googleapis/java-storage/commit/95df758d6753005226556177e68a3e9c630c789b"">95df758</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.24 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2158"">#2158</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4f5682a4f6d6d5372a2d382ae3e47dace490ca0d"">4f5682a</a>)</li>; </ul>; <h2>v2.26.0</h2>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.25.0...v2.26.0"">2.26.0</a> (2023-08-03)</h2>; <h3>Features</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/java-storage/blob/main/CHANGELOG.md"">com.google.cloud:google-cloud-storage's changelog</a>.</em></p>; <blockquote>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.26.1...v2.27.0"">2.27.0</a> (2023-09-12)</h2>; <h3>Features</h3>; <ul>; <li>Add new JournalingBlobWriteSessionConfig usable with gRPC transport (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2194"">#2194</a>) (<a href=""https://githu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13624:5623,plugin,plugin,5623,https://hail.is,https://github.com/hail-is/hail/pull/13624,1,['plugin'],['plugin']
Modifiability,"fe to delete it all and start clean), `build.gradle`, `gradle/`, `gradlew`, `gradlew.bat`, `pgradle`, `settings.gradle`; * run `mill mill.bsp.BSP/install` to generate the `.bsp` config directory (bsp is the Build Server Protocol); * In IntelliJ, go to File->Open, and choose the hail root directory; * When the project is open, go to File->Project Structure; * in the Project pane, set an sdk (8 or 11), and set the language level to 8; * in the Modules pane, delete the existing root module, click the plus sign -> Import Module, choose the `hail/` subdirectory, and choose ""Import module from external model"" and `BSP`; * you should see a progress bar at the bottom as it imports the project; * when it's done, quit and reopen IntelliJ. There should now be a bsp icon (two bars with two arrows between them) on the right, where the gradle elephant used to be. Just like before, sometimes you'll need to click the ""reload"" icon in there if things get wonky.; * if it says ""scalafmt configuration detected"", go ahead and enable the formatter. ## Metals setup. * delete any `.metals` directories; * open the hail repo in VSCode (even if you won't use VSCode, this seems to be the best way to get metals set up initially); * it should ask you to import a Mill build; * when that finishes, at the bottom it should say it's connected to a Bloop build server. In general, I think using Mill as the BSP directly will work best, but I don't have much experience to say for sure. To switch, run `Metals: switch build server` from the command palette. ## Debug and release builds. As before, debug mode adds some (fairly expensive) checking to our native memory system. But now there are a few other differences:; * treat warnings as errors only in release mode, so you can still compile, run tests, etc. during development without fixing all warnings; * enable optimization in scalac only in release mode. The intention is that we use debug mode during development, and release mode ony for published artifac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14147:4582,config,configuration,4582,https://hail.is,https://github.com/hail-is/hail/pull/14147,1,['config'],['configuration']
Modifiability,fer.$plus$plus$eq(ArrayBuffer.scala:105); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49); 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366); 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364); 	at scala.collection.AbstractIterator.to(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358); 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345); 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1431); 	at is.hail.lir.Classx.asBytes(X.scala:120); 	at is.hail.asm4s.ClassBuilder.classBytes(ClassBuilder.scala:488); 	at is.hail.asm4s.ModuleBuilder.$anonfun$classesBytes$1(ClassBuilder.scala:166); 	at is.hail.asm4s.ModuleBuilder.$anonfun$classesBytes$1$adapted(ClassBuilder.scala:166); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator.foreach(Iterator.scala:943); 	at scala.collection.Iterator.foreach$(Iterator.scala:943); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62); 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49); 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366); 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364); 	at scala.collection.AbstractIterator.to(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358); 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14362:9046,adapt,adapted,9046,https://hail.is,https://github.com/hail-is/hail/issues/14362,1,['adapt'],['adapted']
Modifiability,fer.$plus$plus$eq(ArrayBuffer.scala:105); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49); 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366); 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364); 	at scala.collection.AbstractIterator.to(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358); 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345); 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1431); 	at is.hail.lir.Classx.asBytes(X.scala:121); 	at is.hail.asm4s.ClassBuilder.classBytes(ClassBuilder.scala:394); 	at is.hail.asm4s.ModuleBuilder.$anonfun$classesBytes$1(ClassBuilder.scala:151); 	at is.hail.asm4s.ModuleBuilder.$anonfun$classesBytes$1$adapted(ClassBuilder.scala:151); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator.foreach(Iterator.scala:943); 	at scala.collection.Iterator.foreach$(Iterator.scala:943); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62); 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49); 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366); 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364); 	at scala.collection.AbstractIterator.to(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358); 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12531:3969,adapt,adapted,3969,https://hail.is,https://github.com/hail-is/hail/issues/12531,1,['adapt'],['adapted']
Modifiability,fer.$plus$plus$eq(ArrayBuffer.scala:105); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49); 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366); 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364); 	at scala.collection.AbstractIterator.to(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358); 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345); 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1431); 	at is.hail.lir.Classx.asBytes(X.scala:123); 	at is.hail.asm4s.ClassBuilder.classBytes(ClassBuilder.scala:351); 	at is.hail.asm4s.ModuleBuilder.$anonfun$classesBytes$1(ClassBuilder.scala:151); 	at is.hail.asm4s.ModuleBuilder.$anonfun$classesBytes$1$adapted(ClassBuilder.scala:151); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator.foreach(Iterator.scala:943); 	at scala.collection.Iterator.foreach$(Iterator.scala:943); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62); 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49); 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366); 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364); 	at scala.collection.AbstractIterator.to(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358); 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533:17420,adapt,adapted,17420,https://hail.is,https://github.com/hail-is/hail/issues/12533,1,['adapt'],['adapted']
Modifiability,"fix encode/decode of non-wrapped values, some other refactoring changes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6759:52,refactor,refactoring,52,https://hail.is,https://github.com/hail-is/hail/pull/6759,1,['refactor'],['refactoring']
Modifiability,"fix-threaded-client</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/30ce7539778e2a25ff5e6eba4ccb6c08b8a0fe20""><code>30ce753</code></a> fix sphinx 5.0 support</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/a2e90574645052320de861bb84ba1752e25ef2dd""><code>a2e9057</code></a> ignore type error</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/3c6fc38e8dda754aba4a1217733eb1a0146b4c57""><code>3c6fc38</code></a> Run qtconsole test suite as a another downstream project</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/dcb45960b337fb089e04b0c3dde880e8f0f10ae5""><code>dcb4596</code></a> Revert changes related to _handle_recv in ThreadedZMQSocketChannel</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/01bfdd18c2eb8ea34cbb9915cb2bc7d9806f81a4""><code>01bfdd1</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/issues/799"">#799</a> from jupyter/pre-commit-ci-update-config</li>; <li>Additional commits viewable in <a href=""https://github.com/jupyter/jupyter_client/compare/v7.3.1...v7.3.4"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jupyter-client&package-manager=pip&previous-version=7.3.1&new-version=7.3.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12110:9689,config,config,9689,https://hail.is,https://github.com/hail-is/hail/pull/12110,1,['config'],['config']
Modifiability,"fixed numeric aggregations behavior on empty arrays and sets; - modified min, max, mean, and median functions in FunctionRegistry; - added regression tests for empty sets and empty arrays to ExprSuite; - updated and refactored ExpressionLanguage docs to reflect changes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1289:216,refactor,refactored,216,https://hail.is,https://github.com/hail-is/hail/pull/1289,1,['refactor'],['refactored']
Modifiability,fixed sphinx input variable to not run tutorial,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1544:19,variab,variable,19,https://hail.is,https://github.com/hail-is/hail/pull/1544,1,['variab'],['variable']
Modifiability,"fixes #7418. The main change here is fixing #7418 by making nodes which perform aggregation aware of the nearest containing node which defines what is being aggregated over. This is done by making a new variable name---here called ""agg_capability""---which is added to the child context by any node which (re)defines the meaning of aggregation (`TableMapRows`, `AggFilter`, etc.), and which is implicitly referenced by any node which performs an aggregation (`ApplyAggOp`, `AggFilter`, etc.). This requires nodes to be able to bind variables which shadow variables already bound by parents, which it turns out wasn't handled correctly by the CSE algorithm. Fixing this required several changes:; * I moved free variable computation to a lazy value on `BaseIR`. This way, each time we see a subtree `x`, we can recompute the max depth of the binding sites of all of `x`'s free variables, since those binding sites may be different than last time we saw `x`. This also required splitting the free variables into value, agg, and scan sets, so they can be looked up in the correct context (previously context lookup was always done at the `Ref` node, at which point the variable was in the value context).; * Now, in the `CSEPrintPass`, we have to recompute the same binding depth calculation that was done in the analysis pass, so we know which binding site to look at (previously I just searched all binding sites in scope, but with shadowing handled correctly we don't have sufficient information to decide which binding site is valid). This requires maintaining contexts in the print pass, which is annoying because we are now traversing the tree of `Renderable` children, which is not exactly the same as the IR tree.; * To fix this, I duplicated all methods involving binding structure on `BaseIR`. To avoid having to write twice as many methods on concrete IR classes, I made the methods taking the index of the `Renderable` child (e.g. `renderable_bindings`) be the primary methods which are overri",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7479:203,variab,variable,203,https://hail.is,https://github.com/hail-is/hail/pull/7479,5,['variab'],"['variable', 'variables']"
Modifiability,"for better or worse. I believe the behavior is the same and the code and user ergonomics seem better. The one known issue I haven't figured out to fix is the usage output on unknown flags:. ```; $ hailctl --fleep auth login; usage: hailctl [-h] {version,dataproc,auth,dev,batch,curl,config} ...; hailctl: error: unrecognized arguments: --fleep. $ hailctl auth --fleep login; usage: hailctl [-h] {version,dataproc,auth,dev,batch,curl,config} ...; hailctl: error: unrecognized arguments: --fleep; ```. This implies argparse should allow hailctl flags to appear after auth, but that isn't the case. For example:. ```; $ hailctl dataproc connect --beta foo nb; usage: hailctl [-h] {version,dataproc,auth,dev,batch,curl,config} ...; hailctl: error: unrecognized arguments: --beta; ```. I will keep investigating a fix for this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9836:283,config,config,283,https://hail.is,https://github.com/hail-is/hail/pull/9836,3,['config'],['config']
Modifiability,"format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:4269,config,configuration,4269,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['config'],['configuration']
Modifiability,"full control canned ACL to be set when Athena writes query results to S3 buckets.</li>; <li>api-change:<code>keyspaces</code>: [<code>botocore</code>] This release adds support for data definition language (DDL) operations</li>; <li>api-change:<code>ecr</code>: [<code>botocore</code>] This release adds support for tracking images lastRecordedPullTime.</li>; </ul>; <h1>1.21.10</h1>; <ul>; <li>api-change:<code>mediapackage</code>: [<code>botocore</code>] This release adds Hybridcast as an available profile option for Dash Origin Endpoints.</li>; <li>api-change:<code>rds</code>: [<code>botocore</code>] Documentation updates for Multi-AZ DB clusters.</li>; <li>api-change:<code>mgn</code>: [<code>botocore</code>] Add support for GP3 and IO2 volume types. Add bootMode to LaunchConfiguration object (and as a parameter to UpdateLaunchConfigurationRequest).</li>; <li>api-change:<code>kafkaconnect</code>: [<code>botocore</code>] Adds operation for custom plugin deletion (DeleteCustomPlugin) and adds new StateDescription field to DescribeCustomPlugin and DescribeConnector responses to return errors from asynchronous resource creation.</li>; </ul>; <h1>1.21.9</h1>; <ul>; <li>api-change:<code>finspace-data</code>: [<code>botocore</code>] Add new APIs for managing Users and Permission Groups.</li>; <li>api-change:<code>amplify</code>: [<code>botocore</code>] Add repositoryCloneMethod field for hosting an Amplify app. This field shows what authorization method is used to clone the repo: SSH, TOKEN, or SIGV4.</li>; <li>api-change:<code>fsx</code>: [<code>botocore</code>] This release adds support for the following FSx for OpenZFS features: snapshot lifecycle transition messages, force flag for deleting file systems with child resources, LZ4 data compression, custom record sizes, and unsetting volume quotas and reservations.</li>; <li>api-change:<code>fis</code>: [<code>botocore</code>] This release adds logging support for AWS Fault Injection Simulator experiments. Experiment templa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11486:2503,plugin,plugin,2503,https://hail.is,https://github.com/hail-is/hail/pull/11486,2,['plugin'],['plugin']
Modifiability,"fused Deputy. I also partly resolved the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this glob",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:3256,config,configuration,3256,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['config'],['configuration']
Modifiability,"fy entry point naming</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/7160561028ee5db09ea58b8555db08c66ee092b3""><code>7160561</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pycqa/flake8/issues/1649"">#1649</a> from PyCQA/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/84d56a8c25106b5e0a41cdf63b5de261f8da5c99""><code>84d56a8</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/ff6569b87db8ae28c41b548071454de620ad14d5""><code>ff6569b</code></a> Release 5.0.3</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/e76b59ae44f46f7958d13b28bd2d7d9bdc0f5962""><code>e76b59a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pycqa/flake8/issues/1648"">#1648</a> from PyCQA/invalid-syntax-partial-parse</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/25e8ff18b30b58f1dabc1d20546ebc20fd775560""><code>25e8ff1</code></a> ignore config files that partially parse as flake8 configs</li>; <li>Additional commits viewable in <a href=""https://github.com/pycqa/flake8/compare/4.0.1...5.0.4"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flake8&package-manager=pip&previous-version=4.0.1&new-version=5.0.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12105:2056,config,config,2056,https://hail.is,https://github.com/hail-is/hail/pull/12105,2,['config'],"['config', 'configs']"
Modifiability,"fyi @cseed . All of the auxilliary methods have the same arguments as the original emitted function, to preserve the behavior of In and aggregator stuff without change. Any IR that binds a new environment variable (`Let`, `Map`, etc.) now stores those values on a class field so that other methods don't have to worry about whether or not there exist such values in scope. That's basically the only change I made to the structure of Emit; `MakeArray`, `MakeStruct`, and `MakeTuple` now check how big their fields/elements are. I'm a little worried we'll start hitting the class size limit; turning up one of the tests really far already hits it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3308:205,variab,variable,205,https://hail.is,https://github.com/hail-is/hail/pull/3308,1,['variab'],['variable']
Modifiability,g 'hailtop/aiotools/fs/exceptions.py'; adding 'hailtop/aiotools/fs/fs.py'; adding 'hailtop/aiotools/fs/stream.py'; adding 'hailtop/auth/__init__.py'; adding 'hailtop/auth/auth.py'; adding 'hailtop/auth/sql_config.py'; adding 'hailtop/auth/tokens.py'; adding 'hailtop/batch/__init__.py'; adding 'hailtop/batch/backend.py'; adding 'hailtop/batch/batch.py'; adding 'hailtop/batch/batch_pool_executor.py'; adding 'hailtop/batch/conftest.py'; adding 'hailtop/batch/docker.py'; adding 'hailtop/batch/exceptions.py'; adding 'hailtop/batch/globals.py'; adding 'hailtop/batch/hail_genetics_images.py'; adding 'hailtop/batch/job.py'; adding 'hailtop/batch/resource.py'; adding 'hailtop/batch/utils.py'; adding 'hailtop/batch_client/__init__.py'; adding 'hailtop/batch_client/aioclient.py'; adding 'hailtop/batch_client/client.py'; adding 'hailtop/batch_client/globals.py'; adding 'hailtop/batch_client/parse.py'; adding 'hailtop/cleanup_gcr/__init__.py'; adding 'hailtop/cleanup_gcr/__main__.py'; adding 'hailtop/config/__init__.py'; adding 'hailtop/config/deploy_config.py'; adding 'hailtop/config/user_config.py'; adding 'hailtop/fs/__init__.py'; adding 'hailtop/fs/fs.py'; adding 'hailtop/fs/fs_utils.py'; adding 'hailtop/fs/router_fs.py'; adding 'hailtop/fs/stat_result.py'; adding 'hailtop/hailctl/__init__.py'; adding 'hailtop/hailctl/__main__.py'; adding 'hailtop/hailctl/deploy.yaml'; adding 'hailtop/hailctl/describe.py'; adding 'hailtop/hailctl/auth/__init__.py'; adding 'hailtop/hailctl/auth/cli.py'; adding 'hailtop/hailctl/auth/create_user.py'; adding 'hailtop/hailctl/auth/delete_user.py'; adding 'hailtop/hailctl/auth/login.py'; adding 'hailtop/hailctl/batch/__init__.py'; adding 'hailtop/hailctl/batch/batch_cli_utils.py'; adding 'hailtop/hailctl/batch/cli.py'; adding 'hailtop/hailctl/batch/list_batches.py'; adding 'hailtop/hailctl/batch/submit.py'; adding 'hailtop/hailctl/batch/billing/__init__.py'; adding 'hailtop/hailctl/batch/billing/cli.py'; adding 'hailtop/hailctl/config/__init__.py';,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13445:10808,config,config,10808,https://hail.is,https://github.com/hail-is/hail/issues/13445,1,['config'],['config']
Modifiability,"g JUnit 4 Tests using TestNG (Krishnan Mahadevan); Fixed: GITHUB-2847: Deprecate support for running JUnit tests (Krishnan Mahadevan); Fixed: GITHUB-2844: Deprecate support for running Spock Tests (Krishnan Mahadevan); Fixed: GITHUB-550: Weird <a href=""https://github.com/BeforeMethod""><code>@​BeforeMethod</code></a> and <a href=""https://github.com/AfterMethod""><code>@​AfterMethod</code></a> behaviour with dependsOnMethods (Krishnan Mahadevan); Fixed: GITHUB-893: TestNG should provide an Api which allow to find all dependent of a specific test (Krishnan Mahadevan); New: Added .yml file extension for yaml suite files, previously only .yaml was allowed for yaml (Steven Jubb); Fixed: GITHUB-141: regular expression in &quot;dependsOnMethods&quot; does not work (Krishnan Mahadevan); Fixed: GITHUB-2770: FileAlreadyExistsException when report is generated (melloware); Fixed: GITHUB-2825: Programmatically Loading TestNG Suite from JAR File Fails to Delete Temporary Copy of Suite File (Steven Jubb); Fixed: GITHUB-2818: Add configuration key for callback discrepancy behavior (Krishnan Mahadevan); Fixed: GITHUB-2819: Ability to retry a data provider in case of failures (Krishnan Mahadevan); Fixed: GITHUB-2308: StringIndexOutOfBoundsException in findClassesInPackage - Surefire/Maven - JDK 11 fails (Krishnan Mahadevan); Fixed: GITHUB:2788: TestResult.isSuccess() is TRUE when test fails due to expectedExceptions (Krishnan Mahadevan); Fixed: GITHUB-2800: Running Test Classes with Inherited <a href=""https://github.com/Factory""><code>@​Factory</code></a> and <a href=""https://github.com/DataProvider""><code>@​DataProvider</code></a> Annotated Non-Static Methods Fail (Krishnan Mahadevan); New: Ability to provide custom error message for assertThrows\expectThrows methods (Anatolii Yuzhakov); Fixed: GITHUB-2780: Use SpotBugs instead of abandoned FindBugs; Fixed: GITHUB-2801: JUnitReportReporter is too slow; Fixed: GITHUB-2807: buildStackTrace should be fail-safe (Sergey Chernov); Fixed: G",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:11107,config,configuration,11107,https://hail.is,https://github.com/hail-is/hail/pull/12665,1,['config'],['configuration']
Modifiability,"g.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/d",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2147,config,config,2147,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['config'],['config']
Modifiability,g.py'; adding 'hailtop/config/variables.py'; adding 'hailtop/fs/__init__.py'; adding 'hailtop/fs/fs.py'; adding 'hailtop/fs/fs_utils.py'; adding 'hailtop/fs/router_fs.py'; adding 'hailtop/fs/stat_result.py'; adding 'hailtop/hailctl/__init__.py'; adding 'hailtop/hailctl/__main__.py'; adding 'hailtop/hailctl/deploy.yaml'; adding 'hailtop/hailctl/describe.py'; adding 'hailtop/hailctl/auth/__init__.py'; adding 'hailtop/hailctl/auth/cli.py'; adding 'hailtop/hailctl/auth/create_user.py'; adding 'hailtop/hailctl/auth/delete_user.py'; adding 'hailtop/hailctl/auth/login.py'; adding 'hailtop/hailctl/batch/__init__.py'; adding 'hailtop/hailctl/batch/batch_cli_utils.py'; adding 'hailtop/hailctl/batch/cli.py'; adding 'hailtop/hailctl/batch/initialize.py'; adding 'hailtop/hailctl/batch/list_batches.py'; adding 'hailtop/hailctl/batch/submit.py'; adding 'hailtop/hailctl/batch/utils.py'; adding 'hailtop/hailctl/batch/billing/__init__.py'; adding 'hailtop/hailctl/batch/billing/cli.py'; adding 'hailtop/hailctl/config/__init__.py'; adding 'hailtop/hailctl/config/cli.py'; adding 'hailtop/hailctl/config/config_variables.py'; adding 'hailtop/hailctl/dataproc/__init__.py'; adding 'hailtop/hailctl/dataproc/cli.py'; adding 'hailtop/hailctl/dataproc/cluster_config.py'; adding 'hailtop/hailctl/dataproc/connect.py'; adding 'hailtop/hailctl/dataproc/deploy_metadata.py'; adding 'hailtop/hailctl/dataproc/diagnose.py'; adding 'hailtop/hailctl/dataproc/gcloud.py'; adding 'hailtop/hailctl/dataproc/modify.py'; adding 'hailtop/hailctl/dataproc/start.py'; adding 'hailtop/hailctl/dataproc/submit.py'; adding 'hailtop/hailctl/dataproc/utils.py'; adding 'hailtop/hailctl/dev/__init__.py'; adding 'hailtop/hailctl/dev/ci_client.py'; adding 'hailtop/hailctl/dev/cli.py'; adding 'hailtop/hailctl/dev/config.py'; adding 'hailtop/hailctl/hdinsight/__init__.py'; adding 'hailtop/hailctl/hdinsight/cli.py'; adding 'hailtop/hailctl/hdinsight/start.py'; adding 'hailtop/hailctl/hdinsight/submit.py'; adding 'hailtop/utils/__,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:27456,config,config,27456,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['config'],['config']
Modifiability,g.scala:22); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:22); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1921); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annota,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:17855,adapt,adapted,17855,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['adapt'],['adapted']
Modifiability,g.scala:22); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:22); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1921); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:8301,adapt,adapted,8301,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['adapt'],['adapted']
Modifiability,gcloud uses this variable to find the tokens file. We should set it so; that users do not need to configure anything. I also fixed some; missing arguments in the non-async get_userinfo.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9737:17,variab,variable,17,https://hail.is,https://github.com/hail-is/hail/pull/9737,2,"['config', 'variab']","['configure', 'variable']"
Modifiability,"ge:. ```; Traceback (most recent call last):; File ""…/borkscript.py"", line 8, in <module>; my_job, my_output = make_job(batch); File ""…/site-packages/hailtop/batch/job.py"", line 125, in __getitem__; return self._get_resource(item); File ""…/site-packages/hailtop/batch/job.py"", line 118, in _get_resource; r = self._batch._new_job_resource_file(self, value=item); File ""…/site-packages/hailtop/batch/batch.py"", line 405, in _new_job_resource_file; jrf = _resource.JobResourceFile(value, source); File ""…/site-packages/hailtop/batch/resource.py"", line 128, in __init__; super().__init__(value); File ""…/site-packages/hailtop/batch/resource.py"", line 48, in __init__; assert value is None or isinstance(value, str); AssertionError; ```. Of course, in a 400-line script it took a long while to figure out what the traceback that seemed to have little to do with any dubious code of ours was trying to tell us, and to notice that the actual problem was the `return` 200 lines away!. The problem is that these classes define `__getitem__()` so their resources can be accessed as if via a dict. The assignment into multiple variables causes Python to try to interpret the RHS as something iterable, and as `__getitem__` is defined, it will use `__getitem__(0)`, `__getitem__(1)`,... to implement that iteration. These classes are not really iterable, so define a no-op `__iter__()` to prevent this. With this, we get:. ```; Traceback (most recent call last):; File ""…/borkscript.py"", line 8, in <module>; my_job, my_output = make_job(batch); File ""…/site-packages/hailtop/batch/job.py"", line 127, in __iter__; raise TypeError(f'{type(self).__name__!r} object is not iterable'); TypeError: 'BashJob' object is not iterable; ```. Which, while still not pointing directly at the problem, is much clearer. Especially for ResourceGroup, it may be worth defining iteration for these classes in future. But at the moment `__getitem__`-based iteration fails, so this ensures it fails with a clear TypeError message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14390:1449,variab,variables,1449,https://hail.is,https://github.com/hail-is/hail/pull/14390,1,['variab'],['variables']
Modifiability,"gen` or `gs://full/path/to/bgen.file`, which is annoying. I don't have any better way to generically uniquely identify files though. ---; ### Calc Depth Bug. I also had to fix a bug in the indices. Neither my `OnDiskBTreeIndexToValue` nor the existing `IndexBTree` correctly calculated the sizes of the given trees. Recall that a b-tree is a series of layers. Layer 0 is at most `branchingFactor` in size. Layer i is at most `branchingFactor ^ (i+1)` in size. The total size of the b-tree is the sum of the layer sizes. Here's a few max sizes for a branchingFactor of 1024:. - 1 layer tree: 1024; - 2 layer tree: 1024^2 + 1024; - 3 layer tree: 1024^3 + 1024^2 + 1024. If you look carefully at the old `calcDepth` method, it incorrectly concludes that fully populated 3 layer trees have four layers because they have more than 1024^3 total (internal+leaf) elements. This issue rears it's head on an exponentially small number of trees (at depth `i`, the number of leaf elements must lie in `[1024^i-1024^(i-1), 1024^i]`. This discrepancy is what lead to my confusion for the last few days. It shows up quite quickly with very small branching factors (e.g. 3) but with a large branching factor (the default of 1024 and what all the tests were written against) it's fairly rare. ---; ### Summary of Changes. - add `_variants_per_file` which is a map from absolute file paths to lists of variants (identified by their in-file index) to keep; - a test for `_variants_per_file`; - a fixed `calcDepth` which is now used by both index classes; - a set of tests for `calcDepth`; - some clean up in `BgenBlockReader`: use `private[this]` for things that are truly private fields (otherwise they're accessed through `invokevirtual`) and use `using` to manage resources; - teach `BgenInputFormat` to produce splits that describe which variants to keep; - modify `BgenRecord` to use variant filtering information in file splits; - add some groady code to ship variant filter lists through the hadoop configuration",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3813:2617,config,configuration,2617,https://hail.is,https://github.com/hail-is/hail/pull/3813,1,['config'],['configuration']
Modifiability,generic intervals 2: add extended ordering,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2675:25,extend,extended,25,https://hail.is,https://github.com/hail-is/hail/pull/2675,1,['extend'],['extended']
Modifiability,"ging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/config/cli.py; +++ b/hail/python/hailtop/hailctl/dev/config/cli.py; @@ -1,6 +1,6 @@; import os; import json; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; ; def init_parser(parser):; @@ -35,6 +35,6 @@ def main(args):; }; ; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3719,config,config,3719,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['config'],['config']
Modifiability,"github.com/pyasn1/pyasn1-modules/commit/7d8e520aa7d0e71ef7144ce381c8a41464e687dc""><code>7d8e520</code></a> Modernize build and test infra (<a href=""https://redirect.github.com/pyasn1/pyasn1-modules/issues/2"">#2</a>)</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/51f5bfe83178871fe2ee80df6b8e13ed54a2d897""><code>51f5bfe</code></a> Add GitHub Actions CI, test with 3.9 to 3.11 (<a href=""https://redirect.github.com/pyasn1/pyasn1-modules/issues/1"">#1</a>)</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/bdbcc5d9650a8e8382979f089df3307dd4121b49""><code>bdbcc5d</code></a> Bump up coverage percentage cut at tox</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/7c7e4add6cb9f1a47a2303f819c8472491f6ebbb""><code>7c7e4ad</code></a> Add support for RFC 8769 (<a href=""https://redirect.github.com/pyasn1/pyasn1-modules/issues/136"">#136</a>)</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/13ca0da0cc4d0703ca42113f607bde95cf0bfd9c""><code>13ca0da</code></a> Fix tox deps inheritance</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/00fa3b9d15c783389afee7887f5ba3738a005545""><code>00fa3b9</code></a> Run unittests across many Python versions</li>; <li>Additional commits viewable in <a href=""https://github.com/pyasn1/pyasn1-modules/compare/v0.2.8...v0.3.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pyasn1-modules&package-manager=pip&previous-version=0.2.8&new-version=0.3.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12928:8623,inherit,inheritance,8623,https://hail.is,https://github.com/hail-is/hail/pull/12928,1,['inherit'],['inheritance']
Modifiability,"gitt""><code>@​liggitt</code></a>) [SIG API Machinery]</li>; <li>EnvVarSource api doc bug fixes (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91194"">kubernetes/kubernetes#91194</a>, <a href=""https://github.com/wawa0210""><code>@​wawa0210</code></a>) [SIG Apps]</li>; <li>Fix bug in reflector that couldn't recover from &quot;Too large resource version&quot; errors (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/92537"">kubernetes/kubernetes#92537</a>, <a href=""https://github.com/wojtek-t""><code>@​wojtek-t</code></a>) [SIG API Machinery]</li>; <li>Fixed: log timestamps now include trailing zeros to maintain a fixed width (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91207"">kubernetes/kubernetes#91207</a>, <a href=""https://github.com/iamchuckss""><code>@​iamchuckss</code></a>) [SIG Apps and Node]</li>; <li>Generic ephemeral volumes, a new alpha feature under the <code>GenericEphemeralVolume</code> feature gate, provide a more flexible alternative to <code>EmptyDir</code> volumes: as with <code>EmptyDir</code>, volumes are created and deleted for each pod automatically by Kubernetes. But because the normal provisioning process is used (<code>PersistentVolumeClaim</code>), storage can be provided by third-party storage vendors and all of the usual volume features work. Volumes don't need to be empt; for example, restoring from snapshot is supported. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/92784"">kubernetes/kubernetes#92784</a>, <a href=""https://github.com/pohly""><code>@​pohly</code></a>) [SIG API Machinery, Apps, Auth, CLI, Instrumentation, Node, Scheduling, Storage and Testing]</li>; <li>Go1.14.4 is now the minimum version required for building Kubernetes (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/92438"">kubernetes/kubernetes#92438</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>) [SIG API Machinery, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:7314,flexible,flexible,7314,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['flexible'],['flexible']
Modifiability,"goes in ```configureAndCreateSparkContext```. ```sc.uiWebUrl.foreach { s => info(s""SparkUI started at $s"") }```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1142:11,config,configureAndCreateSparkContext,11,https://hail.is,https://github.com/hail-is/hail/issues/1142,1,['config'],['configureAndCreateSparkContext']
Modifiability,"good comments; addressed these and did a fair bit of redesigning of the Python stuff. Refactored the scala to use that method you mentioned, also got mad that we didn't already have a SStackInterval and added that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12376#issuecomment-1295668281:86,Refactor,Refactored,86,https://hail.is,https://github.com/hail-is/hail/pull/12376#issuecomment-1295668281,1,['Refactor'],['Refactored']
Modifiability,"google died:; ```; E Log:	{'main': ""ERROR: (gcloud.auth.activate-service-account) There was a problem refreshing your current auth tokens: {u'status': u'UNAVAILABLE', u'message': u'The service is currently unavailable.', u'code': 503}\nPlease run:\n\n $ gcloud auth login\n\nto obtain new credentials, or if you have already logged in with a\ndifferent account:\n\n $ gcloud config set account ACCOUNT\n\nto select an already authenticated account to use.\n""}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5934#issuecomment-486735019:375,config,config,375,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-486735019,1,['config'],['config']
Modifiability,grafana and prometheus jobs for rendering their nginx configs were producing same name outputs so one would overwrite the other.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10181:54,config,configs,54,https://hail.is,https://github.com/hail-is/hail/pull/10181,1,['config'],['configs']
Modifiability,"h MakeArray?. I have some thoughts on that, but I think it would require a bigger change to how we represent streams in the IR. One idea is to put stream producers and stream consumers on equal footing. Right now, a stream consumer is a node with a stream child that produces a non-stream value, e.g. `ArrayFor(stream, name, body)`. We could let stream consumers be values themselves, `ArrayFor(name, body)`, with other nodes for connecting producers with consumers, e.g. `RunStream(stream, ArrayFor(name, body))`. Then I think we would need two `MakeStream`s, one that can be unrolled, and one that can be zipped. The former just directly takes a consumer and produces a value (and can choose to duplicate the consumer code or wrap it in a method). The later constructs a stream producer, which can be freely used in zips, etc. It would definitely require some thought how to optimize such a representation (is there a normal form?). > I'm not sure I see that. If you don't duplicate the consumer, the push code is the same. The question is, do you use a variable + switch to track where you are in the MakeStream, or do you use the program counter via labels, where you get the latter from the former by code duplication. I think the easiest way to see it is to think about how you would zip two `MakeStream`s, both of which represent their state using the program counter. You could do it by statically fusing them into a single `MakeStream`, but what if it was something more complicated like `Zip(Map(MakeStream(...), ...), Filter(MakeStream(...), ...))`? You can't represent a compound state with a single program counter. At a higher level, you can only do the optimization of unrolling the `MakeStream` loop if that is the outermost loop in the generated code. A producer wanting to create the outermost loop is what I mean by a push stream. By contrast, to be able to zip arbitrary streams, you want the consumer to create the outermost loop, and on each iteration ""pull"" from each producer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8148#issuecomment-591962972:1133,variab,variable,1133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-591962972,1,['variab'],['variable']
Modifiability,"h in dev namespaces and default. Currently, the grafana and prometheus pods have two containers: the app itself (grafana or prometheus) and an nginx container that sits in front of it. The flow is as follows, and since this works the exact same for both prometheus and grafana I will just talk about grafana as the example and the same thing should apply to both:. 1. User sends request to grafana.hail.is; 2. Gateway sees an HTTP request going to a production service and forwards that request to the grafana k8s Service port 443; 3. The grafana K8s Service forwards that request to the grafana pod port 443; 4. Nginx is listening on port 443 in the grafana pod and receives that request. It makes an authorization check to auth to make sure that the request is coming from a developer; 5. Nginx forwards that request to 127.0.0.1:3000, which is where grafana is listening. This PR does not change any behavior, just replaces Nginx with Envoy. Currently, building the nginx container involves running jinja on its config files and building a docker image. With envoy, we can just use the `envoyproxy/envoy` image from DockerHub (which I have copied into our container registries) and feed it a single configmap. The big mess of yaml that is the new configmap for envoy has a lot of boilerplate, but it comprises of the following sections which hopefully on their own are not too bad. ### Envoy config; 1. The top of the `listeners` section shows that Envoy is listening on port 8443 (which is the port that the k8s `Service` will now forward traffic to); 2. The `virtual_hosts` section shows that Envoy will send all paths (prefix ""/"") to the cluster `grafana`; 3. The `http_filters` section says that Envoy will first send an authorization request to the `auth` cluster before allowing the request to pass to grafana; 4. The `clusters` section says that there are two other services Envoy knows about and can send traffic to, one named `auth` that can be found at address `auth` (same as `https://a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12364:1282,config,config,1282,https://hail.is,https://github.com/hail-is/hail/pull/12364,1,['config'],['config']
Modifiability,"hadoop_open has a somewhat strange behavior, when the global fs is; HadoopFS, BGzip and Gzip files are handled transparently by file; extension, so python reads and writes uncompressed data. This is not the; case if the global fs is LocalFS or GoogleCloudStorageFS. We'd; eventually like to move away from this behavior for HadoopFS altogether,; but we cannot change the behavior of hadoop_open without breaking user; code. To that end, rewrite HadoopFS.open to ignore codecs, and add legacy_open; to preserve the old behavior. As a result of this, we also implement the seekable interface in python; for HadoopFS opened files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10812:437,rewrite,rewrite,437,https://hail.is,https://github.com/hail-is/hail/pull/10812,1,['rewrite'],['rewrite']
Modifiability,"hail % gsutil cp ./src/test/resources/ldprune2.vcf gs://danking/chr1.vcf; Copying file://./src/test/resources/ldprune2.vcf [Content-Type=text/x-vcard]...; / [1 files][ 11.5 KiB/ 11.5 KiB] ; Operation completed over 1 objects/11.5 KiB. ; (base) dking@wm28c-761 hail % gsutil cp ./src/test/resources/ldprune2.vcf gs://danking/chr2.vcf; Copying file://./src/test/resources/ldprune2.vcf [Content-Type=text/x-vcard]...; / [1 files][ 11.5 KiB/ 11.5 KiB] ; Operation completed over 1 objects/11.5 KiB. ; (base) dking@wm28c-761 hail % ipython ; Python 3.10.9 (main, Jan 11 2023, 09:18:18) [Clang 14.0.6 ]; Type 'copyright', 'credits' or 'license' for more information; IPython 8.16.1 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl; ...: hl.import_vcf('gs://danking/chr*.vcf').count(); Initializing Hail with default parameters...; /Users/dking/miniconda3/lib/python3.10/site-packages/hailtop/aiocloud/aiogoogle/user_config.py:29: UserWarning: You have specified the GCS requester pays configuration in both your spark-defaults.conf (/Users/dking/miniconda3/lib/python3.10/site-packages/pyspark/conf/spark-defaults.conf) and either an explicit argument or through `hailctl config`. For GCS requester pays configuration, Hail first checks explicit arguments, then `hailctl config`, then spark-defaults.conf.; warnings.warn(; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/Users/dking/miniconda3/lib/python3.10/site-packages/pyspark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13915:1297,config,configuration,1297,https://hail.is,https://github.com/hail-is/hail/issues/13915,1,['config'],['configuration']
Modifiability,hail.expr.ir.TableTextFinalizer.writeMetadata(TableWriter.scala:552); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:748); 	at is.hail.expr.ir.Emit.emitVoid$1(Emit.scala:627); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$5(Emit.scala:644); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$5$adapted(Emit.scala:644); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$4(Emit.scala:644); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$4$adapted(Emit.scala:643); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:1011); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$3(Emit.scala:643); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$3$adapted(Emit.scala:641); 	at scala.collection.Iterator.foreach(Iterator.scala:943); 	at scala.collection.Iterator.foreach$(Iterator.scala:943); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:641); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3(Emit.scala:70); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3$adapted(Emit.scala:68); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:1011); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:68); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:78); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$1(CompileAndEvaluate.scala:50); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:50); 	at is.hail.expr.ir.CompileAndEvaluate$.evalToI,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12531:6966,adapt,adapted,6966,https://hail.is,https://github.com/hail-is/hail/issues/12531,1,['adapt'],['adapted']
Modifiability,hailctl config is too user unfriendly; it should: error or warn on invalid names and document the list of valid names,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13195:8,config,config,8,https://hail.is,https://github.com/hail-is/hail/issues/13195,1,['config'],['config']
Modifiability,hailctl dataproc does not use specified gcloud configuration,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9587:47,config,configuration,47,https://hail.is,https://github.com/hail-is/hail/issues/9587,1,['config'],['configuration']
Modifiability,"hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3232,config,config,3232,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['config'],['config']
Modifiability,"he Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 19776612}},; 'end': {'locus': {'contig': 'chr20', 'position': 21144633}},; 'includeStart': True,; 'includeEnd': True},; ]; parts_str = json.dumps(parts); vcfs = hl.import_vcfs(gvcfs, parts_str). ## Works fine; print(vcfs); ```; 2) Docker install based on Dockerfile.hail-build (built on top of the hail base image). This does use the gcs connector, and some sa key that has access to the bucket I specified. ```python; gvcfs = ['gs://user-nrru16jaxrwmnzkv5f35xfibg/HG00096.g.vcf.gz',; 'gs://user-nrru16jaxrwmnzkv5f35xfibg/HG00268.g.vcf.gz']; # ...; ## Works fine; print(vcfs); ```. ### TODO:; Manually replicate on a Dataproc cluster. Currently working on this, have a Java gateway closed error during hl.init(), which could be caused by a misspecified JAVA_HOME. So at the moment, I either believe it's either a permission issue, or some Dataproc configuration issue. If you have suggestions, I'd love to hear them. cc @tpoterba",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:3900,config,configuration,3900,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803,1,['config'],['configuration']
Modifiability,"he StorageAdmin role in Google Cloud Storage.; Aborted.; ```. Existing remote tmpdir in wrong region:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp/bar/foo; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: y; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-batch-jigold-oxmmp.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-east1; WARNING: remote temporary directory ""gs://hail-batch-jigold-oxmmp/bar/foo"" is not located in the selected compute region for Batch jobs ""us-east1"".; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp/bar/foo; batch/regions=us-east1; batch/backend=service; query/backend=batch; WARNING: Initialized Hail with warnings! The currently specified configuration will result in additional ingress and egress fees when using Hail Batch.; ```. Existing multiregional bucket:. ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-jigold-test-multi-regional; WARNING: remote temporary directory gs://hail-jigold-test-multi-regional is multi-regional. Using this bucket with the Batch Service will incur addtional ingress and egress fees.; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-jigold-test-multi-regional",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:5312,CONFIG,CONFIGURATION,5312,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,1,['CONFIG'],['CONFIGURATION']
Modifiability,"he old values in an _attempt_ to not break current users. At worst, this would require another `hailctl auth login`. This is the patch I _wanted_ to write. ```patch; From aef878903d9249b542522082cba705eaf26d728a Mon Sep 17 00:00:00 2001; From: Christopher Vittal <christopher.vittal@gmail.com>; Date: Wed, 25 Sep 2019 14:55:42 -0400; Subject: [PATCH] [hailctl] Move default location for hail config directory; MIME-Version: 1.0; Content-Type: text/plain; charset=UTF-8; Content-Transfer-Encoding: 8bit. Now we try, in order:; $XDG_CONFIG_HOME/hail; ~/.config/hail. The XDG Base Directory Specification[1] is a freedesktop spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(se",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:1075,config,config,1075,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['config'],['config']
Modifiability,"he results of each benchmark are outputted as json lines (`.jsonl`) to the file specified by the `--output` pytest arg or stdout. The folder structure should be familiar, resembling our `test/` directory.; I believe this is flexible enough to add `hailtop` benchmarks should we so wish:; ```; pytest.ini - hoisted from `test/` to include benchmark marks; benchmark/; - conftest.py for custom pytest command line args ; - hail/; - confest.py for custom plugin that runs hail benchmarks; - benchmark_*.py hail query benchmark code; - tools/; - shared utilites, including the `@benchmark`; ```; Supporting pytest fixtures required writing a custom plugin to run benchmarks, as using off-the-shelf; solutions like `pytest-benchmark` would forbid method level fixtures like `tmp_path` etc.; The plugin is designed to run ""macro-benchmarks"" (ie long-running tests) and fully supports pytest parameterisation.; For each benchmark, the plugin initialises hail and then repeats (for a number of iterations defined by the pytest mark); acquiring fixtures, timing invocation and tearing-down fixtures, finally stopping hail. It is therefore unsuitable for; microbenchmarks, for which we currenly have none in python. If we add them we'd need to tweak this so support them.; Perhaps an inner loop or something. The process of submitting benchmarks to batch is greatly simplified as the old `Makefile` infrastructure for ; building wheels and docker images etc has been replaced with the script `benchmark_in_batch.py`.; Benchmark images are now based off the `hail-dev` image built in CI (or via the `hail-dev-image` make target). ; Furthermore, you can control the number of ""replicate"" jobs created for each benchmark at the benchmark level using; the `@benchmark(batch_jobs=N)` decotator. Limitations/shortcomings:; - Output is currently jsonl only. Some more human friendly output might be nice on a per iteration basis.; - Old `benchmark-hail` utilities are broken. I'll restore these in subsequent changes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14565:1683,plugin,plugin,1683,https://hail.is,https://github.com/hail-is/hail/pull/14565,1,['plugin'],['plugin']
Modifiability,he.lookupOrCompileCachedFunction$(Backend.scala:122); E 	at is.hail.backend.local.LocalBackend.lookupOrCompileCachedFunction(LocalBackend.scala:73); E 	at is.hail.expr.ir.Compile$.apply(Compile.scala:39); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$5(CompileAndEvaluate.scala:66); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:66); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:19); E 	at is.hail.expr.ir.lowering.LowerDistributedSort$.distributedSort(LowerDistributedSort.scala:158); E 	at is.hail.backend.local.LocalBackend.lowerDistributedSort(LocalBackend.scala:331); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:67); E 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:14); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:167); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.LowerAndExecuteSh,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:9892,Rewrite,RewriteBottomUp,9892,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"her):; @@ -967,7 +969,7 @@ date; ); ; ; -class CreateDatabaseStep(Step):; +class CreateDatabase2Step(Step):; def __init__(self, params, database_name, namespace, migrations, shutdowns, inputs, image):; super().__init__(params); ; @@ -989,12 +991,7 @@ class CreateDatabaseStep(Step):; self.create_database_job = None; self.cleanup_job = None; ; - if params.scope == 'dev':; - self.database_server_config_namespace = params.code.namespace; - else:; - self.database_server_config_namespace = DEFAULT_NAMESPACE; -; - self.cant_create_database = is_test_deployment or params.scope == 'dev'; + self.cant_create_database = is_test_deployment; ; # MySQL user name can be up to 16 characters long before MySQL 5.7.8 (32 after); if self.cant_create_database:; @@ -1005,6 +1002,11 @@ class CreateDatabaseStep(Step):; self._name = database_name; self.admin_username = f'{database_name}-admin'; self.user_username = f'{database_name}-user'; + elif params.scope == 'dev':; + dev_username = params.code.config()['user']; + self._name = f'{dev_username}-{database_name}'; + self.admin_username = f'{dev_username}-{database_name}-admin'; + self.user_username = f'{dev_username}-{database_name}-user'; else:; assert params.scope == 'test'; self._name = f'{params.code.short_str()}-{database_name}-{self.token}'; @@ -1030,7 +1032,7 @@ class CreateDatabaseStep(Step):; @staticmethod; def from_json(params: StepParameters):; json = params.json; - return CreateDatabaseStep(; + return CreateDatabase2Step(; params,; json['databaseName'],; json['namespace'],; @@ -1111,12 +1113,12 @@ EOF; attributes={'name': self.name},; secrets=[; {; - 'namespace': self.database_server_config_namespace,; + 'namespace': self.namespace,; 'name': 'database-server-config',; 'mount_path': '/sql-config',; }; ],; - service_account={'namespace': DEFAULT_NAMESPACE, 'name': 'ci-agent'},; + service_account={'namespace': self.namespace, 'name': 'admin'},; input_files=input_files,; parents=[self.create_passwords_job] if self.create_passwords_jo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13022#issuecomment-1542233600:1671,config,config,1671,https://hail.is,https://github.com/hail-is/hail/pull/13022#issuecomment-1542233600,1,['config'],['config']
Modifiability,"hetools-4.2.1 certifi-2020.12.5 chardet-3.0.4 decorator-4.4.2 dill-0.3.3 fsspec-0.8.7 gcsfs-0.7.2 google-api-core-1.26.1 google-auth-1.27.1 google-auth-oauthlib-0.4.3 google-cloud-core-1.6.0 google-cloud-storage-1.25.0 google-resumable-media-0.5.1 googleapis-common-protos-1.53.0 hail-0.2.64 humanize-1.0.0 hurry.filesize-0.9 idna-2.8 ipython-7.21.0 ipython-genutils-0.2.0 jedi-0.18.0 multidict-5.1.0 nest-asyncio-1.5.1 numpy-1.20.1 oauthlib-3.1.0 packaging-20.9 pandas-1.1.4 parsimonious-0.8.1 parso-0.8.1 pexpect-4.8.0 pickleshare-0.7.5 pillow-8.1.2 prompt-toolkit-3.0.17 protobuf-3.15.6 ptyprocess-0.7.0 py4j-0.10.7 pyasn1-0.4.8 pyasn1-modules-0.2.8 pygments-2.8.1 pyparsing-2.4.7 pyspark-2.4.1 python-dateutil-2.8.1 python-json-logger-0.1.11 pytz-2021.1 requests-2.22.0 requests-oauthlib-1.3.0 rsa-4.7.2 scipy-1.6.1 six-1.15.0 tabulate-0.8.3 tornado-6.1 tqdm-4.42.1 traitlets-5.0.5 typing-extensions-3.7.4.3 urllib3-1.25.11 wcwidth-0.2.5 wrapt-1.12.1 yarl-1.6.3; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]; snafu$ ipython ; Python 3.8.6 (default, Jan 27 2021, 15:42:20) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.21.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-e24d842d2b9a> in <module>; ----> 1 import hail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:9445,sandbox,sandbox,9445,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['sandbox'],['sandbox']
Modifiability,"hold off on this for now, regardless. It'll put me in rebase hell with the agg refactoring I'm doing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8012#issuecomment-580371999:79,refactor,refactoring,79,https://hail.is,https://github.com/hail-is/hail/pull/8012#issuecomment-580371999,1,['refactor'],['refactoring']
Modifiability,"http performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster than aiohttp on JSON serialization but cannot pass other tests. Please decide is it important or not. The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :). P.S.; aiohttp has a place for optimization, we are working on it. There is no single bottleneck, the optimization requires careful improvements in many places, with keeping backward compatibility policy. I don't know the best criteria for choosing. My advice is: look on user API and choose what you like. P.P.S.; `Starlette` has own benefits and compromises as well, but the post is pretty huge already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:1750,config,configure,1750,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040,1,['config'],['configure']
Modifiability,https:// is considered invalid by hailctl config set batch/remote_tmpdir,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13049:42,config,config,42,https://hail.is,https://github.com/hail-is/hail/issues/13049,1,['config'],['config']
Modifiability,"https://github.com/chardet/chardet/commit/eca9558cf7569c1f7689bd66e5aaf965a56e903c""><code>eca9558</code></a> Fix missing black formatting</li>; <li><a href=""https://github.com/chardet/chardet/commit/f1f9d4280e11fb3a9b2d9eaf1827dac9263cb1cb""><code>f1f9d42</code></a> slight increase in performance (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/252"">#252</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/f9ef56cfd6c9b24b9c865eae6dc2285c67ffb75c""><code>f9ef56c</code></a> Use Python-3 super() syntax in Latin1Prober (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/240"">#240</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/c5e5d5a8f1b6e135a8bffd8d60b2f726bb168339""><code>c5e5d5a</code></a> Simple maintenance improvements (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/244"">#244</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/49b8341f507bed68f7d3ff7138bb97047a0e04f0""><code>49b8341</code></a> Configure setuptools using the declarative syntax in setup.cfg (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/239"">#239</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/5c73bfcdf819251d1a1d0de672e34480ebafbe1f""><code>5c73bfc</code></a> Run all pre-commit hooks on pull requests (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/236"">#236</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/chardet/chardet/compare/4.0.0...5.0.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=chardet&package-manager=pip&previous-version=4.0.0&new-version=5.0.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12107:4994,Config,Configure,4994,https://hail.is,https://github.com/hail-is/hail/pull/12107,1,['Config'],['Configure']
Modifiability,"https://github.com/hail-is/hail/pull/13610 added the setup for bokeh and plotly into `hailtop/__init__.py`, which causes `IPython` to be imported when invoking `hailctl`, adding unnecessary startup time to it (observed between 0.210s and 0.400s). This change moves the setup into the `__init__.py` files for the plotting modules, with the resulting state stored in a global variable in `hailtop/__init__.py`. The plotting setup is still run upon invoking `import hail`, as this imports all submodules of hail as well, but the `IPython` import no longer happens when invoking `hailctl`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13648:374,variab,variable,374,https://hail.is,https://github.com/hail-is/hail/pull/13648,1,['variab'],['variable']
Modifiability,"https://github.com/hail-is/hail/pull/5196 introduced a bug where `hl.get_reference('GRCh37')` only works *after* a call to `hl.init()`. ```; (hail) dking@wmb16-359 # ipython; import Python 3.6.7 | packaged by conda-forge | (default, Nov 20 2018, 18:37:09) ; Type 'copyright', 'credits' or 'license' for more information; IPython 6.3.1 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl; In [2]: hl.get_reference('GRCh37'); ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-2-3880f3d97a41> in <module>(); ----> 1 hl.get_reference('GRCh37'). ~/anaconda2/envs/hail/lib/python3.6/site-packages/hail/context.py in get_reference(name); 308 return default_reference(); 309 else:; --> 310 return ReferenceGenome._references[name]; 311 ; 312 . KeyError: 'GRCh37'; ```. This issue is considered fixed when:; - [ ] there is a test that would fail against current master `dcf43490c732`; - [ ] there is a fix for the issue. First reported by Claudia Dastmalchi [on Zulip](https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/KeyError.20for.20get_reference).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5252:341,enhance,enhanced,341,https://hail.is,https://github.com/hail-is/hail/issues/5252,1,['enhance'],['enhanced']
Modifiability,https://github.com/jigold/hail/compare/batch-cloud-agnostic-rewrite-all...danking:batch-cloud-agnostic-rewrite-all,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11029#issuecomment-956427427:60,rewrite,rewrite-all,60,https://hail.is,https://github.com/hail-is/hail/pull/11029#issuecomment-956427427,2,['rewrite'],['rewrite-all']
Modifiability,"https://numpydoc.readthedocs.io/en/latest/format.html. ```; def foo(a, b):; """"""One-line summary. Deprecation warning, if any. Extended summary (a few sentences, clarify functionality). Parameters. Returns. Other parameters, for infrequently used parameters on functions with large parameter lists. Warnings. See also (reference related functions that may be what the user actually wants or needs). Notes section, a possibly lengthy discussion of the algorithm and other gotchas. References for the notes section. Examples; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5304:126,Extend,Extended,126,https://hail.is,https://github.com/hail-is/hail/issues/5304,1,['Extend'],['Extended']
Modifiability,"hub.com/psf/requests/commit/b639e66c816514e40604d46f0088fbceec1a5149""><code>b639e66</code></a> test on py3.12 (<a href=""https://redirect.github.com/psf/requests/issues/6448"">#6448</a>)</li>; <li><a href=""https://github.com/psf/requests/commit/d3d504436ef0c2ac7ec8af13738b04dcc8c694be""><code>d3d5044</code></a> Fixed a small typo (<a href=""https://redirect.github.com/psf/requests/issues/6452"">#6452</a>)</li>; <li><a href=""https://github.com/psf/requests/commit/2ad18e0e10e7d7ecd5384c378f25ec8821a10a29""><code>2ad18e0</code></a> v2.30.0</li>; <li><a href=""https://github.com/psf/requests/commit/f2629e9e3c7ce3c3c8c025bcd8db551101cbc773""><code>f2629e9</code></a> Remove strict parameter (<a href=""https://redirect.github.com/psf/requests/issues/6434"">#6434</a>)</li>; <li><a href=""https://github.com/psf/requests/commit/87d63de8739263bbe17034fba2285c79780da7e8""><code>87d63de</code></a> v2.29.0</li>; <li><a href=""https://github.com/psf/requests/commit/51716c4ef390136b0d4b800ec7665dd5503e64fc""><code>51716c4</code></a> enable the warnings plugin (<a href=""https://redirect.github.com/psf/requests/issues/6416"">#6416</a>)</li>; <li><a href=""https://github.com/psf/requests/commit/a7da1ab3498b10ec3a3582244c94b2845f8a8e71""><code>a7da1ab</code></a> try on ubuntu 22.04 (<a href=""https://redirect.github.com/psf/requests/issues/6418"">#6418</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/psf/requests/compare/v2.28.2...v2.31.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=requests&package-manager=pip&previous-version=2.28.2&new-version=2.31.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dep",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13091:6850,plugin,plugin,6850,https://hail.is,https://github.com/hail-is/hail/pull/13091,6,['plugin'],['plugin']
Modifiability,"i>; </ul>; <p>Version 2.0.3; </code></pre></p>; <p>Released on Jun 27, 2022</p>; <ul>; <li>Fix <code>table</code> plugin</li>; <li>Security fix for CVE-2022-34749</li>; </ul>; <p>Version 2.0.2</p>; <pre><code>; Released on Jan 14, 2022; <p>Fix <code>escape_url</code></p>; <p>Version 2.0.1; </code></pre></p>; <p>Released on Dec 30, 2021</p>; <p>XSS fix for image link syntax.</p>; <p>Version 2.0.0</p>; <pre><code>; Released on Dec 5, 2021; <p>This is the first non-alpha release of mistune v2.</p>; <p>Version 2.0.0rc1; </code></pre></p>; <p>Released on Feb 16, 2021</p>; <p>Version 2.0.0a6</p>; <pre><code>; &lt;/tr&gt;&lt;/table&gt; ; </code></pre>; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/lepture/mistune/commit/b92a5febd4da3d7097a3d2b8d7cac6f5d57ea20c""><code>b92a5fe</code></a> Version bump 2.0.4</li>; <li><a href=""https://github.com/lepture/mistune/commit/98a1c0afc51d4be719cb17401a35e62f46206915""><code>98a1c0a</code></a> Fix url plugin render, <a href=""https://github-redirect.dependabot.com/lepture/mistune/issues/308"">#308</a></li>; <li><a href=""https://github.com/lepture/mistune/commit/979d6d3bfc7d6159f38deb8e751611e4205033f6""><code>979d6d3</code></a> Fix * parsing, <a href=""https://github-redirect.dependabot.com/lepture/mistune/issues/312"">#312</a></li>; <li><a href=""https://github.com/lepture/mistune/commit/f857f048ebb2f6f2bb7ab97dcb7a159172a20649""><code>f857f04</code></a> Trigger GitHub dependency graph</li>; <li><a href=""https://github.com/lepture/mistune/commit/3f422f1e84edae0f39756c45be453ecde534b755""><code>3f422f1</code></a> Version bump 2.0.3</li>; <li><a href=""https://github.com/lepture/mistune/commit/a6d43215132fe4f3d93f8d7e90ba83b16a0838b2""><code>a6d4321</code></a> Fix asteris emphasis regex CVE-2022-34749</li>; <li><a href=""https://github.com/lepture/mistune/commit/5638e460459cb59ceb20e4ce4716c802d4d73c53""><code>5638e46</code></a> Merge pull request <a href=""https://gith",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12066:2263,plugin,plugin,2263,https://hail.is,https://github.com/hail-is/hail/pull/12066,2,['plugin'],['plugin']
Modifiability,"i>; <li><a href=""https://github.com/googleapis/java-storage/commit/cf900f4139f30f89e3c0784467ddc12cc00cf81c""><code>cf900f4</code></a> deps: update dependency org.apache.httpcomponents:httpclient to v4.5.14 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1795"">#1795</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/099a6165722464b46d37206af274a637d3f0461a""><code>099a616</code></a> test(deps): update cross product test dependencies (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1792"">#1792</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/3184d65cce1368c2f39ff85a6ed02cf536902244""><code>3184d65</code></a> deps: update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.19...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/7d6742115bcea6b848a289fdf5c4e4bbafc4cf18""><code>7d67421</code></a> build(deps): update dependency com.google.cloud:google-cloud-shared-config to...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/3bf403e94c035e6cf936e062a1ced2b5221b3912""><code>3bf403e</code></a> deps: update dependency org.apache.httpcomponents:httpcore to v4.4.16 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1786"">#1786</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/googleapis/java-storage/compare/v1.106.0...v2.16.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.google.cloud:google-cloud-storage&package-manager=gradle&previous-version=1.106.0&new-version=2.16.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dep",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12545:12318,config,config,12318,https://hail.is,https://github.com/hail-is/hail/pull/12545,1,['config'],['config']
Modifiability,"i>; <li><a href=""https://github.com/pytest-dev/pytest/commit/a65c47a1a40dad1bb5ce0beb83657d492011a425""><code>a65c47a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9783"">#9783</a> from pytest-dev/backport-9780-to-7.1.x</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/30d995ed25e6d76e85da140663e6253fa5b41935""><code>30d995e</code></a> [pre-commit.ci] auto fixes from pre-commit.com hooks</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/10a14d13181fd69dd0eaf48bf5b3d389de896713""><code>10a14d1</code></a> [7.1.x] testing: fix tests when run under <code>-v</code> or <code>-vv</code></li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/f4cfc596c6574abf68ed49503fd1b8ef1484125d""><code>f4cfc59</code></a> [pre-commit.ci] auto fixes from pre-commit.com hooks</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/f1df8074b3d9185313752cbc29b88d889a1879d9""><code>f1df807</code></a> [7.1.x] config: restore pre-pytest 7.1.0 confcutdir exclusion behavior</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/7d4d1ecde6cdc3feae9ee076ee5aab4e05393fa6""><code>7d4d1ec</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9758"">#9758</a> from pytest-dev/release-7.1.0</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/1dbffcc0b4d822b87ad9f90595ffab6d9beee769""><code>1dbffcc</code></a> [pre-commit.ci] auto fixes from pre-commit.com hooks</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/d53a5fb37194faf63ee5d74606cc883138879bc4""><code>d53a5fb</code></a> Prepare release version 7.1.0</li>; <li>Additional commits viewable in <a href=""https://github.com/pytest-dev/pytest/compare/6.2.5...7.1.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pytest&package-manager=pip&previous-version=6.2.5&new-version",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11619:5589,config,config,5589,https://hail.is,https://github.com/hail-is/hail/pull/11619,1,['config'],['config']
Modifiability,"ials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaults. File paths should be determined by the location, which is loaded from HAIL_DEPLOY_CONFIG_FILE. If that isn't set, I look in ~/.hail/deploy_config.json, and if that doesn't exist, use external/default. All other configuration files are determined by the location: the tokens file is in ~/.hail/tokens.json for external, in /user-tokens/tokens.json for k8s, etc. What remains:; - what a `hailctl dev config` to set the (local) deploy config for switching between default and dev namespaces.; - salt session IDs in the database; - dev oauth2 key; - add `dev de",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:1607,config,config,1607,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251,1,['config'],['config']
Modifiability,"iases.; The main breaking change from v5.x is that ""mapping_types"" have been removed, so the ; kt.export_elasticsearch `mapping_type` arg isn't needed anymore (https://www.elastic.co/guide/en/elasticsearch/reference/6.0/breaking-changes-6.0.html). ### Hail version:; 0.1, 0.2. ### What you did:. kt.export_elasticsearch. ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""/hail-elasticsearch-pipelines/hail_scripts/v01/load_clinvar_to_es_pipeline.py"", line 112, in <module>; export_globals_to_index_meta=True,; File ""/hail-elasticsearch-pipelines/hail_scripts/v01/utils/elasticsearch_client.py"", line 142, in export_vds_to_elasticsearch; verbose=verbose); File ""/hail-elasticsearch-pipelines/hail_scripts/v01/utils/elasticsearch_client.py"", line 287, in export_kt_to_elasticsearch; kt.export_elasticsearch(self._host, int(self._port), index_name, index_type_name, block_size, config=elasticsearch_config); File ""<decorator-gen-143>"", line 2, in export_elasticsearch; File ""/hail/build/distributions/hail-python.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: EsHadoopIllegalArgumentException: Unsupported/Unknown Elasticsearch version 6.0.0. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 20050, localhost): org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'; 	at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:247); 	at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:545); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4138:1111,config,config,1111,https://hail.is,https://github.com/hail-is/hail/issues/4138,1,['config'],['config']
Modifiability,"ice] slightly more useful error message when socket dies; - [ ] (@tpoterba) f79c4023cf [shuffler] if we have an ExecuteContext, use it; - [x] (@daniel-goldstein,fyi: @tpoterba) 259f70dd25 [query-service] JSON Logging; - [ ] (@catoverdrive) f5c3ffcbd1 [query-service] pervasively retry all idempotent operations; - [ ] (@tpoterba) 507db4b468 [hail] fix using; - [x] (@jigold) c32a253bb9 [query] when testing, ensure our thread has an event loop; - [ ] (@tpoterba) 110469c2da [query][lir] avoid dumping massive classes onto stderr; - [ ] (@tpoterba) e4aa1c15fe [query] do not print misleading log in RegionPool.finalizer; - [x] (trivial) 33eab9a80e [query-service] better logging information; - [ ] (@catoverdrive) e358e8feeb [query-service] remove race conditions in user management; - [ ] (@tpoterba) b60cb2bae5 [lir] make LIR genName thread-safe; - [ ] (@catoverdrive) 2d82e5faf5 [query-service] send a token for job identifiability; - [x] (@daniel-goldstein) fd78caedcb [query-service] reduce image size by ~2GB; - [ ] (@catoverdrive) 00d1840421 [query-service] retry CLOSE, CLOSED (i.e. connection dropped); - [ ] (@catoverdrive) c985d3e3de [query-service] remove old test code; - [ ] (@catoverdrive) 0a5dc8c651 [query-service] all operations are idempotent; - [ ] (@cseed) 6d02d173fa [make] fix config.mk; - [x] (@daniel-goldstein) d21df54e63 [devbin] teach devbin/functions.py about multiple containers; - [x] (@jigold) 38878f7874 [batch] remove batch_worker_image false dependency on service_base_image; - [x] (@daniel-goldstein) f03defab3d [java-services] avoid NPEs in isTransientError; - [x] (@jigold) e535bdc00d [dependencies] upgrade gcsfs to 0.7.2 to fix GoogleFS rmtree issue; - [x] (@cseed) 743b5ba62f [query-service] enable auto-scaling for PR and dev deploy; - [ ] (@cseed) 6a52d45f6f [query-service] retry EndOfStream errors from java; - [ ] (@jigold) 5853a0bec4 [batch] remove restrictions on PR and dev batch pools; - [ ] (@cseed) 035b19642a [query-service] resolve last two issues",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10100:3597,config,config,3597,https://hail.is,https://github.com/hail-is/hail/pull/10100,1,['config'],['config']
Modifiability,"ield = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the constructor. Because if not, we can have one loadString function. It is confusing (to me), to have multiple versions of a function that differ so greatly in semantics.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:1178,extend,extends,1178,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437,1,['extend'],['extends']
Modifiability,"ig; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2648,config,config,2648,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['config'],['config']
Modifiability,"iguration system. Changes in this PR:. 1. Include JVM output in error logs when the JVM crashes. This should help debugging of JVM crashing in production until the JVM logs are shown on a per-worker page. 2. JVMEntryway is now a real gradle project. I need to compile against log4j, and I didn't want to do that by hand with `javac`. Ignore gradlew, gradlew.bat, and gradle/wrapper, they're programmatically generated by gradle. 3. Add logging to JVMEntryway. JVMEntryway now logs its arguments into the QoB job log. I also log exceptions from the main thread or the cancel thread into the job log. We also flush the logs after the main thread completes, the cancel thread completes, and when the try-catch exits. This should ensure that regardless of what goes wrong (even if both threads fail to start) we at least see the arguments that the JVMEntryway received. 4. Use log4j2 programmatic reconfiguration after every job. This restores log4j2 to well enough working order that, *if you do not try to reconfigure it using log4j1 programmatic configuration*, logs will work. All old versions of Hail use log4j1 programmatic configuration. As a result, **all old versions of Hail will still have no logs**. However, new versions of Hail will log correctly even if an old version of Hail used the JVM before it. 5. `QoBAppender`. This is how we always should have done logging. A custom appender which we can flush and then redirect to a new file at our whim. I followed the log4j2 best practices for creating a new appender. All these annotations, factory methods, and managers are The Right Way, for better or worse. If we ever ban old versions of Hail from the cluster, then we can also eliminate the log4j2 reconfiguration. New versions of Hail work fine without any runtime log configuration (thanks to `QoBAppender`). I would like to eliminate reconfiguration because log4j2 reconfiguration leaves around oprhaned appenders and appender managers. Maybe I'm implementing the Appender or Appender ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12941:1277,config,configuration,1277,https://hail.is,https://github.com/hail-is/hail/pull/12941,1,['config'],['configuration']
Modifiability,il.expr.ir.EmitFunctionBuilder.resultWithIndex(EmitClassBuilder.scala:1078); 	at is.hail.expr.ir.Emit.$anonfun$emitI$238(Emit.scala:2400); 	at is.hail.expr.ir.IEmitCodeGen.map(Emit.scala:336); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:2341); 	at is.hail.expr.ir.Emit.emitI$1(Emit.scala:630); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$26(Emit.scala:748); 	at is.hail.expr.ir.TableTextFinalizer.writeMetadata(TableWriter.scala:552); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:748); 	at is.hail.expr.ir.Emit.emitVoid$1(Emit.scala:627); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$5(Emit.scala:644); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$5$adapted(Emit.scala:644); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$4(Emit.scala:644); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$4$adapted(Emit.scala:643); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:1011); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$3(Emit.scala:643); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$3$adapted(Emit.scala:641); 	at scala.collection.Iterator.foreach(Iterator.scala:943); 	at scala.collection.Iterator.foreach$(Iterator.scala:943); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:641); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3(Emit.scala:70); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3$adapted(Emit.scala:68); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:1011); 	at is.hail.expr,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12531:6604,adapt,adapted,6604,https://hail.is,https://github.com/hail-is/hail/issues/12531,1,['adapt'],['adapted']
Modifiability,il.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:4413,Rewrite,RewriteBottomUp,4413,https://hail.is,https://github.com/hail-is/hail/issues/6458,3,"['Rewrite', 'rewrite']","['RewriteBottomUp', 'rewrite']"
Modifiability,"il/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CO",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2228,config,config,2228,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['config'],['config']
Modifiability,ileAndEvaluate.scala:66); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:66); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:19); E 	at is.hail.expr.ir.lowering.LowerDistributedSort$.distributedSort(LowerDistributedSort.scala:158); E 	at is.hail.backend.local.LocalBackend.lowerDistributedSort(LocalBackend.scala:331); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:67); E 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:14); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:167); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringPass.scala:161); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.mutable.Re,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:10161,Rewrite,RewriteBottomUp,10161,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Rewrite'],['RewriteBottomUp']
Modifiability,ilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.Comp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:4486,Rewrite,RewriteBottomUp,4486,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Rewrite'],['RewriteBottomUp']
Modifiability,iltop/auth/flow.py'; adding 'hailtop/auth/sql_config.py'; adding 'hailtop/auth/tokens.py'; adding 'hailtop/batch/__init__.py'; adding 'hailtop/batch/backend.py'; adding 'hailtop/batch/batch.py'; adding 'hailtop/batch/batch_pool_executor.py'; adding 'hailtop/batch/conftest.py'; adding 'hailtop/batch/docker.py'; adding 'hailtop/batch/exceptions.py'; adding 'hailtop/batch/globals.py'; adding 'hailtop/batch/hail_genetics_images.py'; adding 'hailtop/batch/job.py'; adding 'hailtop/batch/resource.py'; adding 'hailtop/batch/utils.py'; adding 'hailtop/batch_client/__init__.py'; adding 'hailtop/batch_client/aioclient.py'; adding 'hailtop/batch_client/client.py'; adding 'hailtop/batch_client/globals.py'; adding 'hailtop/batch_client/parse.py'; adding 'hailtop/batch_client/types.py'; adding 'hailtop/cleanup_gcr/__init__.py'; adding 'hailtop/cleanup_gcr/__main__.py'; adding 'hailtop/config/__init__.py'; adding 'hailtop/config/deploy_config.py'; adding 'hailtop/config/user_config.py'; adding 'hailtop/config/variables.py'; adding 'hailtop/fs/__init__.py'; adding 'hailtop/fs/fs.py'; adding 'hailtop/fs/fs_utils.py'; adding 'hailtop/fs/router_fs.py'; adding 'hailtop/fs/stat_result.py'; adding 'hailtop/hailctl/__init__.py'; adding 'hailtop/hailctl/__main__.py'; adding 'hailtop/hailctl/deploy.yaml'; adding 'hailtop/hailctl/describe.py'; adding 'hailtop/hailctl/auth/__init__.py'; adding 'hailtop/hailctl/auth/cli.py'; adding 'hailtop/hailctl/auth/create_user.py'; adding 'hailtop/hailctl/auth/delete_user.py'; adding 'hailtop/hailctl/auth/login.py'; adding 'hailtop/hailctl/batch/__init__.py'; adding 'hailtop/hailctl/batch/batch_cli_utils.py'; adding 'hailtop/hailctl/batch/cli.py'; adding 'hailtop/hailctl/batch/initialize.py'; adding 'hailtop/hailctl/batch/list_batches.py'; adding 'hailtop/hailctl/batch/submit.py'; adding 'hailtop/hailctl/batch/utils.py'; adding 'hailtop/hailctl/batch/billing/__init__.py'; adding 'hailtop/hailctl/batch/billing/cli.py'; adding 'hailtop/hailctl/config/__init_,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:26472,config,config,26472,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,2,"['config', 'variab']","['config', 'variables']"
Modifiability,"image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sure how much mileage we can get out of layer squashing. You can take a look at a service-base build [here](https://gist.github.com/danking/830af0688970c176ff25dbfeb4b222e7). Note the stdout comes first and then I `cat` the ""trace"" file which is a very weirdly formatted series of JSON objects that give more detailed information. You can clean it up a bit with `jq -c '(.Logs + .Statuses + .Vertexes) | add | {Timestamp, ID, Name}'`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10502:1763,layers,layers,1763,https://hail.is,https://github.com/hail-is/hail/pull/10502,1,['layers'],['layers']
Modifiability,improve variable names in VariantPackage object,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1326:8,variab,variable,8,https://hail.is,https://github.com/hail-is/hail/pull/1326,1,['variab'],['variable']
Modifiability,"in.doFilter(Filter.java:79); 	at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83); 	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82); 	at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:822); 	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79); 	at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:794); 	at sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:199); 	at sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:544); 	at sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:509); 	at java.lang.Thread.run(Thread.java:750). java.util.NoSuchElementException: Ref with name __iruid_1834 could not be resolved in env BindingEnv((__iruid_1832 -> struct{},__iruid_2157 -> struct{}),None,None,()); 	at is.hail.expr.ir.TypeCheck$.checkSingleNode(TypeCheck.scala:110); 	at is.hail.expr.ir.TypeCheck$.$anonfun$check$4(TypeCheck.scala:37); 	at is.hail.expr.ir.TypeCheck$.$anonfun$check$4$adapted(TypeCheck.scala:29); 	at is.hail.utils.StackSafe$StackFrame.$anonfun$map$1(StackSafe.scala:30); 	at is.hail.utils.StackSafe$StackFrame.flatMap(StackSafe.scala:21); 	at is.hail.utils.StackSafe$StackFrame.map(StackSafe.scala:30); 	at is.hail.expr.ir.TypeCheck$.check(TypeCheck.scala:29); 	at is.hail.expr.ir.TypeCheck$.$anonfun$check$2(TypeCheck.scala:31); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:29); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(Arra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14245:18667,adapt,adapted,18667,https://hail.is,https://github.com/hail-is/hail/issues/14245,1,['adapt'],['adapted']
Modifiability,"in=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp/foo; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. User does not give permissions to existing remote tmpdir:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: n ; WARNING: Please verify service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com has the role ""roles/storage.objectAdmin"" or both ""roles/storage.objectViewer"" and ""roles/storage.objectCreator"" roles for bucket hail-batch-jigold-oxmmp.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-central1; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. Not existing user-specified remote tmpdir:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://my-bucket/foo/bar; ERROR: You do not have sufficient permissions to get information about bucket my-bucket or it does not exist. If the bucket exists, ask a project administrator to give you the permission ""storage.buckets.get"" or assign you the StorageAdmin role in Google Cloud Storage.; Aborted.; ```. Existing remote tmpdir in wrong region:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucke",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:3569,CONFIG,CONFIGURATION,3569,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,1,['CONFIG'],['CONFIGURATION']
Modifiability,"ince 1.20+. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/107527"">kubernetes/kubernetes#107527</a>, <a href=""https://github.com/wojtek-t""><code>@​wojtek-t</code></a>)</li>; <li>Kubelet external Credential Provider feature is moved to Beta. Credential Provider Plugin and Credential Provider Config API's updated from v1alpha1 to v1beta1 with no API changes. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108847"">kubernetes/kubernetes#108847</a>, <a href=""https://github.com/adisky""><code>@​adisky</code></a>)</li>; <li>Make STS available replicas optional again. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/109241"">kubernetes/kubernetes#109241</a>, <a href=""https://github.com/ravisantoshgudimetla""><code>@​ravisantoshgudimetla</code></a>)</li>; <li>MaxUnavailable for StatefulSets, allows faster RollingUpdate by taking down more than 1 pod at a time. The number of pods you want to take down during a RollingUpdate is configurable using maxUnavailable parameter. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/82162"">kubernetes/kubernetes#82162</a>, <a href=""https://github.com/krmayankk""><code>@​krmayankk</code></a>)</li>; <li>Non-graceful node shutdown handling is enabled for stateful workload failovers (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108486"">kubernetes/kubernetes#108486</a>, <a href=""https://github.com/sonasingh46""><code>@​sonasingh46</code></a>)</li>; <li>Omit enum declarations from the static openapi file captured at <a href=""https://git.k8s.io/kubernetes/api/openapi-spec"">https://git.k8s.io/kubernetes/api/openapi-spec</a>. This file is used to generate API clients, and use of enums in those generated clients (rather than strings) can break forward compatibility with additional future values in those fields. See <a href=""https://issue.k8s.io/109177"">https://issue.k8s.io/109177</a> for details. (<a href=""https://gi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12196:9903,config,configurable,9903,https://hail.is,https://github.com/hail-is/hail/pull/12196,1,['config'],['configurable']
Modifiability,"ing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-overview-of-sandboxed-container-technologies/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6720:1753,Sandbox,Sandbox,1753,https://hail.is,https://github.com/hail-is/hail/issues/6720,3,"['Sandbox', 'sandbox']","['Sandbox', 'Sandboxed', 'sandboxed-container-technologies']"
Modifiability,"ingleton subscriptions; e.g., <code>nptyping.Float[64]</code></li>; <li>Resolve forward references</li>; <li>Expand and better handle <code>TypeVar</code></li>; <li>Add intershpinx reference link for <code>...</code> to <code>Ellipsis</code> (as is just an alias)</li>; </ul>; <h2>1.15.3</h2>; <ul>; <li>Prevents reaching inner blocks that contains <code>if TYPE_CHECKING</code></li>; </ul>; <h2>1.15.2</h2>; <ul>; <li>Log a warning instead of crashing when a type guard import fails to resolve</li>; <li>When resolving type guard imports if the target module does not have source code (such is the case for C-extension; modules) do nothing instead of crashing</li>; </ul>; <h2>1.15.1</h2>; <ul>; <li>Fix <code>fully_qualified</code> should be <code>typehints_fully_qualified</code></li>; </ul>; <h2>1.15.0</h2>; <ul>; <li>Resolve type guard imports before evaluating annotations for objects</li>; <li>Remove <code>set_type_checking_flag</code> flag as this is now done by default</li>; <li>Fix crash when the <code>inspect</code> module returns an invalid python syntax source</li>; <li>Made formatting function configurable using the option <code>typehints_formatter</code></li>; </ul>; <h2>1.14.1</h2>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/commit/73aa9b6aea40720ca270b1107c1980b909943cb3""><code>73aa9b6</code></a> Fix mock imports on guarded imports (<a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/issues/225"">#225</a>)</li>; <li><a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/commit/4d5867d5a235040b3e7d3373a56c5b2b580db7b7""><code>4d5867d</code></a> Handle UnionType (<a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/issues/221"">#221</a>)</li>; <li><a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/commit/13ca2b458b0ee9c8d1c980b6a5e97a6ee78f46c7""><code>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11773:2157,config,configurable,2157,https://hail.is,https://github.com/hail-is/hail/pull/11773,1,['config'],['configurable']
Modifiability,"instead we get:; ```; Python 3.7.5 (default, Nov 1 2019, 02:16:32) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: %%time ; ...: import os ; ...: import sys ; ...: ; ...: from hailtop import pipeline as pl ; ...: ; ...: BENCHMARK_IMAGE = 'ubuntu:18.04' ; ...: p = pl.Pipeline(name='download_data_fail', ; ...: backend=pl.BatchBackend(billing_project='hail'), ; ...: default_image=BENCHMARK_IMAGE, ; ...: default_cpu=1) ; ...: ; ...: for i in range(1): ; ...: t = p.new_task(f'replicate_{i}') ; ...: t.command('echo ' + 'a' * 1000) ; ...: p.run(wait=False) ; aenter; aexit; submit jobs timing {'create': 264, 'total': 265}; Traceback (most recent call last):; File ""<timed exec>"", line 15, in <module>; File ""/Users/dking/projects/hail/hail/python/hailtop/pipeline/pipeline.py"", line 395, in run; self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); File ""/Users/dking/projects/hail/hail/python/hailtop/pipeline/backend.py"", line 329, in _run; batch = batch.submit(); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/client.py"", line 164, in submit; async_batch = async_to_blocking(self._async_builder.submit()); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/client.py"", line 7, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py"", line 579, in run_until_complete; return future.result(); File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/futures.py"", line 178, in result; raise self._exception; File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py"", line 249, in __step; result = coro.send(None); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/aioclient.py"", line 500, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7839:152,enhance,enhanced,152,https://hail.is,https://github.com/hail-is/hail/issues/7839,1,['enhance'],['enhanced']
Modifiability,"invar_ht = clinvar_ht.key_by(clinvar_ht.locus, clinvar_ht.alleles); clinvar_mt = hl.MatrixTable.from_rows_table(clinvar_ht); clinvar_mt = split_multi_dynamic(clinvar_mt, left_aligned = False); clinvar_mt = clinvar_mt.repartition(100); clinvar_mt = clinvar_mt.key_rows_by(clinvar_mt.locus, clinvar_mt.alleles); clinvar_vep = hl.vep(clinvar_mt, vep_config); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; In [6]: clinvar_vep = hl.vep(clinvar_mt, vep_config); 2018-03-08 02:46:03 Hail: WARN: property `hail.vep.assembly' not specified. Setting to GRCh37; [Stage 22:======================================================>(99 + 1) / 100]2018-03-08 02:54:37 Hail: INFO: vep: annotated 243477 variants; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-6-a229f1f9de81> in <module>(); ----> 1 clinvar_vep = hl.vep(clinvar_mt, vep_config). <decorator-gen-843> in vep(dataset, config, block_size, name, csq). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/typecheck/check.py in _typecheck(__orig_func__, *args, **kwargs); 491 def _typecheck(__orig_func__, *args, **kwargs):; 492 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=False); --> 493 return __orig_func__(*args_, **kwargs_); 494; 495 return decorator(_typecheck). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/methods/qc.py in vep(dataset, config, block_size, name, csq); 545; 546 require_row_key_variant(dataset, 'vep'); --> 547 return MatrixTable(Env.hail().methods.VEP.apply(dataset._jvds, config, 'va.`{}`'.format(name), csq, block_size)); 548; 549. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_ar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3099:1528,config,config,1528,https://hail.is,https://github.com/hail-is/hail/issues/3099,1,['config'],['config']
Modifiability,"inx-doc/sphinx/issues/10535"">#10535</a> from AA-Turner/css-nav-contents</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/709602437df850d5538a4fe899a50625c01a0f80""><code>7096024</code></a> Update CHANGES for PR <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10539"">#10539</a></li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/d0452276689bfb5b97ca7a3469e1afb505895cdd""><code>d045227</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10539"">#10539</a> from AA-Turner/fix-inherited-attrs</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/29edce9243046962f5f024d510315133448dd3e1""><code>29edce9</code></a> test: Add testcase for autodoc_inherit_docstring and attributes (refs: <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10539"">#10539</a>)</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/3956cf2249d27ed63e8381c07dfde36f6c96f78f""><code>3956cf2</code></a> Fix documenting inherited attributes</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/27f05328d0369ad0db85c27935d52fdadf020f6b""><code>27f0532</code></a> Move <code>aside.topic</code> into the conditional blocks</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/5806f0af2788db40661d62e5e88c2c1560ae46b6""><code>5806f0a</code></a> Add <code>nav.contents</code> everywhere that <code>div.topic</code> is used</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/8da2efb1d71ab2d384ddc90cf4fdebe5d18e91cd""><code>8da2efb</code></a> Rename CSS files to CSS template files</li>; <li>Additional commits viewable in <a href=""https://github.com/sphinx-doc/sphinx/compare/v3.5.4...v5.0.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sphinx&package-manager=pip&previous-version=3.5.4&new-version=5.0.2)](https://docs.github.com/en/github/man",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11925:5552,inherit,inherited,5552,https://hail.is,https://github.com/hail-is/hail/pull/11925,1,['inherit'],['inherited']
Modifiability,"ion strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:2322,config,configuration,2322,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243,4,['config'],['configuration']
Modifiability,"ion: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.project-.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.gbsc-project.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.u",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290:2373,adapt,adapted,2373,https://hail.is,https://github.com/hail-is/hail/issues/12290,1,['adapt'],['adapted']
Modifiability,ions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:11623,Config,Configuration,11623,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['Config'],['Configuration']
Modifiability,"irect.dependabot.com/pytest-dev/pytest/issues/9355"">#9355</a>: Fixed error message prints function decorators when using assert in Python 3.8 and above.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9396"">#9396</a>: Ensure <code>pytest.Config.inifile</code>{.interpreted-text role=&quot;attr&quot;} is available during the <code>pytest_cmdline_main &lt;_pytest.hookspec.pytest_cmdline_main&gt;</code>{.interpreted-text role=&quot;func&quot;} hook (regression during <code>7.0.0rc1</code>).</li>; </ul>; <h2>Improved Documentation</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9404"">#9404</a>: Added extra documentation on alternatives to common misuses of [pytest.warns(None)]{.title-ref} ahead of its deprecation.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9505"">#9505</a>: Clarify where the configuration files are located. To avoid confusions documentation mentions; that configuration file is located in the root of the repository.</li>; </ul>; <h2>Trivial/Internal Changes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9521"">#9521</a>: Add test coverage to assertion rewrite path.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pytest-dev/pytest/commit/3f12087fe0d86a319216653b08b66a96d400bee2""><code>3f12087</code></a> [pre-commit.ci] auto fixes from pre-commit.com hooks</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/bc3021cdfd76507aa3d9e278bd885da9bc1907b2""><code>bc3021c</code></a> Prepare release version 7.0.1</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/591d476f14e3e83d90fbea75d326a93c5e368708""><code>591d476</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9673"">#9673</a> from nicoddemus/backport-9511</li>; <li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11516:3594,config,configuration,3594,https://hail.is,https://github.com/hail-is/hail/pull/11516,3,['config'],['configuration']
Modifiability,"ires additional configuration options in Spark referring; to the path to the Hail Python module directory HAIL_DIR,; e.g. /path/to/python/site-packages/hail:; spark.jars=HAIL_DIR/backend/hail-all-spark.jar; spark.driver.extraClassPath=HAIL_DIR/backend/hail-all-spark.jar; spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 3.3.2-amzn-0.1; SparkUI available at http://ip-192-168-110-167.ap-southeast-1.compute.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.124-e739a95489e4; LOGGING: writing to /mnt/tmp/hail/hail/hail-20231025-0729-0.2.124-e739a95489e4.log; >>> mt = hl.balding_nichols_model(n_populations=3, n_samples=500, n_variants=1_000); 2023-10-25 07:29:48.283 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 500 samples, and 1000 variants...; >>> mt.count(); (1000, 500); ```. it seems working in command line using pyspark !. I need to test on jupyter notebook now... FYI the pyspark configs. ```sh ; - Classification: spark-defaults; ConfigurationProperties:; spark.jars: /opt/hail/backend/hail-all-spark.jar; spark.driver.extraClassPath: /opt/hail/backend/hail-all-spark.jar:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar; spark.executor.extraClassPath: /opt/hail/backend/hail-all-spark.jar:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/u",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949:3362,config,configs,3362,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949,1,['config'],['configs']
Modifiability,"irst commit being checking in package-lock.json); - [ ] Deal with cross-origin tracking issues in Safari. This may require using the ""custom domains"" feature of auth0, paid. Workaround could be to poll/websocket request to api server to refresh tokens. . To run:; ```sh; cd packages/web-client; docker build . -t blah; docker run --env-file=env-example -p 3000:3000 blah npm run start; ```; then navigate to `http://localhost:3000`. \# lines: Most come from the package.json.lock files. These maintain versioning information.; * [It is recommended to check in .lock files]( https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5); * They're huge, sorry.; # Documentation; ### JS; https://javascript.info. We use the subset termed [ES2018](https://flaviocopes.com/es2018/). Compatibility across all browsers is ensured by [transpilation using BabelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. Among those we *may* want to care about, [IE11 has ~2% global use (more if only desktop browsers)](https://caniuse.com/#feat=flexbox); ### NodeJS; We use 10.15. [This is the latest LTS release](https://nodejs.org/en/download/). ### Versioning / dependency management; TL;DR: `npm`; ```sh; npm init # creates a package.json file, which tracks dependencies; npm install next react react-dom # install 3 packages and save them to the dependencies property; ```. #### package.json; The file that tracks dependencies, and their semantic versioning numbers. Shape:; ```json; {; ""name"": ""hail-web-client"",; ""version"": ""0.2.0"",; ""scripts"": {; ""dev"": ""next"",; ""build"": ""next build"",; ""start"": ""NODE_ENV=production SSL=true next start""; },; ""author"": ""Hail Team"",; ""license"": ""MIT"",; ""dependencies"": {; ""next"": ""^7.0.2-canary.50"",;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:1856,config,configurable,1856,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['config'],['configurable']
Modifiability,"is only one IP address, and only one place to open connections. The load-balancer doesn't have the information to actually load-balance once we have a functioning connection pool. This can lead to really unbalanced scenarios when preemptible pods come and go. This leads to our second goal: instead of routing all requests through kube-proxy, use Kubernetes Headless Services to expose all pod IPs underlying a Service so that our proxies can properly load-balance across persistent connections. ## Solution. This PR addresses the two goals outlined above and does so through using Envoy, a load-balancer/proxy that is well-suited to this sort of highly-dynamic cluster configuration. Envoy does not have the constraint that all upstream services must be available at start-time, and has a very convenient API for updating the cluster configuration without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as follows:. 1. Envoy-based gateways and internal-gateways will load their routing configuration from a Kubernetes ConfigMap, which they watch for changes and reconcile their configuration when the ConfigMap changes. The ConfigMap can be populated with a manual deploy and is populated from the beginning with production routes (i.e. batch.hail.is gets routed to batch.default); 2. When running CI, CI will regularly update the ConfigMap with additional routes based on which internal namespaces (dev and PR) are currently active. This requires relatively small changes to CI to track active namespaces but overall is a pretty small change. Note that this does not introduce a dependency on CI to support production traffic, only development traffic.; 3. Deployments that run more than 1 replica (but really can be all of them) are run behind Headless Services, which expose the underl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:3990,config,configuration,3990,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['config'],['configuration']
Modifiability,is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.BackendUtils.$anonfun$collectDArray$15(BackendUtils.scala:90) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Worker$.$anonfun$main$12(Worker.scala:167) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]; 		at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Worker$.$anonfun$main$11(Worker.scala:166) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Worker$.$anonfun$main$11$adapted(Worker.scala:164) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.utils.package$.using(package.scala:637) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Worker$.main(Worker.scala:164) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Main$.main(Main.scala:14) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Main.main(Main.scala) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:23918,adapt,adapted,23918,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['adapt'],['adapted']
Modifiability,is.hail.relocated.com.google.cloud.storage.StorageImpl.writer(StorageImpl.java:674) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.relocated.com.google.cloud.storage.StorageImpl.writer(StorageImpl.java:95) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$doHandlingRequesterPays$2(GoogleStorageFS.scala:300) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$doHandlingRequesterPays$1(GoogleStorageFS.scala:300) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$doHandlingRequesterPays$1$adapted(GoogleStorageFS.scala:299) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.io.fs.GoogleStorageFS.is$hail$io$fs$GoogleStorageFS$$handleRequesterPays(GoogleStorageFS.scala:181) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:304) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$close$1(GoogleStorageFS.scala:326) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]; 		at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:20824,adapt,adapted,20824,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['adapt'],['adapted']
Modifiability,"issues/3217"">#3217</a>)</li>; </ul>; <h3>Configuration</h3>; <ul>; <li>Black now uses the presence of debug f-strings to detect target version (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3215"">#3215</a>)</li>; <li>Fix misdetection of project root and verbose logging of sources in cases involving <code>--stdin-filename</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3216"">#3216</a>)</li>; <li>Immediate <code>.gitignore</code> files in source directories given on the command line are now also respected, previously only <code>.gitignore</code> files in the project root and automatically discovered directories were respected (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3237"">#3237</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Recommend using BlackConnect in IntelliJ IDEs (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3150"">#3150</a>)</li>; </ul>; <h3>Integrations</h3>; <ul>; <li>Vim plugin: prefix messages with <code>Black: </code> so it's clear they come from Black (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3194"">#3194</a>)</li>; <li>Docker: changed to a /opt/venv installation + added to PATH to be available to non-root users (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3202"">#3202</a>)</li>; </ul>; <h3>Output</h3>; <ul>; <li>Change from deprecated <code>asyncio.get_event_loop()</code> to create our event loop which removes DeprecationWarning (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3164"">#3164</a>)</li>; <li>Remove logging from internal <code>blib2to3</code> library since it regularly emits error logs about failed caching that can and should be ignored (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3193"">#3193</a>)</li>; </ul>; <h3>Parser</h3>; <ul>; <li>Type comments are now included in the AST equivalence check consistently so accidental deletion raises an error. Though type comments can't",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12174:3785,plugin,plugin,3785,https://hail.is,https://github.com/hail-is/hail/pull/12174,1,['plugin'],['plugin']
Modifiability,"it is stock hail, but I'm running with a new GRCh38 VEP config + files from; gs://hail-common/vep/vep/GRCh38/vep85-GRCh38-init.sh. though I've run it on another VCF and didn't get that error. ; I'll rerun to make sure it wasn't a misconfiguration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301756372:56,config,config,56,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301756372,1,['config'],['config']
Modifiability,"ithout a palette <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7797"">#7797</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use palette when loading ICO images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7798"">#7798</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use consistent arguments for load_read and load_seek <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7713"">#7713</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Turn off nullability warnings for macOS SDK <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7827"">#7827</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fix shift-sign issue in Convert.c <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7838"">#7838</a> [<a href=""https://github.com/r-barnes""><code>@​r-barnes</code></a>]</li>; <li>winbuild: Refactor dependency versions into constants <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7843"">#7843</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></a>]</li>; <li>Build macOS arm64 wheels natively <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7852"">#7852</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fixed typo <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7855"">#7855</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Open 16-bit grayscale PNGs as I;16 <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7849"">#7849</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Handle truncated chunks at the end of PNG images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7709"">#7709</a> [<a href=""https://github.com/lajiyuan""><code>@​lajiyuan</code></a>]</li>; <li>Match mask size to pasted image si",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14439:7650,Refactor,Refactor,7650,https://hail.is,https://github.com/hail-is/hail/pull/14439,3,['Refactor'],['Refactor']
Modifiability,"ithub-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.5</h2>; <p>Maintenance:</p>; <ul>; <li>Publish signed artifacts to Gradle plugin portal</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.4</h2>; <p>Bug fixes:</p>; <ul>; <li>Fix deadlock in <code>DownloadExtension</code> if <code>max-workers</code> equals 1 (thanks to <a href=""https://github.com/beatbrot""><code>@​beatbrot</code></a> for spotting this, see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/205"">#205</a>)</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/1b5d69760d19cb7f88cbc837ee46456c494c0696""><code>1b5d697</code></a> Bump up version number to 5.2.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/7d6de83037ca41cd2f2f31830b43e43720e45b3a""><code>7d6de83</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:2402,plugin,plugin,2402,https://hail.is,https://github.com/hail-is/hail/pull/12332,1,['plugin'],['plugin']
Modifiability,"ithub-redirect.dependabot.com/pytest-dev/pytest-metadata/issues/50"">#50</a> from BeyondEvil/release-v2.0.2</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/f0b5503452f922a84faa213224a4970c57d8a654""><code>f0b5503</code></a> Release v2.0.2</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/ff493afce81b1bbe7a2f866cd27510e5d9b8feef""><code>ff493af</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-metadata/issues/47"">#47</a> from pytest-dev/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/4591db1fa5546ff372ae95155caed99ce8dc4842""><code>4591db1</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/0414bb9f81cc1856ea021504eecd22d202462f1d""><code>0414bb9</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-metadata/issues/46"">#46</a> from pytest-dev/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/025be8999a22ae395b0e2b8ae4e7c9fa2334f874""><code>025be89</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/429840f4de26276560961929f21aab79ed305875""><code>429840f</code></a> Avoid running nightly on forks</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/c1968f39609978ec9c6a4bcf91c37c6164483f04""><code>c1968f3</code></a> Fix nightly</li>; <li>See full diff in <a href=""https://github.com/pytest-dev/pytest-metadata/compare/v2.0.1...v2.0.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pytest-metadata&package-manager=pip&previous-version=2.0.1&new-version=2.0.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12188:1644,config,config,1644,https://hail.is,https://github.com/hail-is/hail/pull/12188,1,['config'],['config']
Modifiability,"ithub.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Akrassowski+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​krassowski</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ameeseeksmachine+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​meeseeksmachine</code></a></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/notebook/blob/@jupyter-notebook/tree@7.0.7/CHANGELOG.md"">notebook's changelog</a>.</em></p>; <blockquote>; <h2>7.0.7</h2>; <p>(<a href=""https://github.com/jupyter/notebook/compare/@jupyter-notebook/application-extension@7.0.6...089c78c48fd00b2b0d2f33e4463eb42018e86803"">Full Changelog</a>)</p>; <h3>Enhancements made</h3>; <ul>; <li>Update to JupyterLab 4.0.11 <a href=""https://redirect.github.com/jupyter/notebook/pull/7215"">#7215</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Update ruff config and typing <a href=""https://redirect.github.com/jupyter/notebook/pull/7145"">#7145</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Clean up lint handling <a href=""https://redirect.github.com/jupyter/notebook/pull/7142"">#7142</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Adopt ruff format <a href=""https://redirect.github.com/jupyter/notebook/pull/7132"">#7132</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>[7.0.x] Install stable JupyterLab 4.0 in the releaser hook <a href=""https://redirect.github.com/jupyter/notebook/pull/7183"">#7183</a> (<a href=""https://github.com/jtpio""><code>@​jtpio</code></a>)</li>; <li>Update publish-release workflow for PyPI trusted publisher <a href=""https://redirect.github.com/jupyter/notebook/pull/7176"">#7176</a> (<a href=""https://github.com/jtpio""><code>@​jtpio</code></a>)</li>; </ul>; <h3>Contributors to",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14182:3608,config,config,3608,https://hail.is,https://github.com/hail-is/hail/pull/14182,1,['config'],['config']
Modifiability,"ittle unusual in this flow. I still think that it is helpful to set people up with an AR and keep them from footguns, but maybe that can go in a separate command that the initial init command points to once you're done? Something along the lines of ""if you get to the point where you need to upload custom container images, you can use hailctl to set up a registry""?. Another thing that gives me a little pause is the wording around google projects. I get that you need one to create a bucket, but I think we should just make sure to steer clear of the implication that you are ""selecting a GCP project to use for Hail Batch"", because that implies some link or ownership that isn't there. But I think there's a quick fix here: for a given resource that we *are* creating for hail use, like the temp bucket, ask for the name first and then ask which project it should be created in, using the projects listed in gcloud as choices with the option to write in your own. ### Regarding number of checks; I think it'd be good to avoid warnings when possible. From looking at this I see a pattern of; 1. Ask a leading question; 2. Emit a warning if the user selects the alternative option instead of the suggested option. I think I would prefer instead to ask a leading question and in the prompt explain why the alternative option might be undesirable. Then when they make a decision just move on. On a broader note, I think we should focus on having good documentation and linking to it over having perfectly thorough ; explanations in the CLI. At some point in an interactive setup if it gets longwinded I start spamming enter, but if it was quick and at the end it said something to the effect of: ""Your current configuration could result in excess cloud cost. See the documentation <here> about common pitfalls and how to avoid them"", I might decide to read through that FAQ with a more discerning eye. This is just my opinion though, I would be curious to see if other folks disagree regarding the UX.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1648633012:2955,config,configuration,2955,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1648633012,2,['config'],['configuration']
Modifiability,ive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 			at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 			at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 			at is.hail.rvd.RVD.combine(RVD.scala:688); 			at is.hail.expr.ir.Interpret$.run(Interpret.scala:804); 			at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 			at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 			at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); 			at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:39); 			at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 			at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:50); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 			at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 			at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 			at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 			at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:45); 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:12546,rewrite,rewrite,12546,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['rewrite'],['rewrite']
Modifiability,ive(DAGScheduler.scala:2792); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2257); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2289); 	at is.hail.sparkextras.ContextRDD.crunJobWithIndex(ContextRDD.scala:238); 	at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1029); 	at is.hail.rvd.RVD$.makeCoercer(RVD.scala:1104); 	at is.hail.rvd.RVD$.coerce(RVD.scala:1060); 	at is.hail.rvd.RVD.changeKey(RVD.scala:142); 	at is.hail.rvd.RVD.changeKey(RVD.scala:135); 	at is.hail.backend.spark.SparkBackend.lowerDistributedSort(SparkBackend.scala:716); 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:143); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:17); 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:14); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:167); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringPass.scala:161); 	at is.ha,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:12078,Rewrite,RewriteBottomUp,12078,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Rewrite'],['RewriteBottomUp']
Modifiability,jARPACK.dsaupd(F2jARPACK.java:30); 	at org.apache.spark.mllib.linalg.EigenValueDecomposition$.symmetricEigs(EigenValueDecomposition.scala:106); 	at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:385); 	at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:311); 	at org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix.computeSVD(IndexedRowMatrix.scala:231); 	at is.hail.methods.PCA.execute(PCA.scala:41); 	at is.hail.expr.ir.functions.WrappedMatrixToTableFunction.execute(RelationalFunctions.scala:52); 	at is.hail.expr.ir.TableToTableApply.execute(TableIR.scala:3379); 	at is.hail.expr.ir.TableIR.analyzeAndExecute(TableIR.scala:61); 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:865); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:59); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:20); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:58); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:63); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:67); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:62); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); 	at scala.collection.mutable.ResizableArray.foreac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124:8552,rewrite,rewrite,8552,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124,1,['rewrite'],['rewrite']
Modifiability,"java-storage/compare/v2.14.0...v2.15.0"">2.15.0</a> (2022-11-07)</h2>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/java-storage/blob/main/CHANGELOG.md"">google-cloud-storage's changelog</a>.</em></p>; <blockquote>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.15.1...v2.16.0"">2.16.0</a> (2022-12-06)</h2>; <h3>Features</h3>; <ul>; <li>Add {Compose,Rewrite,StartResumableWrite}Request.object_checksums and Bucket.RetentionPolicy.retention_duration (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1790"">#1790</a>) (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; <li>Added a new retention_duration field of Duration type (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; <li>Added object_checksums for compose/rewrite/startResumableWrite request (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>Removed WriteObject routing annotations (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Clarified relative resource names in gRPC IAM RPCs (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; <li>Clarified the object can be deleted via DeleteObject (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; <li>Updated the document link for <code>Naming Guidelines</code> (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; </ul>; <h3>Dependencies</h3>; <",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12545:5845,rewrite,rewrite,5845,https://hail.is,https://github.com/hail-is/hail/pull/12545,1,['rewrite'],['rewrite']
Modifiability,java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2441); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2383); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2372); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:362); 	at is.hail.rvd.RVD.$anonfun$head$1(RVD.scala:526); 	at is.hail.utils.PartitionCounts$.incrementalPCSubsetOffset(PartitionCoun,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682:8621,adapt,adapted,8621,https://hail.is,https://github.com/hail-is/hail/issues/10682,1,['adapt'],['adapted']
Modifiability,java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2204); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2225); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2257); 	at is.hail.sparkextras.ContextRDD.crunJobWithIndex(ContextRDD.scala:238); 	at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1233); 	at is.hail.rvd.RVD$.makeCoercer(RVD.scala:1308); 	at is.hail.rvd.R,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:6489,adapt,adapted,6489,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['adapt'],['adapted']
Modifiability,just one thing to think about -- we plan to extend table.explode to handle nested fields like matrixtable.explode_rows and cols. What are the semantics of `name` in this context? Does it rename the deepest field? That seems reasonable to me.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2985#issuecomment-368351765:44,extend,extend,44,https://hail.is,https://github.com/hail-is/hail/pull/2985#issuecomment-368351765,1,['extend'],['extend']
Modifiability,just pushed some changes to remove the need for all the polymorphism,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7055#issuecomment-531377155:56,polymorphi,polymorphism,56,https://hail.is,https://github.com/hail-is/hail/pull/7055#issuecomment-531377155,1,['polymorphi'],['polymorphism']
Modifiability,"k (most recent call last):; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 584, in arg_check; return checker.check(arg, function_name, arg_name); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/expression_typecheck.py"", line 80, in check; raise TypecheckFailure from e; hail.typecheck.check.TypecheckFailure. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/home/edmund/.pyenv/versions/3.8.16/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/home/edmund/.pyenv/versions/3.8.16/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py"", line 39, in <module>; cli.main(); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 430, in main; run(); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 284, in run_file; runpy.run_path(target, run_name=""__main__""); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 321, in run_path; return _run_module_code(code, init_globals, run_name,; File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 135, in _run_module_code; _run_code(code, mod_globals, init_globals,; File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 124, in _run_code; exec(code, run_globals); File ""/home/ed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:3233,adapt,adapter,3233,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['adapt'],['adapter']
Modifiability,k has been configured); E 	at reactor.core.Exceptions.propagate(Exceptions.java:392); E 	at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:97); E 	at reactor.core.publisher.Flux.blockLast(Flux.java:2519); E 	at com.azure.core.util.paging.ContinuablePagedByIteratorBase.requestPage(ContinuablePagedByIteratorBase.java:94); E 	at com.azure.core.util.paging.ContinuablePagedByItemIterable$ContinuablePagedByItemIterator.<init>(ContinuablePagedByItemIterable.java:50); E 	at com.azure.core.util.paging.ContinuablePagedByItemIterable.iterator(ContinuablePagedByItemIterable.java:37); E 	at com.azure.core.util.paging.ContinuablePagedIterable.iterator(ContinuablePagedIterable.java:106); E 	at java.lang.Iterable.forEach(Iterable.java:74); E 	at is.hail.io.fs.AzureStorageFS.delete(AzureStorageFS.scala:203); E 	at is.hail.backend.OwningTempFileManager.$anonfun$cleanup$1(ExecuteContext.scala:27); E 	at is.hail.backend.OwningTempFileManager.$anonfun$cleanup$1$adapted(ExecuteContext.scala:26); E 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); E 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); E 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); E 	at is.hail.backend.OwningTempFileManager.cleanup(ExecuteContext.scala:26); E 	at is.hail.backend.ExecuteContext.close(ExecuteContext.scala:148); E 	at is.hail.utils.package$.using(package.scala:660); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); E 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$executeOneCommand$1(ServiceBackend.scala:555); E 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); E 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); E 	at i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:1301,adapt,adapted,1301,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['adapt'],['adapted']
Modifiability,"kages/_pytest/runner.py"", line 341 in from_call; File ""/usr/local/lib/python3.9/dist-packages/_pytest/runner.py"", line 261 in call_runtest_hook; File ""/usr/local/lib/python3.9/dist-packages/_pytest/runner.py"", line 222 in call_and_report; File ""/usr/local/lib/python3.9/dist-packages/_pytest/runner.py"", line 133 in runtestprotocol; File ""/usr/local/lib/python3.9/dist-packages/_pytest/runner.py"", line 114 in pytest_runtest_protocol; File ""/usr/local/lib/python3.9/dist-packages/pluggy/_callers.py"", line 102 in _multicall; File ""/usr/local/lib/python3.9/dist-packages/pluggy/_manager.py"", line 119 in _hookexec; File ""/usr/local/lib/python3.9/dist-packages/pluggy/_hooks.py"", line 501 in __call__; File ""/usr/local/lib/python3.9/dist-packages/_pytest/main.py"", line 350 in pytest_runtestloop; File ""/usr/local/lib/python3.9/dist-packages/pluggy/_callers.py"", line 102 in _multicall; File ""/usr/local/lib/python3.9/dist-packages/pluggy/_manager.py"", line 119 in _hookexec; File ""/usr/local/lib/python3.9/dist-packages/pluggy/_hooks.py"", line 501 in __call__; File ""/usr/local/lib/python3.9/dist-packages/_pytest/main.py"", line 325 in _main; File ""/usr/local/lib/python3.9/dist-packages/_pytest/main.py"", line 271 in wrap_session; File ""/usr/local/lib/python3.9/dist-packages/_pytest/main.py"", line 318 in pytest_cmdline_main; File ""/usr/local/lib/python3.9/dist-packages/pluggy/_callers.py"", line 102 in _multicall; File ""/usr/local/lib/python3.9/dist-packages/pluggy/_manager.py"", line 119 in _hookexec; File ""/usr/local/lib/python3.9/dist-packages/pluggy/_hooks.py"", line 501 in __call__; File ""/usr/local/lib/python3.9/dist-packages/_pytest/config/__init__.py"", line 169 in main; File ""/usr/local/lib/python3.9/dist-packages/_pytest/config/__init__.py"", line 192 in console_main; File ""/usr/local/lib/python3.9/dist-packages/pytest/__main__.py"", line 5 in <module>; File ""/usr/lib/python3.9/runpy.py"", line 87 in _run_code; File ""/usr/lib/python3.9/runpy.py"", line 197 in _run_module_as_main; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14299:7381,config,config,7381,https://hail.is,https://github.com/hail-is/hail/issues/14299,2,['config'],['config']
Modifiability,"key: <strong>This is the first release to be signed with Sigstore!</strong> You can verify the distributables using the <code>.sig</code> and <code>.crt</code> files included on this release.</p>; <ul>; <li>Removed support for Python 3.5</li>; <li>Fixed an issue where a <code>ProxyError</code> recommending configuring the proxy as HTTP instead of HTTPS could appear even when an HTTPS proxy wasn't configured.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h1>1.26.11 (2022-07-25)</h1>; <ul>; <li>Fixed an issue where reading more than 2 GiB in a call to <code>HTTPResponse.read</code> would; raise an <code>OverflowError</code> on Python 3.9 and earlier.</li>; </ul>; <h1>1.26.10 (2022-07-07)</h1>; <ul>; <li>Removed support for Python 3.5</li>; <li>Fixed an issue where a <code>ProxyError</code> recommending configuring the proxy as HTTP; instead of HTTPS could appear even when an HTTPS proxy wasn't configured.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/aa3def7d242525e6e854991247c4b68583d15135""><code>aa3def7</code></a> Release 1.26.11</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/6f93b8f450b18b4c9f4c6333d759a911a63d15ae""><code>6f93b8f</code></a> Fix <code>OverflowError</code> when TLS is used on some Python versions</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/0a5f34d2c2ee6457e8365543243eccd3d1dc9430""><code>0a5f34d</code></a> Set GHA token permissions to be read-only</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/ac61b73da703df53707c31030b4ea51aab22d43c""><code>ac61b73</code></a> Backport publish workflow and process to 1.26.x</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/1fd77edc1a1373c9a7e762de148f19f1e2edd418""><code>1fd77ed</code></a> Release 1.26.1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12104:2128,config,configuring,2128,https://hail.is,https://github.com/hail-is/hail/pull/12104,2,['config'],"['configured', 'configuring']"
Modifiability,"kinda feel this should be configurable so we don't need to edit code, but I guess that in itself would involve a PR...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14461#issuecomment-2050061202:26,config,configurable,26,https://hail.is,https://github.com/hail-is/hail/pull/14461#issuecomment-2050061202,1,['config'],['configurable']
Modifiability,"kube-proxy for that IP, it rolls the dice (using `iptables`) and assigns that connection to a particular pod to which it will forward all subsequent packets. From the load-balancer's perspective, there is only one IP address, and only one place to open connections. The load-balancer doesn't have the information to actually load-balance once we have a functioning connection pool. This can lead to really unbalanced scenarios when preemptible pods come and go. This leads to our second goal: instead of routing all requests through kube-proxy, use Kubernetes Headless Services to expose all pod IPs underlying a Service so that our proxies can properly load-balance across persistent connections. ## Solution. This PR addresses the two goals outlined above and does so through using Envoy, a load-balancer/proxy that is well-suited to this sort of highly-dynamic cluster configuration. Envoy does not have the constraint that all upstream services must be available at start-time, and has a very convenient API for updating the cluster configuration without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as follows:. 1. Envoy-based gateways and internal-gateways will load their routing configuration from a Kubernetes ConfigMap, which they watch for changes and reconcile their configuration when the ConfigMap changes. The ConfigMap can be populated with a manual deploy and is populated from the beginning with production routes (i.e. batch.hail.is gets routed to batch.default); 2. When running CI, CI will regularly update the ConfigMap with additional routes based on which internal namespaces (dev and PR) are currently active. This requires relatively small changes to CI to track active namespaces but overall is a pretty small change. Note that this does not introduce a depend",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:3869,config,configuration,3869,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['config'],['configuration']
Modifiability,"kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batch' failed; make: *** [deploy-batch] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:1741,config,configuration,1741,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914,2,['config'],['configuration']
Modifiability,"l use the setup/close hooks to implement region management in the stream emitter. A `Stream` takes two pieces of information from its consumer: a `Code` to run when at the end of the stream, and a `Code` to run when the stream produces a value. (When I say ""run when..."", I really mean ""inline at the point in the control flow at which...""). It also takes an `EmitStreamContext`, which is just a bundle of a `MethodBuilder` and a `JoinPointBuilder`. It then produces six pieces of information for its consumer to use, packaged into a `Source` object. A `Stream` must satisfy the following invariants:; * `close` and `close0` must be okay to emit multiple times, i.e. they should be function calls, or a small number of function calls. (This is because I don't see a way to unify the control paths of the cases where a producer ends the stream, and where the consumer ends it.); * `eos` and `push` must be emitted at most once.; * Any local variables created by the stream must be initialized to dummy values in `setup0`, and initialized to correct initial values in `setup`. The former is needed to satisfy the bytecode verifier (otherwise it can't prove that the local always has a value in the basic blocks in which it's read).; * Any resources created in `setup0` must be freed in `close0`, and similarly for `setup`/`close`. The former will be run exactly once in a stream pipeline, while the latter will be run once per usage of the stream, which can be many times if the stream is used inside a flatMap.; * Both `firstPull` and `pull` must ultimately give control to either `eos` or `push`.; * On any control path leading to `eos`, `close` must have been run. The consumer of a `Stream` must satisfy the invariants:; * All pieces of code besides `setup0` and `setup` must be emitted at most once. This can always be arranged by emitting the code in a `JoinPoint` (label), then jumping to the label at each call site.; * If `firstPull` is defined, it must always be executed before `pull` on any",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8129:1754,variab,variables,1754,https://hail.is,https://github.com/hail-is/hail/pull/8129,1,['variab'],['variables']
Modifiability,l$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:45); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Trave,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:1542,Rewrite,RewriteBottomUp,1542,https://hail.is,https://github.com/hail-is/hail/issues/9128,3,"['Rewrite', 'rewrite']","['RewriteBottomUp', 'rewrite']"
Modifiability,l.expr.ir.Emit.emit$1(Emit.scala:591); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:624); 	at is.hail.expr.ir.Emit.$anonfun$emitVoidInSeparateMethod$1(Emit.scala:549); 	at is.hail.expr.ir.Emit.$anonfun$emitVoidInSeparateMethod$1$adapted(Emit.scala:547); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:985); 	at is.hail.expr.ir.Emit.emitVoidInSeparateMethod(Emit.scala:547); 	at is.hail.expr.ir.Emit.emitInSeparateMethod(Emit.scala:571); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:760); 	at is.hail.expr.ir.Emit.emitI$1(Emit.scala:600); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$26(Emit.scala:715); 	at is.hail.expr.ir.RelationalWriter.writeMetadata(TableWriter.scala:341); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:715); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3(Emit.scala:70); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3$adapted(Emit.scala:68); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:985); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:68); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:77); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$1(CompileAndEvaluate.scala:50); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:50); 	at is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.Low,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533:20517,adapt,adapted,20517,https://hail.is,https://github.com/hail-is/hail/issues/12533,1,['adapt'],['adapted']
Modifiability,l.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2861); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2792); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2257); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2289); 	at is.hail.sparkextras.ContextRDD.crunJobWithIndex(ContextRDD.scala:238); 	at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1029); 	at is.hail.rvd.RVD$.makeCoercer(RVD.scala:1104); 	at is.hail.rvd.R,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:10649,adapt,adapted,10649,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['adapt'],['adapted']
Modifiability,l.rvd.RVD$.coerce(RVD.scala:1264); 	at is.hail.rvd.RVD.changeKey(RVD.scala:144); 	at is.hail.rvd.RVD.changeKey(RVD.scala:137); 	at is.hail.backend.spark.SparkBackend.lowerDistributedSort(SparkBackend.scala:722); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:875); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:731); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1216); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:493); 	at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:717); 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:73); 	at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:18); 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:77); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:27); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(I,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:8457,rewrite,rewrite,8457,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['rewrite'],['rewrite']
Modifiability,la:1141); 	at is.hail.expr.ir.WrappedEmitMethodBuilder.emitWithBuilder$(EmitClassBuilder.scala:1141); 	at is.hail.expr.ir.EmitFunctionBuilder.emitWithBuilder(EmitClassBuilder.scala:1157); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:91); 	at is.hail.expr.ir.Compile$.$anonfun$apply$4(Compile.scala:74); 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction(Backend.scala:125); 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction$(Backend.scala:121); 	at is.hail.backend.spark.SparkBackend.lookupOrCompileCachedFunction(SparkBackend.scala:273); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:40); 	at is.hail.expr.ir.lowering.TableStageToRVD$.apply(RVDToTableStage.scala:112); 	at is.hail.backend.spark.SparkBackend.lowerDistributedSort(SparkBackend.scala:689); 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:110); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:23); 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:20); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:157); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringP,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13633:8828,Rewrite,RewriteBottomUp,8828,https://hail.is,https://github.com/hail-is/hail/issues/13633,1,['Rewrite'],['RewriteBottomUp']
Modifiability,la:463); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:748); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3(Emit.scala:70); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3$adapted(Emit.scala:68); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:1011); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:68); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:78); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$1(CompileAndEvaluate.scala:50); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:50); 	at is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(I,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533:9411,rewrite,rewrite,9411,https://hail.is,https://github.com/hail-is/hail/issues/12533,1,['rewrite'],['rewrite']
Modifiability,"lass org.apache.hadoop.mapred.DirectFileOutputCommitter not found'; full = 'java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.map...mmand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). '. @decorator; def handle_py4j(func, *args, **kwargs):; try:; r = func(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); E FatalError: ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E; E Java stack trace:; E java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2227); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016); E at org.apache.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:3758,Config,Configuration,3758,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['Config'],['Configuration']
Modifiability,lass.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:5249,Rewrite,RewriteBottomUp,5249,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Rewrite'],['RewriteBottomUp']
Modifiability,lass.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.app,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:4598,Rewrite,RewriteBottomUp,4598,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,le$.$anonfun$apply$4(Compile.scala:74); 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction(Backend.scala:125); 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction$(Backend.scala:121); 	at is.hail.backend.spark.SparkBackend.lookupOrCompileCachedFunction(SparkBackend.scala:273); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:40); 	at is.hail.expr.ir.lowering.TableStageToRVD$.apply(RVDToTableStage.scala:112); 	at is.hail.backend.spark.SparkBackend.lowerDistributedSort(SparkBackend.scala:689); 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:110); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:23); 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:20); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:157); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringPass.scala:151); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); 	at scala.collection.mutable.ResizableArray.foreach(Res,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13633:9089,Rewrite,RewriteBottomUp,9089,https://hail.is,https://github.com/hail-is/hail/issues/13633,1,['Rewrite'],['RewriteBottomUp']
Modifiability,le-cloud-sdk/lib/third_party/containerregistry/client/v1/__pycache__/docker_http_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/client/__pycache__/docker_creds_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/client/__pycache__/docker_name_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_puller_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_pusher_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_appender_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_appender_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_puller_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_pusher_.cpython-39.pyc; /usr/local/share/google/dataproc/npd-config/docker-monitor-counter.json; /usr/local/share/google/dataproc/npd-config/docker-monitor.json; /usr/local/share/google/dataproc/npd-config/health-checker-docker.json; /usr/local/share/google/dataproc/npd-config/docker-monitor-filelog.json; /usr/local/share/google/dataproc/bdutil/fluentd/container_logging/plugin/test/Dockerfile; /usr/local/share/google/dataproc/bdutil/components/initialize/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/post-install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/activate/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/shared/docker.sh; /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/configure_docker.sh; /run/docker.sock; /tmp/dataproc/uninstall/docker-ce; /tmp/dataproc/components/uninstall/docker-ce.running; /tmp/dataproc/components/uninstall/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:11716,config,config,11716,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['config'],['config']
Modifiability,"leapis/java-storage/commit/b1d026608a5e3772e8bf77f25f1daf68b007427a"">b1d0266</a>)</li>; <li>Update dependency org.apache.httpcomponents:httpclient to v4.5.14 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1795"">#1795</a>) (<a href=""https://github.com/googleapis/java-storage/commit/cf900f4139f30f89e3c0784467ddc12cc00cf81c"">cf900f4</a>)</li>; <li>Update dependency org.apache.httpcomponents:httpcore to v4.4.16 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1786"">#1786</a>) (<a href=""https://github.com/googleapis/java-storage/commit/3bf403e94c035e6cf936e062a1ced2b5221b3912"">3bf403e</a>)</li>; <li>Update dependency org.apache.httpcomponents:httpmime to v4.5.14 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1796"">#1796</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c9ee3ca8820531cd709bb8f8a58a736813346861"">c9ee3ca</a>)</li>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.18 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1782"">#1782</a>) (<a href=""https://github.com/googleapis/java-storage/commit/5bc517623ef04bdb9a71a51666754b9f753f4c69"">5bc5176</a>)</li>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.19 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1791"">#1791</a>) (<a href=""https://github.com/googleapis/java-storage/commit/3184d65cce1368c2f39ff85a6ed02cf536902244"">3184d65</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.15.0...v2.15.1"">2.15.1</a> (2022-11-17)</h2>; <h3>Bug Fixes</h3>; <ul>; <li>Disable REGAPIC transport in storage v2 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1762"">#1762</a>) (<a href=""https://github.com/googleapis/java-storage/commit/13d630e7ce89273c292acca7a7e048218ece4182"">13d630e</a>)</li>; <li>Update GrpcStorageImpl#get(BlobId) to return null on ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12545:8031,plugin,plugin,8031,https://hail.is,https://github.com/hail-is/hail/pull/12545,1,['plugin'],['plugin']
Modifiability,"leapis/java-storage/commit/b1d026608a5e3772e8bf77f25f1daf68b007427a"">b1d0266</a>)</li>; <li>Update dependency org.apache.httpcomponents:httpclient to v4.5.14 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1795"">#1795</a>) (<a href=""https://github.com/googleapis/java-storage/commit/cf900f4139f30f89e3c0784467ddc12cc00cf81c"">cf900f4</a>)</li>; <li>Update dependency org.apache.httpcomponents:httpcore to v4.4.16 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1786"">#1786</a>) (<a href=""https://github.com/googleapis/java-storage/commit/3bf403e94c035e6cf936e062a1ced2b5221b3912"">3bf403e</a>)</li>; <li>Update dependency org.apache.httpcomponents:httpmime to v4.5.14 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1796"">#1796</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c9ee3ca8820531cd709bb8f8a58a736813346861"">c9ee3ca</a>)</li>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.18 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1782"">#1782</a>) (<a href=""https://github.com/googleapis/java-storage/commit/5bc517623ef04bdb9a71a51666754b9f753f4c69"">5bc5176</a>)</li>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.19 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1791"">#1791</a>) (<a href=""https://github.com/googleapis/java-storage/commit/3184d65cce1368c2f39ff85a6ed02cf536902244"">3184d65</a>)</li>; </ul>; <h2>v2.15.1</h2>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.15.0...v2.15.1"">2.15.1</a> (2022-11-17)</h2>; <h3>Bug Fixes</h3>; <ul>; <li>Disable REGAPIC transport in storage v2 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1762"">#1762</a>) (<a href=""https://github.com/googleapis/java-storage/commit/13d630e7ce89273c292acca7a7e048218ece4182"">13d630e</a>)</li>; <li>Update GrpcStorageImpl#get(BlobId) ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12545:3180,plugin,plugin,3180,https://hail.is,https://github.com/hail-is/hail/pull/12545,1,['plugin'],['plugin']
Modifiability,lect$1(RDD.scala:1021); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:406); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:1020); 	at is.hail.backend.spark.SparkBackend.parallelizeAndComputeWithIndex(SparkBackend.scala:429); 	at is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:82); 	at __C1286Compiled.__m1290split_Block_region18_70(Emit.scala); 	at __C1286Compiled.__m1290split_Block(Emit.scala); 	at __C1286Compiled.apply(Emit.scala); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$4(CompileAndEvaluate.scala:60); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$2(CompileAndEvaluate.scala:60); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$2$adapted(CompileAndEvaluate.scala:58); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:144); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:144); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:58); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:17); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:17); 	at is.hail.expr.ir.TableWriter.apply(TableWriter.scala:51); 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:921); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:66); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:20); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:10870,adapt,adapted,10870,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['adapt'],['adapted']
Modifiability,"lemented for gRPC will result in a runtime error. For those operations which are not yet implemented, please continue to use the existing HTTP transport.</p>; <p>Special thanks (in alphabetical order) to <a href=""https://github.com/BenWhitehead""><code>@​BenWhitehead</code></a>, <a href=""https://github.com/frankyn""><code>@​frankyn</code></a>, <a href=""https://github.com/jesselovelace""><code>@​jesselovelace</code></a> and <a href=""https://github.com/sydney-munro""><code>@​sydney-munro</code></a> for their hard work on this effort.</p>; <h4>Notable Improvements</h4>; <ol>; <li>; <p>For all gRPC media related operations (upload/download) we are now more resource courteous then the corresponding HTTP counterpart. Buffers are fixed to their specified size (can't arbitrarily grow without bounds), are allocated lazily and only if necessary.</p>; <ol>; <li>Investigation into the possibility of backporting these improvements to the HTTP counterparts is ongoing</li>; </ol>; </li>; <li>; <p>Preview support for Accessing GCS via gRPC</p>; <ol>; <li>Set the environment variable <code>GOOGLE_CLOUD_ENABLE_DIRECT_PATH_XDS=true</code>, then run your program.</li>; </ol>; </li>; </ol>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/java-storage/blob/main/CHANGELOG.md"">google-cloud-storage's changelog</a>.</em></p>; <blockquote>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.15.0...v2.15.1"">2.15.1</a> (2022-11-17)</h2>; <h3>Bug Fixes</h3>; <ul>; <li>Disable REGAPIC transport in storage v2 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1762"">#1762</a>) (<a href=""https://github.com/googleapis/java-storage/commit/13d630e7ce89273c292acca7a7e048218ece4182"">13d630e</a>)</li>; <li>Update GrpcStorageImpl#get(BlobId) to return null on 404 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/iss",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12529:5313,variab,variable,5313,https://hail.is,https://github.com/hail-is/hail/pull/12529,1,['variab'],['variable']
Modifiability,"lemented for gRPC will result in a runtime error. For those operations which are not yet implemented, please continue to use the existing HTTP transport.</p>; <p>Special thanks (in alphabetical order) to <a href=""https://github.com/BenWhitehead""><code>@​BenWhitehead</code></a>, <a href=""https://github.com/frankyn""><code>@​frankyn</code></a>, <a href=""https://github.com/jesselovelace""><code>@​jesselovelace</code></a> and <a href=""https://github.com/sydney-munro""><code>@​sydney-munro</code></a> for their hard work on this effort.</p>; <h4>Notable Improvements</h4>; <ol>; <li>; <p>For all gRPC media related operations (upload/download) we are now more resource courteous then the corresponding HTTP counterpart. Buffers are fixed to their specified size (can't arbitrarily grow without bounds), are allocated lazily and only if necessary.</p>; <ol>; <li>Investigation into the possibility of backporting these improvements to the HTTP counterparts is ongoing</li>; </ol>; </li>; <li>; <p>Preview support for Accessing GCS via gRPC</p>; <ol>; <li>Set the environment variable <code>GOOGLE_CLOUD_ENABLE_DIRECT_PATH_XDS=true</code>, then run your program.</li>; <li>When configuring your <code>StorageOptions</code> mimic the following:; <pre><code> StorageOptions.grpc(); .setAttemptDirectPath(true); .build(); </code></pre>; </li>; <li>Internally the default host endpoint <code>https://storage.googleapis.com:443</code> will be transformed to the applicable <code>google-c2p-experimental:///storage.googleapis.com</code></li>; </ol>; </li>; <li>; <p>Support for <code>java.time</code> types on model classes</p>; <ol>; <li>Points in time are now represented with <code>java.time.OffsetDateTime</code>, while durations are represented with <code>java.time.Duration</code></li>; <li>All existing <code>Long</code> centric methods are still present, but have been deprecated in favor of their corresponding <code>java.time</code> variant</li>; <li>At the next major version, these deprecated method",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12456:4283,variab,variable,4283,https://hail.is,https://github.com/hail-is/hail/pull/12456,2,['variab'],['variable']
Modifiability,"lemented for gRPC will result in a runtime error. For those operations which are not yet implemented, please continue to use the existing HTTP transport.</p>; <p>Special thanks (in alphabetical order) to <a href=""https://github.com/BenWhitehead""><code>@​BenWhitehead</code></a>, <a href=""https://github.com/frankyn""><code>@​frankyn</code></a>, <a href=""https://github.com/jesselovelace""><code>@​jesselovelace</code></a> and <a href=""https://github.com/sydney-munro""><code>@​sydney-munro</code></a> for their hard work on this effort.</p>; <h4>Notable Improvements</h4>; <ol>; <li>; <p>For all gRPC media related operations (upload/download) we are now more resource courteous then the corresponding HTTP counterpart. Buffers are fixed to their specified size (can't arbitrarily grow without bounds), are allocated lazily and only if necessary.</p>; <ol>; <li>Investigation into the possibility of backporting these improvements to the HTTP counterparts is ongoing</li>; </ol>; </li>; <li>; <p>Preview support for Accessing GCS via gRPC</p>; <ol>; <li>Set the environment variable <code>GOOGLE_CLOUD_ENABLE_DIRECT_PATH_XDS=true</code>, then run your program.</li>; <li>When configuring your <code>StorageOptions</code> mimic the following:; <pre><code> StorageOptions.grpc(); </code></pre>; </li>; </ol>; </li>; </ol>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/googleapis/java-storage/commit/bfd48a1b5542ff28ffa337eba883c4ca6c3b0aad""><code>bfd48a1</code></a> chore(main): release 2.15.1 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1765"">#1765</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/3b8d137a113376d7dac9010b9207d435df2622f7""><code>3b8d137</code></a> docs: annotate all Option factory methods with their Nullability bounds (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1775"">#1775</a>)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12529:10677,variab,variable,10677,https://hail.is,https://github.com/hail-is/hail/pull/12529,1,['variab'],['variable']
Modifiability,"les generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp/bar/foo; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: y; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-batch-jigold-oxmmp.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-east1; WARNING: remote temporary directory ""gs://hail-batch-jigold-oxmmp/bar/foo"" is not located in the selected compute region for Batch jobs ""us-east1"".; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp/bar/foo; batch/regions=us-east1; batch/backend=service; query/backend=batch; WARNING: Initialized Hail with warnings! The currently specified configuration will result in additional ingress and egress fees when using Hail Batch.; ```. Existing multiregional bucket:. ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-jigold-test-multi-regional; WARNING: remote temporary directory gs://hail-jigold-test-multi-regional is multi-regional. Using this bucket with the Batch Service will incur addtional ingress and egress fees.; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-jigold-test-multi-regional? [y/n]: y; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-jigold-test-multi-regional.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-we",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:5564,config,configuration,5564,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,1,['config'],['configuration']
Modifiability,"lete binary tree with `k` leaves. Conceptually, the leaves hold the current `k` elements (the heads of each of the `k` input streams), while each internal node records the result of the comparison between the ""winners"" of the two subtrees, where in this case the winner is the least element. Thus we can find the smallest of all `k` elements by looking at the root node. If we remove the smallest element, and replace it with the next value from that stream, we change the value in the corresponding leaf node, then we just need to replay the comparisons at all internal nodes on the path to the root. Note that to replay a comparison, we only need to know what element *lost* at this node previously. It must have lost to the previous overall winner, the element we just replaced. Using that observation, we only need to store the `k` current values in the `k` leaves, and in each of the `k-1` internal nodes we store the index of the element which lost the comparison at that node. That just leaves the overall winner, which we can store in a separate variable. Note that each element besides the overall winner loses exactly one ""match"", so the internal nodes store a permutation of the indices 0 to (k-1), minus the overall winner. This is a so-called ""loser tree"". In the implementation, I store the `k` leaves in a `Array[Long]`, where each element is a pointer to the head of the corresponding stream, and I store the `k-1` internal nodes in a `Array[Int]`, in the usual breadth-first order, where each element is the index of the stream which lost the comparison at that node (had the larger value). I use an index of `-1` to represent an imaginary element smaller than all real elements, and similarly an index of `k` is larger than all real elements. The tournament tree begins filled with `-1`, and each stream is advanced once, as their values push out all the `-1`s. When a steam ends, it's leaf is given the value `k`, and once the overall winner is `k`, we know all streams have ended.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9033:1718,variab,variable,1718,https://hail.is,https://github.com/hail-is/hail/pull/9033,1,['variab'],['variable']
Modifiability,"li>; <li><a href=""https://github.com/jupyter/notebook/commit/d2ef92f0b385b7ecd11cbf0f3af181ee8e494623""><code>d2ef92f</code></a> Backport PR <a href=""https://redirect.github.com/jupyter/notebook/issues/7142"">#7142</a>: Clean up lint handling (<a href=""https://redirect.github.com/jupyter/notebook/issues/7185"">#7185</a>)</li>; <li><a href=""https://github.com/jupyter/notebook/commit/8e9390d9af903f34bb1c8414c7e9b49d2fdec32f""><code>8e9390d</code></a> Backport PR <a href=""https://redirect.github.com/jupyter/notebook/issues/7132"">#7132</a>: Adopt ruff format (<a href=""https://redirect.github.com/jupyter/notebook/issues/7184"">#7184</a>)</li>; <li><a href=""https://github.com/jupyter/notebook/commit/4d07f1ee9b6d3dca2736e2bf3a1254451add8259""><code>4d07f1e</code></a> Install stable JupyterLab 4.0 in the releaser hook (<a href=""https://redirect.github.com/jupyter/notebook/issues/7183"">#7183</a>)</li>; <li><a href=""https://github.com/jupyter/notebook/commit/e73d410074e6dbf97273f761d1513ff61db2965c""><code>e73d410</code></a> Updated ui-tests Configuration in Contributing.md (<a href=""https://redirect.github.com/jupyter/notebook/issues/7124"">#7124</a>)</li>; <li><a href=""https://github.com/jupyter/notebook/commit/ea1a1538ef56084654b7486e1d0b96f06b33acbe""><code>ea1a153</code></a> Set <code>navigation_with_keys</code> to <code>False</code> (<a href=""https://redirect.github.com/jupyter/notebook/issues/7129"">#7129</a>)</li>; <li><a href=""https://github.com/jupyter/notebook/commit/d717c6b3613f3609139bc2b9fe8d0a126aaeeae2""><code>d717c6b</code></a> Add Python 3.12 classifier (<a href=""https://redirect.github.com/jupyter/notebook/issues/7111"">#7111</a>)</li>; <li>See full diff in <a href=""https://github.com/jupyter/notebook/compare/@jupyter-notebook/tree@7.0.6...@jupyter-notebook/tree@7.0.7"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=notebook&package-manager=pip&previo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14182:7871,Config,Configuration,7871,https://hail.is,https://github.com/hail-is/hail/pull/14182,1,['Config'],['Configuration']
Modifiability,"li>; <li>Fix conversions for custom metrics. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/94654"">kubernetes/kubernetes#94654</a>, <a href=""https://github.com/wojtek-t""><code>@​wojtek-t</code></a>) [SIG Instrumentation]</li>; <li>A new alpha-level field, <code>SupportsFsGroup</code>, has been introduced for CSIDrivers to allow them to specify whether they support volume ownership and permission modifications. The <code>CSIVolumeSupportFSGroup</code> feature gate must be enabled to allow this field to be used. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/92001"">kubernetes/kubernetes#92001</a>, <a href=""https://github.com/huffmanca""><code>@​huffmanca</code></a>) [SIG API Machinery, CLI and Storage]</li>; <li>Added pod version skew strategy for seccomp profile to synchronize the deprecated annotations with the new API Server fields. Please see the corresponding section <a href=""https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/135-seccomp/README.md#version-skew-strategy"">in the KEP</a> for more detailed explanations. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91408"">kubernetes/kubernetes#91408</a>, <a href=""https://github.com/saschagrunert""><code>@​saschagrunert</code></a>) [SIG Apps, Auth, CLI and Node]</li>; <li>Adds the ability to disable Accelerator/GPU metrics collected by Kubelet (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91930"">kubernetes/kubernetes#91930</a>, <a href=""https://github.com/RenaudWasTaken""><code>@​RenaudWasTaken</code></a>) [SIG Node]</li>; <li>Admission webhooks can now return warning messages that are surfaced to API clients, using the <code>.response.warnings</code> field in the admission review response. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/92667"">kubernetes/kubernetes#92667</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>) [SIG API Machinery a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:3007,enhance,enhancements,3007,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['enhance'],['enhancements']
Modifiability,"libs/aioredis-py/issues/1101"">#1101</a>)</li>; <li>Synchronized reading the responses from a connection (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1106"">#1106</a>)</li>; </ul>; <h2>Fixes</h2>; <ul>; <li>Remove del from Redis (Fixes <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1115"">#1115</a>) (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1227"">#1227</a>)</li>; <li>fix socket.error raises (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1129"">#1129</a>)</li>; <li>Fix buffer is closed error when using PythonParser class (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1213"">#1213</a>)</li>; </ul>; <h2>Version v2.0.0</h2>; <p>Version 2.0 is a complete rewrite of aioredis. Starting with this version, aioredis now follows the API of <a href=""https://github.com/andymccurdy/redis-py"">redis-py</a>, so you can easily adapt synchronous code that uses redis-py for async applications with aioredis-py.</p>; <p><strong>NOTE:</strong> This version is <em>not</em> compatible with earlier versions of aioredis. If you upgrade, you will need to make code changes.</p>; <p>For more details, read our <a href=""https://aioredis.readthedocs.io/en/latest/migration/"">documentation on migrating to version 2.0</a>.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aioredis-py/blob/master/CHANGELOG.md"">aioredis's changelog</a>.</em></p>; <blockquote>; <h2>2.0.1 - (2021-12-20)</h2>; <h3>Features</h3>; <ul>; <li>Added Python 3.10 to CI &amp; Updated the Docs; (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1160"">#1160</a>)</li>; <li>Enable mypy in CI (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1101"">#1101</a>)</li>; <li>Synchronized reading the responses from a connection; (see <a href",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11569:1540,adapt,adapt,1540,https://hail.is,https://github.com/hail-is/hail/pull/11569,1,['adapt'],['adapt']
Modifiability,"lics detected.; 2018-06-26 01:49:09 Hail: INFO: interval filter loaded 27 of 586 partitions; ```. I'll kill what I have and run a single regression, since this will take a long time.; ```; 2018-07-18 15:39:30 Hail: INFO: Number of BGEN files parsed: 1; 2018-07-18 15:39:30 Hail: INFO: Number of samples in BGEN files: 487409; 2018-07-18 15:39:30 Hail: INFO: Number of variants across all BGEN files: 1255683; 2018-07-18 15:40:37 Hail: INFO: Coerced almost-sorted dataset; 2018-07-18 15:40:39 Hail: INFO: interval filter loaded 5 of 293 partitions; 2018-07-18 15:43:13 Hail: WARN: 126215 of 487409 samples have a missing phenotype or covariate.; 2018-07-18 15:43:13 Hail: INFO: linear_regression: running on 361194 samples for 110 response variables y,; with input variable x, intercept, and 25 additional covariates...; 2018-07-18 15:44:06 Hail: WARN: 132571 of 487409 samples have a missing phenotype or covariate.; 2018-07-18 15:44:06 Hail: INFO: linear_regression: running on 354838 samples for 1 response variable y,; with input variable x, intercept, and 25 additional covariates...; 2018-07-18 15:44:59 Hail: WARN: 132781 of 487409 samples have a missing phenotype or covariate.; 2018-07-18 15:44:59 Hail: INFO: linear_regression: running on 354628 samples for 1 response variable y,; with input variable x, intercept, and 25 additional covariates...; 2018-07-18 15:45:42 Hail: WARN: 133165 of 487409 samples have a missing phenotype or covariate.; 2018-07-18 15:45:42 Hail: INFO: linear_regression: running on 354244 samples for 1 response variable y,; with input variable x, intercept, and 25 additional covariates...; 2018-07-18 15:46:57 Hail: WARN: 132601 of 487409 samples have a missing phenotype or covariate.; 2018-07-18 15:46:57 Hail: INFO: linear_regression: running on 354808 samples for 1 response variable y,; with input variable x, intercept, and 25 additional covariates...; ```. Also, the default parallelism in 0.1 was better for the same interval: 27 partitions to 5 partition",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3945#issuecomment-405979965:1376,variab,variable,1376,https://hail.is,https://github.com/hail-is/hail/pull/3945#issuecomment-405979965,2,['variab'],['variable']
Modifiability,"lier PR are responded to there and addressed in the code for this one. This PR is to enable `hail-az/https` Azure file references to contain SAS tokens to enable bearer-auth style file access to Azure storage. Basic summary of the changes:; - Update `AzureAsyncFS` url parsing function to look for and separate out a SAS-token-like query string. Note: made fairly specific to SAS tokens - generic detection of query string syntax interferes with glob support and '?' characters in file names; - Added `generate_sas_token` convenience function to `AzureAsyncFS`. Adds new azure-mgmt-storage package requirement.; - Updated `AzureAsyncFS` to use `(account, container, credential)` tuple as internal `BlobServiceClient` cache key; - Updated `AzureAsyncFSURL` and `AzureFileListEntry` to track the token separately from the name, and extend the base classes to allow returning url with or without a token; - Update `RouterFS.ls` function and associated listfiles function to allow for trailing query strings during path traversal; - Update `AsyncFS.open_from` function to handle query-string urls in zero-length case; - Change to existing behavior: `LocalAsyncFSURL.__str__` no longer returns 'file:' prefix. Done to make `str()` output be appropriate for input to `fs` functions across all subclasses; - Updated `InputResource` to not include the SAS token as part of the destination file name; - Updated `inter_cloud/test_fs.py` to generically use query-string-friendly file path building functions to respect the new model, where it is no longer safe to extend URLs by just appending new segments with `+ ""/""` because there may be a query string, and added `'sas/azure-https'` test case to the fixture. Running tests for the SAS case requires some new test variables to allow the test code to generate SAS tokens (`build.yaml/test_hail_python_fs`):; ```; # Required for SAS testing on Azure; export HAIL_TEST_AZURE_RESGRP=haildev; export HAIL_TEST_AZURE_SUBID=12ab51c6-da79-4a99-8dec-3d2decc97343; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13140:1745,extend,extend,1745,https://hail.is,https://github.com/hail-is/hail/pull/13140,2,"['extend', 'variab']","['extend', 'variables']"
Modifiability,"llib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/batch/server.py"", line 417, in run_forever; target(*args, **kwargs); File ""/batch/server.py"", line 441, in kube_event_loop; requests.post('http://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-10-23 03:58:08,124 | server.py | run_forever:416 | run_forever: run target kube_event_loop; INFO | 2018-10-23 03:59:30,729 | server.py | mark_complete:175 | wrote log for job 153 to logs/job-153.log; INFO | 2018-10-23 03:59:30,730 | server.py | set_state:141 | job 153 changed state: Created -> Complete; INFO | 2018-10-23 03:59:30,730 | server.py | mark_complete:184 | job 153 complete, exit_code 2; INFO | 2018-10-23 03:59:30,737 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,806 | _internal.py | _log:88 | 10.56.142.33 - - [23/Oct/2018 03:59:30] ""GET /jobs HTTP/1.1"" 200 -; INFO | 2018-10-23 03:59:30,815 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; I",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:1798,adapt,adapters,1798,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038,1,['adapt'],['adapters']
Modifiability,"localhost, executor driver, partition 0, ANY, 4726 bytes); 2018-10-09 14:46:42 Executor: INFO: Running task 0.0 in stage 4.0 (TID 4); 2018-10-09 14:46:42 ShuffleBlockFetcherIterator: INFO: Getting 1 non-empty blocks out of 1 blocks; 2018-10-09 14:46:42 ShuffleBlockFetcherIterator: INFO: Started 0 remote fetches in 0 ms; 2018-10-09 14:46:42 Executor: INFO: Finished task 0.0 in stage 4.0 (TID 4). 1539 bytes result sent to driver; 2018-10-09 14:46:42 TaskSetManager: INFO: Finished task 0.0 in stage 4.0 (TID 4) in 7 ms on localhost (executor driver) (1/1); 2018-10-09 14:46:42 TaskSchedulerImpl: INFO: Removed TaskSet 4.0, whose tasks have all completed, from pool ; 2018-10-09 14:46:42 DAGScheduler: INFO: ResultStage 4 (collect at utils.scala:197) finished in 0.007 s; 2018-10-09 14:46:42 DAGScheduler: INFO: Job 2 finished: collect at utils.scala:197, took 0.053572 s; 2018-10-09 14:46:42 CodeGenerator: INFO: Code generated in 5.955541 ms; 2018-10-09 14:46:42 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:42 SparkSqlParser: INFO: Parsing command: SELECT *; FROM `table7e606a8b83f4` AS `zzz1`; WHERE (0 = 1); 2018-10-09 14:46:42 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:42 SparkSqlParser: INFO: Parsing command: SELECT *; FROM `table7e606a8b83f4`; 2018-10-09 14:46:43 root: INFO: optimize: before:; (TableCount; (TableKeyBy () False; (TableLiteral))); 2018-10-09 14:46:43 root: INFO: optimize: after:; (TableCount; (TableLiteral)); 2018-10-09 14:46:43 SparkContext: INFO: Starting job: fold at RVD.scala:361; 2018-10-09 14:46:43 DAGScheduler: INFO: Got job 3 (fold at RVD.scala:361) with 1 output partitions; 2018-10-09 14:46:43 DAGScheduler: INFO: Final stage: ResultStage 5 (fold at RVD.scala:361); 2018-10-09 14:46:43 DAGScheduler: INFO: Parents of final stage: List(); 2018-10-09 14:46:43 DAGScheduler: INFO: Missing parents: List(); 2018-10",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:45951,config,configuration,45951,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['config'],['configuration']
Modifiability,"localhost, executor driver, partition 0, ANY, 4726 bytes); 2018-10-09 15:04:37 Executor: INFO: Running task 0.0 in stage 4.0 (TID 4); 2018-10-09 15:04:37 ShuffleBlockFetcherIterator: INFO: Getting 1 non-empty blocks out of 1 blocks; 2018-10-09 15:04:37 ShuffleBlockFetcherIterator: INFO: Started 0 remote fetches in 0 ms; 2018-10-09 15:04:37 Executor: INFO: Finished task 0.0 in stage 4.0 (TID 4). 1539 bytes result sent to driver; 2018-10-09 15:04:37 TaskSetManager: INFO: Finished task 0.0 in stage 4.0 (TID 4) in 7 ms on localhost (executor driver) (1/1); 2018-10-09 15:04:37 TaskSchedulerImpl: INFO: Removed TaskSet 4.0, whose tasks have all completed, from pool ; 2018-10-09 15:04:37 DAGScheduler: INFO: ResultStage 4 (collect at utils.scala:197) finished in 0.008 s; 2018-10-09 15:04:37 DAGScheduler: INFO: Job 2 finished: collect at utils.scala:197, took 0.051042 s; 2018-10-09 15:04:37 CodeGenerator: INFO: Code generated in 5.011153 ms; 2018-10-09 15:04:37 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:37 SparkSqlParser: INFO: Parsing command: SELECT *; FROM `table8508c46074` AS `zzz1`; WHERE (0 = 1); 2018-10-09 15:04:37 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:37 SparkSqlParser: INFO: Parsing command: SELECT *; FROM `table8508c46074`; 2018-10-09 15:04:38 root: INFO: optimize: before:; (TableCount; (TableKeyBy () False; (TableLiteral))); 2018-10-09 15:04:38 root: INFO: optimize: after:; (TableCount; (TableLiteral)); 2018-10-09 15:04:38 SparkContext: INFO: Starting job: fold at RVD.scala:361; 2018-10-09 15:04:38 DAGScheduler: INFO: Got job 3 (fold at RVD.scala:361) with 1 output partitions; 2018-10-09 15:04:38 DAGScheduler: INFO: Final stage: ResultStage 5 (fold at RVD.scala:361); 2018-10-09 15:04:38 DAGScheduler: INFO: Parents of final stage: List(); 2018-10-09 15:04:38 DAGScheduler: INFO: Missing parents: List(); 2018-10-09 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:28428,config,configuration,28428,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['config'],['configuration']
Modifiability,"looked into the failing tests concerning randomness, and tracked down the source of the failures:. - `ApplySeeded` is an `AbstractApplyNode`. this means the interpreter will try to ""memoize"" the function definition so as to generate it only once even if you call it in multiple places in the IR. ; - memoization is based on referential equality, so they need to be the exact same IR object in order for reuse to trigger; - the ""seeded function"" implementations used by the test suite will create a new randomness state per generated function; - the Interpret pipeline used to rewrite each ApplySeeded node to be different objects, but with your changes they are the same, so their functions get memoized, thus sharing the same state. im not entirely sure which of these was doing the wrong thing, but IMO your changes in this PR should not have introduced any problems",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7567#issuecomment-556266727:576,rewrite,rewrite,576,https://hail.is,https://github.com/hail-is/hail/pull/7567#issuecomment-556266727,1,['rewrite'],['rewrite']
Modifiability,"looks like the differences between `--unsafe-fixes` and my manual edits based on the feedback the linter gave were:; * `assert <boolean value> == True` becomes `assert <boolean value> is True`, not `assert <boolean value>`; * `if <value> == foo or <value> == bar` becomes `if <value> in (foo, bar)`, not `if <value> in {foo, bar}`; * `<unused variable> = foo` becomes `foo` instead of being deleted outright. those seem fine, though i think the manual version of the latter two is better in the cases where i had added it, as i only used sets for hashable types and deleted things that didn't have side effects, afaik",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14128#issuecomment-1883396355:343,variab,variable,343,https://hail.is,https://github.com/hail-is/hail/pull/14128#issuecomment-1883396355,2,['variab'],['variable']
Modifiability,lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:45); at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:600); at is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:636); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:631); at is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:630); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); at is.hail.utils.package$.using(package.scala:664); at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78); at is.hail.utils.package$.using(package.scala:664); at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:65); at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:407); at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:393); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:630); at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:88); at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79); at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14249:5886,adapt,adapted,5886,https://hail.is,https://github.com/hail-is/hail/issues/14249,1,['adapt'],['adapted']
Modifiability,lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:45); at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:601); at is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:637); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:632); at is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:631); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:77); at is.hail.utils.package$.using(package.scala:665); at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:77); at is.hail.utils.package$.using(package.scala:665); at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:64); at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:407); at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:393); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:631); at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:89); at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79); at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:7148,adapt,adapted,7148,https://hail.is,https://github.com/hail-is/hail/issues/14168,3,['adapt'],['adapted']
Modifiability,"ls/htsjdk/issues/1622"">#1622</a>); 347c0ac57 Fix EdgeReadIterator (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1616"">#1616</a>); d15a5bacb Added ULTIMA and ELEMENT as valid value for RG-PL according to SAM spec. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1619"">#1619</a>)</p>; <h2>3.0.0</h2>; <p>Htsjdk 3.0.0: Revenge of the Simple Allele</p>; <p>This is the first htsjdk with a major version increase in a long time. We bumped it to indicate there are some breaking changes that will potentially require downstream code changes. Notably, <code>Allele</code> became an interface instead of a concrete class. <code>SimpleAllele</code> may be used as a replacement if you have classes which previously subclassed allele.</p>; <p>New Plugin Infrastructure:; 6a60de7c2 Move API marker annotations into new annotation package. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1558"">#1558</a>); 7ac95d5f7 Plugin framework and interfaces for versioned file format codecs (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1525"">#1525</a>); d40fe5412 Beta implementation of Bundles. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1546"">#1546</a>)</p>; <p>CRAM; 489c4192d Support CRAM reference regions. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1605"">#1605</a>); 22aec6782 Fix decoding of CRAM Scores read feature during normalization. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1592"">#1592</a>); 6507249a4 Make the CRAM MD5 failure message more user friendly. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1607"">#1607</a>); b5af659e6 Fix restoration of read base feature code. <a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1379"">#1379</a> (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1590"">#1590</a>); e63c34a92 Ignore TC, TN on CRAM read (<a hr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12229:1797,Plugin,Plugin,1797,https://hail.is,https://github.com/hail-is/hail/pull/12229,2,['Plugin'],['Plugin']
Modifiability,"lt even though a DesignSpace v5 did contain 'STAT' definitions (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3045"">#3045</a>, <a href=""https://redirect.github.com/fonttools/fonttools/issues/3046"">#3046</a>).</li>; </ul>; <p><strong>NOTE</strong>: The 4.39.1 distribution was &quot;yanked&quot; from PyPI to prevent users from accidentally upgrading to it.</p>; <h2>4.39.1</h2>; <ul>; <li>[avar2] Added experimental support for reading/writing avar version 2 as specified in this draft proposal:; <a href=""https://github.com/harfbuzz/boring-expansion-spec/blob/main/avar2.md"">https://github.com/harfbuzz/boring-expansion-spec/blob/main/avar2.md</a></li>; <li>[glifLib] Wrap underlying XML library exceptions with GlifLibError when parsing GLIFs, and also print the name and path of the glyph that fails to be parsed (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3029"">fonttools/fonttools#3029</a>).</li>; <li>[feaLib] Consult avar for normalizing user-space values in ConditionSets and in VariableScalars (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3042"">fonttools/fonttools#3042</a>, <a href=""https://redirect.github.com/fonttools/fonttools/pull/3043"">fonttools/fonttools#3043</a>).</li>; <li>[ttProgram] Handle string input to Program.fromAssembly() (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3038"">fonttools/fonttools#3038</a>).</li>; <li>[otlLib] Added a config option to emit GPOS 7 lookups, currently disabled by default because of a macOS bug (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3034"">fonttools/fonttools#3034</a>).</li>; <li>[COLRv1] Added method to automatically compute ClipBoxes (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3027"">fonttools/fonttools#3027</a>).</li>; <li>[ttFont] Fixed getGlyphID to raise KeyError on missing glyphs instead of returning None. The regression was introduced in v4.27.0 (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3032"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12910:1935,Variab,VariableScalars,1935,https://hail.is,https://github.com/hail-is/hail/pull/12910,1,['Variab'],['VariableScalars']
Modifiability,"lt;- <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1177"">#1177</a>)</li>; <li>misc speed improvements/optimisations</li>; </ul>; <h2>tqdm v4.63.0 stable</h2>; <ul>; <li>add <code>__reversed__()</code></li>; <li>add efficient <code>__contains__()</code></li>; <li>improve CLI startup time (replace <code>pkg_resources</code> =&gt; <code>importlib</code>)</li>; <li><code>tqdm.autonotebook</code> warning &amp; <code>std</code> fallback on missing <code>ipywidgets</code> (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1218"">#1218</a> &lt;- <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1082"">#1082</a>, <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1217"">#1217</a>)</li>; <li>warn on positional CLI arguments</li>; <li>misc build/test framework updates; <ul>; <li>enable <code>py3.10</code> tests</li>; <li>add <code>conda</code> dependencies</li>; <li>update pre-commit hooks</li>; <li>fix <code>pytest</code> config (<code>nbval</code>, <code>asyncio</code>)</li>; <li>fix dependencies &amp; tests</li>; <li>fix site deployment</li>; </ul>; </li>; </ul>; <h2>tqdm v4.62.3 stable</h2>; <ul>; <li>fix minor typo (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1246"">#1246</a>)</li>; <li>minor example fix (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1246"">#1246</a>)</li>; <li>misc tidying &amp; refactoring</li>; <li>misc build/dev framework updates; <ul>; <li>update dependencies</li>; <li>update linters</li>; <li>update docs deployment branches</li>; </ul>; </li>; <li>misc test/ci updates; <ul>; <li>test forks</li>; <li>tidy OS &amp; Python version tests</li>; <li>bump primary python version 3.7 =&gt; 3.8</li>; <li>beta py3.10 testing</li>; <li>fix py2.7 tests</li>; <li>better timeout handling</li>; </ul>; </li>; </ul>; <h2>tqdm v4.62.2 stable</h2>; <ul>; <li>fix notebook memory leak (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1216"">#1216</a>)</li>; </",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12260:2853,config,config,2853,https://hail.is,https://github.com/hail-is/hail/pull/12260,1,['config'],['config']
Modifiability,ltop/fs/fs.py'; adding 'hailtop/fs/fs_utils.py'; adding 'hailtop/fs/router_fs.py'; adding 'hailtop/fs/stat_result.py'; adding 'hailtop/hailctl/__init__.py'; adding 'hailtop/hailctl/__main__.py'; adding 'hailtop/hailctl/deploy.yaml'; adding 'hailtop/hailctl/describe.py'; adding 'hailtop/hailctl/auth/__init__.py'; adding 'hailtop/hailctl/auth/cli.py'; adding 'hailtop/hailctl/auth/create_user.py'; adding 'hailtop/hailctl/auth/delete_user.py'; adding 'hailtop/hailctl/auth/login.py'; adding 'hailtop/hailctl/batch/__init__.py'; adding 'hailtop/hailctl/batch/batch_cli_utils.py'; adding 'hailtop/hailctl/batch/cli.py'; adding 'hailtop/hailctl/batch/initialize.py'; adding 'hailtop/hailctl/batch/list_batches.py'; adding 'hailtop/hailctl/batch/submit.py'; adding 'hailtop/hailctl/batch/utils.py'; adding 'hailtop/hailctl/batch/billing/__init__.py'; adding 'hailtop/hailctl/batch/billing/cli.py'; adding 'hailtop/hailctl/config/__init__.py'; adding 'hailtop/hailctl/config/cli.py'; adding 'hailtop/hailctl/config/config_variables.py'; adding 'hailtop/hailctl/dataproc/__init__.py'; adding 'hailtop/hailctl/dataproc/cli.py'; adding 'hailtop/hailctl/dataproc/cluster_config.py'; adding 'hailtop/hailctl/dataproc/connect.py'; adding 'hailtop/hailctl/dataproc/deploy_metadata.py'; adding 'hailtop/hailctl/dataproc/diagnose.py'; adding 'hailtop/hailctl/dataproc/gcloud.py'; adding 'hailtop/hailctl/dataproc/modify.py'; adding 'hailtop/hailctl/dataproc/start.py'; adding 'hailtop/hailctl/dataproc/submit.py'; adding 'hailtop/hailctl/dataproc/utils.py'; adding 'hailtop/hailctl/dev/__init__.py'; adding 'hailtop/hailctl/dev/ci_client.py'; adding 'hailtop/hailctl/dev/cli.py'; adding 'hailtop/hailctl/dev/config.py'; adding 'hailtop/hailctl/hdinsight/__init__.py'; adding 'hailtop/hailctl/hdinsight/cli.py'; adding 'hailtop/hailctl/hdinsight/start.py'; adding 'hailtop/hailctl/hdinsight/submit.py'; adding 'hailtop/utils/__init__.py'; adding 'hailtop/utils/filesize.py'; adding 'hailtop/utils/process.py'; adding,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:27541,config,config,27541,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['config'],['config']
Modifiability,"lucene_grammar.py raised when run...</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/e4f3ce2a0805bf1561b96b8f13210f07fe3651ab""><code>e4f3ce2</code></a> Prep for 3.1.0 release</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/40babe02aa8a7905c5b46d010888516036be02ab""><code>40babe0</code></a> More example updates, PEP-8 names, f-strings.</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/85c2ef19a3e11faad0cb405a5ab66bdca7e49f45""><code>85c2ef1</code></a> Minor formatting change, bug-fix on 0000 time</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/2fc41a02f32b4f7a769f036daec58ba4f233b106""><code>2fc41a0</code></a> Update ci.yml</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/08e7cfdb94fecf2d99c324856c37ec66fac0eeed""><code>08e7cfd</code></a> Minor changes in examples, conversion to PEP8 names, etc.</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/a8b05ccbe380117dac40b4cf6d9ffe08266fd7ed""><code>a8b05cc</code></a> Slight perf enhancement in Empty</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/801863aa4582a8ce5e6a7408d4966afcd247ea90""><code>801863a</code></a> Make htmlStripper.py and html_table_parser examples use PEP-8 names, add comm...</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/7d4da80b2bca8a2767134f4a181ea9aac4bbb230""><code>7d4da80</code></a> Prep for release</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/be0310a83436bb4893d0068bb5da3059199e4c0b""><code>be0310a</code></a> Add bf parser/executor example</li>; <li>Additional commits viewable in <a href=""https://github.com/pyparsing/pyparsing/compare/pyparsing_3.0.9...3.1.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pyparsing&package-manager=pip&previous-version=3.0.9&new-version=3.1.0)](https://docs.github.com/en/github/managing-security-vulnerabilit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13334:8159,enhance,enhancement,8159,https://hail.is,https://github.com/hail-is/hail/pull/13334,1,['enhance'],['enhancement']
Modifiability,"ly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in any reasonable language as a ""function""). It is intended to be used like this:. ```; VERSION = 30; $(eval $(call ENV_VAR,VERSION)). build: env/VERSION; build:; ... $(VERSION) ...; ```. Each time this Makefile is executed, at Makefile parse-time, `make` evaluates the `ifneq` to compare the current value of the variable to the previously used value (if any). If they differ, a phony (ergo always needs to be rebuilt) target is dynamically generated. That target will force a execution of any dependent targets, in the example above, it will force `build` to be executed. If the variable's current value and it's previous value do not differ, no target is generated and thus nothing is executed. #### MAKEFLAGS += --no-builtin-rules. This disables all the automatic rules, making Makefile parsing/in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5130:2572,variab,variable,2572,https://hail.is,https://github.com/hail-is/hail/pull/5130,1,['variab'],['variable']
Modifiability,"m/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/Other.20genome/near/397467764). The following is seemingly correct code but it doesn't work:; ```python3; import hail as hl. rgwheat = hl.ReferenceGenome('Wheat', ...). hl.init(default_reference=rgwheat); ```; The first problem is that the `@typecheck` on `hl.init`, `hl.init_spark`, etc. only allows a built-in reference genome. . Even if we relax that requirement, we encounter a deeper problem: creating the reference genome initializes Hail. In particular, [we call `Env.backend()`](https://github.com/hail-is/hail/blob/main/hail/python/hail/genetics/reference_genome.py#L117-L118) (which calls `Env.hc()`, which forces initialization) so that we can call `add_reference`. What does initialization mean? Historically, it meant connection to or starting a JVM/Spark process. In QoB/ServiceBackend, initialization just loads configurations, it doesn't really do anything irreversible. Regardless of what it does, we only allow initialization *once*. OK, so, there's two possible routes to fix this problem:; 1. Rewrite `ReferenceGenome.__init__` such that it does not initialize Hail. You have to decide how reference genomes are ultimately communicated to the backend. Do you hang a list of all created reference genomes off of the `ReferenceGenome` class? Do you require explicit registering a la `hl.register_reference`? The latter seems a bit silly. The former seems OK, but you could also ...; 2. Allow modification of the default reference after initialization. The default reference genome is just a field on the HailContext: `_default_ref` which is accessed through `hl.default_reference()`. Just modify `hl.default_reference` to *return* the reference with no arguments and *set* the reference with one argument. Now this works:. ```python3; import hail as hl; rgwheat = hl.ReferenceGenome('Wheat', ...); hl.default_reference(rgwheat); mt = hl.import_vcf('wheat.vcf'); ```. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13856:1160,Rewrite,Rewrite,1160,https://hail.is,https://github.com/hail-is/hail/issues/13856,1,['Rewrite'],['Rewrite']
Modifiability,"m/python-pillow/Pillow/issues/7928"">#7928</a>; [radarhere, hugovk]</p>; </li>; <li>; <p>Deprecate <code>eval()</code>, replacing it with <code>lambda_eval()</code> and <code>unsafe_eval()</code> <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7927"">#7927</a>; [radarhere, hugovk]</p>; </li>; <li>; <p>Raise <code>ValueError</code> if seeking to greater than offset-sized integer in TIFF <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7883"">#7883</a>; [radarhere]</p>; </li>; <li>; <p>Add <code>--report</code> argument to <code>__main__.py</code> to omit supported formats <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7818"">#7818</a>; [nulano, radarhere, hugovk]</p>; </li>; <li>; <p>Added RGB to I;16, I;16L, I;16B and I;16N conversion <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7918"">#7918</a>, <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7920"">#7920</a>; [radarhere]</p>; </li>; <li>; <p>Fix editable installation with custom build backend and configuration options <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7658"">#7658</a>; [nulano, radarhere]</p>; </li>; <li>; <p>Fix putdata() for I;16N on big-endian <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7209"">#7209</a>; [Yay295, hugovk, radarhere]</p>; </li>; <li>; <p>Determine MPO size from markers, not EXIF data <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7884"">#7884</a>; [radarhere]</p>; </li>; <li>; <p>Improved conversion from RGB to RGBa, LA and La <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7888"">#7888</a>; [radarhere]</p>; </li>; <li>; <p>Support FITS images with GZIP_1 compression <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7894"">#7894</a>; [radarhere]</p>; </li>; <li>; <p>Use I;16 mode for 9-bit JPEG 2000 images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7900"">#7900</a>; [scaramallion, radarhere]</p>; </l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14439:11303,config,configuration,11303,https://hail.is,https://github.com/hail-is/hail/pull/14439,3,['config'],['configuration']
Modifiability,"mance</h3>; <ul>; <li>Speed-up the new backtracking parser about 4X in general (enabled when <code>--target-version</code> is set to 3.10 and higher). (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2728"">#2728</a>)</li>; <li>Black is now compiled with mypyc for an overall 2x speed-up. 64-bit Windows, MacOS, and Linux (not including musl) are supported. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/1009"">#1009</a>, <a href=""https://github-redirect.dependabot.com/psf/black/issues/2431"">#2431</a>)</li>; </ul>; <h3>Configuration</h3>; <ul>; <li>Do not accept bare carriage return line endings in pyproject.toml (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2408"">#2408</a>)</li>; <li>Add configuration option (<code>python-cell-magics</code>) to format cells with custom magics in Jupyter Notebooks (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2744"">#2744</a>)</li>; <li>Allow setting custom cache directory on all platforms with environment variable <code>BLACK_CACHE_DIR</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2739"">#2739</a>).</li>; <li>Enable Python 3.10+ by default, without any extra need to specify -<code>-target-version=py310</code>. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2758"">#2758</a>)</li>; <li>Make passing <code>SRC</code> or <code>--code</code> mandatory and mutually exclusive (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2804"">#2804</a>)</li>; </ul>; <h3>Output</h3>; <ul>; <li>Improve error message for invalid regular expression (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2678"">#2678</a>)</li>; <li>Improve error message when parsing fails during AST safety check by embedding the underlying SyntaxError (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2693"">#2693</a>)</li>; <li>No longer color diff headers white as it's unreadable in light themed terminals (<a href=""https:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11468:5006,variab,variable,5006,https://hail.is,https://github.com/hail-is/hail/pull/11468,1,['variab'],['variable']
Modifiability,"mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesn’t work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation · Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O · GitHub](https://gist.github.com/kunalkushwaha/66629a90e0f8f5cc5dc512ef1c346f2f). [Measuring the Horizontal Attack Profile of Nabla Containers](https://outlookseries.com/A0784/Infrastructure/3868.htm); * 10-30% cost for networking-heavy operations. ### Example implementations of sandboxed containers; https://github.com/kata-containers/documentation/blob/master/how-to/how-to-use-k8s-with-cri-containerd-and-kata.md. [CRI installation - Kubernetes](https://kubernetes.io/docs/setup/cri/). ### References:; [Kata Containers - Why Kata Containers doesn’t replace Kubernetes: A Kata Containers explainer](https://katacontainers.io/posts/why-kata-containers-doesnt-replace-kubernetes/). [Kubernetes Container Runtimes - kubedex.com](https://kubedex.com/kubernetes-container-runtimes/). [GitHub - containerd/cri: Containerd Plugin for Kubernetes Container Runtime Interface](https://github.com/containerd/cri). https://github.com/kubernetes/community/blob/master/contributors/devel/container-runtime-interface.md",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5111:2271,sandbox,sandboxed,2271,https://hail.is,https://github.com/hail-is/hail/issues/5111,2,"['Plugin', 'sandbox']","['Plugin', 'sandboxed']"
Modifiability,"me directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/sub",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1550,config,configuration,1550,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170,1,['config'],['configuration']
Modifiability,"ment>;; const good3 = () => <span><div>OK</div><span>GOOD!</span></span>;; ```. #### JSX naming conventions; 1. Lowercase components are just built-in html elements. i.e `<span>` is a an HTML `<span>` on output.; 2. Uppercase components are javascript functions. This makes composing components really simple. ```jsx; const CoolComponent = () => <span>Hello World</span>;. export default () => <CoolComponent>;; ```. You can pass state to these user-defined components, much like you would in HTML, using attributes. These attributes can have arbitrary names, except they must start with a lowercase letter, and follow camel-case convention. These attributes are called `props`. ```jsx; const CoolComponent = (props) => <span>Hello {props.name}!</span>;. export default () => <CoolComponent name=""Alex"">; #mounts <span>Hello Alex!</span> in DOM; ```. #### PureComponent / shallow watch; React's reconciler is triggered whenever this.setState is called, resulting in a walk down the descendent nodes, based on either the presence of that state variable as a ""prop"" (i.e `<MyComponent name={this.state.name}/>`), or its use directly within the component (i.e `{this.state.name === 'Alex' ? <div>Do stuff</div> : <div>Do other stuff</div>). To give the reconciler less work to do, when accepting objects as props, use a `<PureComponent>`. This will tell React to check the reference for diff, rather than deep value compare. Obviously much faster to do the latter. You can do even better than `PureComponent`. Use a regular `Component`, and specific a `shouldComponentUpdate() { }` method in that component. Within that method, write whatever checks needed, so that when a prop, or state changes, you return `true`, otherwise `false`. When true, the component will re-render. However, this allows you to react in a more fine-grained way, i.e instead of checking reference, check for the update of a specific property, or don't react to that object changing at all. Behind the scenes, PureComponent is in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:9213,variab,variable,9213,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['variab'],['variable']
Modifiability,mer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:74); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:19); E 	at is.hail.expr.ir.lowering.LowerDistributedSort$.distributedSort(LowerDistributedSort.scala:163); E 	at is.hail.backend.service.ServiceBackend.lowerDistributedSort(ServiceBackend.scala:354); E 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:100); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:23); E 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$4(RewriteBottomUp.scala:26); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:36); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:20); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:157); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringPass.scala:151); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.mutable.Re,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:1866,Rewrite,RewriteBottomUp,1866,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['Rewrite'],['RewriteBottomUp']
Modifiability,"method to automatically compute ClipBoxes (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3027"">fonttools/fonttools#3027</a>).</li>; <li>[ttFont] Fixed getGlyphID to raise KeyError on missing glyphs instead of returning None. The regression was introduced in v4.27.0 (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3032"">fonttools/fonttools#3032</a>).</li>; <li>[sbix] Fixed UnboundLocalError: cannot access local variable 'rawdata' (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3031"">fonttools/fonttools#3031</a>).</li>; <li>[varLib] When building VF, do not overwrite a pre-existing <code>STAT</code> table that was built with feaLib from FEA feature file. Also, added support for building multiple VFs defined in Designspace v5 from <code>fonttools varLib</code> script (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3024"">fonttools/fonttools#3024</a>).</li>; <li>[mtiLib] Only add <code>Debg</code> table with lookup names when <code>FONTTOOLS_LOOKUP_DEBUGGING</code> env variable is set (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3023"">fonttools/fonttools#3023</a>).</li>; </ul>; <h2>4.39.0</h2>; <ul>; <li>[mtiLib] Optionally add <code>Debg</code> debug info for MTI feature builds (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3018"">#3018</a>).</li>; <li>[ttx] Support reading input file from standard input using special <code>-</code> character, similar to existing <code>-o -</code> option to write output to standard output (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3020"">#3020</a>).</li>; <li>[cython] Prevent <code>cython.compiled</code> raise AttributeError if cython not installed properly (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3017"">#3017</a>).</li>; <li>[OS/2] Guard against ZeroDivisionError when calculating xAvgCharWidth in the unlikely scenario no glyph has non-zero advance (<a href=""https://redirect.github.com/fonttools/fontt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12910:3599,variab,variable,3599,https://hail.is,https://github.com/hail-is/hail/pull/12910,1,['variab'],['variable']
Modifiability,"methods much more effectively than large methods (and so splitting a large method into two small ones can make an order of magnitude or more in performance difference). We have three forms of method splitting in the Hail Query compiler. The first is a heuristic and greedy IR-level method splitter that generates new methods every X IR nodes, simply based on node count. However, the size of code generated by each IR can vary widely (`I32` vs `LowerBoundOnOrderedCollection` for instance), and so we have two other kinds of splitting that operate on the LIR level. The first is region splitting, which is used to split large blocks of LIR. In order to insert a split, any variables on the stack are stored in local variables before the split and loaded from those locals after the split. The second is method splitting, which is used to split large single methods. A single-exit group of blocks can be split into a separate method, and we have some machinery for replacing control flow instructions (which I will not go into here, for they are not relevant now), as well as handling local variables that are used across a method split. These shared Local variables are replaced by fields on a ""spills"" class which is allocated any time a split method is called. Spilled local `store`s are rewritten as field `store`s, and `load`s are rewritten as field `load`s. # What was the problem here?. A region split was inserted *directly between* the `I2B` instruction and the call to `OutputBuffer.write`. This meant that the result of `I2B` was stored in a local variable and read in the subsequent block. **The incorrect TypeInfo of Boolean was used for that local variable**, but this seems not to pose a problem -- both Boolean and Byte use a single slot, and so the code still works even with the wrong variable type. However, the method splitter then **generated a method split at the same point where the region was split**. This means that the local variable resulting from I2B is spilled to a class",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:2507,variab,variables,2507,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['variab'],['variables']
Modifiability,"mitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[28] at mapPartitions at ContextRDD.scala:137) (first 15 tasks are for partitions Vector(0)); 2018-10-09 15:04:38 TaskSchedulerImpl: INFO: Adding task set 5.0 with 1 tasks; 2018-10-09 15:04:38 TaskSetManager: INFO: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4777 bytes); 2018-10-09 15:04:38 Executor: INFO: Running task 0.0 in stage 5.0 (TID 5); 2018-10-09 15:04:38 BlockManager: INFO: Found block rdd_9_0 locally; 2018-10-09 15:04:38 CodeGenerator: INFO: Code generated in 14.135243 ms; 2018-10-09 15:04:38 CodeGenerator: INFO: Code generated in 8.306294 ms; 2018-10-09 15:04:38 Executor: INFO: Finished task 0.0 in stage 5.0 (TID 5). 1119 bytes result sent to driver; ```; </details>. <details>; <summary>Broken hail.log</summary>. ```; 2018-10-09 14:46:38 Hail: INFO: SparkUI: http://10.32.119.167:4040; 2018-10-09 14:46:38 Hail: INFO: Running Hail version devel-e7552fd55a9d; 2018-10-09 14:46:38 SharedState: INFO: loading hive config file: file:/Users/michafla/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml; 2018-10-09 14:46:38 SharedState: INFO: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 14:46:38 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@28f0ac7{/SQL,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@49a30f89{/SQL/json,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4495af6e{/SQL/execution,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6baf9f3b{/SQL/execution/json,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHan",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:31125,config,config,31125,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['config'],['config']
Modifiability,"mmands or subgroups, like `hailctl dataproc`, `hailctl batch`, etc. Groups are defined like this:; ; ```; @hailctl.group(; help=""Manage the Hail Batch service.""); def batch():; pass; ```; - A command in a group is defined like this:. ```; @batch.command(; help=""Get a particular batch's info.""); @click.argument('batch_id', type=int); @click.option('--output-format', '-o',; type=click.Choice(['yaml', 'json']),; default='yaml', show_default=True,; help=""Specify output format"",); def get(batch_id, output_format):; ...; ```. The command decorator replaces the function with one that takes a `List[str]` of command line parameters, parses them, and calls the original function. The option options are pretty self-explanatory. - To access an argument to a group (like `dataproc --beta`) in a (sub)command, use the decorator `click.pass_context` to pass the click context which allows you to access parent group parameters. `dataproc start` is an example:. ```; @dataproc.command(; help=""Start a Dataproc cluster configured for Hail.""); @click.argument('cluster_name'); ...; @click.pass_context; def start(ctx, cluster_name, ...):; beta = ctx.parent.params['beta']; ```. The help output for a group looks like this:. ```; $ hailctl dataproc --help; Usage: hailctl dataproc [OPTIONS] COMMAND [ARGS]... Manage and monitor Hail deployments. Options:; --beta Force use of `beta` in gcloud commands; --help Show this message and exit. Commands:; connect Connect to a running Dataproc cluster; describe Gather information about a Hail (Table or MatrixTable) file...; diagnose Diagnose problems in a Dataproc cluster.; list List Dataproc clusters.; modify; start Start a Dataproc cluster configured for Hail.; stop Shut down a Dataproc cluster.; submit Submit a Python script to a running Dataproc cluster.; ```. The help output for a command looks like:. ```; $ hailctl batch get --help; Usage: hailctl batch get [OPTIONS] BATCH_ID. Get a particular batch's info. Options:; -o, --output-format [yaml|json]; Sp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842:2494,config,configured,2494,https://hail.is,https://github.com/hail-is/hail/pull/9842,1,['config'],['configured']
Modifiability,"mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:5010,config,configuration,5010,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['config'],['configuration']
Modifiability,"mples by one but only D will receive ""own time"" (I believe). Sorting by samples sort of reveals the call stack. You have to be careful to focus on the own time as you scan down the stack though!. My take on the relatively expensive operations:. 1. Struct decoding (the fourth generated code one, with 399 own time and 229 samples) is pretty branchy: it checks a bit for each field. I'm not sure how to speed this up. Consider a struct of 8 optional fields. There are 2^8 possible missingness pattern. Each pattern corresponds to a different sequence of field-decoders. I suppose we could generate 256 different patterns and jump to them? That seems excessive. We could maybe generate 16 patterns but that only saves 3/4 of the branches. Maybe that's enough for a substantial speedup?; 3. `LEB128InputBuffer.readByte` (365, 139). `readByte` just calls its child `InputBuffer`'s `readByte`. My best explanation: we call `readByte` *a lot*. That kinda makes sense: `LEB128InputBuffer` issues a `readByte` for each byte in the variable length encoding of the integer. ; 4. Optional Array of Optional Int32 (450, 138). These arrays will have a bunch of LEB128 encoded integers, which, as stated, has a bunch of branches making this fairly expensive.; 5. `BlockingInputBuffer.ensure` is expensive. (705, 130). The blocking happens *after* Zstd. This dataset is about 16 GiB when uncompressed. The block size is 65kB (ergo 244k blocks the uncompressed dataset) When reading a large numbers of bytes, `ensure` is called infrequently compared to actually reading the bytes. However, LEB128 relies heavily on `readByte` and thus issues O(N_BYTES) calls to `ensure`.; 6. `Zstd.decompressByteArray` (249, 111). This is eliminated in main by commit 507744f2d7. Patrick's branch (on which I ran these experiments) is out of date.; 7. `BlockingInputBuffer.readBytes` (798, 88). I think this is a bit unavoidable: we have to copy bytes from input to the region. We could avoid the intermediary buffer: copy as much a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13792:4971,variab,variable,4971,https://hail.is,https://github.com/hail-is/hail/issues/13792,1,['variab'],['variable']
Modifiability,"ms as much as ~12 GB/s of aggregate bandwidth. We seem to have room for improvement, but this seems good enough for now. # On AWS, GCS -> S3. | Files | Bytes | Time | Rate |; | ----- | ----- | ---- | ---- |; | 1 | 5.4 GB | 34 seconds | 154.5 MB/s |; | 1 | 42.9 GB | 4 minutes | 161.6 MB/s |; | 200 | 5.4 GB | 35 seconds | 151.1 MB/s |; | 40000 | 5.4 GB | 4 minutes | 22.0 MB/s |. # On GCP, S3 -> GCS. | Files | Bytes | Time | Rate |; | ----- | ----- | ---- | ---- |; | 1 | 5.4 GB | 17 seconds | 304.2 MB/s |; | 1 | 42.9 GB | 3 minutes | 235.5 MB/s |; | 200 | 5.4 GB | 20 seconds | 267.8 MB/s |; | 40000 | 5.4 GB | 6 minutes | 13.3 MB/s |. # machine parsable form; ```; [{'config': 'one',; 'from': 'gs://1-day/tmp/test-copy/dking-benchmark/one',; 'times': [34.76],; 'to': 's3://hail-test-dy5rg/tmp/target/dking-benchmark/one'},; {'config': 'some',; 'from': 'gs://1-day/tmp/test-copy/dking-benchmark/some',; 'times': [35.527],; 'to': 's3://hail-test-dy5rg/tmp/target/dking-benchmark/some'},; {'config': 'many',; 'from': 'gs://1-day/tmp/test-copy/dking-benchmark/many',; 'times': [244.154],; 'to': 's3://hail-test-dy5rg/tmp/target/dking-benchmark/many'},; {'config': 'huge',; 'from': 'gs://1-day/tmp/test-copy/dking-benchmark/huge',; 'times': [265.719],; 'to': 's3://hail-test-dy5rg/tmp/target/dking-benchmark/huge'},; {'config': 'one',; 'from': 's3://hail-test-dy5rg/tmp/test-copy/dking-benchmark/one',; 'times': [17.65],; 'to': 'gs://1-day/tmp/test-copy/target/dking-benchmark/one'},; {'config': 'some',; 'from': 's3://hail-test-dy5rg/tmp/test-copy/dking-benchmark/some',; 'times': [20.048],; 'to': 'gs://1-day/tmp/test-copy/target/dking-benchmark/some'},; {'config': 'many',; 'from': 's3://hail-test-dy5rg/tmp/test-copy/dking-benchmark/many',; 'times': [402.267],; 'to': 'gs://1-day/tmp/test-copy/target/dking-benchmark/many'},; {'config': 'huge',; 'from': 's3://hail-test-dy5rg/tmp/test-copy/dking-benchmark/huge',; 'times': [182.355],; 'to': 'gs://1-day/tmp/test-copy/target/dking-benchmark/huge'}].",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10752#issuecomment-897651697:1247,config,config,1247,https://hail.is,https://github.com/hail-is/hail/pull/10752#issuecomment-897651697,1,['config'],['config']
Modifiability,"much detail as possible. -----------------------------------------------------------------------------. Package Info:; Name: hail; Version: 0.2.93; Summary: Scalable library for exploring and analyzing genomic data.; Home-page: https://hail.is; Author: Hail Team; Author-email: hail@broadinstitute.org; License: UNKNOWN; Location: /Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages; Requires: dill, bokeh, scipy, azure-storage-blob, janus, parsimonious, botocore, google-cloud-storage, tabulate, Jinja2, python-json-logger, plotly, avro, azure-identity, PyJWT, orjson, tqdm, aiohttp-session, google-auth, nest-asyncio, uvloop, humanize, hurry.filesize, decorator, requests, Deprecated, aiohttp, asyncinit, numpy, pyspark, sortedcontainers, boto3, pandas. -----------------------------------------------------------------------------. Importing hail via the IPython console in Spyder causes the following error:. Python 3.8.12 (default, Oct 12 2021, 06:23:56) ; IPython 8.2.0 -- An enhanced Interactive Python. In [1]: `import hail`. > [SpyderKernelApp] ERROR | Exception in message handler:; > Traceback (most recent call last):; > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages/spyder_kernels/comms/frontendcomm.py"", line 164, in poll_one; > asyncio.run(handler(out_stream, ident, msg)); > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages/nest_asyncio.py"", line 36, in run; > task = asyncio.ensure_future(main); > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/asyncio/tasks.py"", line 684, in ensure_future; > raise TypeError('An asyncio.Future, a coroutine or an awaitable is '; > TypeError: An asyncio.Future, a coroutine or an awaitable is required; > [SpyderKernelApp] ERROR | Exception in message handler:; > Traceback (most recent call last):; > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages/spyder_kernels/comms/frontendcomm.py"", line 164, in poll_one; > asyncio.run(handler(out_stream, ident, msg)); > File ""/Users/jacobb",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11758:1179,enhance,enhanced,1179,https://hail.is,https://github.com/hail-is/hail/issues/11758,1,['enhance'],['enhanced']
Modifiability,"my strategy here was just to undo everything added in [this commit](https://github.com/hail-is/hail/commit/12e0f497db0f3e5453f870495e48e44191b315f4), except the version upgrades, so i'm not sure if there are some changes i'm making here that are unnecessary or produce weird results as far as what all ends up in the jar or anything. from running `jar -tf` on the jar produced by the current `main` and the one produced by the commit prior to the one i'm partially reverting, it looked like the updates to the config only added things to the jar, rather than removing any, so hopefully that should be fine",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13759#issuecomment-1743654792:510,config,config,510,https://hail.is,https://github.com/hail-is/hail/pull/13759#issuecomment-1743654792,2,['config'],['config']
Modifiability,"n local variables before the split and loaded from those locals after the split. The second is method splitting, which is used to split large single methods. A single-exit group of blocks can be split into a separate method, and we have some machinery for replacing control flow instructions (which I will not go into here, for they are not relevant now), as well as handling local variables that are used across a method split. These shared Local variables are replaced by fields on a ""spills"" class which is allocated any time a split method is called. Spilled local `store`s are rewritten as field `store`s, and `load`s are rewritten as field `load`s. # What was the problem here?. A region split was inserted *directly between* the `I2B` instruction and the call to `OutputBuffer.write`. This meant that the result of `I2B` was stored in a local variable and read in the subsequent block. **The incorrect TypeInfo of Boolean was used for that local variable**, but this seems not to pose a problem -- both Boolean and Byte use a single slot, and so the code still works even with the wrong variable type. However, the method splitter then **generated a method split at the same point where the region was split**. This means that the local variable resulting from I2B is spilled to a class field on the spills class. Our incorrectly-Boolean local becomes an incorrectly-Boolean **field**, and this is where things go wrong -- it seems as though Boolean class fields (appropriately) truncate on store and load a single bit. Our value of `3` was stored as a class Boolean, and came out `1`. The fact that a single field's missingness was flipped was a red herring -- all higher bits are flipped to 0 (defined)! Here's a look at the LIR looks like, though it was ultimately the JVM class file printout that tipped me off to the problem:. ```code. # I2B is stored as a class field on spills. The Z at the end of the next line indicates this field is a Boolean, not a byte. 31017 (PutFieldX PUTFIELD _",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:3078,variab,variable,3078,https://hail.is,https://github.com/hail-is/hail/pull/11328,2,['variab'],['variable']
Modifiability,"n the LIR level. The first is region splitting, which is used to split large blocks of LIR. In order to insert a split, any variables on the stack are stored in local variables before the split and loaded from those locals after the split. The second is method splitting, which is used to split large single methods. A single-exit group of blocks can be split into a separate method, and we have some machinery for replacing control flow instructions (which I will not go into here, for they are not relevant now), as well as handling local variables that are used across a method split. These shared Local variables are replaced by fields on a ""spills"" class which is allocated any time a split method is called. Spilled local `store`s are rewritten as field `store`s, and `load`s are rewritten as field `load`s. # What was the problem here?. A region split was inserted *directly between* the `I2B` instruction and the call to `OutputBuffer.write`. This meant that the result of `I2B` was stored in a local variable and read in the subsequent block. **The incorrect TypeInfo of Boolean was used for that local variable**, but this seems not to pose a problem -- both Boolean and Byte use a single slot, and so the code still works even with the wrong variable type. However, the method splitter then **generated a method split at the same point where the region was split**. This means that the local variable resulting from I2B is spilled to a class field on the spills class. Our incorrectly-Boolean local becomes an incorrectly-Boolean **field**, and this is where things go wrong -- it seems as though Boolean class fields (appropriately) truncate on store and load a single bit. Our value of `3` was stored as a class Boolean, and came out `1`. The fact that a single field's missingness was flipped was a red herring -- all higher bits are flipped to 0 (defined)! Here's a look at the LIR looks like, though it was ultimately the JVM class file printout that tipped me off to the problem:. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:2975,variab,variable,2975,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['variab'],['variable']
Modifiability,"n, payload); 208 path = action_routes[action]; 209 port = self._backend_server_port; --> 210 resp = self._requests_session.post(f'http://localhost:{port}{path}', data=data); 211 if resp.status_code >= 400:; 212 error_json = orjson.loads(resp.content). File /opt/conda/lib/python3.10/site-packages/requests/sessions.py:635, in Session.post(self, url, data, json, **kwargs); 624 def post(self, url, data=None, json=None, **kwargs):; 625 r""""""Sends a POST request. Returns :class:`Response` object.; 626 ; 627 :param url: URL for the new :class:`Request` object.; (...); 632 :rtype: requests.Response; 633 """"""; --> 635 return self.request(""POST"", url, data=data, json=json, **kwargs). File /opt/conda/lib/python3.10/site-packages/requests/sessions.py:587, in Session.request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json); 582 send_kwargs = {; 583 ""timeout"": timeout,; 584 ""allow_redirects"": allow_redirects,; 585 }; 586 send_kwargs.update(settings); --> 587 resp = self.send(prep, **send_kwargs); 589 return resp. File /opt/conda/lib/python3.10/site-packages/requests/sessions.py:701, in Session.send(self, request, **kwargs); 698 start = preferred_clock(); 700 # Send the request; --> 701 r = adapter.send(request, **kwargs); 703 # Total elapsed time of the request (approximately); 704 elapsed = preferred_clock() - start. File /opt/conda/lib/python3.10/site-packages/requests/adapters.py:502, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies); 487 resp = conn.urlopen(; 488 method=request.method,; 489 url=url,; (...); 498 chunked=chunked,; 499 ); 501 except (ProtocolError, OSError) as err:; --> 502 raise ConnectionError(err, request=request); 504 except MaxRetryError as e:; 505 if isinstance(e.reason, ConnectTimeoutError):; 506 # TODO: Remove this in 3.0.0: see #2811. ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960:12774,adapt,adapter,12774,https://hail.is,https://github.com/hail-is/hail/issues/13960,2,['adapt'],"['adapter', 'adapters']"
Modifiability,n.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.lambda$getPropsWithPrefix$3(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getLookupKey(HadoopConfigurationProperty.java:120); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getPropsWithPrefix(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:421); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsFsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:383); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1516); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1486); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:541); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:494); at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669); at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94); at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703); at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685); at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373); at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); at is.hail.io.fs.HadoopFS.is$hail$io$fs$HadoopFS$$_fileSystem(HadoopFS.scala:157); at is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:244); at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:226); at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:225); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8343:3432,config,configure,3432,https://hail.is,https://github.com/hail-is/hail/issues/8343,1,['config'],['configure']
Modifiability,"n3.10/site-packages/notebook/notebook/handlers.py"", line 95, in get; May 16 14:17:07 mw116-m python[8309]: self.write(self.render_template('notebook.html',; May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/notebook/base/handlers.py"", line 507, in render_template; May 16 14:17:07 mw116-m python[8309]: return template.render(**ns); May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/jinja2/environment.py"", line 1301, in render; May 16 14:17:07 mw116-m python[8309]: self.environment.handle_exception(); May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/jinja2/environment.py"", line 936, in handle_exception; May 16 14:17:07 mw116-m python[8309]: raise rewrite_traceback_stack(source=source); May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/notebook/templates/notebook.html"", line 1, in top-level template code; May 16 14:17:07 mw116-m python[8309]: {% extends ""page.html"" %}; May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/notebook/templates/page.html"", line 154, in top-level template code; May 16 14:17:07 mw116-m python[8309]: {% block header %}; May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/notebook/templates/notebook.html"", line 114, in block 'header'; May 16 14:17:07 mw116-m python[8309]: {% for exporter in get_frontend_exporters() %}; May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/notebook/notebook/handlers.py"", line 23, in get_frontend_exporters; May 16 14:17:07 mw116-m python[8309]: from nbconvert.exporters.base import get_export_names, get_exporter; May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/nbconvert/__init__.py"", line 4, in <module>; May 16 14:17:07 mw116-m python[8309]: from .exporters import *; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13059:2482,extend,extends,2482,https://hail.is,https://github.com/hail-is/hail/issues/13059,1,['extend'],['extends']
Modifiability,"n; Python 3.9.18; ```; I activate java 11.0.20.1; ```sh; $ java -version; openjdk version ""11.0.20.1"" 2023-08-22 LTS; OpenJDK Runtime Environment Corretto-11.0.20.9.1 (build 11.0.20.1+9-LTS); OpenJDK 64-Bit Server VM Corretto-11.0.20.9.1 (build 11.0.20.1+9-LTS, mixed mode); ```; * I clone hail; ```sh; $ cd /tmp; $ git clone --branch 0.2.124 --depth 1 https://github.com/broadinstitute/hail.git; ```; * I build hail; ```sh; $ cd hail/hail/; $ make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.15 SPARK_VERSION=3.3.0; [...]; Successfully installed hail-0.2.124; hailctl config set query/backend spark; ```; * At this point Hail seems correcly installed; ```sh; $ pip show hail; Name: hail; Version: 0.2.124; Summary: Scalable library for exploring and analyzing genomic data.; Home-page: https://hail.is; Author: Hail Team; Author-email: hail@broadinstitute.org; License: UNKNOWN; Location: /home/hadoop/.local/lib/python3.9/site-packages; ```; * For sake of configuration I create a symlink of the hail backend; ```sh; sudo ln -sf /home/hadoop/.local/lib/python3.9/site-packages/hail/backend /opt/hail/backend; ```; * Confident of the. installation I try to run spark shell; ```sh; $ spark-shell; [...]; Exception in thread ""main"" java.lang.NoSuchMethodError: 'scala.reflect.internal.settings.MutableSettings ; ```. I am out of idea on how to solve the current situation. ; Thanks. ### Version. 0.2.124. ### Relevant log output. ```shell; $ spark-shell; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Setting default log level to ""WARN"".; To adju",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837:1268,config,configuration,1268,https://hail.is,https://github.com/hail-is/hail/issues/13837,1,['config'],['configuration']
Modifiability,nCompilable.scala:27); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:45); at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:600); at is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:636); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:631); at is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:630); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); at is.hail.utils.package$.using(package.scala:664); at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14249:5113,adapt,adapted,5113,https://hail.is,https://github.com/hail-is/hail/issues/14249,1,['adapt'],['adapted']
Modifiability,nCompilable.scala:27); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:45); at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:601); at is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:637); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:632); at is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:631); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:77); at is.hail.utils.package$.using(package.scala:665); at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:6375,adapt,adapted,6375,https://hail.is,https://github.com/hail-is/hail/issues/14168,3,['adapt'],['adapted']
Modifiability,nCompilable.scala:67); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:53); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:13); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:416); at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:452); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:70); at is.hail.utils.package$.using(package.scala:646); at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); at is.hail.utils.package$.using(package.scala:646); at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); at is.hail.backend.spark.Sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12280:4207,adapt,adapted,4207,https://hail.is,https://github.com/hail-is/hail/issues/12280,1,['adapt'],['adapted']
Modifiability,"n[T] -> (U -> Boolean) -> Gen[Unit]`; - A `Gen[Unit]` is a bit artificial because the test framework halts execution (presumably with an exception) when a counter-example is found. I instead prefer that `Prop.forAll` has type: `Gen[T] -> (U -> Boolean) -> Gen[Boolean]`; - Now `Prop.forAll` has the same type as `Gen.flatMap[Boolean]`. It seems the difference between `forAll` and `flatMap` is that `forAll` conceptually preforms a product operation while `flatMap` performs a sampling. However, I think they are, in reality, the same operation: sampling. The implementation for `GenProp3` looks like:. ``` scala; for (i <- 0 until p.count) {; val v1 = g1(p); val v2 = g2(p); val v3 = g3(p); val r = f(v1, v2, v3); if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; }; ```. Which could be re-written as:. ``` scala; for (i <- 0 until p.count) {; (for (v1 <- g1; v2 <- g2; v3 <- g3) {; if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; })(p); }; ```. The primary difference between `flatMap` and `forAll` seems to be in error reporting. We can fix this by noting `Gen[T]` is currently a Reader monad on `Parameters`. If we add a ""forAll stack"" to `Parameters` we could implement `forAll` as:. ``` scala; def forAll[T,U](gt: Gen[T], gu: T -> Gen[U]): Gen[U] =; for (t <- gt; u <- local(pushQuantified(t), gu(t)) yield u. def pushQuantified(x: Any)(Parameters p): Paramters =; new Parameters(p.rng, p.size, p.count, (x :: p.quanitifed)); ```. We complete the Reader monad transformation by adding the `local` operation to `class Gen[T]`. ``` scala; // in class Gen; def local(modify: Parameters -> Parameters, gu: Gen[U]): Gen[U] =; Gen { p => gu(modify(p)) }; ```. Finally, the `check` method can access this stack of quantified variables to provide a useful error message. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/400#issuecomment-238901220:2144,variab,variables,2144,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-238901220,1,['variab'],['variables']
Modifiability,"namespaces and services are active in our cluster at a given point in time. TL;DR Switching from NGINX to Envoy with CI acting as the ""control plane"" for our internal networking allows us to more easily dynamically configure our Kubernetes networking and achieve proper connection pooling/load-balancing over TLS, which translates to less resource consumption and lower request latencies. ## Motivation; This is primarily a performance-motivated change, and one largely based on our (ab)use of NGINX in order to work with our dynamically-generated Kubernetes test namespaces. Currently, we configure NGINX by creating server blocks that dynamically resolve and dispatch requests based on matching regular expressions on the host and path headers. This is in large part due that at gateway deploy time we do not statically know all of the namespaces and namespace-service combinations that will exist in the cluster in the future. This is true for `default`, but not test namespaces, and NGINX will refuse to start with statically-configured clusters that it cannot reach. Making the server blocks make the routing decisions dynamically circumvents this limitation. However, this prevents usage of NGINX [upstream](http://nginx.org/en/docs/http/ngx_http_upstream_module.html) blocks that provide connection pooling, at least in the community edition, and as a result the gateways will create and terminate a TCP connection per http request. This likely causes minor delays on the front-end through gateway, but this hampers performance greatly in job scheduling. The batch driver is forced to establish a new TCP connection and do an SSL handshake with the internal-gateway multiple times per job, which is expensive and slow. We currently have to dedicate a 2-core NGINX sidecar for the batch-driver just to terminate TLS with internal-gateway and free up cycles in the batch-driver python process. By using proper persistent connections, we can reduce the TLS overhead to single-digit percents of a ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:1214,config,configured,1214,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['config'],['configured']
Modifiability,"nats are now not types and align more with the ReferenceGenome structure, making for better function signatures. There's a `NatBase` with `Nat` and `NatVariable` as extending classes. For `NatVariable`, instances mutate a single class variable `_nat`. Type variables behave similarly using using a map of name -> box, but I didn't really see the point of ever needing more than one `nat` variable in the same context. Either way it seems a little weird so glad to take feedback.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5737#issuecomment-479528708:165,extend,extending,165,https://hail.is,https://github.com/hail-is/hail/pull/5737#issuecomment-479528708,8,"['extend', 'variab']","['extending', 'variable', 'variables']"
Modifiability,"nb, from [Authorization Overview](https://kubernetes.io/docs/reference/access-authn-authz/authorization/):. > Caution: System administrators, use care when granting access to pod creation. A user granted permission to create pods (or controllers that create pods) in the namespace can: read all secrets in the namespace; read all config maps in the namespace; and impersonate any service account in the namespace and take any action the account could take. This applies regardless of authorization mode. Permission to create a pod gives you permission to mount any secrets in said namespace. Pod creation is a dangerous and powerful permission. See this [recently closed ticket on k8s](https://github.com/kubernetes/kubernetes/issues/4957). [An issue from June 2018](https://github.com/kubernetes/community/pull/1604) notes this is an issue for multi-tenant clusters. The k8s maintainers don't have bandwidth to iterate on a solution right now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5753#issuecomment-479640539:330,config,config,330,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479640539,1,['config'],['config']
Modifiability,"nd Functionality:</p>; <ul>; <li>Updating axios requests to be delayed by pre-emptive promise creation (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Adding &quot;synchronous&quot; and &quot;runWhen&quot; options to interceptors api (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Updating of transformResponse (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3377"">#3377</a>)</li>; <li>Adding ability to omit User-Agent header (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3703"">#3703</a>)</li>; <li>Adding multiple JSON improvements (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3688"">#3688</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3763"">#3763</a>)</li>; <li>Fixing quadratic runtime and extra memory usage when setting a maxContentLength (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3738"">#3738</a>)</li>; <li>Adding parseInt to config.timeout (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3781"">#3781</a>)</li>; <li>Adding custom return type support to interceptor (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3783"">#3783</a>)</li>; <li>Adding security fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>; <li>Updating follow rediect version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3694"">#3694</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3771"">#3771</a>)</li>; <li>Updating karma sauce launcher to fix failing sauce tests (<a href=""https://github-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11080:1326,config,config,1326,https://hail.is,https://github.com/hail-is/hail/pull/11080,4,['config'],['config']
Modifiability,nd.scala:430); E 	at is.hail.backend.service.Main$.main(Main.scala:33); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.concurrent.TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.handleTimeout(FluxTimeout.java:294); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.doTimeout(FluxTimeout.java:279); E 	at reactor.core.publisher.FluxTimeout$TimeoutTimeoutSubscriber.onNext(FluxTimeout.java:418); E 	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onNext(FluxOnErrorResume.java:79); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.propagateDelay(MonoDelay.java:270); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.run(MonoDelay.java:285); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:68); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:28); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180); E 	at java.util.concurrent.ScheduledThreadPo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:4636,config,configured,4636,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['config'],['configured']
Modifiability,"nddistrict/allow-passing-in-additionl-jwt-headers</li>; <li><a href=""https://github.com/lepture/authlib/commit/38c6444bf4ae193c55c96a4b0d849ea9800d85f8""><code>38c6444</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/333"">#333</a> from jeffsawatzky/maintain-0.15</li>; <li><a href=""https://github.com/lepture/authlib/commit/2ea533c4f658206ec0f09265bd3b3f0c48844667""><code>2ea533c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/388"">#388</a> from minddistrict/backport-httpx-oauth2-client-fixes-...</li>; <li><a href=""https://github.com/lepture/authlib/commit/fca7f8523042cd9eadc68a1be091af9bc4ecab8b""><code>fca7f85</code></a> fix assertion client for httpx</li>; <li><a href=""https://github.com/lepture/authlib/commit/4d8a6ef7775dbe7033bb6d3efbc1919ff13579aa""><code>4d8a6ef</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/390"">#390</a> from minddistrict/parameterize-signing-algoritm-for-r...</li>; <li><a href=""https://github.com/lepture/authlib/commit/1e511edf07afcf14b4ad6704a7333eb5e78ef99a""><code>1e511ed</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/393"">#393</a> from nam3less/maintain-0.15-bugfix-377</li>; <li>Additional commits viewable in <a href=""https://github.com/lepture/authlib/compare/v0.11...v0.15.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=authlib&package-manager=pip&previous-version=0.11&new-version=0.15.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-sta",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11483:5788,parameteriz,parameterize-signing-algoritm-for-r,5788,https://hail.is,https://github.com/hail-is/hail/pull/11483,1,['parameteriz'],['parameterize-signing-algoritm-for-r']
Modifiability,"nding spans.</li>; <li>Fixed TimeElapsedColumn from showing negative.</li>; <li>Fix for escaping strings with a trailing backslash <a href=""https://redirect.github.com/Textualize/rich/issues/2987"">Textualize/rich#2987</a></li>; <li>Fixed exception in Markdown with partial table <a href=""https://redirect.github.com/Textualize/rich/issues/3053"">Textualize/rich#3053</a></li>; <li>Fixed the HTML export template so that the <code>&lt;html&gt;</code> tag comes before the <code>&lt;head&gt;</code> tag <a href=""https://redirect.github.com/Textualize/rich/issues/3021"">Textualize/rich#3021</a></li>; <li>Fixed issue with custom classes overwriting <code>__eq__</code> <a href=""https://redirect.github.com/Textualize/rich/issues/2875"">Textualize/rich#2875</a></li>; <li>Fix rich.pretty.install breakage in iPython <a href=""https://redirect.github.com/Textualize/rich/issues/3013"">Textualize/rich#3013</a></li>; </ul>; <h3>Added</h3>; <ul>; <li>Added Text.extend_style method.</li>; <li>Added Span.extend method.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Text.tab_size now defaults to <code>None</code> to indicate that Console.tab_size should be used.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/Textualize/rich/blob/master/CHANGELOG.md"">rich's changelog</a>.</em></p>; <blockquote>; <h2>[13.5.3] - 2023-09-17</h2>; <h3>Fixed</h3>; <ul>; <li>Markdown table rendering issue with inline styles and links <a href=""https://redirect.github.com/Textualize/rich/issues/3115"">Textualize/rich#3115</a></li>; <li>Fix Markdown code blocks on a light background <a href=""https://redirect.github.com/Textualize/rich/issues/3123"">Textualize/rich#3123</a></li>; </ul>; <h2>[13.5.2] - 2023-08-01</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs assertion error</li>; </ul>; <h2>[13.5.1] - 2023-07-31</h2>; <h3>Fixed</h3>; <ul>; <li>Fix tilde character (<code>~</code>) not includ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13651:2398,extend,extend,2398,https://hail.is,https://github.com/hail-is/hail/pull/13651,2,['extend'],['extend']
Modifiability,"nding spans.</li>; <li>Fixed TimeElapsedColumn from showing negative.</li>; <li>Fix for escaping strings with a trailing backslash <a href=""https://redirect.github.com/Textualize/rich/issues/2987"">Textualize/rich#2987</a></li>; <li>Fixed exception in Markdown with partial table <a href=""https://redirect.github.com/Textualize/rich/issues/3053"">Textualize/rich#3053</a></li>; <li>Fixed the HTML export template so that the <code>&lt;html&gt;</code> tag comes before the <code>&lt;head&gt;</code> tag <a href=""https://redirect.github.com/Textualize/rich/issues/3021"">Textualize/rich#3021</a></li>; <li>Fixed issue with custom classes overwriting <code>__eq__</code> <a href=""https://redirect.github.com/Textualize/rich/issues/2875"">Textualize/rich#2875</a></li>; <li>Fix rich.pretty.install breakage in iPython <a href=""https://redirect.github.com/Textualize/rich/issues/3013"">Textualize/rich#3013</a></li>; </ul>; <h3>Added</h3>; <ul>; <li>Added Text.extend_style method.</li>; <li>Added Span.extend method.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Text.tab_size now defaults to <code>None</code> to indicate that Console.tab_size should be used.</li>; </ul>; <h2>[13.4.2] - 2023-06-12</h2>; <h3>Changed</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/Textualize/rich/commit/e9f75c9912ed25b9777bc0257853370951220b17""><code>e9f75c9</code></a> Merge branch 'py312'</li>; <li><a href=""https://github.com/Textualize/rich/commit/35b64f1237f1c64326329b4668b116809a7fc596""><code>35b64f1</code></a> Merge pull request <a href=""https://redirect.github.com/Textualize/rich/issues/3139"">#3139</a> from Textualize/py312</li>; <li><a href=""https://github.com/Textualize/rich/commit/c8ff546416b086c92d1ab9a1a0bb24447da12826""><code>c8ff546</code></a> version bump</li>; <li><a href=""https://github.com/Textualize/rich/commit/3f8c4af45133590590257de90fcbdc40834c0775""><code>3f8c4af</code></a> tests for ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13758:4703,extend,extend,4703,https://hail.is,https://github.com/hail-is/hail/pull/13758,2,['extend'],['extend']
Modifiability,"nding spans.</li>; <li>Fixed TimeElapsedColumn from showing negative.</li>; <li>Fix for escaping strings with a trailing backslash <a href=""https://redirect.github.com/Textualize/rich/issues/2987"">Textualize/rich#2987</a></li>; <li>Fixed exception in Markdown with partial table <a href=""https://redirect.github.com/Textualize/rich/issues/3053"">Textualize/rich#3053</a></li>; <li>Fixed the HTML export template so that the <code>&lt;html&gt;</code> tag comes before the <code>&lt;head&gt;</code> tag <a href=""https://redirect.github.com/Textualize/rich/issues/3021"">Textualize/rich#3021</a></li>; <li>Fixed issue with custom classes overwriting <code>__eq__</code> <a href=""https://redirect.github.com/Textualize/rich/issues/2875"">Textualize/rich#2875</a></li>; <li>Fix rich.pretty.install breakage in iPython <a href=""https://redirect.github.com/Textualize/rich/issues/3013"">Textualize/rich#3013</a></li>; </ul>; <h3>Added</h3>; <ul>; <li>Added Text.extend_style method.</li>; <li>Added Span.extend method.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Text.tab_size now defaults to <code>None</code> to indicate that Console.tab_size should be used.</li>; </ul>; <h2>[13.4.2] - 2023-06-12</h2>; <h3>Changed</h3>; <ul>; <li>Relaxed markdown-it-py dependency</li>; </ul>; <h2>[13.4.1] - 2023-05-31</h2>; <h3>Fixed</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/Textualize/rich/commit/ec91917deb47b43188312e0e3f03bbab7e4e2e7e""><code>ec91917</code></a> changelog</li>; <li><a href=""https://github.com/Textualize/rich/commit/5360fe6fe4f582e5a5bec591cf7433ed85e6863d""><code>5360fe6</code></a> version bump</li>; <li><a href=""https://github.com/Textualize/rich/commit/e0d3aee1eccd424c98d05b94910f9d5ddb821a40""><code>e0d3aee</code></a> Merge pull request <a href=""https://redirect.github.com/Textualize/rich/issues/3132"">#3132</a> from Textualize/fix-markdown-on-light</li>; <li><a href=""https://g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13651:4633,extend,extend,4633,https://hail.is,https://github.com/hail-is/hail/pull/13651,2,['extend'],['extend']
Modifiability,"nding spans.</li>; <li>Fixed TimeElapsedColumn from showing negative.</li>; <li>Fix for escaping strings with a trailing backslash <a href=""https://redirect.github.com/Textualize/rich/issues/2987"">Textualize/rich#2987</a></li>; <li>Fixed exception in Markdown with partial table <a href=""https://redirect.github.com/Textualize/rich/issues/3053"">Textualize/rich#3053</a></li>; <li>Fixed the HTML export template so that the <code>&lt;html&gt;</code> tag comes before the <code>&lt;head&gt;</code> tag <a href=""https://redirect.github.com/Textualize/rich/issues/3021"">Textualize/rich#3021</a></li>; <li>Fixed issue with custom classes overwriting <code>__eq__</code> <a href=""https://redirect.github.com/Textualize/rich/issues/2875"">Textualize/rich#2875</a></li>; <li>Fix rich.pretty.install breakage in iPython <a href=""https://redirect.github.com/Textualize/rich/issues/3013"">Textualize/rich#3013</a></li>; </ul>; <h3>Added</h3>; <ul>; <li>Added Text.extend_style method.</li>; <li>Added Span.extend method.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Text.tab_size now defaults to <code>None</code> to indicate that Console.tab_size should be used.</li>; </ul>; <h2>[13.4.2] - 2023-06-12</h2>; <h3>Changed</h3>; <ul>; <li>Relaxed markdown-it-py dependency</li>; </ul>; <h2>[13.4.1] - 2023-05-31</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed typing extensions import in markdown <a href=""https://redirect.github.com/Textualize/rich/issues/2979"">Textualize/rich#2979</a></li>; </ul>; <h2>[13.4.0] - 2023-05-31</h2>; <h3>Added</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/Textualize/rich/commit/720800e6930d85ad027b1e9bd0cbb96b5e994ce3""><code>720800e</code></a> fix tab size issue</li>; <li><a href=""https://github.com/Textualize/rich/commit/4037906f2be2d07f864687ef324da2abdf2028b9""><code>4037906</code></a> Merge pull request <a href=""https://redirect.github.com/Textualize/rich/issues/3071"">#30",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13380:4037,extend,extend,4037,https://hail.is,https://github.com/hail-is/hail/pull/13380,2,['extend'],['extend']
Modifiability,"nding spans.</li>; <li>Fixed TimeElapsedColumn from showing negative.</li>; <li>Fix for escaping strings with a trailing backslash <a href=""https://redirect.github.com/Textualize/rich/issues/2987"">Textualize/rich#2987</a></li>; <li>Fixed exception in Markdown with partial table <a href=""https://redirect.github.com/Textualize/rich/issues/3053"">Textualize/rich#3053</a></li>; <li>Fixed the HTML export template so that the <code>&lt;html&gt;</code> tag comes before the <code>&lt;head&gt;</code> tag <a href=""https://redirect.github.com/Textualize/rich/issues/3021"">Textualize/rich#3021</a></li>; <li>Fixed issue with custom classes overwriting <code>__eq__</code> <a href=""https://redirect.github.com/Textualize/rich/issues/2875"">Textualize/rich#2875</a></li>; <li>Fix rich.pretty.install breakage in iPython <a href=""https://redirect.github.com/Textualize/rich/issues/3013"">Textualize/rich#3013</a></li>; </ul>; <h3>Added</h3>; <ul>; <li>Added Text.extend_style method.</li>; <li>Added Span.extend method.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Text.tab_size now defaults to <code>None</code> to indicate that Console.tab_size should be used.</li>; </ul>; <h2>v13.4.2</h2>; <h2>[13.4.2] - 2023-06-12</h2>; <h3>Changed</h3>; <ul>; <li>Relaxed markdown-it-py dependency</li>; </ul>; <h2>Hot fix for typing extension issue</h2>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/Textualize/rich/blob/master/CHANGELOG.md"">rich's changelog</a>.</em></p>; <blockquote>; <h2>[13.5.2] - 2023-08-01</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tab assertion error</li>; </ul>; <h2>[13.5.1] - 2023-07-31</h2>; <h3>Fixed</h3>; <ul>; <li>Fix tilde character (<code>~</code>) not included in link regex when printing to console <a href=""https://redirect.github.com/Textualize/rich/issues/3057"">Textualize/rich#3057</a></li>; </ul>; <h2>[13.5.0] - 2023-07-29</h2>; <h3>Fixed</h3>; <ul>; <",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13380:2003,extend,extend,2003,https://hail.is,https://github.com/hail-is/hail/pull/13380,2,['extend'],['extend']
Modifiability,"ne rules). > Sometimes you want to split a long line inside of single quotes, but you don’t want the backslash/newline to appear in the quoted content. This is often the case when passing scripts to languages such as Perl, where extraneous backslashes inside the script can change its meaning or even be a syntax error. One simple way of handling this is to place the quoted string, or even the entire command, into a make variable then use the variable in the recipe. In this situation the newline quoting rules for makefiles will be used, and the backslash/newline will be removed. If we rewrite our example above using this method:; > ; > ```; > HELLO = 'hello \; > world'; > ; > all : ; @echo $(HELLO); > ```; > ; > we will get output like this:; > ; > ```; > hello world; > ```; >; > If you like, you can also use target-specific variables (see [Target-specific Variable Values](https://www.gnu.org/software/make/manual/html_node/Target_002dspecific.html)) to obtain a tighter correspondence between the variable and the recipe that uses it. It seems to me like there are not any great choices. Putting the JSON into a Make variable seems too magical and likely to confuse a newbie editing this file. Using escaped double quotes is less legible than literal JSON. Putting the whole JSON array on one line is quite long. I guess we can go with double quotes for now. I tested on Make 3.81 and Make 4.4.1. The first EDIT and the original comment follow for context. ---. EDIT: Nope, I still appear to be wrong. Hold on. ---. I have bash 3.2.57; ```; (base) dking@wm28c-761 /tmp % make print-shell; /bin/sh; (base) dking@wm28c-761 /tmp % /bin/sh --version; GNU bash, version 3.2.57(1)-release (arm64-apple-darwin22); Copyright (C) 2007 Free Software Foundation, Inc.; ```. Looks like this was an intentionally backwards incompatible change [in Make 4.0](https://git.savannah.gnu.org/cgit/make.git/tree/NEWS?h=4.0&id=52191d9d613819a77a321ad6c3ab16e1bc73c381#n18) which removed the POSIX-compatible be",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324:1765,variab,variable,1765,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324,2,['variab'],['variable']
Modifiability,"needs to be rebased. also I originally tried to leave things uninitialized but the ASM byte code verifier disapproved of referencing local variables it couldn't prove would have the correct type (i.e. because it was potentially uninitialized it was treated as having type `.`, I guess byte code doesn't actually have type annotations on registers?)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2385#issuecomment-341597070:139,variab,variables,139,https://hail.is,https://github.com/hail-is/hail/pull/2385#issuecomment-341597070,1,['variab'],['variables']
Modifiability,nfun$apply$2(Optimize.scala:22); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:18); E 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:40); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:36); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); E 	at is.hail.expr.ir.Compile$.$anonfun$apply$4(Compile.scala:45); E 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction(Backend.scala:126); E 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction$(Backend.scala:122); E 	at is.hail.backend.local.LocalBackend.lookupOrCompileCachedFunction(LocalBackend.scala:73); E 	at is.hail.expr.ir.Compile$.apply(Compile.scala:39); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$5(CompileAndEvaluate.scala:66); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:66); E 	at,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:8341,adapt,adapted,8341,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['adapt'],['adapted']
Modifiability,"ng. All artifacts, such as HTML reports are served through the batch outputs mechanism described above. Job file dependencies are handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not cancelled when its parents are cancelled.; - [ ] add namespace dependencies. CI allocates anonymous namespaces as requested by the build process. All `exec`s are, by default, run in an anonymous namespace. CI adds a finalizer job that deletes namespaces when all relevant `exec`s are finished; - [ ] add image dependencies. CI can create a batch job that builds a docker image with an anonymous name and pushes it to the project's GCR. CI adds a finalizer job that deletes the image when all relevant `exec`s are finished.; - [ ] batch and notebook are parameterized by their worker namespace so they can use the namespaces described above; - [ ] hail's build steps are parameterized in a way that permits them to use a jar not built locally on this machine (hopefully the Make PR makes this easy, otherwise we have to fool gradle into not rebuilding the jar). To reliably handle clean up, we *must* persist batch jobs, so I think that should be either higher priority or at least happening in parallel to the above (i.e. two developers working in parallel). - [ ] persist batch jobs in a durable store with all of the fields in the beginning of `Job.__init__`. When batch starts up, before serving any requests, it restores its state from the durable store and then refreshes from k8s. The k8s label `hail.is/batch-instance` is retired. Instead, pods have `hail.is/batch-version` which is a monotonically increasing natural number. It is only incremented if batch is backwards incompatible with the pod specs. Probably batch should destroy any pods that are alive from an out-of-date version of batch.; - [ ] persist CI information in a durable store [this needs more detail]; - [ ] How do ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5193:2553,parameteriz,parameterized,2553,https://hail.is,https://github.com/hail-is/hail/issues/5193,2,['parameteriz'],['parameterized']
Modifiability,ng.LowerAndExecuteShufflesPass.apply(LoweringPass.scala:161); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:50); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:517); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:546); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:542); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:541); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:62); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$3(SparkBackend.scala:368); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:364); 	at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:541); 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:81); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:14006,adapt,adapted,14006,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['adapt'],['adapted']
Modifiability,nsion(VariantDataset.scala:425); E at is.hail.variant.VariantDatasetFunctions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$ano,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:7613,Config,Configuration,7613,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['Config'],['Configuration']
Modifiability,nstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:1648,Rewrite,RewriteBottomUp,1648,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,nstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$cl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:1694,Rewrite,RewriteBottomUp,1694,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"nt over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:6497,config,config,6497,https://hail.is,https://github.com/hail-is/hail/pull/8561,1,['config'],['config']
Modifiability,"nt.py\"", line 1285, in _send_request\n self.endheaders(body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1234, in endheaders\n self._send_output(message_body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1026, in _send_output\n self.send(msg)\n File \""/usr/lib/python3.6/http/client.py\"", line 964, in send\n self.connect()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 181, in connect\n conn = self._new_conn()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 168, in _new_conn\n self, \""Failed to establish a new connection: %s\"" % e)\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/requests/adapters.py\"", line 449, in send\n timeout=timeout\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 638, in urlopen\n _stacktrace=sys.exc_info()[2])\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\"", line 399, in increment\n raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1341, in polling_event_loop\n await refresh_k8s_state()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1332, in refresh_k8s_state\n await refresh_k8s_pods()\n Fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6754:2488,adapt,adapters,2488,https://hail.is,https://github.com/hail-is/hail/issues/6754,1,['adapt'],['adapters']
Modifiability,"nt/compare/v7.3.2...37ca37d865db260e7da6fa85339be450d6fd3c3c"">Full Changelog</a>)</p>; <h3>Bugs fixed</h3>; <ul>; <li>Add local-provisioner entry point to pyproject.toml Fixes <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/issues/800"">#800</a> <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/801"">#801</a> (<a href=""https://github.com/utkonos""><code>@​utkonos</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2022-06-06&amp;to=2022-06-07&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Autkonos+updated%3A2022-06-06..2022-06-07&amp;type=Issues""><code>@​utkonos</code></a></p>; <h2>v7.3.2</h2>; <h2>7.3.2</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v7.3.1...c81771416d9e09e0e92be799f3e8549d0db57e43"">Full Changelog</a>)</p>; <h3>Enhancements made</h3>; <ul>; <li>Correct <code>Any</code> type annotations. <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/791"">#791</a> (<a href=""https://github.com/joouha""><code>@​joouha</code></a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/jupyter_client/blob/main/CHANGELOG.md"">jupyter-client's changelog</a>.</em></p>; <blockquote>; <h2>7.3.4</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v7.3.3...ca4cb2d6a4b95a6925de85a47b323d2235032c74"">Full Changelog</a>)</p>; <h3>Bugs fixed</h3>; <ul>; <li>Revert latest changes to <code>ThreadedZMQSocketChannel</code> because they break Qtconsole <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/803"">#803</a> (<a href=""https://github.com/ccordoba12""><code>@​ccordoba12</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improveme",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12110:3031,Enhance,Enhancements,3031,https://hail.is,https://github.com/hail-is/hail/pull/12110,1,['Enhance'],['Enhancements']
Modifiability,"ntaining the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:5929,config,config,5929,https://hail.is,https://github.com/hail-is/hail/pull/8513,2,['config'],"['config', 'configurations']"
Modifiability,ntext.scala:164) sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) java.lang.reflect.Method.invoke(Method.java:498) py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) py4j.Gateway.invoke(Gateway.java:280) py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) py4j.commands.CallCommand.execute(CallCommand.java:79) py4j.GatewayConnection.run(GatewayConnection.java:214) java.lang.Thread.run(Thread.java:745) at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2278) at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2274) at scala.Option.foreach(Option.scala:257) at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2274) at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2353) at org.apache.spark.SparkContext.<init>(SparkContext.scala:85) at is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:84) at is.hail.HailContext$.apply(HailContext.scala:164) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at py4j.Gateway.invoke(Gateway.java:280) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:214) at java.lang.Thread.run(Thread.java:745). Thank you so much!!,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1525:6337,config,configureAndCreateSparkContext,6337,https://hail.is,https://github.com/hail-is/hail/issues/1525,1,['config'],['configureAndCreateSparkContext']
Modifiability,"ntrol flow instructions (which I will not go into here, for they are not relevant now), as well as handling local variables that are used across a method split. These shared Local variables are replaced by fields on a ""spills"" class which is allocated any time a split method is called. Spilled local `store`s are rewritten as field `store`s, and `load`s are rewritten as field `load`s. # What was the problem here?. A region split was inserted *directly between* the `I2B` instruction and the call to `OutputBuffer.write`. This meant that the result of `I2B` was stored in a local variable and read in the subsequent block. **The incorrect TypeInfo of Boolean was used for that local variable**, but this seems not to pose a problem -- both Boolean and Byte use a single slot, and so the code still works even with the wrong variable type. However, the method splitter then **generated a method split at the same point where the region was split**. This means that the local variable resulting from I2B is spilled to a class field on the spills class. Our incorrectly-Boolean local becomes an incorrectly-Boolean **field**, and this is where things go wrong -- it seems as though Boolean class fields (appropriately) truncate on store and load a single bit. Our value of `3` was stored as a class Boolean, and came out `1`. The fact that a single field's missingness was flipped was a red herring -- all higher bits are flipped to 0 (defined)! Here's a look at the LIR looks like, though it was ultimately the JVM class file printout that tipped me off to the problem:. ```code. # I2B is stored as a class field on spills. The Z at the end of the next line indicates this field is a Boolean, not a byte. 31017 (PutFieldX PUTFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2355__l2315split_large_block Z; 31018 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;); 31019 25774 (InsnX I2B; 31020 25775 (InsnX IOR; 31021 25776 (I",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:3369,variab,variable,3369,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['variab'],['variable']
Modifiability,"numpy-1.20.1 oauthlib-3.1.0 packaging-20.9 pandas-1.1.4 parsimonious-0.8.1 parso-0.8.1 pexpect-4.8.0 pickleshare-0.7.5 pillow-8.1.2 prompt-toolkit-3.0.17 protobuf-3.15.6 ptyprocess-0.7.0 py4j-0.10.7 pyasn1-0.4.8 pyasn1-modules-0.2.8 pygments-2.8.1 pyparsing-2.4.7 pyspark-2.4.1 python-dateutil-2.8.1 python-json-logger-0.1.11 pytz-2021.1 requests-2.22.0 requests-oauthlib-1.3.0 rsa-4.7.2 scipy-1.6.1 six-1.15.0 tabulate-0.8.3 tornado-6.1 tqdm-4.42.1 traitlets-5.0.5 typing-extensions-3.7.4.3 urllib3-1.25.11 wcwidth-0.2.5 wrapt-1.12.1 yarl-1.6.3; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]; snafu$ ipython ; Python 3.8.6 (default, Jan 27 2021, 15:42:20) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.21.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-e24d842d2b9a> in <module>; ----> 1 import hail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.acc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:9880,sandbox,sandbox,9880,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['sandbox'],['sandbox']
Modifiability,o_struct_of_o_binaryANDo_int32END_TO_SIndexablePointer(Unknown Source); E 	at __C92844etypeDecode.apply(Unknown Source); E 	at is.hail.io.CompiledDecoder.readRegionValue(Decoder.scala:31); E 	at is.hail.io.AbstractTypedCodecSpec.decodeArrays(CodecSpec.scala:57); E 	at is.hail.io.AbstractTypedCodecSpec.decodeArrays$(CodecSpec.scala:54); E 	at is.hail.io.TypedCodecSpec.decodeArrays(TypedCodecSpec.scala:19); E 	at is.hail.expr.ir.Interpret$.$anonfun$run$1(Interpret.scala:80); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:79); E 	at is.hail.expr.ir.Interpret$.interpret$1(Interpret.scala:67); E 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:110); E 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:58); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$foldConstants$1(FoldConstants.scala:47); E 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); E 	at is.hail.expr.ir.FoldConstants$.foldConstants(FoldConstants.scala:13); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$apply$1(FoldConstants.scala:10); E 	at is.hail.backend.ExecuteContext$.$anonfun$scopedNewRegion$1(ExecuteContext.scala:86); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.backend.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:83); E 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:9); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$4(Optimize.scala:22); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$1(Optimize.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:6238,Rewrite,RewriteBottomUp,6238,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"oal of this PR is to make this work:. ```; HAIL_QUERY_BACKEND=service \; python3 -c 'import hail as hl; hl.utils.range_table(10).write(""gs://foo/bar.t"")`; ```. In particular, a normal user should not need to know the location of a Hail Query JAR. Currently, you must specify two environment variables: `HAIL_SHA` and `HAIL_JAR_URL`. This PR takes advantage of the well known location of a Hail Query JAR [1]. We use the newly introduced `hl.revision()` to determine the SHA-1 of the currently installed Hail. This PR includes the revision in the driver job spec. The front end has been modified to convert the revision into a cloud storage URL. This PR also provides three escape hatches to the aforementioned default behavior. These escape hatches should more or less only be used by developers. They're specified from highest priority to lowest.; 1. Specify the `jar_url` parameter to `ServiceBackend`.; 2. Specify the `HAIL_JAR_URL` environment variable.; 3. Specify a JAR url in the user config: `hailctl config set query/jar_url gs://...`. While writing this PR, I decided to clean up five bits of cruft I left when I first built the service backend. First, I took the JAR URL out of the ""command"" of the job spec. This ""command"" is just an array of strings. The fact that certain parts of that array *must* be the JAR URL and the SHA-1 is confusing. Instead, there are now two keys in a JVM process specification:; 1. `jar_spec`, which may be either `{""type"": ""jar_url"", ""value"": ""gs://..../abc123....jar""}` or `{""type"":""git_revision"", ""value"": ""abc123...""}`.; 2. `argv`, an opaque list of strings which are passed, by the JVMEntryway, along with a few more args, to `is.hail.backend.service.Main`. The `Main` class dispatches to either `ServiceBackendSocketAPI2` or the `Worker` based on the first element of `argv`. Each class expects different contents in `argv` that suits its needs. Second, I completely eliminated the HAIL_SHA/revision from the Worker and Hail Query Java code. This was on",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11645:1129,config,config,1129,https://hail.is,https://github.com/hail-is/hail/pull/11645,2,['config'],['config']
Modifiability,"ocal computer using singularity instead of docker, it worked. Also, for the the other chromosomes with less variants but same samples, such as chr13, it worked well in Terra. Since we will convert multiple plink files to hailmatrix table using Terra platform in future, I need to figure the problem out. Any advise would be appreciated. ### Version. 0.2.127. ### Relevant log output. ```shell; 2024/01/17 20:20:25 Starting container setup.; 2024/01/17 20:20:26 Done container setup.; 2024/01/17 20:20:27 Starting localization.; 2024/01/17 20:20:34 Localization script execution started...; 2024/01/17 20:20:34 Localizing input gs://fc-5a8938eb-1299-4afc-957f-afb53ef602b9/submissions/e8747e74-47d1-4f52-acfc-1ac7f81d79ba/VUMCBed2HailMatrix/683447d9-9342-4058-bcfc-ba21422d3121/call-Bed2HailMatrix/script -> /cromwell_root/script; 2024/01/17 20:20:36 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bed -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bed; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.fam -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.fam; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bim -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bim; Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.fam...; / [0 files][ 0.0 B/910.3 KiB] / [1 files][910.3 KiB/910.3 KiB] Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.bim...; / [1 files][910.3 KiB/369.7 MiB] - - [1 files][ 51.9 MiB/369.7 MiB] \ | | [1 files][107.6 MiB/369.7 MiB] / - - [1 files][162.3 MiB/369.7 MiB] \ \ [1 files][213.9 MiB/369.7 MiB] | / / [1 files][286.6 MiB/369.7 MiB] - \ \ [1 files][342.1 MiB/369.7 MiB] |; Operation completed over 2 objects/369.7 MiB.; | [2 files][369.7 MiB/369.7 MiB] 2024/01/17 20:59:27 Localization script execution complete.; 2024/01/17 20:59:38 Done localization.; 2024/01/17 20:59:39 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint=/bin/bash hailgenetics/hail@sha256:3f22576793ce3161893aed2bd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:9783,sandbox,sandbox,9783,https://hail.is,https://github.com/hail-is/hail/issues/14168,1,['sandbox'],['sandbox']
Modifiability,"ockquote>; <h2>[2.0.2] - 2021-07-27</h2>; <h3>Added</h3>; <ul>; <li>Officially supporting 3.9 - <a href=""https://github.com/felixonmars""><code>@​felixonmars</code></a>.</li>; <li>You can now add static fields to log objects - <a href=""https://github.com/cosimomeli""><code>@​cosimomeli</code></a>.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Dropped 3.4 support.</li>; <li>Dropped Travis CI for Github Actions.</li>; <li>Wheel should build for python 3 instead of just 3.4 now.</li>; </ul>; <h2>[2.0.1] - 2020-10-12</h2>; <h3>Added</h3>; <ul>; <li>Support Pypi long descripton - <a href=""https://github.com/ereli-cb""><code>@​ereli-cb</code></a></li>; </ul>; <h3>Changed</h3>; <ul>; <li>You can now rename output fields - <a href=""https://github.com/schlitzered""><code>@​schlitzered</code></a></li>; </ul>; <h2>[2.0.0] - 2020-09-26</h2>; <h3>Added</h3>; <ul>; <li>New Changelog</li>; <li>Added timezone support to timestamps - <a href=""https://github.com/lalten""><code>@​lalten</code></a></li>; <li>Refactored log record to function - <a href=""https://github.com/georgysavva""><code>@​georgysavva</code></a></li>; <li>Add python 3.8 support - <a href=""https://github.com/tommilligan""><code>@​tommilligan</code></a></li>; </ul>; <h3>Removed</h3>; <ul>; <li>Support for Python 2.7</li>; <li>Debian directory</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/madzak/python-json-logger/commit/076b407aa7f34bc64a729cc77da336fb159d7597""><code>076b407</code></a> Release 2.0.2</li>; <li><a href=""https://github.com/madzak/python-json-logger/commit/f51d8fe76154380cac2fe6a30a944d67dc09df2d""><code>f51d8fe</code></a> added test/build requirements to ci file</li>; <li><a href=""https://github.com/madzak/python-json-logger/commit/b07b580670c6c4e340c372c73d0e76cdddc8b456""><code>b07b580</code></a> moved release out of test workflow. setup.cfg specifies a proper wheel now</li>; <li><a href=""https://github.com/madzak/python-json-logger/commit/4df12f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11467:2310,Refactor,Refactored,2310,https://hail.is,https://github.com/hail-is/hail/pull/11467,1,['Refactor'],['Refactored']
Modifiability,"ode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:4886,config,configuration,4886,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['config'],['configuration']
Modifiability,"ode></a> Streamline overloaded assertion methods for Groovy</li>; <li><a href=""https://github.com/cbeust/testng/commit/5ac0021d14f7eb00804fe235aaefc5c2fbce57d1""><code>5ac0021</code></a> Adding release notes</li>; <li><a href=""https://github.com/cbeust/testng/commit/c0e1e772f1fc0ab2142f3a4114a2b8cfe60fa7e1""><code>c0e1e77</code></a> Adjust version reference in deprecation msgs.</li>; <li><a href=""https://github.com/cbeust/testng/commit/011527d9bf0f91a40539f5e5467cc106888810d9""><code>011527d</code></a> Bump version to 7.7.0 for release</li>; <li><a href=""https://github.com/cbeust/testng/commit/7846c444a411647f7e401a097224702188c93835""><code>7846c44</code></a> Deprecate support for running JUnit tests</li>; <li><a href=""https://github.com/cbeust/testng/commit/8630a7e8fe12985d71c00212f9362fd38fb0cb9e""><code>8630a7e</code></a> Ensure ITestContext available for JUnit4 tests</li>; <li><a href=""https://github.com/cbeust/testng/commit/7070b020def0089d0d9dc695a5762ad16e974ce6""><code>7070b02</code></a> Streamline dependsOnMethods for configurations</li>; <li><a href=""https://github.com/cbeust/testng/commit/d7e0bb1cbcd7933d34d704678e75cbaf42704505""><code>d7e0bb1</code></a> Deprecate support for running Spock Tests</li>; <li><a href=""https://github.com/cbeust/testng/commit/ca7a3a293008389096be75fea4936af8e5f79650""><code>ca7a3a2</code></a> Ensure All tests run all the time</li>; <li>Additional commits viewable in <a href=""https://github.com/cbeust/testng/compare/testng-6.8.21...7.7.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.testng:testng&package-manager=gradle&previous-version=6.8.21&new-version=7.7.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase m",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:16082,config,configurations,16082,https://hail.is,https://github.com/hail-is/hail/pull/12665,1,['config'],['configurations']
Modifiability,"ode>build_encrypted_persistence()</code> API. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/87"">#87</a>, <a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/110"">#110</a>)</li>; <li>Remove: Old TokenCache API which has been deprecated for 2 years. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/110"">#110</a>)</li>; <li>Enhancement: Make all platform-dependent parameters optional (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/103"">#103</a>)</li>; <li>Enhancement: Provide <code>PersistenceEncryptError</code> and <code>PersistenceDecryptError</code>, currently raised when encryption on Windows fails. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/108"">#108</a>)</li>; <li>Enhancement: The data file will be created with <code>600</code> permission when running in Unix-like systems. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/107"">#107</a>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/AzureAD/microsoft-authentication-extensions-for-python/commit/a88fa673af3602fe7c8c922314599b0c245e7add""><code>a88fa67</code></a> Merge branch 'release-1.0.0'</li>; <li><a href=""https://github.com/AzureAD/microsoft-authentication-extensions-for-python/commit/bd5b4074dbb7d03c9d91ce6a75378851be92552a""><code>bd5b407</code></a> Update README to reflect the new APIs</li>; <li><a href=""https://github.com/AzureAD/microsoft-authentication-extensions-for-python/commit/6f77b1e70be086aae752dcf7e08d7f06bcabdcd7""><code>6f77b1e</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11992:1486,Enhance,Enhancement,1486,https://hail.is,https://github.com/hail-is/hail/pull/11992,1,['Enhance'],['Enhancement']
Modifiability,"ode>html_style</code> to an iterable of strings.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10366"">#10366</a>: std domain: Add support for emphasising placeholders in :rst:dir:<code>option</code>; directives through a new :confval:<code>option_emphasise_placeholders</code> configuration; option.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10439"">#10439</a>: std domain: Use the repr of some variables when displaying warnings,; making whitespace issues easier to identify.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10571"">#10571</a>: quickstart: Reduce content in the generated <code>conf.py</code> file. Patch by; Pradyun Gedam.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10648"">#10648</a>: LaTeX: CSS-named-alike additional :ref:<code>'sphinxsetup' &lt;latexsphinxsetup&gt;</code>; keys allow to configure four separate border-widths, four paddings, four; corner radii, a shadow (possibly inset), colours for border, background, shadow; for each of the code-block, topic, attention, caution, danger, error and warning; directives.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10655"">#10655</a>: LaTeX: Explain non-standard encoding in LatinRules.xdy</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10599"">#10599</a>: HTML Theme: Wrap consecutive footnotes in an <code>&lt;aside&gt;</code> element when; using Docutils 0.18 or later, to allow for easier styling. This matches the; behaviour introduced in Docutils 0.19. Patch by Adam Turner.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10518"">#10518</a>: config: Add <code>include_patterns</code> as the opposite of <code>exclude_patterns</code>.; Patch by Adam Turner.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <su",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12165:4104,config,configure,4104,https://hail.is,https://github.com/hail-is/hail/pull/12165,1,['config'],['configure']
Modifiability,"ode>sphinx.util.stemmer</code> in favour of <code>snowballstemmer</code>.; Patch by Adam Turner.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9856"">#9856</a>: Deprecated <code>sphinx.ext.napoleon.iterators</code>.</li>; </ul>; <h2>Features added</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10444"">#10444</a>: html theme: Allow specifying multiple CSS files through the <code>stylesheet</code>; setting in <code>theme.conf</code> or by setting <code>html_style</code> to an iterable of strings.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10366"">#10366</a>: std domain: Add support for emphasising placeholders in :rst:dir:<code>option</code>; directives through a new :confval:<code>option_emphasise_placeholders</code> configuration; option.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10439"">#10439</a>: std domain: Use the repr of some variables when displaying warnings,; making whitespace issues easier to identify.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10571"">#10571</a>: quickstart: Reduce content in the generated <code>conf.py</code> file. Patch by; Pradyun Gedam.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10648"">#10648</a>: LaTeX: CSS-named-alike additional :ref:<code>'sphinxsetup' &lt;latexsphinxsetup&gt;</code>; keys allow to configure four separate border-widths, four paddings, four; corner radii, a shadow (possibly inset), colours for border, background, shadow; for each of the code-block, topic, attention, caution, danger, error and warning; directives.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10655"">#10655</a>: LaTeX: Explain non-standard encoding in LatinRules.xdy</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10599"">#10599</a>: HTML Theme: Wrap c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12165:3616,variab,variables,3616,https://hail.is,https://github.com/hail-is/hail/pull/12165,1,['variab'],['variables']
Modifiability,"ods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""rbac.authorization.k8s.io/v1, Resource=roles"", GroupVersionKind: ""rbac.authorization.k8s.io/v1, Kind=Role""; Name: ""batch-pods-admin"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""rbac.authorization.k8s.io/v1"" ""kind"":""Role"" ""metadata"":map[""name"":""batch-pods-admin"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""rules"":[map[""apiGroups"":[""""] ""resources"":[""pods""] ""verbs"":[""get"" ""list"" ""watch"" ""create"" ""update"" ""patch"" ""delete""]] map[""apiGroups"":[""""] ""resources"":[""pods/log""] ""verbs"":[""get""]]]]}; from server for: ""deployment.yaml"": roles.rbac.authorization.k8s.io ""batch-pods-admin"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get roles.rbac.authorization.k8s.io in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""rbac.authorization.k8s.io/v1, Resource=rolebindings"", GroupVersionKind: ""rbac.authorization.k8s.io/v1, Kind=RoleBinding""; Name: ""batch-pods-admin-binding"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""rbac.authorization.k8s.io/v1"" ""kind"":""RoleBinding"" ""metadata"":map[""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""name"":""batch-pods-admin-binding"" ""namespace"":""batch-pods""] ""roleRef"":map[""apiGroup"":"""" ""kind"":""Role"" ""name"":""batch-pods-admin""] ""subjects"":[map[""kind"":""ServiceAccount"" ""name"":""batch-svc"" ""namespace"":""default""]]]}; from server for: ""deployment.yaml"": rolebindings.rbac.authorization.k8s.io ""batch-pods-admin-binding"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get rolebindings.rbac.authorization.k8s.io in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error wh",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:2295,config,configuration,2295,https://hail.is,https://github.com/hail-is/hail/issues/4609,1,['config'],['configuration']
Modifiability,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8335:2061,extend,extends,2061,https://hail.is,https://github.com/hail-is/hail/pull/8335,10,['extend'],['extends']
Modifiability,"of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed serv",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:7117,config,configuration,7117,https://hail.is,https://github.com/hail-is/hail/pull/8561,2,['config'],"['configuration', 'configures']"
Modifiability,"oft-authentication-extensions-for-python/releases"">msal-extensions's releases</a>.</em></p>; <blockquote>; <h2>MSAL Extensions for Python, 1.0.0</h2>; <p>This package is now considered stable and production-ready.</p>; <ul>; <li>New: Add a new platform-independent <code>build_encrypted_persistence()</code> API. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/87"">#87</a>, <a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/110"">#110</a>)</li>; <li>Remove: Old TokenCache API which has been deprecated for 2 years. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/110"">#110</a>)</li>; <li>Enhancement: Make all platform-dependent parameters optional (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/103"">#103</a>)</li>; <li>Enhancement: Provide <code>PersistenceEncryptError</code> and <code>PersistenceDecryptError</code>, currently raised when encryption on Windows fails. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/108"">#108</a>)</li>; <li>Enhancement: The data file will be created with <code>600</code> permission when running in Unix-like systems. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/107"">#107</a>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/AzureAD/microsoft-authentication-extensions-for-python/commit/a88fa673af3602fe7c8c922314599b0c245e7add""><code>a88fa67</code></a> Merge branch 'release-1.0.0'</li>; <li><a href=""https://github.com/AzureAD/microsoft-authentication-extensions-for-python/commit/bd5b4074dbb7d03c9d91ce6a75378851be92552a""><code>bd5b407</code></a> Update README to reflect the new APIs</li>; <li><a href=""https:/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11992:1199,Enhance,Enhancement,1199,https://hail.is,https://github.com/hail-is/hail/pull/11992,1,['Enhance'],['Enhancement']
Modifiability,"ok, so back to the same error. I think your Spark cluster must be configured incorrectly, which is something we can't really help with. You should be able to do:; ```; pyspark; ```; and then:; ```python; sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); ```; without error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321224012:66,config,configured,66,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321224012,1,['config'],['configured']
Modifiability,"okay @daniel-goldstein https://github.com/hail-is/hail/pull/14132 replaces just the formatting/format checking with `ruff`, and i'll make another PR once that goes in to extend the `ruff` linting to the `hail` folder. thanks so much for your help on this! :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14131#issuecomment-1883560111:170,extend,extend,170,https://hail.is,https://github.com/hail-is/hail/pull/14131#issuecomment-1883560111,1,['extend'],['extend']
Modifiability,"ole <a href=""https://redirect.github.com/Textualize/rich/issues/3057"">Textualize/rich#3057</a></li>; </ul>; <h2>Mostly cake, one or two puppies</h2>; <p><a href=""https://textual.textualize.io/blog/2023/07/29/pull-requests-are-cake-or-puppies/"">https://textual.textualize.io/blog/2023/07/29/pull-requests-are-cake-or-puppies/</a></p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/Textualize/rich/blob/master/CHANGELOG.md"">rich's changelog</a>.</em></p>; <blockquote>; <h2>[13.7.0] - 2023-11-15</h2>; <h3>Added</h3>; <ul>; <li>Adds missing parameters to Panel.fit <a href=""https://redirect.github.com/Textualize/rich/issues/3142"">Textualize/rich#3142</a></li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Some text goes missing during wrapping when it contains double width characters <a href=""https://redirect.github.com/Textualize/rich/issues/3176"">Textualize/rich#3176</a></li>; <li>Ensure font is correctly inherited in exported HTML <a href=""https://redirect.github.com/Textualize/rich/issues/3104"">Textualize/rich#3104</a></li>; <li>Fixed typing for <code>FloatPrompt</code>.</li>; </ul>; <h2>[13.6.0] - 2023-09-30</h2>; <h3>Added</h3>; <ul>; <li>Added Python 3.12 to classifiers.</li>; </ul>; <h2>[13.5.3] - 2023-09-17</h2>; <h3>Fixed</h3>; <ul>; <li>Markdown table rendering issue with inline styles and links <a href=""https://redirect.github.com/Textualize/rich/issues/3115"">Textualize/rich#3115</a></li>; <li>Fix Markdown code blocks on a light background <a href=""https://redirect.github.com/Textualize/rich/issues/3123"">Textualize/rich#3123</a></li>; </ul>; <h2>[13.5.2] - 2023-08-01</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs assertion error</li>; </ul>; <h2>[13.5.1] - 2023-07-31</h2>; <h3>Fixed</h3>; <ul>; <li>Fix tilde character (<code>~</code>) not included in link regex when printing to console <a href=""https://redirect.github.com/Textualize/rich/issues/3057",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14012:2874,inherit,inherited,2874,https://hail.is,https://github.com/hail-is/hail/pull/14012,2,['inherit'],['inherited']
Modifiability,"ols/fonttools/issues/3034"">#3034</a>).</li>; <li>[COLRv1] Added method to automatically compute ClipBoxes (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3027"">#3027</a>).</li>; <li>[ttFont] Fixed getGlyphID to raise KeyError on missing glyphs instead of returning; None. The regression was introduced in v4.27.0 (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3032"">#3032</a>).</li>; <li>[sbix] Fixed UnboundLocalError: cannot access local variable 'rawdata' (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3031"">#3031</a>).</li>; <li>[varLib] When building VF, do not overwrite a pre-existing <code>STAT</code> table that was built; with feaLib from FEA feature file. Also, added support for building multiple VFs; defined in Designspace v5 from <code>fonttools varLib</code> script (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3024"">#3024</a>).</li>; <li>[mtiLib] Only add <code>Debg</code> table with lookup names when <code>FONTTOOLS_LOOKUP_DEBUGGING</code>; env variable is set (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3023"">#3023</a>).</li>; </ul>; <h2>4.39.0 (released 2023-03-06)</h2>; <ul>; <li>[mtiLib] Optionally add <code>Debg</code> debug info for MTI feature builds (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3018"">#3018</a>).</li>; <li>[ttx] Support reading input file from standard input using special <code>-</code> character,; similar to existing <code>-o -</code> option to write output to standard output (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3020"">#3020</a>).</li>; <li>[cython] Prevent <code>cython.compiled</code> raise AttributeError if cython not installed; properly (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3017"">#3017</a>).</li>; <li>[OS/2] Guard against ZeroDivisionError when calculating xAvgCharWidth in the unlikely; scenario no glyph has non-zero advance (<a href=""https://redirect.github.com/fontto",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12910:13433,variab,variable,13433,https://hail.is,https://github.com/hail-is/hail/pull/12910,1,['variab'],['variable']
Modifiability,"ols/fonttools/pull/3042"">fonttools/fonttools#3042</a>, <a href=""https://redirect.github.com/fonttools/fonttools/pull/3043"">fonttools/fonttools#3043</a>).</li>; <li>[ttProgram] Handle string input to Program.fromAssembly() (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3038"">fonttools/fonttools#3038</a>).</li>; <li>[otlLib] Added a config option to emit GPOS 7 lookups, currently disabled by default because of a macOS bug (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3034"">fonttools/fonttools#3034</a>).</li>; <li>[COLRv1] Added method to automatically compute ClipBoxes (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3027"">fonttools/fonttools#3027</a>).</li>; <li>[ttFont] Fixed getGlyphID to raise KeyError on missing glyphs instead of returning None. The regression was introduced in v4.27.0 (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3032"">fonttools/fonttools#3032</a>).</li>; <li>[sbix] Fixed UnboundLocalError: cannot access local variable 'rawdata' (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3031"">fonttools/fonttools#3031</a>).</li>; <li>[varLib] When building VF, do not overwrite a pre-existing <code>STAT</code> table that was built with feaLib from FEA feature file. Also, added support for building multiple VFs defined in Designspace v5 from <code>fonttools varLib</code> script (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3024"">fonttools/fonttools#3024</a>).</li>; <li>[mtiLib] Only add <code>Debg</code> table with lookup names when <code>FONTTOOLS_LOOKUP_DEBUGGING</code> env variable is set (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3023"">fonttools/fonttools#3023</a>).</li>; </ul>; <h2>4.39.0</h2>; <ul>; <li>[mtiLib] Optionally add <code>Debg</code> debug info for MTI feature builds (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3018"">#3018</a>).</li>; <li>[ttx] Support reading input file from standard input using special <code",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12910:3004,variab,variable,3004,https://hail.is,https://github.com/hail-is/hail/pull/12910,1,['variab'],['variable']
Modifiability,"om the exception chain in; 447 # Python 3 (including for exceptions like SystemExit).; 448 # Otherwise it looks like a bug in the code. File /opt/conda/lib/python3.10/http/client.py:1375, in HTTPConnection.getresponse(self); 1374 try:; -> 1375 response.begin(); 1376 except ConnectionError:. File /opt/conda/lib/python3.10/http/client.py:318, in HTTPResponse.begin(self); 317 while True:; --> 318 version, status, reason = self._read_status(); 319 if status != CONTINUE:. File /opt/conda/lib/python3.10/http/client.py:287, in HTTPResponse._read_status(self); 284 if not line:; 285 # Presumably, the server closed the connection before; 286 # sending a valid response.; --> 287 raise RemoteDisconnected(""Remote end closed connection without""; 288 "" response""); 289 try:. RemoteDisconnected: Remote end closed connection without response. During handling of the above exception, another exception occurred:. ProtocolError Traceback (most recent call last); File /opt/conda/lib/python3.10/site-packages/requests/adapters.py:487, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies); 486 try:; --> 487 resp = conn.urlopen(; 488 method=request.method,; 489 url=url,; 490 body=request.body,; 491 headers=request.headers,; 492 redirect=False,; 493 assert_same_host=False,; 494 preload_content=False,; 495 decode_content=False,; 496 retries=self.max_retries,; 497 timeout=timeout,; 498 chunked=chunked,; 499 ); 501 except (ProtocolError, OSError) as err:. File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:787, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw); 785 e = ProtocolError(""Connection aborted."", e); --> 787 retries = retries.increment(; 788 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]; 789 ); 790 retries.sleep(). File /opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py:550, in Retry.increment(self, m",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960:6273,adapt,adapters,6273,https://hail.is,https://github.com/hail-is/hail/issues/13960,1,['adapt'],['adapters']
Modifiability,"om/danielduhh""><code>@​danielduhh</code></a>) (<a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/133"">#133</a> <a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/134"">#134</a>)</li>; </ul>; <p>Version 0.8.0 (released 2019-03-17)</p>; <ul>; <li>Respect <code>dot_notation</code> flag in ignore argument (<a href=""https://github.com/yoyonel""><code>@​yoyonel</code></a>) (<a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/107"">#107</a>)</li>; <li>Adds argument for toggling dot notation in diff. (<a href=""https://github.com/robinchew""><code>@​robinchew</code></a>)</li>; </ul>; <p>Version 0.7.2 (released 2019-02-22)</p>; <ul>; <li>Two NaN values are considered the same, hence they are not shown in <code>diff</code>; output. (<a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/114"">#114</a>) (<a href=""https://github.com/t-b""><code>@​t-b</code></a>)</li>; <li>Refactors <code>diff</code> method to reduce recursive call stack size. (<a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/112"">#112</a>); (<a href=""https://github.com/yoyonel""><code>@​yoyonel</code></a>)</li>; <li>Python porting best practice use feature detection instead; of version detection to save an import and pass both PyLint; and Flake8 tests with neither 'pragma' nor 'noqa'. (<a href=""https://github.com/cclauss""><code>@​cclauss</code></a>)</li>; </ul>; <p>Version 0.7.1 (released 2018-05-04)</p>; <ul>; <li>Resolves issue with keys containing dots. (<a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/101"">#101</a>)</li>; </ul>; <p>Version 0.7.0 (released 2017-10-16)</p>; <ul>; <li>Fixes problem with diff results that reference the original structure by; introduction of <code>deepcopy</code> for all possibly unhashable items. Thus the diff; does not change later when the diffed structures change.</li>; <li>Adds new option for patchi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11485:3929,Refactor,Refactors,3929,https://hail.is,https://github.com/hail-is/hail/pull/11485,1,['Refactor'],['Refactors']
Modifiability,"om/hail-is/hail/pull/12883/commits/ae51e0a9af12e4c89a44e7ce3235f3f665ff4830). ---. [VPC Flow Logs](https://cloud.google.com/vpc/docs/flow-logs):. > VPC Flow Logs records a sample of network flows sent from and received by VM instances, including; > instances used as Google Kubernetes Engine nodes. These logs can be used for network monitoring,; > forensics, real-time security analysis, and expense optimization. I found the collection process the most elucidating part of the documentation. My summary of that; process follows:. 1. Packets are sampled on the network interface of a VM. Google claims an average sampling rate of; 1/30. This rate reduces if the VM is under load. This rate is immutable to us. 2. Within an ""aggregation interval"", packets are aggregated into ""records"" which are keyed (my term); by source & destination. There are currently six choices for aggregation interval: 5s, 30s, 1m,; 5m, 10m, and 15m. 3. Records are sampled. The sampling rate is a user configured floating point number (precision; unclear) between 0 and 1. 4. Metadata is optionally added to the records. The metadata captures information about the source; and destination VM such as project id, VM name, zone, region, GKE pod, GKE service, and geographic; information of external parties. The user may elect to receive all metadata, no metadata, or a; specific set of metadata fields. 5. The records are written to Google Cloud Logging. The pricing of VPC Flow Logs is described at the [network pricing page](https://cloud.google.com/vpc/network-pricing#network-telemetry). Notice that, if logs are only sent to Cloud Logging (not to BigQuery, Pub/Sub, or Cloud Storage):. > If you store your logs in Cloud Logging, logs generation charges are waived, and only Logging charges apply. I believe in this phrase ""logs generation charges"" refers to *VPC Flow logs* generation charges. The Google Cloud Logging [pricing page](https://cloud.google.com/stackdriver/pricing#google-clouds-operations-suite-pricing) ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12883:1066,config,configured,1066,https://hail.is,https://github.com/hail-is/hail/pull/12883,1,['config'],['configured']
Modifiability,"omize RFC7523 <code>alg</code> value</li>; </ul>; <h2>Version 0.15.4</h2>; <p><strong>Released on Jul 17, 2021.</strong></p>; <ul>; <li>Security fix when JWT claims is None.</li>; </ul>; <h2>Version 0.15.3</h2>; <p><strong>Released on Jan 15, 2021.</strong></p>; <ul>; <li>Fixed <code>.authorize_access_token</code> for OAuth 1.0 services, via :gh:<code>issue#308</code>.</li>; </ul>; <h2>Version 0.15.2</h2>; <p><strong>Released on Oct 18, 2020.</strong></p>; <ul>; <li>Fixed HTTPX authentication bug, via :gh:<code>issue#283</code>.</li>; </ul>; <h2>Version 0.15.1</h2>; <p><strong>Released on Oct 14, 2020.</strong></p>; <ul>; <li>Backward compitable fix for using JWKs in JWT, via :gh:<code>issue#280</code>.</li>; </ul>; <h2>Version 0.15</h2>; <p><strong>Released on Oct 10, 2020.</strong></p>; <p>This is the last release before v1.0. In this release, we added more RFCs; implementations and did some refactors for JOSE:</p>; <ul>; <li>RFC8037: CFRG Elliptic Curve Diffie-Hellman (ECDH) and Signatures in JSON Object Signing and Encryption (JOSE)</li>; <li>RFC7638: JSON Web Key (JWK) Thumbprint</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/lepture/authlib/commit/d8e428c9350c792fc3d25dbaaffa3bfefaabd8e3""><code>d8e428c</code></a> Version bump 0.15.5</li>; <li><a href=""https://github.com/lepture/authlib/commit/f24962835fd0725349cb1b368ee69ba0cc8670f9""><code>f249628</code></a> Add changelog</li>; <li><a href=""https://github.com/lepture/authlib/commit/38ac0d22837a2a5400205bdbd11bb63e5aa7660f""><code>38ac0d2</code></a> Improve rfc7523 parameters compatibility.</li>; <li><a href=""https://github.com/lepture/authlib/commit/e880f1640168683d2d568fd88d1dd0ec0b29f0a6""><code>e880f16</code></a> Rename parameters of InsufficientScopeError</li>; <li><a href=""https://github.com/lepture/authlib/commit/afaeaf98c88baa3ea2fa9f9ac3e65f9a55c8773f""><code>afaeaf9</code></a> Merge pul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11483:3580,refactor,refactors,3580,https://hail.is,https://github.com/hail-is/hail/pull/11483,1,['refactor'],['refactors']
Modifiability,ommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:834). is.hail.utils.HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advan,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:5814,adapt,adapted,5814,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['adapt'],['adapted']
Modifiability,"on Linux <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7200"">#7200</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>If the clipboard fails to open on Windows, wait and try again <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7141"">#7141</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fixed saving multiple 1 mode frames to GIF <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7181"">#7181</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Replaced absolute PIL import with relative import <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7173"">#7173</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Removed files and types override <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7194"">#7194</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Removed duplicate config <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7193"">#7193</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Replaced deprecated Py_FileSystemDefaultEncoding for Python &gt;= 3.12 <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7192"">#7192</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Improved wl-paste mimetype handling in ImageGrab <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7094"">#7094</a> [<a href=""https://github.com/rrcgat""><code>@​rrcgat</code></a>]</li>; <li>Updated redirected URLs <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7178"">#7178</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Added <em>repr_jpeg</em>() for IPython display_jpeg <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7135"">#7135</a> [<a href=""https://github.com/n3011""><code>@​n3011</code></",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13321:7758,config,config,7758,https://hail.is,https://github.com/hail-is/hail/pull/13321,1,['config'],['config']
Modifiability,"on't quite match (e.g. an input is let bound, or the fold implementing ""locus is contained in a set of intervals"" is written slightly differently), this uses a standard abstract interpretation framework, which is almost completely insensitive to the form of the IR, only depending on the semantics. It also correctly handles missing key fields, where the previous implementation often produced an unsound transformation of the IR. Also adds a much more thorough test suite than we had before. At the top level, the analysis takes a boolean typed IR `cond` in an environment where there is a reference to some `key`, and produces a set `intervals`, such that `cond` is equivalent to `cond & intervals.contains(key)` (in other words `cond` implies `intervals.contains(key)`, or `intervals` contains all rows where `cond` is true). This means for instance it is safe to replace `TableFilter(t, cond)` with `TableFilter(TableFilterIntervals(t, intervals), cond)`. Then in a second pass it rewrites `cond` to `cond2`, such that `cond & (intervals.contains(key))` is equivalent to `cond2 & intervals.contains(key)` (in other words `cond` implies `cond2`, and `cond2 & intervals.contains(key)` implies `cond`). This means it is safe to replace the `TableFilter(t, cond)` with `TableFilter(TableFilterIntervals(t, intervals), cond2)`. A common example is when `cond` can be completely captured by the interval filter, i.e. `cond` is equivant to `intervals.contains(key)`, in which case we can take `cond2 = True`, and the `TableFilter` can be optimized away. This all happens in the function; ```scala; def extractPartitionFilters(ctx: ExecuteContext, cond: IR, ref: Ref, key: IndexedSeq[String]): Option[(IR, IndexedSeq[Interval])] = {; if (key.isEmpty) None; else {; val extract = new ExtractIntervalFilters(ctx, ref.typ.asInstanceOf[TStruct].typeAfterSelectNames(key)); val trueSet = extract.analyze(cond, ref.name); if (trueSet == extract.KeySetLattice.top); None; else {; val rw = extract.Rewrites(mutabl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13355:1220,rewrite,rewrites,1220,https://hail.is,https://github.com/hail-is/hail/pull/13355,1,['rewrite'],['rewrites']
Modifiability,on.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:5226,Rewrite,RewriteBottomUp,5226,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Rewrite'],['RewriteBottomUp']
Modifiability,on.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optim,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:4575,Rewrite,RewriteBottomUp,4575,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"onda3/lib/python3.10/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 63 tpl = Env.jutils().handleForPython(e.java_exception); 64 deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 65 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 66 except pyspark.sql.utils.CapturedException as e:; 67 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: Chain file 'grch37_to_grch38.over.chain.gz' does not exist. Java stack trace:; is.hail.utils.HailException: Chain file 'grch37_to_grch38.over.chain.gz' does not exist.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.variant.ReferenceGenome.addLiftover(ReferenceGenome.scala:407); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$2(SparkBackend.scala:613); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$2$adapted(SparkBackend.scala:612); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:62); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$1(SparkBackend.scala:347); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$1(SparkBackend.scala:612); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$1$adapted(SparkBackend.scala:611); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); 	at is.hail.backend.spark.SparkBackend.pyAddLiftover(SparkBackend.scala:611); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13993:3808,adapt,adapted,3808,https://hail.is,https://github.com/hail-is/hail/issues/13993,1,['adapt'],['adapted']
Modifiability,"onsistencies (like #9587). A nitpick though... is there a better name for the click context attribute than ""obj""?. > hailctl no longer takes --region (for gcloud dataproc commands). I compute region in GcloudRunner by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this). If consistency with `gcloud dataproc` is desired, I think the opposite (determining zone from cluster region) would be preferable. `gcloud dataproc` commands take a `--region` argument. [`--zone` is an optional argument for `gcloud dataproc clusters create`](https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters/create#--zone). When a cluster's zone is needed to run `gcloud compute` commands, it can be determined using `gcloud dataproc clusters describe <cluster> --format json`. `hailctl dataproc diagnose` currently does this. I believe the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dataproc modify. There aren't any left. Invoking modify now looks like:; > ; > ```; > hailctl dataproc modify my-cluster \; > --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; > ```; >; > The extra in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of modify --help is included below. I have mixed feelings on this one. On the one hand, `--extra-gcloud-update-args` sounds like it is extra arguments for a `gcloud update` command, which isn't a thing. On the ot",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:1713,config,configuration,1713,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393,2,['config'],['configuration']
Modifiability,ontainerregistry/client/__pycache__/docker_creds_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/client/__pycache__/docker_name_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_puller_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_pusher_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_appender_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_appender_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_puller_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_pusher_.cpython-39.pyc; /usr/local/share/google/dataproc/npd-config/docker-monitor-counter.json; /usr/local/share/google/dataproc/npd-config/docker-monitor.json; /usr/local/share/google/dataproc/npd-config/health-checker-docker.json; /usr/local/share/google/dataproc/npd-config/docker-monitor-filelog.json; /usr/local/share/google/dataproc/bdutil/fluentd/container_logging/plugin/test/Dockerfile; /usr/local/share/google/dataproc/bdutil/components/initialize/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/post-install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/activate/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/shared/docker.sh; /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/configure_docker.sh; /run/docker.sock; /tmp/dataproc/uninstall/docker-ce; /tmp/dataproc/components/uninstall/docker-ce.running; /tmp/dataproc/components/uninstall/docker-ce.done; /tmp/dataproc/components/pre-uninstall/docker-ce.running; /tmp/dataproc/components/pre-uninstall/docker-ce.done; /etc/apt/pre,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:11853,config,config,11853,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['config'],['config']
Modifiability,"oogleapis/java-storage/commit/82aacd7922573d6f4779f21cdc83de10616d7a08"">82aacd7</a>)</li>; <li>Update retries for Notifications (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1734"">#1734</a>) (<a href=""https://github.com/googleapis/java-storage/commit/0fb2f1823f9eff8534f15240321003f120fed3f4"">0fb2f18</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency com.google.cloud:google-cloud-shared-dependencies to v3.0.6 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1761"">#1761</a>) (<a href=""https://github.com/googleapis/java-storage/commit/803a90b7747b8972f51d1407616c51084d97c589"">803a90b</a>)</li>; <li>Update dependency net.jqwik:jqwik to v1.7.1 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1758"">#1758</a>) (<a href=""https://github.com/googleapis/java-storage/commit/140e90911229c876de7b674dd1e61b278e8b07fd"">140e909</a>)</li>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.17 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1759"">#1759</a>) (<a href=""https://github.com/googleapis/java-storage/commit/7e3175a56a06dac0aa0841f221a486bb69b5c9bf"">7e3175a</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.13.1...v2.14.0"">2.14.0</a> (2022-10-26)</h2>; <h3>Google Cloud Storage gRPC API Preview</h3>; <p>The first release of <code>google-cloud-storage</code> with support for a subset of the Google Cloud Storage gRPC API which is in private preview. The most common operations have all been implemented and are available for experimentation.</p>; <p>Given not all public api surface of <code>google-cloud-storage</code> classes are supported for gRPC a new annotation <code>@TransportCompatibility</code> has been added to various classes, methods and fields/enum values to signal where that thing can be expected to work. As we implement more of the operations these annotations will be upd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12456:7124,plugin,plugin,7124,https://hail.is,https://github.com/hail-is/hail/pull/12456,2,['plugin'],['plugin']
Modifiability,"oogleapis/java-storage/commit/82aacd7922573d6f4779f21cdc83de10616d7a08"">82aacd7</a>)</li>; <li>Update retries for Notifications (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1734"">#1734</a>) (<a href=""https://github.com/googleapis/java-storage/commit/0fb2f1823f9eff8534f15240321003f120fed3f4"">0fb2f18</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency com.google.cloud:google-cloud-shared-dependencies to v3.0.6 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1761"">#1761</a>) (<a href=""https://github.com/googleapis/java-storage/commit/803a90b7747b8972f51d1407616c51084d97c589"">803a90b</a>)</li>; <li>Update dependency net.jqwik:jqwik to v1.7.1 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1758"">#1758</a>) (<a href=""https://github.com/googleapis/java-storage/commit/140e90911229c876de7b674dd1e61b278e8b07fd"">140e909</a>)</li>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.17 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1759"">#1759</a>) (<a href=""https://github.com/googleapis/java-storage/commit/7e3175a56a06dac0aa0841f221a486bb69b5c9bf"">7e3175a</a>)</li>; </ul>; <h2>v2.14.0</h2>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.13.1...v2.14.0"">2.14.0</a> (2022-10-26)</h2>; <h3>Google Cloud Storage gRPC API Preview</h3>; <p>The first release of <code>google-cloud-storage</code> with support for a subset of the Google Cloud Storage gRPC API which is in private preview. The most common operations have all been implemented and are available for experimentation.</p>; <p>Given not all public api surface of <code>google-cloud-storage</code> classes are supported for gRPC a new annotation <code>@TransportCompatibility</code> has been added to various classes, methods and fields/enum values to signal where that thing can be expected to work. As we implement more of the operations these annot",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12456:1612,plugin,plugin,1612,https://hail.is,https://github.com/hail-is/hail/pull/12456,2,['plugin'],['plugin']
Modifiability,"ool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:310); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:449); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.backend.spark.SparkBackend.executeEncode(SparkBackend.scala:448); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.98-f8833c1ae16b; Error summary: SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.project.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:; ```; We also tried using a similar cluster configuration with dynamic scaling and received the same error. Do you have a recommended cluster configuration to run king() on this many samples?. To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290:8238,config,configuration,8238,https://hail.is,https://github.com/hail-is/hail/issues/12290,2,['config'],['configuration']
Modifiability,op/hailctl/batch/batch_cli_utils.py'; adding 'hailtop/hailctl/batch/cli.py'; adding 'hailtop/hailctl/batch/list_batches.py'; adding 'hailtop/hailctl/batch/submit.py'; adding 'hailtop/hailctl/batch/billing/__init__.py'; adding 'hailtop/hailctl/batch/billing/cli.py'; adding 'hailtop/hailctl/config/__init__.py'; adding 'hailtop/hailctl/config/cli.py'; adding 'hailtop/hailctl/dataproc/__init__.py'; adding 'hailtop/hailctl/dataproc/cli.py'; adding 'hailtop/hailctl/dataproc/cluster_config.py'; adding 'hailtop/hailctl/dataproc/connect.py'; adding 'hailtop/hailctl/dataproc/deploy_metadata.py'; adding 'hailtop/hailctl/dataproc/diagnose.py'; adding 'hailtop/hailctl/dataproc/gcloud.py'; adding 'hailtop/hailctl/dataproc/modify.py'; adding 'hailtop/hailctl/dataproc/start.py'; adding 'hailtop/hailctl/dataproc/submit.py'; adding 'hailtop/hailctl/dataproc/utils.py'; adding 'hailtop/hailctl/dev/__init__.py'; adding 'hailtop/hailctl/dev/ci_client.py'; adding 'hailtop/hailctl/dev/cli.py'; adding 'hailtop/hailctl/dev/config.py'; adding 'hailtop/hailctl/hdinsight/__init__.py'; adding 'hailtop/hailctl/hdinsight/cli.py'; adding 'hailtop/hailctl/hdinsight/start.py'; adding 'hailtop/hailctl/hdinsight/submit.py'; adding 'hailtop/utils/__init__.py'; adding 'hailtop/utils/process.py'; adding 'hailtop/utils/rate_limiter.py'; adding 'hailtop/utils/rates.py'; adding 'hailtop/utils/rich_progress_bar.py'; adding 'hailtop/utils/serialization.py'; adding 'hailtop/utils/time.py'; adding 'hailtop/utils/utils.py'; adding 'hailtop/utils/validate/__init__.py'; adding 'hailtop/utils/validate/validate.py'; adding 'hail-0.2.120.dist-info/METADATA'; adding 'hail-0.2.120.dist-info/WHEEL'; adding 'hail-0.2.120.dist-info/entry_points.txt'; adding 'hail-0.2.120.dist-info/top_level.txt'; adding 'hail-0.2.120.dist-info/RECORD'; removing build/bdist.linux-x86_64/wheel; python3 -m pip install 'pip-tools==6.13.0' && bash ../check_pip_requirements.sh python; Requirement already satisfied: pip-tools==6.13.0 in /usr/loca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13445:12509,config,config,12509,https://hail.is,https://github.com/hail-is/hail/issues/13445,1,['config'],['config']
Modifiability,"or SparkR, use setLogLevel(newLevel).; 17/10/19 08:45:43 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.2.0.cloudera1; /_/. Using Python version 2.7.5 (default, Aug 4 2017 00:39:18); SparkSession available as 'spark'.; >>> import hail; >>> hc = hail.HailContext(); log4j:ERROR setFile(null,false) call failed.; java.io.FileNotFoundException: hail.log (Permission denied); 	at java.io.FileOutputStream.open0(Native Method); 	at java.io.FileOutputStream.open(FileOutputStream.java:270); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:213); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:133); 	at org.apache.log4j.FileAppender.setFile(FileAppender.java:294); 	at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165); 	at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104); 	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842); 	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768); 	at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648); 	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514); 	at org.apache.log4j.PropertyConfigurator.configure(PropertyConfigurator.java:440); 	at is.hail.HailContext$.configureLogging(HailContext.scala:132); 	at is.hail.HailContext$.apply(HailContext.scala:159); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198:1757,config,config,1757,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198,1,['config'],['config']
Modifiability,"or assertThrows\expectThrows methods (Anatolii Yuzhakov); Fixed: GITHUB-2780: Use SpotBugs instead of abandoned FindBugs; Fixed: GITHUB-2801: JUnitReportReporter is too slow; Fixed: GITHUB-2807: buildStackTrace should be fail-safe (Sergey Chernov); Fixed: GITHUB-2830: TestHTMLReporter parameter toString should be fail-safe (Sergey Chernov); Fixed: GITHUB-2798: Parallel executions coupled with retry analyzer results in duplicate retry analyzer instances being created (Krishnan Mahadevan)</p>; <p>7.6.1; Fixed: GITHUB-2761: Exception: ERROR java.nio.file.NoSuchFileException: /tmp/testngXmlPathInJar-15086412835569336174 (Krishnan Mahadevan); 7.6.0; Fixed: GITHUB-2741: Show fully qualified name of the test instead of just the function name for better readability of test output.(Krishnan Mahadevan); Fixed: GITHUB-2725: Honour custom attribute values in TestNG default reports (Krishnan Mahadevan); Fixed: GITHUB-2726: <a href=""https://github.com/AfterClass""><code>@​AfterClass</code></a> config method is executed for EACH <a href=""https://github.com/Test""><code>@​Test</code></a> method when parallel == methods (Krishnan Mahadevan); Fixed: GITHUB-2752: TestListener is being lost when implenting both IClassListener and ITestListener (Krishnan Mahadevan); New: GITHUB-2724: DataProvider: possibility to unload dataprovider class, when done with it (Dzmitry Sankouski); Fixed: GITHUB-217: Configure TestNG to fail when there's a failure in data provider (Krishnan Mahadevan); Fixed: GITHUB-2743: SuiteRunner could not be initial by default Configuration (Nan Liang); Fixed: GITHUB-2729: beforeConfiguration() listener method should be invoked for skipped configurations as well(Nan Liang); Fixed: assertEqualsNoOrder for Collection and Iterators size check was missing (Adam Kaczmarek); Fixed: GITHUB-2709: Testnames not working together with suites in suite (Martin Aldrin); Fixed: GITHUB-2704: IHookable and IConfigurable callback discrepancy (Krishnan Mahadevan); Fixed: GITHUB-2637: Upgrade",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:12815,config,config,12815,https://hail.is,https://github.com/hail-is/hail/pull/12665,1,['config'],['config']
Modifiability,or domain specified. The key bug here was a check for `'domain' not in config` instead of `config['domain'] is None`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13585:71,config,config,71,https://hail.is,https://github.com/hail-is/hail/pull/13585,2,['config'],['config']
Modifiability,or replace the `PYSPARK_PYTHON` variable to your python2 location,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321420671:32,variab,variable,32,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321420671,1,['variab'],['variable']
Modifiability,orImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFun,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:11700,Config,Configuration,11700,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['Config'],['Configuration']
Modifiability,"orcing the global uniqueness of names as a basic invariant of the IR (typecheck could also check this invariant); * keep a string in the `Name`, but no longer require it to be unique. Instead it's just a suggestion for how to show the name in printouts, adding a uniqueifying suffix as needed. With `NormalizeNames` gone, this would let us preserve meaningful variable names further in the lowering pipeline.; * possibly keep other state in the `Name`, for example to allow a more efficient implementation of environments, similar to the `mark` state on `BaseIR`. This is obviously a large change, but there are only a few conceptual pieces (appologies for not managing to separate these out):; * attempt to minimize the number of locations in which the `Name` constructor is called, to make future refactorings easier; * add `freshName()`, which just wraps `genUID()`, returning a `Name`; * convert IR construction to use the convenience methods in `ir.package`, which take scala lambdas to represent blocks with bound variables, instead of manually creating new variable names; * replace uses of the magic constant variable names (`row`, `va`, `sa`, `g`, `global`) with constants (`TableIR.{rowName, globalName}`, `MatrixIR.{rowName, colName, entryName, globalName}`); * the above changes modified the names we use for bound variables in many places. That shouldn't matter, but it cought a couple bugs where it did.; * `NormalizeNames` optionally allows the IR to contain free variables. But it didn't do anything to ensure the newly generated variable names are distinct from any contained free variables. Thus it was possible to rename a bound variable to mistakenly capture a contained free variable. I've fixed that.; * `SimplifySuite` compared simplified IR with the pre-constructed expected IR, carefully controlling the `genUID` global state to make simplify generate exactly the names expected. I've replaced that by just comparing with the expected IR using a alpha-equivalence comparison.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14547:1593,variab,variables,1593,https://hail.is,https://github.com/hail-is/hail/pull/14547,9,['variab'],"['variable', 'variables']"
Modifiability,"orker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,598	job.py	schedule_job:443	schedule job (97, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,656	job.py	schedule_job:443	schedule job (99, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,662	job.py	schedule_job:443	schedule job (100, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,663	job.py	schedule_job:443	schedule job (98, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:32,455	job.py	schedule_job:443	schedule job (101, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:32,455	job.py	schedule_job:443	schedule job (102, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:33,456	hail_logging.py	log:40	https POST /pr-11438-default-g6cibyji6520/batch-driver/api/v1alpha/instances/activate done in 3.2369999999998527s: 200; ERROR	2022-03-02 19:06:33,492	job.py	schedule_job:473	error while scheduling job (95, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 449, in schedule_job\n time",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:2741,config,config,2741,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['config'],['config']
Modifiability,"orm in future, I need to figure the problem out. Any advise would be appreciated. ### Version. 0.2.127. ### Relevant log output. ```shell; 2024/01/17 20:20:25 Starting container setup.; 2024/01/17 20:20:26 Done container setup.; 2024/01/17 20:20:27 Starting localization.; 2024/01/17 20:20:34 Localization script execution started...; 2024/01/17 20:20:34 Localizing input gs://fc-5a8938eb-1299-4afc-957f-afb53ef602b9/submissions/e8747e74-47d1-4f52-acfc-1ac7f81d79ba/VUMCBed2HailMatrix/683447d9-9342-4058-bcfc-ba21422d3121/call-Bed2HailMatrix/script -> /cromwell_root/script; 2024/01/17 20:20:36 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bed -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bed; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.fam -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.fam; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bim -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bim; Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.fam...; / [0 files][ 0.0 B/910.3 KiB] / [1 files][910.3 KiB/910.3 KiB] Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.bim...; / [1 files][910.3 KiB/369.7 MiB] - - [1 files][ 51.9 MiB/369.7 MiB] \ | | [1 files][107.6 MiB/369.7 MiB] / - - [1 files][162.3 MiB/369.7 MiB] \ \ [1 files][213.9 MiB/369.7 MiB] | / / [1 files][286.6 MiB/369.7 MiB] - \ \ [1 files][342.1 MiB/369.7 MiB] |; Operation completed over 2 objects/369.7 MiB.; | [2 files][369.7 MiB/369.7 MiB] 2024/01/17 20:59:27 Localization script execution complete.; 2024/01/17 20:59:38 Done localization.; 2024/01/17 20:59:39 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint=/bin/bash hailgenetics/hail@sha256:3f22576793ce3161893aed2bd40949b1fc822d2b7e6517dc0ac993b62badaff8 /cromwell_root/script; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.81879b1c; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.81879b1c; 24/01/17 20:59:51 WARN NativeCodeLoader: Unable to ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:10024,sandbox,sandbox,10024,https://hail.is,https://github.com/hail-is/hail/issues/14168,1,['sandbox'],['sandbox']
Modifiability,"ormat):; ...; ```. The command decorator replaces the function with one that takes a `List[str]` of command line parameters, parses them, and calls the original function. The option options are pretty self-explanatory. - To access an argument to a group (like `dataproc --beta`) in a (sub)command, use the decorator `click.pass_context` to pass the click context which allows you to access parent group parameters. `dataproc start` is an example:. ```; @dataproc.command(; help=""Start a Dataproc cluster configured for Hail.""); @click.argument('cluster_name'); ...; @click.pass_context; def start(ctx, cluster_name, ...):; beta = ctx.parent.params['beta']; ```. The help output for a group looks like this:. ```; $ hailctl dataproc --help; Usage: hailctl dataproc [OPTIONS] COMMAND [ARGS]... Manage and monitor Hail deployments. Options:; --beta Force use of `beta` in gcloud commands; --help Show this message and exit. Commands:; connect Connect to a running Dataproc cluster; describe Gather information about a Hail (Table or MatrixTable) file...; diagnose Diagnose problems in a Dataproc cluster.; list List Dataproc clusters.; modify; start Start a Dataproc cluster configured for Hail.; stop Shut down a Dataproc cluster.; submit Submit a Python script to a running Dataproc cluster.; ```. The help output for a command looks like:. ```; $ hailctl batch get --help; Usage: hailctl batch get [OPTIONS] BATCH_ID. Get a particular batch's info. Options:; -o, --output-format [yaml|json]; Specify output format [default: yaml]; --help Show this message and exit.; ```. I also made `BatchClient` a context manager and made the default limit unbounded in `BatchClient.list_batches`. I have marked this WIP until we are happy with the interface changes and how we're going to communicate them to the users. CHANGELOG: Rewrote the hailctl argument parsing code, which made some incompatible changes to the way hailctl handles arguments. For more details, see [the URL of this PR]. Spice level: medium.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842:3162,config,configured,3162,https://hail.is,https://github.com/hail-is/hail/pull/9842,1,['config'],['configured']
Modifiability,"ositoryCloneMethod field for hosting an Amplify app. This field shows what authorization method is used to clone the repo: SSH, TOKEN, or SIGV4.</li>; <li>api-change:<code>fsx</code>: [<code>botocore</code>] This release adds support for the following FSx for OpenZFS features: snapshot lifecycle transition messages, force flag for deleting file systems with child resources, LZ4 data compression, custom record sizes, and unsetting volume quotas and reservations.</li>; <li>api-change:<code>fis</code>: [<code>botocore</code>] This release adds logging support for AWS Fault Injection Simulator experiments. Experiment templates can now be configured to send experiment activity logs to Amazon CloudWatch Logs or to an S3 bucket.</li>; <li>api-change:<code>route53-recovery-cluster</code>: [<code>botocore</code>] This release adds a new API option to enable overriding safety rules to allow routing control state updates.</li>; <li>api-change:<code>amplifyuibuilder</code>: [<code>botocore</code>] We are adding the ability to configure workflows and actions for components.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/boto3/commit/67b84e02c185294c54a8e49510d4cb962e89cee2""><code>67b84e0</code></a> Merge branch 'release-1.21.13'</li>; <li><a href=""https://github.com/boto/boto3/commit/99acd545b20fe30ffa2f589a674c5a7ad74c266b""><code>99acd54</code></a> Bumping version to 1.21.13</li>; <li><a href=""https://github.com/boto/boto3/commit/83a8f662655bada44d442df7f33cb20d71ead257""><code>83a8f66</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/261b0f2ffe079b6940d683657fcad358195f882e""><code>261b0f2</code></a> Merge branch 'release-1.21.12'</li>; <li><a href=""https://github.com/boto/boto3/commit/a972b1bed4caacf0c97f1056cabdfe4b5ccc2681""><code>a972b1b</code></a> Merge branch 'release-1.21.12' into develop</li>; <li",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11504:5395,config,configure,5395,https://hail.is,https://github.com/hail-is/hail/pull/11504,1,['config'],['configure']
Modifiability,"ositoryCloneMethod field for hosting an Amplify app. This field shows what authorization method is used to clone the repo: SSH, TOKEN, or SIGV4.</li>; <li>api-change:<code>fsx</code>: [<code>botocore</code>] This release adds support for the following FSx for OpenZFS features: snapshot lifecycle transition messages, force flag for deleting file systems with child resources, LZ4 data compression, custom record sizes, and unsetting volume quotas and reservations.</li>; <li>api-change:<code>fis</code>: [<code>botocore</code>] This release adds logging support for AWS Fault Injection Simulator experiments. Experiment templates can now be configured to send experiment activity logs to Amazon CloudWatch Logs or to an S3 bucket.</li>; <li>api-change:<code>route53-recovery-cluster</code>: [<code>botocore</code>] This release adds a new API option to enable overriding safety rules to allow routing control state updates.</li>; <li>api-change:<code>amplifyuibuilder</code>: [<code>botocore</code>] We are adding the ability to configure workflows and actions for components.</li>; <li>api-change:<code>athena</code>: [<code>botocore</code>] This release adds support for updating an existing named query.</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] This release adds support for new AMI property 'lastLaunchedTime'</li>; <li>api-change:<code>servicecatalog-appregistry</code>: [<code>botocore</code>] AppRegistry is deprecating Application and Attribute-Group Name update feature. In this release, we are marking the name attributes for Update APIs as deprecated to give a heads up to our customers.</li>; </ul>; <h1>1.21.8</h1>; <ul>; <li>api-change:<code>elasticache</code>: [<code>botocore</code>] Doc only update for ElastiCache</li>; <li>api-change:<code>panorama</code>: [<code>botocore</code>] Added NTP server configuration parameter to ProvisionDevice operation. Added alternate software fields to DescribeDevice response</li>; </ul>; <!-- raw HTML omitted -->; </blockq",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11486:3948,config,configure,3948,https://hail.is,https://github.com/hail-is/hail/pull/11486,1,['config'],['configure']
Modifiability,"ough using Envoy, a load-balancer/proxy that is well-suited to this sort of highly-dynamic cluster configuration. Envoy does not have the constraint that all upstream services must be available at start-time, and has a very convenient API for updating the cluster configuration without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as follows:. 1. Envoy-based gateways and internal-gateways will load their routing configuration from a Kubernetes ConfigMap, which they watch for changes and reconcile their configuration when the ConfigMap changes. The ConfigMap can be populated with a manual deploy and is populated from the beginning with production routes (i.e. batch.hail.is gets routed to batch.default); 2. When running CI, CI will regularly update the ConfigMap with additional routes based on which internal namespaces (dev and PR) are currently active. This requires relatively small changes to CI to track active namespaces but overall is a pretty small change. Note that this does not introduce a dependency on CI to support production traffic, only development traffic.; 3. Deployments that run more than 1 replica (but really can be all of them) are run behind Headless Services, which expose the underlying pod IPs so Envoy can handle load-balancing instead of kube-proxy. This allows Envoy to make smart load-balancing decisions and correctly enforce rate-limiting when using connection pools. The namespace tracking in CI in Point 2 is possible before we make any changes to our networking, so that comes first in #12093. Point 3 is taken care of in #12094, and the rest of Point 2 and Point 1, everything to do with Envoy, is in this PR. ### Additional QoL improvements; - Envoy by default exposes Prometheus metrics that we can use to easily monitor things like rate-li",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:4578,Config,ConfigMap,4578,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['Config'],['ConfigMap']
Modifiability,"ound. In the world of low-level container runtimes there exists the term ""bundle"", which basically means the pair of a root filesystem and a `config.json` file containing all of the other necessary information to run the container. If you invoke `crun` or `runc` with `--bundle /path/to/bundle`, the runtime assumes the following:. - The configuration file for the container is located at `/path/to/bundle/config.json`; - That `config.json` contains a field [`root.path`](https://github.com/opencontainers/runtime-spec/blob/main/config.md#root) that specifies the location of the root filesystem, most commonly as a path relative to `/path/to/bundle`. `crun` offers a way to explicitly reference the location of `config.json` through its `--config` flag. This seems fairly innocuous, but specifying a custom `--config` path can have some unfortunate unintended consequences because it invalidates the assumption in the specification that the configuration resides at `/path/to/bundle/config.json`. Specifically, it breaks [Hooks](https://github.com/opencontainers/runtime-spec/blob/main/config.md#posix-platform-hooks). When a hook is run, the runtime (crun) feeds it the [container state](https://github.com/opencontainers/runtime-spec/blob/main/runtime.md#state), a JSON of information about the container including the `bundle` path. Any hook that attempts to load the `config.json`, like for example, the `nvidia-container-runtime-hook`, will crash. ### Change. This change stops using the `--config` flag for crun and instead does the following to create a well-formed bundle:. - Instead of the bundle being the merged directory of the container overlay, it is the container's scratch directory; - `root.path` is adjusted inside of `config.json` to now point to the merged directory of the container overlay. I've opted to use an absolute path here because why use a relative path.; - Move `config.json` into the container scratch directory so that it is inside the root of the bundle directory.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13438:1097,config,config,1097,https://hail.is,https://github.com/hail-is/hail/pull/13438,5,['config'],['config']
Modifiability,"our AWS organization in a decentralized way. You can now allow member accounts to manage Organizations policies.</li>; <li>api-change:<code>rds</code>: [<code>botocore</code>] This release enables new Aurora and RDS feature called Blue/Green Deployments that makes updates to databases safer, simpler and faster.</li>; <li>api-change:<code>textract</code>: [<code>botocore</code>] This release adds support for classifying and splitting lending documents by type, and extracting information by using the Analyze Lending APIs. This release also includes support for summarized information of the processed lending document package, in addition to per document results.</li>; <li>api-change:<code>transcribe</code>: [<code>botocore</code>] This release adds support for 'inputType' for post-call and real-time (streaming) Call Analytics within Amazon Transcribe.</li>; </ul>; <h1>1.26.16</h1>; <ul>; <li>api-change:<code>grafana</code>: [<code>botocore</code>] This release includes support for configuring a Grafana workspace to connect to a datasource within a VPC as well as new APIs for configuring Grafana settings.</li>; <li>api-change:<code>rbin</code>: [<code>botocore</code>] This release adds support for Rule Lock for Recycle Bin, which allows you to lock retention rules so that they can no longer be modified or deleted.</li>; </ul>; <h1>1.26.15</h1>; <ul>; <li>bugfix:Endpoints: [<code>botocore</code>] Resolve endpoint with default partition when no region is set</li>; <li>bugfix:s3: [<code>botocore</code>] fixes missing x-amz-content-sha256 header for s3 object lambda</li>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] Adding support for Amazon AppFlow to transfer the data to Amazon Redshift databases through Amazon Redshift Data API service. This feature will support the Redshift destination connector on both public and private accessible Amazon Redshift Clusters and Amazon Redshift Serverless.</li>; <li>api-change:<code>kinesisanalyticsv2</code>: [<code>botoco",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12507:4207,config,configuring,4207,https://hail.is,https://github.com/hail-is/hail/pull/12507,2,['config'],['configuring']
Modifiability,"owerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:3492,config,config,3492,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['config'],['config']
Modifiability,owering.EvalRelationalLetsPass.apply(LoweringPass.scala:162); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:45); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:600); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:636); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:631); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:630); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:65); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:407); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:393); 	at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:630); 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:88); 	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79); 	at sun.net.httpserver.AuthFilter.doFilter(AuthFi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14529:11232,adapt,adapted,11232,https://hail.is,https://github.com/hail-is/hail/issues/14529,1,['adapt'],['adapted']
Modifiability,owering.EvalRelationalLetsPass.apply(LoweringPass.scala:164); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:45); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:601); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:637); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:632); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:631); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:77); 	at is.hail.utils.package$.using(package.scala:665); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:77); 	at is.hail.utils.package$.using(package.scala:665); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:64); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:407); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:393); 	at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:631); 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:89); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpser,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14362:17994,adapt,adapted,17994,https://hail.is,https://github.com/hail-is/hail/issues/14362,1,['adapt'],['adapted']
Modifiability,"owever, I am; pretty certain I understand the bug. I have seen this happen in GCP; and in Azure. Take a look at an interval of driver logs:; ```; INFO	2022-03-02 19:06:30,198	main.py	get_credentials_1:226	returning azure credentials to activating instance instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q; INFO	2022-03-02 19:06:30,199	hail_logging.py	log:40	https GET /pr-11438-default-g6cibyji6520/batch-driver/api/v1alpha/instances/credentials done in 0.005999999999858119s: 200; INFO	2022-03-02 19:06:30,226	main.py	activate_instance_1:237	activating instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q; INFO	2022-03-02 19:06:30,991	base.py	check:335	checking on instance batch-worker-pr-11438-default-g6cibyji6520-highcpu-z0idl, last updated 60.151s ago; INFO	2022-03-02 19:06:31,526	pool.py	schedule_loop_body:371	schedule pool standard: starting; INFO	2022-03-02 19:06:31,583	job.py	schedule_job:443	schedule job (94, 2) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,584	job.py	schedule_job:443	schedule job (95, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,585	job.py	schedule_job:443	schedule job (93, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,586	job.py	schedule_job:443	schedule job (90, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,586	job.py	schedule_job:443	schedule job (94, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,598	job.py	schedule_job:443	schedule job (97, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,656	job.py	schedule_job:443	schedule job (99, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:1118,config,config,1118,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['config'],['config']
Modifiability,"p itself (grafana or prometheus) and an nginx container that sits in front of it. The flow is as follows, and since this works the exact same for both prometheus and grafana I will just talk about grafana as the example and the same thing should apply to both:. 1. User sends request to grafana.hail.is; 2. Gateway sees an HTTP request going to a production service and forwards that request to the grafana k8s Service port 443; 3. The grafana K8s Service forwards that request to the grafana pod port 443; 4. Nginx is listening on port 443 in the grafana pod and receives that request. It makes an authorization check to auth to make sure that the request is coming from a developer; 5. Nginx forwards that request to 127.0.0.1:3000, which is where grafana is listening. This PR does not change any behavior, just replaces Nginx with Envoy. Currently, building the nginx container involves running jinja on its config files and building a docker image. With envoy, we can just use the `envoyproxy/envoy` image from DockerHub (which I have copied into our container registries) and feed it a single configmap. The big mess of yaml that is the new configmap for envoy has a lot of boilerplate, but it comprises of the following sections which hopefully on their own are not too bad. ### Envoy config; 1. The top of the `listeners` section shows that Envoy is listening on port 8443 (which is the port that the k8s `Service` will now forward traffic to); 2. The `virtual_hosts` section shows that Envoy will send all paths (prefix ""/"") to the cluster `grafana`; 3. The `http_filters` section says that Envoy will first send an authorization request to the `auth` cluster before allowing the request to pass to grafana; 4. The `clusters` section says that there are two other services Envoy knows about and can send traffic to, one named `auth` that can be found at address `auth` (same as `https://auth`), and a cluster called `grafana` which can be found at `127.0.0.1:3000` (same as `localhost:3000`)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12364:1469,config,configmap,1469,https://hail.is,https://github.com/hail-is/hail/pull/12364,3,['config'],"['config', 'configmap']"
Modifiability,"p support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <ul>; <li>Fixed an issue where reading more than 2 GiB in a call to HTTPResponse.read would raise an OverflowError on Python 3.9 and earlier.</li>; </ul>; <h2>1.26.10</h2>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a>.</strong></p>; <p>:warning: <strong>urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <p>:closed_lock_with_key: <strong>This is the first release to be signed with Sigstore!</strong> You can verify the distributables using the <code>.sig</code> and <code>.crt</code> files included on this release.</p>; <ul>; <li>Removed support for Python 3.5</li>; <li>Fixed an issue where a <code>ProxyError</code> recommending configuring the proxy as HTTP instead of HTTPS could appear even when an HTTPS proxy wasn't configured.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h1>1.26.11 (2022-07-25)</h1>; <ul>; <li>Fixed an issue where reading more than 2 GiB in a call to <code>HTTPResponse.read</code> would; raise an <code>OverflowError</code> on Python 3.9 and earlier.</li>; </ul>; <h1>1.26.10 (2022-07-07)</h1>; <ul>; <li>Removed support for Python 3.5</li>; <li>Fixed an issue where a <code>ProxyError</code> recommending configuring the proxy as HTTP; instead of HTTPS could appear even when an HTTPS proxy wasn't configured.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/aa3def7d242525e6e854991247c4b68583d15135""><code>aa3def7</code></a> Release 1.26.11</li>; <li><a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12104:1452,config,configuring,1452,https://hail.is,https://github.com/hail-is/hail/pull/12104,2,['config'],"['configured', 'configuring']"
Modifiability,"p support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <ul>; <li>Fixed an issue where reading more than 2 GiB in a call to HTTPResponse.read would raise an OverflowError on Python 3.9 and earlier.</li>; </ul>; <h2>1.26.10</h2>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a>.</strong></p>; <p>:warning: <strong>urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <p>:closed_lock_with_key: <strong>This is the first release to be signed with Sigstore!</strong> You can verify the distributables using the <code>.sig</code> and <code>.crt</code> files included on this release.</p>; <ul>; <li>Removed support for Python 3.5</li>; <li>Fixed an issue where a <code>ProxyError</code> recommending configuring the proxy as HTTP instead of HTTPS could appear even when an HTTPS proxy wasn't configured.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h1>1.26.12 (2022-08-22)</h1>; <ul>; <li>Deprecated the <code>urllib3[secure]</code> extra and the <code>urllib3.contrib.pyopenssl</code> module.; Both will be removed in v2.x. See this <code>GitHub issue &lt;https://github.com/urllib3/urllib3/issues/2680&gt;</code>_; for justification and info on how to migrate.</li>; </ul>; <h1>1.26.11 (2022-07-25)</h1>; <ul>; <li>Fixed an issue where reading more than 2 GiB in a call to <code>HTTPResponse.read</code> would; raise an <code>OverflowError</code> on Python 3.9 and earlier.</li>; </ul>; <h1>1.26.10 (2022-07-07)</h1>; <ul>; <li>Removed support for Python 3.5</li>; <li>Fixed an issue where a <code>ProxyError</code> recommending configuring ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12140:1777,config,configuring,1777,https://hail.is,https://github.com/hail-is/hail/pull/12140,2,['config'],"['configured', 'configuring']"
Modifiability,"p/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 20 times, most recent failure: Lost task 8.19 in stage 1.0 (TID 2899) (hail-test-w-1.australia-southeast1-a.c.pb-dev-312200.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:4085,Plugin,Plugins,4085,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,['Plugin'],['Plugins']
Modifiability,"p>; </li>; <li>; <p>Do not close provided file handles with libtiff <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7199"">#7199</a>; [radarhere]</p>; </li>; <li>; <p>Convert to HSV if mode is HSV in getcolor() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7226"">#7226</a>; [radarhere]</p>; </li>; <li>; <p>Added alpha_only argument to getbbox() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7123"">#7123</a>; [radarhere. hugovk]</p>; </li>; <li>; <p>Prioritise speed in <em>repr_png</em> <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7242"">#7242</a>; [radarhere]</p>; </li>; <li>; <p>Do not use CFFI access by default on PyPy <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7236"">#7236</a>; [radarhere]</p>; </li>; <li>; <p>Limit size even if one dimension is zero in decompression bomb check <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7235"">#7235</a>; [radarhere]</p>; </li>; <li>; <p>Use --config-settings instead of deprecated --global-option <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7171"">#7171</a>; [radarhere]</p>; </li>; <li>; <p>Better C integer definitions <a href=""https://redirect.github.com/python-pillow/Pillow/issues/6645"">#6645</a>; [Yay295, hugovk]</p>; </li>; <li>; <p>Fixed finding dependencies on Cygwin <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7175"">#7175</a>; [radarhere]</p>; </li>; <li>; <p>Changed grabclipboard() to use PNG instead of JPG compression on macOS <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7219"">#7219</a>; [abey79, radarhere]</p>; </li>; <li>; <p>Added in_place argument to ImageOps.exif_transpose() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7092"">#7092</a>; [radarhere]</p>; </li>; <li>; <p>Fixed calling putpalette() on L and LA images before load() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7187"">#7187</a>; [radarhere]</p>; </li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13321:11169,config,config-settings,11169,https://hail.is,https://github.com/hail-is/hail/pull/13321,1,['config'],['config-settings']
Modifiability,"p[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""hail.is/sha"":""1c6dbf20333a"" ""app"":""batch""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""1c6dbf20333a""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:4b4139c73fe9be3bee6c2895aa74059e157eb861d2bdac7d2304ba44b5421f88"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""labels"":map[""app"":""batch""] ""name"":""batch""] ""spec"":map[""ports"":[map[""port"":'P' ""protocol"":""TCP"" ""targetPort"":'\u1388']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; make: *** [deploy-batch] Error 1; Makefile:45: recipe for target 'deploy-batch' failed; ```; [deploy.log](https://github.com/hail-is/hail/files/2504429/deploy.log). Service accounts:; ```; error: the server doesn't have a resource type ""service-accounts""; # kubectl get serviceaccounts ; NAME SECRETS AGE; batch-svc 1 9h; default 1 113d; # kubectl get serviceaccounts ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:4392,config,configuration,4392,https://hail.is,https://github.com/hail-is/hail/issues/4609,1,['config'],['configuration']
Modifiability,parameterize TableRead with a TableReader,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5139:0,parameteriz,parameterize,0,https://hail.is,https://github.com/hail-is/hail/pull/5139,1,['parameteriz'],['parameterize']
Modifiability,parameterize en/decoder,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2859:0,parameteriz,parameterize,0,https://hail.is,https://github.com/hail-is/hail/pull/2859,1,['parameteriz'],['parameterize']
Modifiability,parameterize some IRSuite tests to avoid c++ recompilation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5582:0,parameteriz,parameterize,0,https://hail.is,https://github.com/hail-is/hail/pull/5582,1,['parameteriz'],['parameterize']
Modifiability,parameterized (matrix)table file by rvd spec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2828:0,parameteriz,parameterized,0,https://hail.is,https://github.com/hail-is/hail/pull/2828,1,['parameteriz'],['parameterized']
Modifiability,"part of the work for getting TableMapPartitions working. you can use `Ref` or `In` nodes that have type `TStream` as emittable streams. the variable/argument must be bound to a `Iterator[RegionValue]`, which will be iterated over and emitted as part of the stream. this stream can then be composed like other streams. there's no fancy region management going on in this PR since i'm going to handle that by deep copying in TableMapPartitions",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7677:140,variab,variable,140,https://hail.is,https://github.com/hail-is/hail/pull/7677,1,['variab'],['variable']
Modifiability,pass spark configuration options to HailContext,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1510:11,config,configuration,11,https://hail.is,https://github.com/hail-is/hail/issues/1510,1,['config'],['configuration']
Modifiability,"patch; From aef878903d9249b542522082cba705eaf26d728a Mon Sep 17 00:00:00 2001; From: Christopher Vittal <christopher.vittal@gmail.com>; Date: Wed, 25 Sep 2019 14:55:42 -0400; Subject: [PATCH] [hailctl] Move default location for hail config directory; MIME-Version: 1.0; Content-Type: text/plain; charset=UTF-8; Content-Transfer-Encoding: 8bit. Now we try, in order:; $XDG_CONFIG_HOME/hail; ~/.config/hail. The XDG Base Directory Specification[1] is a freedesktop spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailto",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:1244,config,config,1244,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['config'],['config']
Modifiability,"pendabot.com/pytest-dev/pytest-html/issues/230"">#230</a>) <a href=""https://github.com/hugovk""><code>@​hugovk</code></a></li>; </ul>; <h2>Minor Changes</h2>; <ul>; <li>Add support for py39 (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/345"">#345</a>) <a href=""https://github.com/ssbarnea""><code>@​ssbarnea</code></a></li>; <li>Enable py38 testing (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/326"">#326</a>) <a href=""https://github.com/ssbarnea""><code>@​ssbarnea</code></a></li>; <li>Strip ANSI escape sequences when ansi2html is missing (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/315"">#315</a>) <a href=""https://github.com/BeyondEvil""><code>@​BeyondEvil</code></a></li>; <li>Make the links column in the results table sortable (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/324"">#324</a>) <a href=""https://github.com/gnikonorov""><code>@​gnikonorov</code></a></li>; <li>Make the maximum asset filename length configurable. (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/313"">#313</a>) <a href=""https://github.com/D3X""><code>@​D3X</code></a></li>; <li>Fix broken development docs (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/316"">#316</a>) <a href=""https://github.com/BeyondEvil""><code>@​BeyondEvil</code></a></li>; <li>Update link to Tox (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/301"">#301</a>) <a href=""https://github.com/gnikonorov""><code>@​gnikonorov</code></a></li>; </ul>; <h2>Bugfixes</h2>; <ul>; <li>Add ESLint to project (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/367"">#367</a>) <a href=""https://github.com/gnikonorov""><code>@​gnikonorov</code></a></li>; <li>Assure scm versioning is pypa compatible (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/364"">#364</a>) <a href=""https://github.com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11524:2127,config,configurable,2127,https://hail.is,https://github.com/hail-is/hail/pull/11524,1,['config'],['configurable']
Modifiability,perationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:176); 	at is.hail.utils.richUtils.RichContextRDD.writePartitions(RichContextRDD.scala:105); 	at is.hail.utils.richUtils.RichRDD$.writePartitions$extension(RichRDD.scala:209); 	at is.hail.linalg.BlockMatrix.write(BlockMatrix.scala:871); 	at is.hail.expr.ir.BlockMatrixNativeWriter.apply(BlockMatrixWriter.scala:46); 	at is.hail.expr.ir.BlockMatrixNativeWriter.apply(BlockMatrixWriter.scala:39); 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:858); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:57); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:20); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(I,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290:4838,rewrite,rewrite,4838,https://hail.is,https://github.com/hail-is/hail/issues/12290,1,['rewrite'],['rewrite']
Modifiability,piled.apply(Emit.scala); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$7(CompileAndEvaluate.scala:74); E 	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:74); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:19); E 	at is.hail.expr.ir.lowering.LowerDistributedSort$.distributedSort(LowerDistributedSort.scala:163); E 	at is.hail.backend.service.ServiceBackend.lowerDistributedSort(ServiceBackend.scala:354); E 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:100); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:23); E 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$4(RewriteBottomUp.scala:26); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:36); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:20); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:157); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(Lowering,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:1631,Rewrite,RewriteBottomUp,1631,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['Rewrite'],['RewriteBottomUp']
Modifiability,"ping @chrisvittal ; There is 1 failure left, which seems difficult to replicate, and for which I have developed a (small) prior belief that it may be uncorrelated with this PR's changes. . The failure is in the Dataproc test of [cluster-tests/cluster-read-vcfs-check.py, line 143 of HadoopFS, in _fileSystem, new hadoop.fs.Path(filename).getFileSystem(conf)](https://ci2.hail.is/jobs/17044/log). This calls hadoop.fs.FileSystem.get , which in turn calls Configuration.get (instance method). ```java; // In Path.java; public FileSystem getFileSystem(Configuration conf) throws IOException {; return FileSystem.get(this.toUri(), conf);; }. // In FileSystem.java; public static FileSystem get(Configuration conf) throws IOException {; return get(getDefaultUri(conf), conf);; }. public static FileSystem get(final URI uri, final Configuration conf,; final String user) throws IOException, InterruptedException {; String ticketCachePath =; conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);; UserGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configurat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:454,Config,Configuration,454,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803,4,['Config'],['Configuration']
Modifiability,"pistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster than aiohttp on JSON serialization but cannot pass other tests. Please decide is it important or not. The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:1234,config,configured,1234,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040,2,['config'],"['config', 'configured']"
Modifiability,"ple plink files to hailmatrix table using Terra platform in future, I need to figure the problem out. Any advise would be appreciated. ### Version. 0.2.127. ### Relevant log output. ```shell; 2024/01/17 20:20:25 Starting container setup.; 2024/01/17 20:20:26 Done container setup.; 2024/01/17 20:20:27 Starting localization.; 2024/01/17 20:20:34 Localization script execution started...; 2024/01/17 20:20:34 Localizing input gs://fc-5a8938eb-1299-4afc-957f-afb53ef602b9/submissions/e8747e74-47d1-4f52-acfc-1ac7f81d79ba/VUMCBed2HailMatrix/683447d9-9342-4058-bcfc-ba21422d3121/call-Bed2HailMatrix/script -> /cromwell_root/script; 2024/01/17 20:20:36 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bed -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bed; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.fam -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.fam; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bim -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bim; Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.fam...; / [0 files][ 0.0 B/910.3 KiB] / [1 files][910.3 KiB/910.3 KiB] Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.bim...; / [1 files][910.3 KiB/369.7 MiB] - - [1 files][ 51.9 MiB/369.7 MiB] \ | | [1 files][107.6 MiB/369.7 MiB] / - - [1 files][162.3 MiB/369.7 MiB] \ \ [1 files][213.9 MiB/369.7 MiB] | / / [1 files][286.6 MiB/369.7 MiB] - \ \ [1 files][342.1 MiB/369.7 MiB] |; Operation completed over 2 objects/369.7 MiB.; | [2 files][369.7 MiB/369.7 MiB] 2024/01/17 20:59:27 Localization script execution complete.; 2024/01/17 20:59:38 Done localization.; 2024/01/17 20:59:39 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint=/bin/bash hailgenetics/hail@sha256:3f22576793ce3161893aed2bd40949b1fc822d2b7e6517dc0ac993b62badaff8 /cromwell_root/script; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.81879b1c; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.81879b1c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:9973,sandbox,sandbox,9973,https://hail.is,https://github.com/hail-is/hail/issues/14168,1,['sandbox'],['sandbox']
Modifiability,"ple-darwin15; Configured with: /opt/local/var/macports/build/_opt_mports_dports_lang_gcc49/gcc49/work/gcc-4.9.3/configure --prefix=/opt/local --build=x86_64-apple-darwin15 --enable-languages=c,c++,objc,obj-c++,lto,fortran,java --libdir=/opt/local/lib/gcc49 --includedir=/opt/local/include/gcc49 --infodir=/opt/local/share/info --mandir=/opt/local/share/man --datarootdir=/opt/local/share/gcc-4.9 --with-local-prefix=/opt/local --with-system-zlib --disable-nls --program-suffix=-mp-4.9 --with-gxx-include-dir=/opt/local/include/gcc49/c++/ --with-gmp=/opt/local --with-mpfr=/opt/local --with-mpc=/opt/local --with-isl=/opt/local --disable-isl-version-check --with-cloog=/opt/local --disable-cloog-version-check --enable-stage1-checking --disable-multilib --enable-lto --enable-libstdcxx-time --with-as=/opt/local/bin/as --with-ld=/opt/local/bin/ld --with-ar=/opt/local/bin/ar --with-bugurl=https://trac.macports.org/newticket --with-pkgversion='MacPorts gcc49 4.9.3_0' --with-build-config=bootstrap-debug; Thread model: posix; gcc version 4.9.3 (MacPorts gcc49 4.9.3_0) . **sysctl -a | grep machdep.cpu**; machdep.cpu.tsc_ccc.denominator: 0; machdep.cpu.tsc_ccc.numerator: 0; machdep.cpu.thread_count: 8; machdep.cpu.core_count: 4; machdep.cpu.address_bits.virtual: 48; machdep.cpu.address_bits.physical: 36; machdep.cpu.tlb.shared: 512; machdep.cpu.tlb.data.large: 32; machdep.cpu.tlb.data.small: 64; machdep.cpu.tlb.inst.large: 8; machdep.cpu.tlb.inst.small: 64; machdep.cpu.cache.size: 256; machdep.cpu.cache.L2_associativity: 8; machdep.cpu.cache.linesize: 64; machdep.cpu.arch_perf.fixed_width: 48; machdep.cpu.arch_perf.fixed_number: 3; machdep.cpu.arch_perf.events: 0; machdep.cpu.arch_perf.events_number: 7; machdep.cpu.arch_perf.width: 48; machdep.cpu.arch_perf.number: 4; machdep.cpu.arch_perf.version: 3; machdep.cpu.xsave.extended_state1: 1 0 0 0; machdep.cpu.xsave.extended_state: 7 832 832 0; machdep.cpu.thermal.energy_policy: 0; machdep.cpu.thermal.hardware_feedback: 0; machdep.cpu.ther",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1274#issuecomment-274242543:1133,config,config,1133,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-274242543,1,['config'],['config']
Modifiability,plugins,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/353:0,plugin,plugins,0,https://hail.is,https://github.com/hail-is/hail/issues/353,1,['plugin'],['plugins']
Modifiability,plus some router config fixes. Mainly I'm PR'ing this so I can use the cached base image when testing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6936:17,config,config,17,https://hail.is,https://github.com/hail-is/hail/pull/6936,1,['config'],['config']
Modifiability,port GRCh38 VEP properties config to new json format,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4245:27,config,config,27,https://hail.is,https://github.com/hail-is/hail/issues/4245,1,['config'],['config']
Modifiability,"ps://github-redirect.dependabot.com/cbeust/testng/pull/2814"">cbeust/testng#2814</a></li>; <li>Streamline TestResult due to expectedExceptions by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2815"">cbeust/testng#2815</a></li>; <li>Unexpected test runs count with retry analyzer by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2816"">cbeust/testng#2816</a></li>; <li>Make PackageUtils compliant with JPMS by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2817"">cbeust/testng#2817</a></li>; <li>Ability to retry a data provider during failures by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2820"">cbeust/testng#2820</a></li>; <li>Refactoring by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2821"">cbeust/testng#2821</a></li>; <li>Fixing bug with DataProvider retry by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2822"">cbeust/testng#2822</a></li>; <li>Add config key for callback discrepancy behavior by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2823"">cbeust/testng#2823</a></li>; <li>Upgrading versions by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2824"">cbeust/testng#2824</a></li>; <li>Fix <a href=""https://github-redirect.dependabot.com/cbeust/testng/issues/2770"">#2770</a>: FileAlreadyExistsException on copy by <a href=""h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:3523,Refactor,Refactoring,3523,https://hail.is,https://github.com/hail-is/hail/pull/12665,1,['Refactor'],['Refactoring']
Modifiability,"ps://github.com/googleapis/java-storage/commit/3345ac9eec286ee3108c08bdbe263eba59085ad3""><code>3345ac9</code></a> test: add test to verify <code>lifecycle.rule.condition.age_days = 0</code> (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1846"">#1846</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/45dc983a4af8e7feb937263ce611bd34eda37e03""><code>45dc983</code></a> feat: update GrpcBlobReadChannel to allow seek/limit after read (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1834"">#1834</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/b8f43169a504080c55eadc3428d0d7966efdc3d4""><code>b8f4316</code></a> build(deps): update dependency org.apache.maven.plugins:maven-dependency-plug...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/e532a590fd351bb2020b571d21662fbee629038e""><code>e532a59</code></a> build(deps): update dependency org.apache.maven.plugins:maven-surefire-plugin...</li>; <li>Additional commits viewable in <a href=""https://github.com/googleapis/java-storage/compare/v1.106.0...v2.17.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.google.cloud:google-cloud-storage&package-manager=gradle&previous-version=1.106.0&new-version=2.17.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will rec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12598:15980,plugin,plugins,15980,https://hail.is,https://github.com/hail-is/hail/pull/12598,2,['plugin'],"['plugin', 'plugins']"
Modifiability,"ps://github.com/googleapis/java-storage/commit/c8bf3c70cca81ed87a52939fe7da58889c8f55ce""><code>c8bf3c7</code></a> fix: update GrpcStorageImpl#update to support fine-grained update of BucketIn...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/3345ac9eec286ee3108c08bdbe263eba59085ad3""><code>3345ac9</code></a> test: add test to verify <code>lifecycle.rule.condition.age_days = 0</code> (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1846"">#1846</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/45dc983a4af8e7feb937263ce611bd34eda37e03""><code>45dc983</code></a> feat: update GrpcBlobReadChannel to allow seek/limit after read (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1834"">#1834</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/b8f43169a504080c55eadc3428d0d7966efdc3d4""><code>b8f4316</code></a> build(deps): update dependency org.apache.maven.plugins:maven-dependency-plug...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/e532a590fd351bb2020b571d21662fbee629038e""><code>e532a59</code></a> build(deps): update dependency org.apache.maven.plugins:maven-surefire-plugin...</li>; <li>Additional commits viewable in <a href=""https://github.com/googleapis/java-storage/compare/v1.106.0...v2.17.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.google.cloud:google-cloud-storage&package-manager=gradle&previous-version=1.106.0&new-version=2.17.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12598:15763,plugin,plugins,15763,https://hail.is,https://github.com/hail-is/hail/pull/12598,1,['plugin'],['plugins']
Modifiability,"ps://redirect.github.com/python-pillow/Pillow/issues/7497"">#7497</a>; [ZachNagengast, nulano, radarhere]</p>; </li>; <li>; <p>Attempt memory mapping when tile args is a string <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7565"">#7565</a>; [radarhere]</p>; </li>; <li>; <p>Fill identical pixels with transparency in subsequent frames when saving GIF <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7568"">#7568</a>; [radarhere]</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/python-pillow/Pillow/commit/6956d0b2853f5c7ec5f6ec4c60725c5a7ee73aeb""><code>6956d0b</code></a> 10.2.0 version bump</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/31c8dacdc727673e9099f1ac86019714cdccec67""><code>31c8dac</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7675"">#7675</a> from python-pillow/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/40a3f91af2c78870676a13629b5902bab4ab4cf0""><code>40a3f91</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7674"">#7674</a> from nulano/url-example</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/cb41b0cc78eeefbd9ed2ce8c10f8d6d4c405a706""><code>cb41b0c</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/de62b25ed318f1604aa4ccd6f942a04c6b2c8b59""><code>de62b25</code></a> fix image url in &quot;Reading from URL&quot; example</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/7c526a6c6bdc7cb947f0aee1d1ee17c266ff6c61""><code>7c526a6</code></a> Update CHANGES.rst [ci skip]</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/d93a5ad70bf94dbb63bdbfb19491a02976574d6d""><code>d93a5ad</code></a> Merge pull request <a href=""https://redirect.github.com/pyth",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14191:13358,config,config,13358,https://hail.is,https://github.com/hail-is/hail/pull/14191,3,['config'],['config']
Modifiability,"puteOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.130-bea04d9c79b5; Error summary: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:20032,plugin,plugin,20032,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,"['Plugin', 'plugin']","['Plugins', 'plugin']"
Modifiability,"puty; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. Fo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:3326,config,config-proxy,3326,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['config'],['config-proxy']
Modifiability,"py"", line 3269, in run_cell_async; has_raised = await self.run_ast_nodes(code_ast.body, cell_name,; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3448, in run_ast_nodes; if await self.run_code(code, result, async_=asy):; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3508, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-2-40c87378e50b>"", line 1, in <module>; aiohttp.ClientSession(); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x104dac8b0>; source_traceback: Object created at (most recent call last):; File ""/Users/dking/miniconda3/bin/ipython"", line 8, in <module>; sys.exit(start_ipython()); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/__init__.py"", line 128, in start_ipython; return launch_new_instance(argv=argv, **kwargs); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/traitlets/config/application.py"", line 1043, in launch_instance; app.start(); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/terminal/ipapp.py"", line 318, in start; self.shell.mainloop(); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/terminal/interactiveshell.py"", line 888, in mainloop; self.interact(); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/terminal/interactiveshell.py"", line 881, in interact; self.run_cell(code, store_history=True); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3009, in run_cell; result = self._run_cell(; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3064, in _run_cell; result = runner(coro); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner; coro.send(None); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/cor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13421:4111,config,config,4111,https://hail.is,https://github.com/hail-is/hail/pull/13421,1,['config'],['config']
Modifiability,"py"", line 3269, in run_cell_async; has_raised = await self.run_ast_nodes(code_ast.body, cell_name,; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3448, in run_ast_nodes; if await self.run_code(code, result, async_=asy):; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3508, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-3-40c87378e50b>"", line 1, in <module>; aiohttp.ClientSession(); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x104daeec0>; source_traceback: Object created at (most recent call last):; File ""/Users/dking/miniconda3/bin/ipython"", line 8, in <module>; sys.exit(start_ipython()); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/__init__.py"", line 128, in start_ipython; return launch_new_instance(argv=argv, **kwargs); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/traitlets/config/application.py"", line 1043, in launch_instance; app.start(); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/terminal/ipapp.py"", line 318, in start; self.shell.mainloop(); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/terminal/interactiveshell.py"", line 888, in mainloop; self.interact(); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/terminal/interactiveshell.py"", line 881, in interact; self.run_cell(code, store_history=True); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3009, in run_cell; result = self._run_cell(; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3064, in _run_cell; result = runner(coro); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner; coro.send(None); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/cor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13421:6130,config,config,6130,https://hail.is,https://github.com/hail-is/hail/pull/13421,1,['config'],['config']
Modifiability,"py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8053:1293,adapt,adapters,1293,https://hail.is,https://github.com/hail-is/hail/issues/8053,3,['adapt'],['adapters']
Modifiability,"py4j-0.10.9.5-src.zip/py4j/java_gateway.py"", line 1321, in __call__; File ""/opt/conda/default/lib/python3.10/site-packages/hail/backend/py4j_backend.py"", line 35, in deco; raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; hail.utils.java.FatalError: HailException: zip: length mismatch: 62164, 104. Java stack trace:; is.hail.utils.HailException: zip: length mismatch: 62164, 104; 	at __C8160Compiled.__m8201split_ToArray(Emit.scala); 	at __C8160Compiled.__m8169split_CollectDistributedArray(Emit.scala); 	at __C8160Compiled.__m8164split_Let(Emit.scala); 	at __C8160Compiled.apply(Emit.scala); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$4(CompileAndEvaluate.scala:61); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$2(CompileAndEvaluate.scala:61); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$2$adapted(CompileAndEvaluate.scala:59); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:140); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:140); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:59); 	at is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:33); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:58); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:63); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:67); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13486:3104,adapt,adapted,3104,https://hail.is,https://github.com/hail-is/hail/issues/13486,1,['adapt'],['adapted']
Modifiability,"questreview-104364784) (what; the hell?), ergo Confused Deputy. I also partly resolved the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABL",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:3235,config,config,3235,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['config'],['config']
Modifiability,"r simplifying the implementation of `ArrayFilter`. There is basically no nice way to implement filter otherwise without introducing some significant code duplication.; - ~~The stream ""parameter"", as well as the `Empty` return, are not very useful for the basic streams in this PR. However, they simplify the implementation of `ArrayFlatMap` (aka ""composing"" two parameterized streams).~~ NOTE (to Patrick): I decided to abandon the ""empty"" return idea in favor of just providing ""default states"" that always yield empty streams. **Implementation Notes**. - The implementation makes great use of Scala's type system. Most of the streams are implemented first in a very type aware manner, where it is easy to reason about the types of data flowing in and out, before being instantiated with EmitTriplets and Envs which don't hold very much type information. For instance, we have the following helper for `map`:; ```scala; Parameterized[P, A].map(f: A => B): Parameterized[P, B]; ```; The emitter instantiates P = `Any`, A = `EmitTriplet`, B = `EmitTriplet` :/. - Complex streams will have non trivial control flow and jumps. Therefore `init` and `step` both take `JoinPointBuilder`s and return `Code[Ctrl]`, to indicate that they may create join points and do jumps. - I've utilized a cute continuation passing style trick in multiple functions. Instead of `init` ""returning"" `Missing`/`Start(s0)` (which is impossible, since these are compile time data structures; they won't exist during JVM runtime), init takes a continuation `k: Init[S] => Code[Ctrl]`, which it must call with one of these values. To use `init`, you can simply provide a pattern matching lambda, which closely resembles the syntax you would normally use to pattern match on a returned value, e.g. (from `contMap`):; ```scala; self.init(mb, jb, param) {; case Missing => k(Missing); case Start(s) => Code(setup, k(Start(s))); }; ```; This technique actually cleans up the implementation significantly, especially moving forward to",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7228:2662,Parameteriz,Parameterized,2662,https://hail.is,https://github.com/hail-is/hail/pull/7228,1,['Parameteriz'],['Parameterized']
Modifiability,"r, I like the greater flexibility in providing parameters. A few concerns though:; - For hailctl arguments that are also gcloud arguments (for example, `--project` to `hailctl dataproc start`), what happens if a user provides them in both places (for example, `hailctl dataproc start --project=project-a cluster-name -- --project=project-b`)? One nice attribute of the current parsing method is that it does not allow this, since in most cases the hailctl argument shadows the gcloud argument of the same name.; - It looks like this creates some inconsistency in how the same argument must be provided to different `hailctl dataproc` commands. For example, `--project` can be directly provided to `hailctl dataproc start`, but it would have to go after the `--` for `hailctl dataproc list` or `hailctl dataproc modify`. That seems likely to be surprising/annoying for users. This could be solved by moving such flags (`--project`, `--region`, and `--configuration` are the ones that immediately come to mind) to the `hailctl dataproc` group level or by adding definitions for those arguments to all `hailctl dataproc` commands.; - On the subject of `--`, a current issue with `hailctl dataproc submit` is that it does not support `--` for specifying parameters to the submitted script like `gcloud dataproc jobs submit` does. Thus, for example, you cannot currently submit a script that has a `--files` argument because `--files` will be interpreted as an argument to `hailctl dataproc submit` instead of the submitted script. It would be nice to support that behavior in `hailctl dataproc submit`. However, with this parsing approach, supporting script arguments like that might conflict with accepting pass through arguments to `gcloud dataproc jobs submit` such as `--async`, `--bucket`, etc. And more minor:; - For `hailctl dataproc start` especially, it could seem pretty arbitrary to a user which arguments go before `--` vs after. For example, `--num-worker-local-ssds` is a `hailctl dataproc s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034:1176,config,configuration,1176,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034,1,['config'],['configuration']
Modifiability,"r-space values in ConditionSets and in; VariableScalars (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3042"">#3042</a>, <a href=""https://redirect.github.com/fonttools/fonttools/issues/3043"">#3043</a>).</li>; <li>[ttProgram] Handle string input to Program.fromAssembly() (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3038"">#3038</a>).</li>; <li>[otlLib] Added a config option to emit GPOS 7 lookups, currently disabled by default; because of a macOS bug (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3034"">#3034</a>).</li>; <li>[COLRv1] Added method to automatically compute ClipBoxes (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3027"">#3027</a>).</li>; <li>[ttFont] Fixed getGlyphID to raise KeyError on missing glyphs instead of returning; None. The regression was introduced in v4.27.0 (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3032"">#3032</a>).</li>; <li>[sbix] Fixed UnboundLocalError: cannot access local variable 'rawdata' (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3031"">#3031</a>).</li>; <li>[varLib] When building VF, do not overwrite a pre-existing <code>STAT</code> table that was built; with feaLib from FEA feature file. Also, added support for building multiple VFs; defined in Designspace v5 from <code>fonttools varLib</code> script (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3024"">#3024</a>).</li>; <li>[mtiLib] Only add <code>Debg</code> table with lookup names when <code>FONTTOOLS_LOOKUP_DEBUGGING</code>; env variable is set (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3023"">#3023</a>).</li>; </ul>; <h2>4.39.0 (released 2023-03-06)</h2>; <ul>; <li>[mtiLib] Optionally add <code>Debg</code> debug info for MTI feature builds (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3018"">#3018</a>).</li>; <li>[ttx] Support reading input file from standard input using special <code>-</code> character,; simi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12910:12869,variab,variable,12869,https://hail.is,https://github.com/hail-is/hail/pull/12910,1,['variab'],['variable']
Modifiability,r.ir.WrappedEmitMethodBuilder.emitWithBuilder$(EmitClassBuilder.scala:1141); 	at is.hail.expr.ir.EmitFunctionBuilder.emitWithBuilder(EmitClassBuilder.scala:1157); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:91); 	at is.hail.expr.ir.Compile$.$anonfun$apply$4(Compile.scala:74); 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction(Backend.scala:125); 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction$(Backend.scala:121); 	at is.hail.backend.spark.SparkBackend.lookupOrCompileCachedFunction(SparkBackend.scala:273); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:40); 	at is.hail.expr.ir.lowering.TableStageToRVD$.apply(RVDToTableStage.scala:112); 	at is.hail.backend.spark.SparkBackend.lowerDistributedSort(SparkBackend.scala:689); 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:110); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:23); 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:20); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:157); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringPass.scala:151); 	at is.ha,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13633:8862,Rewrite,RewriteBottomUp,8862,https://hail.is,https://github.com/hail-is/hail/issues/13633,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"r::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 20 times, most recent failure: Lost task 8.19 in stage 1.0 (TID 2899) (hail-test-w-1.australia-southeast1-a.c.pb-dev-312200.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:3851,plugin,plugin,3851,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,"['Plugin', 'plugin']","['Plugins', 'plugin']"
Modifiability,"rGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:1991,Config,Configuration,1991,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803,1,['Config'],['Configuration']
Modifiability,ractIntervalFilters$.minimumValueByType(ExtractIntervalFilters.scala:42); 	at is.hail.expr.ir.ExtractIntervalFilters$.openInterval(ExtractIntervalFilters.scala:94); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:205); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:201); 	at scala.Option.flatMap(Option.scala:171); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:201); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:151); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractPartitionFilters(ExtractIntervalFilters.scala:249); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Trav,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:3672,Rewrite,RewriteBottomUp,3672,https://hail.is,https://github.com/hail-is/hail/issues/6458,3,"['Rewrite', 'rewrite']","['RewriteBottomUp', 'rewrite']"
Modifiability,"randomly assigned Patrick. I expected to get an error like:; ```; ExpressionException: Key type mismatch: cannot index table with given expressions:; Table key: <<empty key>>; Index Expressions: int32; ```; But instead got:; ```; # ipython; import hail Python 3.7.3 (default, Mar 27 2019, 09:23:15) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl ; hl.utils.ra; In [2]: t = hl.utils.range_table(10) . In [3]: t.describe() ; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'idx': int32 ; ----------------------------------------; Key: ['idx']; ----------------------------------------. In [4]: t = t.key_by() . In [5]: t[t.idx] ; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-5-a44cae835751> in <module>; ----> 1 t[t.idx]. /usr/local/lib/python3.7/site-packages/hail/table.py in __getitem__(self, item); 366 else:; 367 try:; --> 368 return self.index(*wrap_to_tuple(item)); 369 except TypeError as e:; 370 raise TypeError(f""Table.__getitem__: invalid index argument(s)\n"". /usr/local/lib/python3.7/site-packages/hail/table.py in index(self, all_matches, *exprs); 1530 """"""; 1531 try:; -> 1532 return self._index(*exprs, all_matches=all_matches); 1533 except TableIndexKeyError as err:; 1534 key_type, exprs = err.args. /usr/local/lib/python3.7/site-packages/hail/table.py in _index(self, all_matches, *exprs); 1554 ; 1555 is_interval = (len(exprs) == 1; -> 1556 and isinstance(self.key[0].dtype, hl.tinterval); 1557 and exprs[0].dtype == self.key[0].dtype.point_type); 1558 . </usr/local/lib/python3.7/site-packages/decorator.py:decorator-gen-570> in __getitem__(self, item). /usr/local/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kw",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6663:384,enhance,enhanced,384,https://hail.is,https://github.com/hail-is/hail/issues/6663,1,['enhance'],['enhanced']
Modifiability,rator.scala:490); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.hasNext(RichContextRDD.scala:69); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator.foreach(Iterator.scala:943); 	at scala.collection.Iterator.foreach$(Iterator.scala:943); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); 	at is.hail.io.RichContextRDDRegionValue$.writeRowsPartition(RichContextRDDRegionValue.scala:37); 	at is.hail.io.RichContextRDDLong$.$anonfun$writeRows$2(RichContextRDDRegionValue.scala:234); 	at is.hail.utils.richUtils.RichContextRDD$.writeParts(RichContextRDD.scala:42); 	at is.hail.utils.richUtils.RichContextRDD.$anonfun$writePartitions$1(RichContextRDD.scala:107); 	at is.hail.utils.richUtils.RichContextRDD.$anonfun$writePartitions$1$adapted(RichContextRDD.scala:105); 	at is.hail.sparkextras.ContextRDD.$anonfun$cmapPartitionsWithIndex$2(ContextRDD.scala:259); 	at is.hail.utils.richUtils.RichContextRDD.$anonfun$cleanupRegions$2(RichContextRDD.scala:60); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.hasNext(RichContextRDD.scala:69); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator.foreach(Iterator.scala:943); 	at scala.collection.Iterator.foreach$(Iterator.scala:943); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62); 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936:2609,adapt,adapted,2609,https://hail.is,https://github.com/hail-is/hail/issues/12936,1,['adapt'],['adapted']
Modifiability,raversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:2388,Rewrite,RewriteBottomUp,2388,https://hail.is,https://github.com/hail-is/hail/issues/9128,2,['Rewrite'],['RewriteBottomUp']
Modifiability,raversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldC,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:3868,Rewrite,RewriteBottomUp,3868,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"rc1 but wasn't documented.</p>; </li>; </ul>; <h2>Bug Fixes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9355"">#9355</a>: Fixed error message prints function decorators when using assert in Python 3.8 and above.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9396"">#9396</a>: Ensure <code>pytest.Config.inifile</code>{.interpreted-text role=&quot;attr&quot;} is available during the <code>pytest_cmdline_main &lt;_pytest.hookspec.pytest_cmdline_main&gt;</code>{.interpreted-text role=&quot;func&quot;} hook (regression during <code>7.0.0rc1</code>).</li>; </ul>; <h2>Improved Documentation</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9404"">#9404</a>: Added extra documentation on alternatives to common misuses of [pytest.warns(None)]{.title-ref} ahead of its deprecation.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9505"">#9505</a>: Clarify where the configuration files are located. To avoid confusions documentation mentions; that configuration file is located in the root of the repository.</li>; </ul>; <h2>Trivial/Internal Changes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9521"">#9521</a>: Add test coverage to assertion rewrite path.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pytest-dev/pytest/commit/3f12087fe0d86a319216653b08b66a96d400bee2""><code>3f12087</code></a> [pre-commit.ci] auto fixes from pre-commit.com hooks</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/bc3021cdfd76507aa3d9e278bd885da9bc1907b2""><code>bc3021c</code></a> Prepare release version 7.0.1</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/591d476f14e3e83d90fbea75d326a93c5e368708""><code>591d476</code></a> Merge pull request <a href=""https://github-re",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11516:3512,config,configuration,3512,https://hail.is,https://github.com/hail-is/hail/pull/11516,3,['config'],['configuration']
Modifiability,"rce (or it may not exist)."",; E ""reason"": ""forbidden""; E }; E ],; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist).""; E }; E ; E Java stack trace:; E java.io.IOException: Error accessing gs://hail-test-requester-pays-fds32/zero-to-nine; E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:1986); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:1882); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystemImpl.getFileInfoInternal(GoogleCloudStorageFileSystemImpl.java:861); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystemImpl.getFileInfo(GoogleCloudStorageFileSystemImpl.java:833); E 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem.getFileStatus(GoogleHadoopFileSystem.java:724); E 	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:115); E 	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:349); E 	at org.apache.hadoop.fs.Globber.glob(Globber.java:202); E 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2142); E 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem.globStatus(GoogleHadoopFileSystem.java:759); E 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem.globStatus(GoogleHadoopFileSystem.java:1277); E 	at is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:162); E 	at is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:85); E 	at is.hail.io.fs.FS.glob(FS.scala:402); E 	at is.hail.io.fs.FS.glob$(FS.scala:402); E 	at is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:85); E 	at is.hail.io.fs.HadoopFS.$anonfun$globAll$1(HadoopFS.scala:154); E 	at is.hail.io.fs.HadoopFS.$anonfun$globAll$1$adapted(HadoopFS.scala:153). ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236:3197,adapt,adapted,3197,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236,1,['adapt'],['adapted']
Modifiability,"rdance[:2].map(lambda x: hl.sum(x[:2])))>0)). /home/hail/hail.zip/hail/table.py in aggregate(self, expr); 1107 analyze('Table.aggregate', expr, self._global_indices, {self._row_axis}); 1108 ; -> 1109 result_json = base._jt.aggregateJSON(expr._ast.to_hql()); 1110 return expr.dtype._from_json(result_json); 1111 . /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: VerifyError: Bad local variable type; Exception Details:; Location:; is/hail/codegen/generated/C423.apply(Lis/hail/annotations/Region;[Lis/hail/annotations/aggregators/RegionValueAggregator;JZJZ)V @2710: iload; Reason:; Type top (current frame, locals[130]) is not assignable to integer; Current Frame:; bci: @2710; flags: { }; locals: { 'is/hail/codegen/generated/C423', 'is/hail/annotations/Region', '[Lis/hail/annotations/aggregators/RegionValueAggregator;', long, long_2nd, integer, long, long_2nd, integer, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, integer, long, long_2nd, top, integer, integer, top, top, top, integer, long, long_2nd, integer, top, integer, integer, integer, long, long_2nd, integer, long, long_2nd, integer, integer, integer }; stack: { 'is/hail/codegen/generated/C423' }; Bytecode:; 0x0000000: 1508 3630 1530 9900 06a7 0008 1606 a700; 0x0000010: 0614 0020 3731 1530",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3729:2387,variab,variable,2387,https://hail.is,https://github.com/hail-is/hail/issues/3729,1,['variab'],['variable']
Modifiability,"re the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](https://github.com/kubernetes/kubernetes/pull/61231#pullrequestreview-104364784) (what; the hell?), ergo Confused Deputy. I also partly resolved the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:2890,config,config,2890,https://hail.is,https://github.com/hail-is/hail/pull/8513,3,['config'],"['config', 'configuration']"
Modifiability,"re-PEP8 methods (such as <code>ParserElement.parseString</code>) will start to raise <code>DeprecationWarnings</code>. 3.2.0 should get released some time later in 2023. I currently plan to completely drop the pre-PEP8 methods in pyparsing 4.0, though we won't see that release until at least late 2023 if not 2024. So there is plenty of time to convert existing parsers to the new function names before the old functions are completely removed. (Big help from Devin J. Pohly in structuring the code to enable this peaceful transition.)</p>; <p>Version 3.2.0 will also discontinue support for Python versions 3.6 and 3.7.</p>; <ul>; <li>; <p>API ENHANCEMENT: <code>Optional(expr)</code> may now be written as <code>expr | &quot;&quot;</code></p>; <p>This will make this code:</p>; <pre><code>&quot;{&quot; + Optional(Literal(&quot;A&quot;) | Literal(&quot;a&quot;)) + &quot;}&quot;; </code></pre>; <p>writable as:</p>; <pre><code>&quot;{&quot; + (Literal(&quot;A&quot;) | Literal(&quot;a&quot;) | &quot;&quot;) + &quot;}&quot;; </code></pre>; <p>Some related changes implemented as part of this work:</p>; <ul>; <li><code>Literal(&quot;&quot;)</code> now internally generates an <code>Empty()</code> (and no longer raises an exception)</li>; <li><code>Empty</code> is now a subclass of <code>Literal</code></li>; </ul>; <p>Suggested by Antony Lee (issue <a href=""https://redirect.github.com/pyparsing/pyparsing/issues/412"">#412</a>), PR (<a href=""https://redirect.github.com/pyparsing/pyparsing/issues/413"">#413</a>) by Devin J. Pohly.</p>; </li>; <li>; <p>Added new class property <code>identifier</code> to all Unicode set classes in <code>pyparsing.unicode</code>, using the class's values for <code>cls.identchars</code> and <code>cls.identbodychars</code>. Now Unicode-aware parsers that formerly wrote:</p>; <pre><code>ppu = pyparsing.unicode; ident = Word(ppu.Greek.identchars, ppu.Greek.identbodychars); </code></pre>; <p>can now write:</p>; <pre><code>ident = ppu.Greek.identifier; # or; # id",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13334:985,ENHANCE,ENHANCEMENT,985,https://hail.is,https://github.com/hail-is/hail/pull/13334,2,['ENHANCE'],['ENHANCEMENT']
Modifiability,read SPARK_HOME from environment variable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1119:33,variab,variable,33,https://hail.is,https://github.com/hail-is/hail/pull/1119,1,['variab'],['variable']
Modifiability,"reate and delete API instead of copying the user's gsa from the production namespace. This relies on / tests that the delete user endpoint is properly deleting cloud identities when the users are deleted (previously broken in GCP but fixed in this PR.; - The developer role no longer implicitly deletes and recreates a corresponding namespace. I wanted adding developers to test namespaces not to have side-effects that leaked out of the namespace. A follow-up PR will incorporate the ability for a developer to request an on-demand dev namespace, which should be made a lot easier after these changes. I think this also means that we can remove some permissions from the auth K8s ServiceAccount since it no longer needs the ability to create and delete namespaces.; - A fixed-but-sufficient number of oauth2 callbacks are hard-coded into the oauth2 secret from GCP/azure and then allocated to a given namespace. This is fairly self-contained, all that needs to happen is to tell `auth` what callback to use and rewrite those callback urls in gateway to route back to the appropriate auth. This is done only for test namespaces, production still just uses `auth.hail.is/oauth2callback`. This gets around a long-standing limitation of Google oauth2 clients where there is no programmatic way to change the oauth2 callbacks. ### What has stayed the same; - The lifecycle of dev and test namespaces has not changed (future PR).; - The semantics of a dev deploy has not changed (other than a different oauth2 callback which the user will not notice). ### Testing; I've tested this branch in my own google project both by deploying main and updating to this branch and by deploying this branch from the beginning. The CI in my project ran both some dev deploys and a test batch because I PR'd this branch against my fork. Tested that I could access both my dev namespace and the test namespace in that deployment. I manually verified that dev cloud identities in the test namespace were deleted after the n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12751:1873,rewrite,rewrite,1873,https://hail.is,https://github.com/hail-is/hail/pull/12751,1,['rewrite'],['rewrite']
Modifiability,"recursive flag for glob notebook search by <a href=""https://github.com/paoloalba""><code>@​paoloalba</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1785"">jupyter/nbconvert#1785</a></li>; <li>Updates for sphinx 5.0 support by <a href=""https://github.com/blink1073""><code>@​blink1073</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1788"">jupyter/nbconvert#1788</a></li>; <li>Fixed unique div ids in lab template, fixed <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/issues/1759"">#1759</a> by <a href=""https://github.com/veghdev""><code>@​veghdev</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1761"">jupyter/nbconvert#1761</a></li>; <li>WebPDFExporter: Emulate media print by <a href=""https://github.com/martinRenou""><code>@​martinRenou</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1791"">jupyter/nbconvert#1791</a></li>; <li>Fix fonts overriden by user stylesheet by inheriting styles by <a href=""https://github.com/dakoop""><code>@​dakoop</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1793"">jupyter/nbconvert#1793</a></li>; <li>Fix lab template output alignment by <a href=""https://github.com/dakoop""><code>@​dakoop</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1795"">jupyter/nbconvert#1795</a></li>; <li>Add qtpdf and qtpng exporters by <a href=""https://github.com/davidbrochart""><code>@​davidbrochart</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1611"">jupyter/nbconvert#1611</a></li>; <li>Fix linters by <a href=""https://github.com/martinRenou""><code>@​martinRenou</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1825"">jupyter/nbconvert#1825</a></li>; <li>Remove downloaded CSS from repository by <a href=""https://github.com/martinRenou""><code>@​martinRenou</code></a> in <a hr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12126:2220,inherit,inheriting,2220,https://hail.is,https://github.com/hail-is/hail/pull/12126,1,['inherit'],['inheriting']
Modifiability,"recursive flag for glob notebook search by <a href=""https://github.com/paoloalba""><code>@​paoloalba</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1785"">jupyter/nbconvert#1785</a></li>; <li>Updates for sphinx 5.0 support by <a href=""https://github.com/blink1073""><code>@​blink1073</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1788"">jupyter/nbconvert#1788</a></li>; <li>Fixed unique div ids in lab template, fixed <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/issues/1759"">#1759</a> by <a href=""https://github.com/veghdev""><code>@​veghdev</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1761"">jupyter/nbconvert#1761</a></li>; <li>WebPDFExporter: Emulate media print by <a href=""https://github.com/martinRenou""><code>@​martinRenou</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1791"">jupyter/nbconvert#1791</a></li>; <li>Fix fonts overriden by user stylesheet by inheriting styles by <a href=""https://github.com/dakoop""><code>@​dakoop</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1793"">jupyter/nbconvert#1793</a></li>; <li>Fix lab template output alignment by <a href=""https://github.com/dakoop""><code>@​dakoop</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1795"">jupyter/nbconvert#1795</a></li>; <li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1796"">jupyter/nbconvert#1796</a></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/jupyter/nbconvert/commit/27a7fcbd9cb55d1c30818b1c8b5918bb178a243f""><code>27a7fcb</code></a> Release 7.0.0</li>; <li><a href=""https://github.com/jupyter/nbconve",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12126:8376,inherit,inheriting,8376,https://hail.is,https://github.com/hail-is/hail/pull/12126,1,['inherit'],['inheriting']
Modifiability,"recy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:6315,config,config-NAME,6315,https://hail.is,https://github.com/hail-is/hail/pull/8561,1,['config'],['config-NAME']
Modifiability,"redirect.dependabot.com/psf/black/issues/3168"">#3168</a>)</li>; <li>When using <code>--skip-magic-trailing-comma</code> or <code>-C</code>, trailing commas are stripped from; subscript expressions with more than 1 element (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3209"">#3209</a>)</li>; <li>Implicitly concatenated strings inside a list, set, or tuple are now wrapped inside; parentheses (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3162"">#3162</a>)</li>; <li>Fix a string merging/split issue when a comment is present in the middle of implicitly; concatenated strings on its own line (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3227"">#3227</a>)</li>; </ul>; <h3><em>Blackd</em></h3>; <ul>; <li><code>blackd</code> now supports enabling the preview style via the <code>X-Preview</code> header (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3217"">#3217</a>)</li>; </ul>; <h3>Configuration</h3>; <ul>; <li>Black now uses the presence of debug f-strings to detect target version (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3215"">#3215</a>)</li>; <li>Fix misdetection of project root and verbose logging of sources in cases involving; <code>--stdin-filename</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3216"">#3216</a>)</li>; <li>Immediate <code>.gitignore</code> files in source directories given on the command line are now; also respected, previously only <code>.gitignore</code> files in the project root and; automatically discovered directories were respected (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3237"">#3237</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Recommend using BlackConnect in IntelliJ IDEs (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3150"">#3150</a>)</li>; </ul>; <h3>Integrations</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12174:7863,Config,Configuration,7863,https://hail.is,https://github.com/hail-is/hail/pull/12174,1,['Config'],['Configuration']
Modifiability,"reemptible pods come and go. This leads to our second goal: instead of routing all requests through kube-proxy, use Kubernetes Headless Services to expose all pod IPs underlying a Service so that our proxies can properly load-balance across persistent connections. ## Solution. This PR addresses the two goals outlined above and does so through using Envoy, a load-balancer/proxy that is well-suited to this sort of highly-dynamic cluster configuration. Envoy does not have the constraint that all upstream services must be available at start-time, and has a very convenient API for updating the cluster configuration without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as follows:. 1. Envoy-based gateways and internal-gateways will load their routing configuration from a Kubernetes ConfigMap, which they watch for changes and reconcile their configuration when the ConfigMap changes. The ConfigMap can be populated with a manual deploy and is populated from the beginning with production routes (i.e. batch.hail.is gets routed to batch.default); 2. When running CI, CI will regularly update the ConfigMap with additional routes based on which internal namespaces (dev and PR) are currently active. This requires relatively small changes to CI to track active namespaces but overall is a pretty small change. Note that this does not introduce a dependency on CI to support production traffic, only development traffic.; 3. Deployments that run more than 1 replica (but really can be all of them) are run behind Headless Services, which expose the underlying pod IPs so Envoy can handle load-balancing instead of kube-proxy. This allows Envoy to make smart load-balancing decisions and correctly enforce rate-limiting when using connection pools. The namespace tracking in CI in Point ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:4233,config,configuration,4233,https://hail.is,https://github.com/hail-is/hail/pull/12095,4,"['Config', 'config']","['ConfigMap', 'configuration']"
Modifiability,"ref=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105687"">kubernetes/kubernetes#105687</a>, <a href=""https://github.com/alculquicondor""><code>@​alculquicondor</code></a>)</li>; <li>Kube-apiserver: Fixes handling of CRD schemas containing literal null values in enums. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104969"">kubernetes/kubernetes#104969</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>)</li>; <li>Kube-apiserver: The <code>rbac.authorization.k8s.io/v1alpha1</code> API version is removed; use the <code>rbac.authorization.k8s.io/v1</code> API, available since v1.8. The <code>scheduling.k8s.io/v1alpha1</code> API version is removed; use the <code>scheduling.k8s.io/v1</code> API, available since v1.14. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104248"">kubernetes/kubernetes#104248</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>)</li>; <li>Kube-scheduler: support for configuration file version <code>v1beta1</code> is removed. Update configuration files to v1beta2(xref: <a href=""https://github-redirect.dependabot.com/kubernetes/enhancements/issues/2901"">kubernetes/enhancements#2901</a>) or v1beta3 before upgrading to 1.23. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104782"">kubernetes/kubernetes#104782</a>, <a href=""https://github.com/kerthcet""><code>@​kerthcet</code></a>)</li>; <li>KubeSchedulerConfiguration provides a new field <code>MultiPoint</code> which will register a plugin for all valid extension points (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105611"">kubernetes/kubernetes#105611</a>, <a href=""https://github.com/damemi""><code>@​damemi</code></a>) [SIG Scheduling and Testing]</li>; <li>Kubelet should reject pods whose OS doesn't match the node's OS label. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105292"">kubernetes/kubernetes#105292",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:8383,config,configuration,8383,https://hail.is,https://github.com/hail-is/hail/pull/11957,1,['config'],['configuration']
Modifiability,"ref=""https://github.com/krzysztof-pawlik-gat""><code>@​krzysztof-pawlik-gat</code></a></li>; <li>Keep sort preference for previously sorted columns (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/220"">#220</a>) <a href=""https://github.com/wanam""><code>@​wanam</code></a></li>; <li>Fix assets file naming to work across both *nix and windows (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/223"">#223</a>) <a href=""https://github.com/BeyondEvil""><code>@​BeyondEvil</code></a></li>; <li>Remove unused and undocumented markers (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/224"">#224</a>) <a href=""https://github.com/BeyondEvil""><code>@​BeyondEvil</code></a></li>; <li>Append a line break after captured log sections (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/217"">#217</a>) <a href=""https://github.com/borntyping""><code>@​borntyping</code></a></li>; <li>Handle when report title is stored as an environment variable (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/203"">#203</a>) <a href=""https://github.com/BeyondEvil""><code>@​BeyondEvil</code></a></li>; <li>Removed extraneous space from anchor tag (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/192"">#192</a>) <a href=""https://github.com/chardbury""><code>@​chardbury</code></a></li>; <li>Stop filtering out falsy environment values (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/180"">#180</a>) <a href=""https://github.com/crazymerlyn""><code>@​crazymerlyn</code></a></li>; <li>Always define <strong>version</strong> even if get_distribution() fails (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/178"">#178</a>) <a href=""https://github.com/nicoddemus""><code>@​nicoddemus</code></a></li>; <li>Disable sort on environment table when metadata is ordered (<a href=""https://github-redirect.dependabot.c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11524:7228,variab,variable,7228,https://hail.is,https://github.com/hail-is/hail/pull/11524,1,['variab'],['variable']
Modifiability,refactor ArraySort IR to take comparison expression,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5084:0,refactor,refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/5084,1,['refactor'],['refactor']
Modifiability,refactor backend stuff in preparation for scheduler's backend,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6301:0,refactor,refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/6301,1,['refactor'],['refactor']
Modifiability,refactor collect as set bool,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3821:0,refactor,refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/3821,1,['refactor'],['refactor']
Modifiability,refactored linreg as a map rather than a join,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/989:0,refactor,refactored,0,https://hail.is,https://github.com/hail-is/hail/pull/989,1,['refactor'],['refactored']
Modifiability,"refactored the four block matrix operators to match for now, I'll later change this interface to be numpy-like and match local matrix",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2995#issuecomment-369619464:0,refactor,refactored,0,https://hail.is,https://github.com/hail-is/hail/pull/2995#issuecomment-369619464,1,['refactor'],['refactored']
Modifiability,refactored with flattenOrNull in Utils. Back to you.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/591#issuecomment-240544764:0,refactor,refactored,0,https://hail.is,https://github.com/hail-is/hail/pull/591#issuecomment-240544764,1,['refactor'],['refactored']
Modifiability,"regex is way more flexible (it was string containment before). Chris added this feature as a quick one-off to help with some stuff he was working on, but being able to write inclusion/exclusion regexes has been very useful!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6894#issuecomment-522472904:18,flexible,flexible,18,https://hail.is,https://github.com/hail-is/hail/pull/6894#issuecomment-522472904,1,['flexible'],['flexible']
Modifiability,"relates to this (which we need to update to Python, I made a separate issue):; http://discuss.hail.is/t/save-pcs-for-projection/46. There are several issues requesting more flexible PCA at high abstraction level. We should make a game plan.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/442#issuecomment-279578856:173,flexible,flexible,173,https://hail.is,https://github.com/hail-is/hail/issues/442#issuecomment-279578856,1,['flexible'],['flexible']
Modifiability,"remove unused let; push aggfilter into aggmap (if possible); fuse aggmaps; Added ir.Binds (list of variables bound by an IR); Added ir.Mentions (whether a variable is free in an IR); propagate NA where strict; fuse ArrayMaps. Also improved ir.Pretty (more headers, was missing some closing parens). This cleans up most of the small IR opportunities I've seen in @konradjk's scripts, although probably won't make a huge difference (except maybe pushing AggFilter into AggMap).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3429:99,variab,variables,99,https://hail.is,https://github.com/hail-is/hail/pull/3429,2,['variab'],"['variable', 'variables']"
Modifiability,remove vep configs human_ancestor and conservation file for 0.2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1728:11,config,configs,11,https://hail.is,https://github.com/hail-is/hail/issues/1728,1,['config'],['configs']
Modifiability,removed unused variable buffer from ReadRowsRDD,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2258:15,variab,variable,15,https://hail.is,https://github.com/hail-is/hail/pull/2258,1,['variab'],['variable']
Modifiability,"reordering output (<a href=""https://redirect.github.com/pygments/pygments/issues/2407"">#2407</a>, <a href=""https://redirect.github.com/pygments/pygments/issues/2410"">#2410</a>, <a href=""https://redirect.github.com/pygments/pygments/issues/2412"">#2412</a>)</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pygments/pygments/commit/04a75bd5a75bfe27f0b582dd83c85e62f9475581""><code>04a75bd</code></a> Prepare 2.15.1 release.</li>; <li><a href=""https://github.com/pygments/pygments/commit/fdf182a7af85b1deeeb637ca970d31935e7c9d52""><code>fdf182a</code></a> Improve Java properties lexer (<a href=""https://redirect.github.com/pygments/pygments/issues/2404"">#2404</a>)</li>; <li><a href=""https://github.com/pygments/pygments/commit/c5a2b23adaaadc08a7586a5eda72e9f7d6171012""><code>c5a2b23</code></a> Update CHANGES</li>; <li><a href=""https://github.com/pygments/pygments/commit/c97762448b1e4eac8d74b8d88415f23c32aa0cdd""><code>c977624</code></a> Refactor PythonConsoleLexer as a DelegatingLexer (<a href=""https://redirect.github.com/pygments/pygments/issues/2412"">#2412</a>)</li>; <li><a href=""https://github.com/pygments/pygments/commit/50dd4d80e25c4c4afab503d41b471a536ed2af13""><code>50dd4d8</code></a> Python console: do not require output that looks like a traceback to be valid...</li>; <li><a href=""https://github.com/pygments/pygments/commit/96a0cdf200ab8a36dc5f6f748f3b9d01c05cb91b""><code>96a0cdf</code></a> PythonTracebackLexer: minor tweak in docstring</li>; <li><a href=""https://github.com/pygments/pygments/commit/569eea6ee85ec4d679bb38a890c167b58ee727dd""><code>569eea6</code></a> Enable Sphinx nitpicky mode and fix warnings (<a href=""https://redirect.github.com/pygments/pygments/issues/2403"">#2403</a>)</li>; <li><a href=""https://github.com/pygments/pygments/commit/b018a65cb6ef51596c2cb8d6c97f0d79d9fa2ae7""><code>b018a65</code></a> Prepare for next release.</li>; <li>See full diff in <a href=""https://github.com/pygment",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12909:2515,Refactor,Refactor,2515,https://hail.is,https://github.com/hail-is/hail/pull/12909,1,['Refactor'],['Refactor']
Modifiability,"request and response bodies from; being written to the API audit log. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/94986"">kubernetes/kubernetes#94986</a>, <a href=""https://github.com/tkashem""><code>@​tkashem</code></a>) [SIG API Machinery, Auth, Cloud Provider and Testing]</li>; <li>A small regression in Service updates was fixed. The circumstances are so unlikely that probably nobody would ever hit it. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104601"">kubernetes/kubernetes#104601</a>, <a href=""https://github.com/thockin""><code>@​thockin</code></a>)</li>; <li>Added a feature gate <code>StatefulSetAutoDeletePVC</code>, which allows PVCs automatically created for StatefulSet pods to be automatically deleted. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/99728"">kubernetes/kubernetes#99728</a>, <a href=""https://github.com/mattcary""><code>@​mattcary</code></a>)</li>; <li>Client-go impersonation config can specify a UID to pass impersonated uid information through in requests. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104483"">kubernetes/kubernetes#104483</a>, <a href=""https://github.com/margocrawf""><code>@​margocrawf</code></a>)</li>; <li>Create HPA v2 from v2beta2 with some fields changed. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/102534"">kubernetes/kubernetes#102534</a>, <a href=""https://github.com/wangyysde""><code>@​wangyysde</code></a>) [SIG API Machinery, Apps, Auth, Autoscaling and Testing]</li>; <li>Ephemeral containers graduated to beta and are now available by default. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105405"">kubernetes/kubernetes#105405</a>, <a href=""https://github.com/verb""><code>@​verb</code></a>)</li>; <li>Fix kube-proxy regression on UDP services because the logic to detect stale connections was not considering if the endpoint was ready. (<a href=""https://g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:2835,config,config,2835,https://hail.is,https://github.com/hail-is/hail/pull/11957,1,['config'],['config']
Modifiability,"resolves #14749. Leave the override in Backend as well to avoid duplication. Future enhancements may enable us to construct a ServiceBackend from argv alone, allowing this to be reverted. ## Security Assessment. Delete all except the correct answer:; - This change has no security impact. ### Impact Description; This change restores known good functionality.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14750:84,enhance,enhancements,84,https://hail.is,https://github.com/hail-is/hail/pull/14750,1,['enhance'],['enhancements']
Modifiability,"resting thing is, when I tried to convert the exactly same data in local computer using singularity instead of docker, it worked. Also, for the the other chromosomes with less variants but same samples, such as chr13, it worked well in Terra. Since we will convert multiple plink files to hailmatrix table using Terra platform in future, I need to figure the problem out. Any advise would be appreciated. ### Version. 0.2.127. ### Relevant log output. ```shell; 2024/01/17 20:20:25 Starting container setup.; 2024/01/17 20:20:26 Done container setup.; 2024/01/17 20:20:27 Starting localization.; 2024/01/17 20:20:34 Localization script execution started...; 2024/01/17 20:20:34 Localizing input gs://fc-5a8938eb-1299-4afc-957f-afb53ef602b9/submissions/e8747e74-47d1-4f52-acfc-1ac7f81d79ba/VUMCBed2HailMatrix/683447d9-9342-4058-bcfc-ba21422d3121/call-Bed2HailMatrix/script -> /cromwell_root/script; 2024/01/17 20:20:36 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bed -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bed; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.fam -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.fam; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bim -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bim; Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.fam...; / [0 files][ 0.0 B/910.3 KiB] / [1 files][910.3 KiB/910.3 KiB] Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.bim...; / [1 files][910.3 KiB/369.7 MiB] - - [1 files][ 51.9 MiB/369.7 MiB] \ | | [1 files][107.6 MiB/369.7 MiB] / - - [1 files][162.3 MiB/369.7 MiB] \ \ [1 files][213.9 MiB/369.7 MiB] | / / [1 files][286.6 MiB/369.7 MiB] - \ \ [1 files][342.1 MiB/369.7 MiB] |; Operation completed over 2 objects/369.7 MiB.; | [2 files][369.7 MiB/369.7 MiB] 2024/01/17 20:59:27 Localization script execution complete.; 2024/01/17 20:59:38 Done localization.; 2024/01/17 20:59:39 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:9703,sandbox,sandbox,9703,https://hail.is,https://github.com/hail-is/hail/issues/14168,1,['sandbox'],['sandbox']
Modifiability,"rets/ConfigMaps feature to Beta and enable the feature by default.; This allows to set <code>Immutable</code> field in Secrets or ConfigMap object to mark their contents as immutable. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/89594"">kubernetes/kubernetes#89594</a>, <a href=""https://github.com/wojtek-t""><code>@​wojtek-t</code></a>) [SIG Apps and Testing]</li>; <li>Remove <code>BindTimeoutSeconds</code> from schedule configuration <code>KubeSchedulerConfiguration</code> (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91580"">kubernetes/kubernetes#91580</a>, <a href=""https://github.com/cofyc""><code>@​cofyc</code></a>) [SIG Scheduling and Testing]</li>; <li>Remove kubescheduler.config.k8s.io/v1alpha1 (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/89298"">kubernetes/kubernetes#89298</a>, <a href=""https://github.com/gavinfish""><code>@​gavinfish</code></a>) [SIG Scheduling]</li>; <li>Reserve plugins that fail to reserve will trigger the unreserve extension point (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/92391"">kubernetes/kubernetes#92391</a>, <a href=""https://github.com/adtac""><code>@​adtac</code></a>) [SIG Scheduling and Testing]</li>; <li>Resolve regression in <code>metadata.managedFields</code> handling in update/patch requests submitted by older API clients (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91748"">kubernetes/kubernetes#91748</a>, <a href=""https://github.com/apelisse""><code>@​apelisse</code></a>)</li>; <li>Scheduler: optionally check for available storage capacity before scheduling pods which have unbound volumes (alpha feature with the new <code>CSIStorageCapacity</code> feature gate, only works for CSI drivers and depends on support for the feature in a CSI driver deployment) (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/92387"">kubernetes/kubernetes#92387</a>, <a href=""https:/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:12140,plugin,plugins,12140,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['plugin'],['plugins']
Modifiability,rettyPrint.Doc.render(PrettyPrintWriter.scala:167); at is.hail.expr.ir.Pretty.sexprStyle(Pretty.scala:466); at is.hail.expr.ir.Pretty.apply(Pretty.scala:429); at is.hail.expr.ir.Pretty$.apply(Pretty.scala:22); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:45); at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:30); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:13); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:450); at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:486); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:70); at is.hail.utils.package$.using(package.scala:635); at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); at is.hail.utils.package$.using(package.scala:635); at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); at is.hail.backend.spark.Sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046:3520,adapt,adapted,3520,https://hail.is,https://github.com/hail-is/hail/issues/13046,1,['adapt'],['adapted']
Modifiability,"return CreateDatabaseStep(; + return CreateDatabase2Step(; params,; json['databaseName'],; json['namespace'],; @@ -1111,12 +1113,12 @@ EOF; attributes={'name': self.name},; secrets=[; {; - 'namespace': self.database_server_config_namespace,; + 'namespace': self.namespace,; 'name': 'database-server-config',; 'mount_path': '/sql-config',; }; ],; - service_account={'namespace': DEFAULT_NAMESPACE, 'name': 'ci-agent'},; + service_account={'namespace': self.namespace, 'name': 'admin'},; input_files=input_files,; parents=[self.create_passwords_job] if self.create_passwords_job else self.deps_parents(),; network='private',; @@ -1125,42 +1127,4 @@ EOF; ); ; def cleanup(self, batch, scope, parents):; - if scope in ['deploy', 'dev'] or self.cant_create_database:; - return; -; - cleanup_script = f'''; -set -ex; -; -commands=$(mktemp); -; -cat >$commands <<EOF; -DROP DATABASE IF EXISTS \\`{self._name}\\`;; -DROP USER IF EXISTS '{self.admin_username}';; -DROP USER IF EXISTS '{self.user_username}';; -EOF; -; -until mysql --defaults-extra-file=/sql-config/sql-config.cnf <$commands; -do; - echo 'failed, will sleep 2 and retry'; - sleep 2; -done; -; -'''; -; - self.cleanup_job = batch.create_job(; - CI_UTILS_IMAGE,; - command=['bash', '-c', cleanup_script],; - attributes={'name': f'cleanup_{self.name}'},; - secrets=[; - {; - 'namespace': self.database_server_config_namespace,; - 'name': 'database-server-config',; - 'mount_path': '/sql-config',; - }; - ],; - service_account={'namespace': DEFAULT_NAMESPACE, 'name': 'ci-agent'},; - parents=parents,; - always_run=True,; - network='private',; - regions=[REGION],; - ); + pass; diff --git a/ci/test/resources/build.yaml b/ci/test/resources/build.yaml; index e6f67bb486..662c873590 100644; --- a/ci/test/resources/build.yaml; +++ b/ci/test/resources/build.yaml; @@ -190,7 +190,7 @@ steps:; to: /io/pyproject.toml; dependsOn:; - hello_image; - - kind: createDatabase; + - kind: createDatabase2; name: hello_database; databaseName: hello; image:; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13022#issuecomment-1542233600:3158,config,config,3158,https://hail.is,https://github.com/hail-is/hail/pull/13022#issuecomment-1542233600,4,['config'],['config']
Modifiability,rewrite MatrixExplodeRows using compiled IR functions; add tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4069:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/4069,1,['rewrite'],['rewrite']
Modifiability,rewrite README to focus more on software dev,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4600:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/4600,1,['rewrite'],['rewrite']
Modifiability,rewrite Region.scala to build on the RegionPool backed region.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4859:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/4859,1,['rewrite'],['rewrite']
Modifiability,rewrite add_index as scans,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4149:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/4149,1,['rewrite'],['rewrite']
Modifiability,rewrite annotate_rows in terms of select_rows,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3230:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/3230,1,['rewrite'],['rewrite']
Modifiability,rewrite cpp decoder stuff to take input stream directly,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4676:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/4676,1,['rewrite'],['rewrite']
Modifiability,rewrite cxx iterator,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4956:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/4956,1,['rewrite'],['rewrite']
Modifiability,rewrite hl.agg.corr in python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6729:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/6729,1,['rewrite'],['rewrite']
Modifiability,rewrite hl.agg.hist in python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6730:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/6730,1,['rewrite'],['rewrite']
Modifiability,rewrite interval joins,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4487:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/4487,1,['rewrite'],['rewrite']
Modifiability,rewrite sample in python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4133:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/4133,1,['rewrite'],['rewrite']
Modifiability,rewrite sample names programatically,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/945:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/issues/945,1,['rewrite'],['rewrite']
Modifiability,rewrite select/annotate/drop to go through `_select_cols` in Python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3336:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/3336,1,['rewrite'],['rewrite']
Modifiability,rewrite staged aggregator interface,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6652:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/6652,1,['rewrite'],['rewrite']
Modifiability,rewrite trio_matrix in Python.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5132:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/5132,1,['rewrite'],['rewrite']
Modifiability,"rg/en/master/changes.html"">https://www.sphinx-doc.org/en/master/changes.html</a></p>; <h2>v4.4.0</h2>; <p>Changelog: <a href=""https://www.sphinx-doc.org/en/master/changes.html"">https://www.sphinx-doc.org/en/master/changes.html</a></p>; <h2>v4.3.1</h2>; <p>No release notes provided.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/sphinx-doc/sphinx/blob/5.x/CHANGES"">sphinx's changelog</a>.</em></p>; <blockquote>; <h1>Release 5.0.2 (released Jun 17, 2022)</h1>; <h2>Features added</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10523"">#10523</a>: HTML Theme: Expose the Docutils's version info tuple as a template; variable, <code>docutils_version_info</code>. Patch by Adam Turner.</li>; </ul>; <h2>Bugs fixed</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10538"">#10538</a>: autodoc: Inherited class attribute having docstring is documented even; if :confval:<code>autodoc_inherit_docstring</code> is disabled</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10509"">#10509</a>: autosummary: autosummary fails with a shared library</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10497"">#10497</a>: py domain: Failed to resolve strings in Literal. Patch by Adam Turner.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10523"">#10523</a>: HTML Theme: Fix double brackets on citation references in Docutils 0.18+.; Patch by Adam Turner.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10534"">#10534</a>: Missing CSS for nav.contents in Docutils 0.18+. Patch by Adam Turner.</li>; </ul>; <h1>Release 5.0.1 (released Jun 03, 2022)</h1>; <h2>Bugs fixed</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10498"">#10498</a>: gettext: TypeError is raised when sorting warni",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11925:1755,Inherit,Inherited,1755,https://hail.is,https://github.com/hail-is/hail/pull/11925,1,['Inherit'],['Inherited']
Modifiability,ring.LowerTableIR$.lower$2(LowerTableIR.scala:1050); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2192); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1050); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1361); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1050); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1654); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:728); 	at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:823); 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:27); 	at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:11); 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:91); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:27); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14529:8146,rewrite,rewrite,8146,https://hail.is,https://github.com/hail-is/hail/issues/14529,1,['rewrite'],['rewrite']
Modifiability,"rk releases</li>; <li><a href=""https://github.com/apache/spark/commit/001d8b0cddcec46a44e7c6e31612dc2baada05d5""><code>001d8b0</code></a> [SPARK-37554][BUILD] Add PyArrow, pandas and plotly to release Docker image d...</li>; <li><a href=""https://github.com/apache/spark/commit/9dd4c07475c82f922c29d67a4db4bb42676c5c07""><code>9dd4c07</code></a> [SPARK-37730][PYTHON][FOLLOWUP] Split comments to comply pycodestyle check</li>; <li><a href=""https://github.com/apache/spark/commit/bc54a3f0c2e08893702c3929bfe7a9d543a08cdb""><code>bc54a3f</code></a> [SPARK-37730][PYTHON] Replace use of MPLPlot._add_legend_handle with MPLPlot....</li>; <li><a href=""https://github.com/apache/spark/commit/c5983c1691f20590abf80b17bdc029b584b89521""><code>c5983c1</code></a> [SPARK-38018][SQL][3.2] Fix ColumnVectorUtils.populate to handle CalendarInte...</li>; <li><a href=""https://github.com/apache/spark/commit/32aff86477ac001b0ee047db08591d89e90c6eb8""><code>32aff86</code></a> [SPARK-39447][SQL][3.2] Avoid AssertionError in AdaptiveSparkPlanExec.doExecu...</li>; <li><a href=""https://github.com/apache/spark/commit/be891ad99083564a7bf7f421e00b2cc4759a679f""><code>be891ad</code></a> [SPARK-39551][SQL][3.2] Add AQE invalid plan check</li>; <li><a href=""https://github.com/apache/spark/commit/1c0bd4c15a28d7c6a2dca846a5b8d0eb1d152aae""><code>1c0bd4c</code></a> [SPARK-39656][SQL][3.2] Fix wrong namespace in DescribeNamespaceExec</li>; <li><a href=""https://github.com/apache/spark/commit/3d084fe3217bea9af4c544f10ead8a2e5b97dad4""><code>3d084fe</code></a> [SPARK-39677][SQL][DOCS][3.2] Fix args formatting of the regexp and like func...</li>; <li>Additional commits viewable in <a href=""https://github.com/apache/spark/compare/v3.1.3...v3.2.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pyspark&package-manager=pip&previous-version=3.1.3&new-version=3.2.2)](https://docs.github.com/en/github/managing-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12452:1466,Adapt,AdaptiveSparkPlanExec,1466,https://hail.is,https://github.com/hail-is/hail/pull/12452,1,['Adapt'],['AdaptiveSparkPlanExec']
Modifiability,"rk.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": {; ""zoneUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f"",; ""networkUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/global/networks/default"",; ""serviceAccountScopes"": [; ""https://www.googleapis.com/auth/bigquery"",; ""https://www.googleapis.com/auth/bigtable.admin.table"",; ""https://www.googleapis.com/auth/bigtable.data"",; ""https://www.googleapis.com/auth/cloud.useraccounts.readonly"",; ""https://www.googleapis.com/auth/devstorage.full_control"",; ""https://www.googleapis.com/auth/devstorage.read_write"",; ""https://www.googleapis.com/auth/logging.write""; ]; },; ""masterConfig"": {; ""numInstances"": 1,; ""instanceNames"": [; ""cluster-2-m""; ],; ""imageUri"": ""https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/ma",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:3156,config,config,3156,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,4,['config'],"['config', 'configBucket']"
Modifiability,"rm aggregation aware of the nearest containing node which defines what is being aggregated over. This is done by making a new variable name---here called ""agg_capability""---which is added to the child context by any node which (re)defines the meaning of aggregation (`TableMapRows`, `AggFilter`, etc.), and which is implicitly referenced by any node which performs an aggregation (`ApplyAggOp`, `AggFilter`, etc.). This requires nodes to be able to bind variables which shadow variables already bound by parents, which it turns out wasn't handled correctly by the CSE algorithm. Fixing this required several changes:; * I moved free variable computation to a lazy value on `BaseIR`. This way, each time we see a subtree `x`, we can recompute the max depth of the binding sites of all of `x`'s free variables, since those binding sites may be different than last time we saw `x`. This also required splitting the free variables into value, agg, and scan sets, so they can be looked up in the correct context (previously context lookup was always done at the `Ref` node, at which point the variable was in the value context).; * Now, in the `CSEPrintPass`, we have to recompute the same binding depth calculation that was done in the analysis pass, so we know which binding site to look at (previously I just searched all binding sites in scope, but with shadowing handled correctly we don't have sufficient information to decide which binding site is valid). This requires maintaining contexts in the print pass, which is annoying because we are now traversing the tree of `Renderable` children, which is not exactly the same as the IR tree.; * To fix this, I duplicated all methods involving binding structure on `BaseIR`. To avoid having to write twice as many methods on concrete IR classes, I made the methods taking the index of the `Renderable` child (e.g. `renderable_bindings`) be the primary methods which are overridden. For all IR nodes, there is a map from IR child index to Renderable chil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7479:994,variab,variables,994,https://hail.is,https://github.com/hail-is/hail/pull/7479,2,['variab'],"['variable', 'variables']"
Modifiability,"rn pip will not pick it; up automatically. The reason is that is has <code>multidict &lt; 6</code> set in; the distribution package metadata (see :pr:<code>6950</code>). Please, use; <code>aiohttp ~= 3.8.3, != 3.8.1</code> instead, if you can.</p>; <h2>Bugfixes</h2>; <ul>; <li>Added support for registering :rfc:<code>OPTIONS &lt;9110#OPTIONS&gt;</code>; HTTP method handlers via :py:class:<code>~aiohttp.web.RouteTableDef</code>.; (<a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp/issues/4663"">#4663</a>)</li>; <li>Started supporting :rfc:<code>authority-form &lt;9112#authority-form&gt;</code> and; :rfc:<code>absolute-form &lt;9112#absolute-form&gt;</code> URLs on the server-side.; (<a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp/issues/6227"">#6227</a>)</li>; <li>Fixed Python 3.11 incompatibilities by using Cython 0.29.25.; (<a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp/issues/6396"">#6396</a>)</li>; <li>Extended the <code>sock</code> argument typing declaration of the</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/blob/master/CHANGES.rst"">aiohttp's changelog</a>.</em></p>; <blockquote>; <h1>3.8.3 (2022-09-21)</h1>; <p>.. attention::</p>; <p>This is the last :doc:<code>aiohttp &lt;index&gt;</code> release tested under; Python 3.6. The 3.9 stream is dropping it from the CI and the; distribution package metadata.</p>; <h2>Bugfixes</h2>; <ul>; <li>; <p>Increased the upper boundary of the :doc:<code>multidict:index</code> dependency; to allow for the version 6 -- by :user:<code>hugovk</code>.</p>; <p>It used to be limited below version 7 in :doc:<code>aiohttp &lt;index&gt;</code> v3.8.1 but; was lowered in v3.8.2 via :pr:<code>6550</code> and never brought back, causing; problems with dependency pins when upgrading. :doc:<code>aiohttp &lt;index&gt;</code> v3.8.3; fixes that by",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12296:2359,Extend,Extended,2359,https://hail.is,https://github.com/hail-is/hail/pull/12296,1,['Extend'],['Extended']
Modifiability,"rom Dockerfile.base. Replace uses with either the gcloud-sdk image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sure how much mileage we can get out of layer squashing. You can take a look at a service-base build [here](https://gist.github.com/danking/830af0688970c176ff25dbfeb4b222e7). Note the stdout comes first and then I `cat` the ""trace"" file which is a very weirdly formatted series of JSON objects that give more detailed information. You can clean it up a bit with `jq -c '(.Lo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10502:1394,layers,layers,1394,https://hail.is,https://github.com/hail-is/hail/pull/10502,1,['layers'],['layers']
Modifiability,"roposal for an IR representation. I'll say ""sum"" everywhere, but that could be any aggregation. Let T be a 1-tensor. O represents the output of the operation. Then. * `T(i) -> O()` is the sum of T. * `T(i) -> O(i)` is identity. * `T(i) -> O(i, i)` makes a square matrix whose diagonal is T. * `T(i) -> O(i, j)` and `T(i) -> O(j, i)` broadcast T over a matrix in the two possible directions. Now let T be a 2-tensor. * `T(i, j) -> O(i)` is the vector of row-sums of T. * `T(i, i) -> O(i)` is the diagonal of T. * `T(i, i) -> O()` is the trace of T. * `T(i, j) -> O(j, i)` is transposition. How do we represent matrix multiplication? Let T1 and T2 be 2-tensors. Then letting `T = Out(T1, T2, ""and"").map((x, y) => x * y)`, the matrix product is given by. * `T(i, j, j, k) -> O(i, k)`. In general, an index operation on T requires specifying an output tensor O (including its shape, though you can deduce that in non-broadcasting cases), a set of index variables (eg. ""i, j, k""), and an assignment of a variable to each dimension of T and O. . More abstractly, let DT and DO be the sets of dimensions of T and O. An index operation consists of a set I and two functions DT -> I <- DO, a ""cospan"". These operations compose by cospan composition, which involves a pushout (a disjoint union and a quotient). Let i: DT -> I and o: DO -> I be the two maps assigning index variables. It helps to consider some special cases (compare to the examples above):. * If i is surjective, and o is identity, this is extracting a diagonal from T. * If i is injective, and o is identity, this is a broadcast. * If i is identity, and o is surjective, this embeds T as a diagonal of a higher-dimensional output tensor. * If i is identity, and o is injective, this is a pure aggregation, summing out some dimensions of T. To make composition easy to compute, we could represent an index operation using a union-find structure for I. In other words, an index operation consists of a union-find structure I, and two arrays of ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772:3045,variab,variable,3045,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772,2,['variab'],['variable']
Modifiability,rows_table / cols_table should inherit globals,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2810:31,inherit,inherit,31,https://hail.is,https://github.com/hail-is/hail/issues/2810,1,['inherit'],['inherit']
Modifiability,"rring, the main container appears to fail. `/vep/vep` is returning exit code -9 but providing no further debug information. We need to assess who is generating the -9. Based on the contents of `docker/hailgenetics/vep/grch38/95/Dockerfile`, `/vep/vep` appears to be; ```; #!/bin/bash; export PERL5LIB=$PERL5LIB:/vep/ensembl-vep/Plugins/; exec perl /vep/ensembl-vep/vep \""\$@\""""; ```; It seems likely that `/vep/esnembl-vep/vep` is returning exit code -9. We need to determine under what conditions that happens. Main container output:; ```; Smartmatch is experimental at /vep/ensembl-vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 241. Traceback (most recent call last):; File ""/hail-vep/vep.py"", line 218, in <module>; main(action, consequence, tolerate_parse_error, block_size, input_file, output_file, part_id, vep_cmd); File ""/hail-vep/vep.py"", line 199, in main; results = run_vep(vep_cmd, input_file, block_size, consequence, tolerate_parse_error, part_id, os.environ); File ""/hail-vep/vep.py"", line 127, in run_vep; raise ValueError(f'VEP command {vep_cmd} failed with non-zero exit status {proc.returncode}\n'; ValueError: VEP command ['/vep/vep', '--input_file', '/io/input', '--format', 'vcf', '--json', '--everything', '--allele_number', '--no_stats', '--cache', '--offline', '--minimal', '--assembly', 'GRCh38', '--fasta', '/vep_data//homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz', '--plugin', 'LoF,loftee_path:/vep/ensembl-vep/Plugins/,gerp_bigwig:/vep_data//gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/vep_data//human_ancestor.fa.gz",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13989#issuecomment-1802287224:1063,Plugin,Plugins,1063,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1802287224,1,['Plugin'],['Plugins']
Modifiability,"rror from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=namespaces"", GroupVersionKind: ""/v1, Kind=Namespace""; Name: ""batch-pods"", Namespace: """"; Object: &{map[""apiVersion"":""v1"" ""kind"":""Namespace"" ""metadata"":map[""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""name"":""batch-pods"" ""namespace"":""""]]}; from server for: ""deployment.yaml"": namespaces ""batch-pods"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get namespaces in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=serviceaccounts"", GroupVersionKind: ""/v1, Kind=ServiceAccount""; Name: ""batch-svc"", Namespace: ""batch-pods""; Object: &{map[""kind"":""ServiceAccount"" ""metadata"":map[""name"":""batch-svc"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""apiVersion"":""v1""]}; from server for: ""deployment.yaml"": serviceaccounts ""batch-svc"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get serviceaccounts in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""rbac.authorization.k8s.io/v1, Resource=roles"", GroupVersionKind: ""rbac.authorization.k8s.io/v1, Kind=Role""; Name: ""batch-pods-admin"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""rbac.authorization.k8s.io/v1"" ""kind"":""Role"" ""metadata"":map[""name"":""batch-pods-admin"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""rules"":[map[""apiGroups"":[""""] ""resources"":[""pods""] ""verbs"":[""get"" ""list"" ""watch"" ""create"" ""update"" ""patch"" ""delete""]] map[""apiGroups"":[""""] ""resources"":[""pods/log""] ""verbs"":[""get""]]]]}; from server for: ""deployment.yaml"": roles.rbac.authorization.k8s.io ""batch-pods-admin"" is fo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:1013,config,configuration,1013,https://hail.is,https://github.com/hail-is/hail/issues/4609,1,['config'],['configuration']
Modifiability,"rry for the wall of text but I found these a little hard to articulate. ### Regarding number of prompts. I think this is my primary concern. There's a lot of great automation here, but it's a lot right off the bat. I think what this is aiming to do is make it quick and simple to start running batches and every time someone has to stop and ask someone a question as to how they should respond to some prompt that process gets longer and more complicated. I think it's worth considering what the first batch people should run might be and design for a minimal first experience. IMO, a temp bucket is an absolutely crucial piece of configuration before you can do anything interesting and configuring a temp bucket is something that `hailctl` can easily be very opinionated about. Container registry… I feel like there's harder questions there, and you can run a lot of cool batches before having to worry about provisioning your own. It's also not actually a part of the hailctl config (unless something has changed recently) so it feels a little unusual in this flow. I still think that it is helpful to set people up with an AR and keep them from footguns, but maybe that can go in a separate command that the initial init command points to once you're done? Something along the lines of ""if you get to the point where you need to upload custom container images, you can use hailctl to set up a registry""?. Another thing that gives me a little pause is the wording around google projects. I get that you need one to create a bucket, but I think we should just make sure to steer clear of the implication that you are ""selecting a GCP project to use for Hail Batch"", because that implies some link or ownership that isn't there. But I think there's a quick fix here: for a given resource that we *are* creating for hail use, like the temp bucket, ask for the name first and then ask which project it should be created in, using the projects listed in gcloud as choices with the option to write in you",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1648633012:1184,config,config,1184,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1648633012,2,['config'],['config']
Modifiability,"rt it properly due to <a href=""https://github-redirect.dependabot.com/urllib3/urllib3/issues/2282"">reasons listed in this issue</a>. If you are a user of this module please leave a comment.</li>; <li>Changed <code>HTTPConnection.request_chunked()</code> to not erroneously emit multiple <code>Transfer-Encoding</code> headers in the case that one is already specified.</li>; <li>Fixed typo in deprecation message to recommend <code>Retry.DEFAULT_ALLOWED_METHODS</code>.</li>; </ul>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a></strong></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h2>1.26.8 (2022-01-07)</h2>; <ul>; <li>Added extra message to <code>urllib3.exceptions.ProxyError</code> when urllib3 detects that; a proxy is configured to use HTTPS but the proxy itself appears to only use HTTP.</li>; <li>Added a mention of the size of the connection pool when discarding a connection due to the pool being full.</li>; <li>Added explicit support for Python 3.11.</li>; <li>Deprecated the <code>Retry.MAX_BACKOFF</code> class property in favor of <code>Retry.DEFAULT_MAX_BACKOFF</code>; to better match the rest of the default parameter names. <code>Retry.MAX_BACKOFF</code> is removed in v2.0.</li>; <li>Changed location of the vendored <code>ssl.match_hostname</code> function from <code>urllib3.packages.ssl_match_hostname</code>; to <code>urllib3.util.ssl_match_hostname</code> to ensure Python 3.10+ compatibility after being repackaged; by downstream distributors.</li>; <li>Fixed absolute imports, all imports are now relative.</li>; </ul>; <h1>1.26.7 (2021-09-22)</h1>; <ul>; <li>Fixed a bug with HTTPS hostname verification involving IP addresses and lack; of SNI. (Issue <a href=""https://github-redirect.dependabot.com/url",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11532:3549,config,configured,3549,https://hail.is,https://github.com/hail-is/hail/pull/11532,1,['config'],['configured']
Modifiability,rtual.TLocus); 	at is.hail.expr.ir.ExtractIntervalFilters$.minimumValueByType(ExtractIntervalFilters.scala:42); 	at is.hail.expr.ir.ExtractIntervalFilters$.openInterval(ExtractIntervalFilters.scala:94); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:205); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:201); 	at scala.Option.flatMap(Option.scala:171); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:201); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:151); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractPartitionFilters(ExtractIntervalFilters.scala:249); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.Tra,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:3639,Rewrite,RewriteBottomUp,3639,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"ructural since they rarely influence the bytecode generated. Here is an exception, and that's where the method splitter comes in. Method splitting exists in the Hail compiler because not only does the JVM have limits on how large methods can be, but also the JIT compiler handles small methods much more effectively than large methods (and so splitting a large method into two small ones can make an order of magnitude or more in performance difference). We have three forms of method splitting in the Hail Query compiler. The first is a heuristic and greedy IR-level method splitter that generates new methods every X IR nodes, simply based on node count. However, the size of code generated by each IR can vary widely (`I32` vs `LowerBoundOnOrderedCollection` for instance), and so we have two other kinds of splitting that operate on the LIR level. The first is region splitting, which is used to split large blocks of LIR. In order to insert a split, any variables on the stack are stored in local variables before the split and loaded from those locals after the split. The second is method splitting, which is used to split large single methods. A single-exit group of blocks can be split into a separate method, and we have some machinery for replacing control flow instructions (which I will not go into here, for they are not relevant now), as well as handling local variables that are used across a method split. These shared Local variables are replaced by fields on a ""spills"" class which is allocated any time a split method is called. Spilled local `store`s are rewritten as field `store`s, and `load`s are rewritten as field `load`s. # What was the problem here?. A region split was inserted *directly between* the `I2B` instruction and the call to `OutputBuffer.write`. This meant that the result of `I2B` was stored in a local variable and read in the subsequent block. **The incorrect TypeInfo of Boolean was used for that local variable**, but this seems not to pose a problem -- b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:2090,variab,variables,2090,https://hail.is,https://github.com/hail-is/hail/pull/11328,2,['variab'],['variables']
Modifiability,runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:944); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:223); 	at is.hail.rvd.RVD.writeRowsSplit(RVD.scala:915); 	at is.hail.expr.ir.MatrixValue.write(MatrixValue.scala:214); 	at is.hail.expr.ir.MatrixNativeWriter.apply(MatrixWriter.scala:39); 	at is.hail.expr.ir.WrappedMatrixWriter.apply(MatrixWriter.scala:24); 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:583); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:54); 	at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:48); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:11); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:11); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:43); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20); 	at is.hai,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8545:11864,rewrite,rewrite,11864,https://hail.is,https://github.com/hail-is/hail/issues/8545,1,['rewrite'],['rewrite']
Modifiability,runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:989); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:166); 	at is.hail.rvd.RVD.writeRowsSplit(RVD.scala:936); 	at is.hail.expr.ir.MatrixValue.write(MatrixValue.scala:214); 	at is.hail.expr.ir.MatrixNativeWriter.apply(MatrixWriter.scala:39); 	at is.hail.expr.ir.WrappedMatrixWriter.apply(MatrixWriter.scala:24); 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:586); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:54); 	at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:50); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:45); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20); 	at is.hai,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:84691,rewrite,rewrite,84691,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['rewrite'],['rewrite']
Modifiability,"rver: Fixes handling of CRD schemas containing literal null values in enums. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104969"">kubernetes/kubernetes#104969</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>)</li>; <li>Kube-apiserver: The <code>rbac.authorization.k8s.io/v1alpha1</code> API version is removed; use the <code>rbac.authorization.k8s.io/v1</code> API, available since v1.8. The <code>scheduling.k8s.io/v1alpha1</code> API version is removed; use the <code>scheduling.k8s.io/v1</code> API, available since v1.14. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104248"">kubernetes/kubernetes#104248</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>)</li>; <li>Kube-scheduler: support for configuration file version <code>v1beta1</code> is removed. Update configuration files to v1beta2(xref: <a href=""https://github-redirect.dependabot.com/kubernetes/enhancements/issues/2901"">kubernetes/enhancements#2901</a>) or v1beta3 before upgrading to 1.23. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104782"">kubernetes/kubernetes#104782</a>, <a href=""https://github.com/kerthcet""><code>@​kerthcet</code></a>)</li>; <li>KubeSchedulerConfiguration provides a new field <code>MultiPoint</code> which will register a plugin for all valid extension points (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105611"">kubernetes/kubernetes#105611</a>, <a href=""https://github.com/damemi""><code>@​damemi</code></a>) [SIG Scheduling and Testing]</li>; <li>Kubelet should reject pods whose OS doesn't match the node's OS label. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105292"">kubernetes/kubernetes#105292</a>, <a href=""https://github.com/ravisantoshgudimetla""><code>@​ravisantoshgudimetla</code></a>) [SIG Apps and Node]</li>; <li>Kubelet: turn the KubeletConfiguration v1beta1 <code>ResolverConfig</code> field from",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:8546,enhance,enhancements,8546,https://hail.is,https://github.com/hail-is/hail/pull/11957,2,['enhance'],['enhancements']
Modifiability,ry/client/v1/__pycache__/docker_image_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/client/v1/__pycache__/docker_http_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/client/__pycache__/docker_creds_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/client/__pycache__/docker_name_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_puller_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_pusher_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_appender_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_appender_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_puller_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_pusher_.cpython-39.pyc; /usr/local/share/google/dataproc/npd-config/docker-monitor-counter.json; /usr/local/share/google/dataproc/npd-config/docker-monitor.json; /usr/local/share/google/dataproc/npd-config/health-checker-docker.json; /usr/local/share/google/dataproc/npd-config/docker-monitor-filelog.json; /usr/local/share/google/dataproc/bdutil/fluentd/container_logging/plugin/test/Dockerfile; /usr/local/share/google/dataproc/bdutil/components/initialize/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/post-install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/activate/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/shared/docker.sh; /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/configure_docker.sh; /run/docker.sock; /tmp/dataproc/uninstall/docker-ce; /tmp/dataproc/compon,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:11643,config,config,11643,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['config'],['config']
Modifiability,"ry>; <ul>; <li><a href=""https://github.com/sass/libsass-python/commit/b18db090672676d7c58fcd52e6ae0eb505993886""><code>b18db09</code></a> 0.22.0</li>; <li><a href=""https://github.com/sass/libsass-python/commit/22adb66fac69d058e8dccc0014563cd76e78349e""><code>22adb66</code></a> correct version number</li>; <li><a href=""https://github.com/sass/libsass-python/commit/b2436e282ae19ccb7be9318f3ddd0eb6cdb48be3""><code>b2436e2</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/406"">#406</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/980b41f462ae07939515993781e72654b117bdce""><code>980b41f</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/cfffd417e56b7fd3aaf6034fa49083185714f6b7""><code>cfffd41</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/405"">#405</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/20b3cdade8a199e832521d0e44a2507bc75315e0""><code>20b3cda</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/940ef2e9f9dd4143d642a29156c94d0a3133a691""><code>940ef2e</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/404"">#404</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/5f8470b48cb576f188ad7424327babb056ce152a""><code>5f8470b</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/7c76abf3a14b1606cc6053f5d95d2e0fa231b97f""><code>7c76abf</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/403"">#403</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/40be5dc3e2c3260",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12482:1978,config,config,1978,https://hail.is,https://github.com/hail-is/hail/pull/12482,1,['config'],['config']
Modifiability,"s [msal-extensions](https://github.com/AzureAD/microsoft-authentication-extensions-for-python) from 0.3.1 to 1.0.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/AzureAD/microsoft-authentication-extensions-for-python/releases"">msal-extensions's releases</a>.</em></p>; <blockquote>; <h2>MSAL Extensions for Python, 1.0.0</h2>; <p>This package is now considered stable and production-ready.</p>; <ul>; <li>New: Add a new platform-independent <code>build_encrypted_persistence()</code> API. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/87"">#87</a>, <a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/110"">#110</a>)</li>; <li>Remove: Old TokenCache API which has been deprecated for 2 years. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/110"">#110</a>)</li>; <li>Enhancement: Make all platform-dependent parameters optional (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/103"">#103</a>)</li>; <li>Enhancement: Provide <code>PersistenceEncryptError</code> and <code>PersistenceDecryptError</code>, currently raised when encryption on Windows fails. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/108"">#108</a>)</li>; <li>Enhancement: The data file will be created with <code>600</code> permission when running in Unix-like systems. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/107"">#107</a>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/AzureAD/microsoft-authentication-extensions-for-python/commit/a88fa673af3602fe7c8c922314599b0c245e7add""><code>a88fa67</code></a> Merge branch 'release-1.0.0'</li>; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11992:1002,Enhance,Enhancement,1002,https://hail.is,https://github.com/hail-is/hail/pull/11992,1,['Enhance'],['Enhancement']
Modifiability,"s begin at Fri 2019-03-01 19:54:49 UTC, end at Fri 2019-03-01 20:11:51 UTC. --; Mar 01 19:59:03 dk-m systemd[1]: Started Jupyter Notebook.; Mar 01 19:59:04 dk-m python[5149]: [I 19:59:04.630 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.796 NotebookApp] All authentication is disabled. Anyone who can connect to this server will be able to run code.; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.802 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.803 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /usr/local/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /opt/conda/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /root/.jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.904 NotebookApp] Error loading server extension jupyter_spark; Mar 01 19:59:04 dk-m python[5149]: Traceback (most recent call last):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/notebook/notebookapp.py"", line 1575, in init_server_extensions; Mar 01 19:59:04 dk-m python[5149]: func(self); Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/__init__.py"", line 30, in load_jupyter_server_extension; Mar 01 19:59:04 dk-m python[5149]: from .handlers import SparkHandler; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/handlers.py"", line 8, in <",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5505:1385,config,configuration,1385,https://hail.is,https://github.com/hail-is/hail/issues/5505,1,['config'],['configuration']
Modifiability,"s per the logs (see below); ```; pyspark==3.3.3; # via -r hail/hail/python/requirements.txt`; ``` . * I do not see any `pyspark` in `/hail/hail/python/hailtop/hailctl/deploy.yaml`. * Checking in `/usr/lib/spark` I see reference of scala 2.12.15 same as in the hail logs; ```sh; $ cat /usr/lib/spark/RELEASE ; Spark 3.3.2-amzn-0.1 built for Hadoop 3.3.3-amzn-3.1; Build flags: -Divy.home=/home/release/.ivy2 -Dsbt.ivy.home=/home/release/.ivy2 -Duser.home=/home/release -Drepo.maven.org= -Dreactor.repo=file:///home/release/.m2/repository -Dhadoop.version=3.3.3-amzn-3.1 -Dyarn.version=3.3.3-amzn-3.1 -Dhive.version=2.3.9-amzn-3 -Dparquet.version=1.12.2-amzn-3 -Dprotobuf.version=2.5.0 -Dfasterxml.jackson.version=2.13.4 -Dfasterxml.jackson.databind.version=2.13.4 -Dcommons.httpclient.version=4.5.9 -Dcommons.httpcore.version=4.4.11 -Daws.java.sdk.version=1.12.446 -Daws.kinesis.client.version=1.12.0 -Daws.kinesis.producer.version=0.12.9 -Dscala.version=2.12.15 -DrecompileMode=all -Dmaven.deploy.plugin.version=2.8.2 -Dmaven.scaladoc.skip -Pyarn -Phadoop-3.2 -Phive -Phive-thriftserver -Psparkr -Pspark-ganglia-lgpl -Pnetlib-lgpl -Pscala-2.12 -Pkubernetes -Pvolcano -Pkinesis-asl -DskipTests; ```; I still did not found why scala is downgraded to 2.12.13. <details><summary>Hail logs</summary>; <p>; # Build Hail #; WARNING: Package(s) not found: hail; REVISION is set to ""13536b531342a263b24a7165bfeec7bd02723e4b"" which is different from old value """"; printf ""13536b531342a263b24a7165bfeec7bd02723e4b"" > env/REVISION; echo 13536b531342a263b24a7165bfeec7bd02723e4b > python/hail/hail_revision; SHORT_REVISION is set to ""13536b531342"" which is different from old value """"; printf ""13536b531342"" > env/SHORT_REVISION; HAIL_PIP_VERSION is set to ""0.2.124"" which is different from old value """"; printf ""0.2.124"" > env/HAIL_PIP_VERSION; echo 0.2.124-13536b531342 > python/hail/hail_version; echo 0.2.124 > python/hail/hail_pip_version; cp -f python/hail/hail_version python/hailtop/hail_version; printf 'h",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:1240,plugin,plugin,1240,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['plugin'],['plugin']
Modifiability,"s specified in this draft proposal:; <a href=""https://github.com/harfbuzz/boring-expansion-spec/blob/main/avar2.md"">https://github.com/harfbuzz/boring-expansion-spec/blob/main/avar2.md</a></li>; <li>[glifLib] Wrap underlying XML library exceptions with GlifLibError when parsing GLIFs, and also print the name and path of the glyph that fails to be parsed (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3029"">fonttools/fonttools#3029</a>).</li>; <li>[feaLib] Consult avar for normalizing user-space values in ConditionSets and in VariableScalars (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3042"">fonttools/fonttools#3042</a>, <a href=""https://redirect.github.com/fonttools/fonttools/pull/3043"">fonttools/fonttools#3043</a>).</li>; <li>[ttProgram] Handle string input to Program.fromAssembly() (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3038"">fonttools/fonttools#3038</a>).</li>; <li>[otlLib] Added a config option to emit GPOS 7 lookups, currently disabled by default because of a macOS bug (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3034"">fonttools/fonttools#3034</a>).</li>; <li>[COLRv1] Added method to automatically compute ClipBoxes (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3027"">fonttools/fonttools#3027</a>).</li>; <li>[ttFont] Fixed getGlyphID to raise KeyError on missing glyphs instead of returning None. The regression was introduced in v4.27.0 (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3032"">fonttools/fonttools#3032</a>).</li>; <li>[sbix] Fixed UnboundLocalError: cannot access local variable 'rawdata' (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3031"">fonttools/fonttools#3031</a>).</li>; <li>[varLib] When building VF, do not overwrite a pre-existing <code>STAT</code> table that was built with feaLib from FEA feature file. Also, added support for building multiple VFs defined in Designspace v5 from <code>fonttools varLib</code> script (<a href=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12910:2344,config,config,2344,https://hail.is,https://github.com/hail-is/hail/pull/12910,1,['config'],['config']
Modifiability,"s work to do, when accepting objects as props, use a `<PureComponent>`. This will tell React to check the reference for diff, rather than deep value compare. Obviously much faster to do the latter. You can do even better than `PureComponent`. Use a regular `Component`, and specific a `shouldComponentUpdate() { }` method in that component. Within that method, write whatever checks needed, so that when a prop, or state changes, you return `true`, otherwise `false`. When true, the component will re-render. However, this allows you to react in a more fine-grained way, i.e instead of checking reference, check for the update of a specific property, or don't react to that object changing at all. Behind the scenes, PureComponent is in effect implementing a shouldComponentUpdate that checks reference equality (prevProp !== currentProp). References: ; 1. https://reactjs.org/docs/react-api.html#reactpurecomponent. #### Memo stateless components; Stateless/functional components (i.e those than don't extend React.Component or React.PureComponent, i.e `(props) => <div>Hello {props.name}</div>`), can be memoized. As in a typical memoized function, given one set of input (props), the result is cached, and the cached result is returned for n + 1 calls. References; 1. https://reactjs.org/docs/react-api.html#reactmemo; 2. https://scotch.io/tutorials/react-166-reactmemo-for-functional-components-rendering-control. ### Typescript; 2. https://reactjs.org/docs/react-api.html#reactmemo. #### And React Component prop definitions; https://levelup.gitconnected.com/ultimate-react-component-patterns-with-typescript-2-8-82990c516935. ### NextJS; https://nextjs.org/docs/; Next has 4 deviations from normal react:; 1) _app.js: Can be omitted. Wraps all other components. Is useful for global functions, because it is not reloaded when you change pages. Good place to place a header component, a footer, global data stores, or handle page transitions.; it has this shape:; ```js; <Container>; <Header />;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:10437,extend,extend,10437,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['extend'],['extend']
Modifiability,"s://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:6784,config,configuration,6784,https://hail.is,https://github.com/hail-is/hail/pull/8561,2,['config'],['configuration']
Modifiability,"s</summary>; <ul>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/13f39349c6950a881c1fe4fcd5984af2e8b7c220""><code>13f3934</code></a> Remove unnecessary skip from test_logfinish_hook as we require pytest&gt;=6.2</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/c76d5622f17135e892965d742377870eb9b07933""><code>c76d562</code></a> Skip test_warning_captured_deprecated_in_pytest_6 in pytest&gt;=7.1</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/5f78c7155e66ab73bdc7631c4ac6bfe684b82500""><code>5f78c71</code></a> Fix CHANGELOG header</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/c8bbc03e49d5a53b5da808c7328e8f3ad6ed2d7e""><code>c8bbc03</code></a> Release 2.5.0</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/8dbf3677dc7cc26ada33cf8a27d7ac51a9be467b""><code>8dbf367</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-xdist/issues/738"">#738</a> from pytest-dev/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/a25c14bef59ad728e39cabc64f71190aaad73b0a""><code>a25c14b</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/110c114025202d11570737be823de158d1bb8d99""><code>110c114</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-xdist/issues/734"">#734</a> from nicoddemus/revamp-readme</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/83bdbf4b95c914a889d1faa8fba8d506bcc2f8c7""><code>83bdbf4</code></a> Revamp README</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/630c1eb6f2c31dcb4c38c75bb62f868237cdde94""><code>630c1eb</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-xdist/issues/733"">#733</a> from baekdohyeop/feature-loadgroup</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/62e50d00977b41e17",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11491:4526,config,config,4526,https://hail.is,https://github.com/hail-is/hail/pull/11491,2,['config'],['config']
Modifiability,"s`: this parameter now defaults to `True` and includes the; response body text in the error message. Both; Both parameters may be overridden on a per-request basis. - `httpx.ResponseManager` and `httpx.ClientSession` work together to enable; `retry_transient` and `raise_for_status`. Aiohttp has this unusual structure where; all the request methods are synchronous but they return an object that is both; awaitable and an async context manager. I mirror their structure exactly. The; `httpx.ResponseManager` is both awaitable and an async context manager. Its; `response_coroutine` field is a coroutine that includes the retry and; raise-for-status logic. - The `HailResolver` overrides domain name resolution to first consult the Hail; `address` service. `address` is effectively a domain name server. It watches; kubernetes services and publishes the pod IPs. It supports two name styles:; `service` and `service.namespace`. The former uses the deploy config to; determine in which namespace to find the given service. Currently, the; client-side library only looks up IPs for `shuffler` and `address`. - `BlockingClientSession` and `BlockingContextManager` wrap the; aforementioned `httpx` classes. `BlockingClientResponse` wraps an; `aiohttp.ClientResponse`. ---. Examples of correct usage:. A blocking HTTPS request:. ```python3; with httpx.blocking_client_session() as session:; with session.post(url, json=config, headers=headers) as resp:; assert resp.status == 200; print(resp.text()); ```. An asynchronous HTTPS request to auth:; ```python3; async with httpx.client_session() as session:; async with session.get(; deploy_config.url('auth', '/api/v1alpha/userinfo'),; headers=headers) as resp:; assert resp.status == 200; print(await resp.json()); ```. A blocking HTTPS session with a large default timeout:. ```python3; httpx.blocking_client_session(; headers=service_auth_headers(deploy_config, 'query'),; timeout=aiohttp.ClientTimeout(total=600)); ```. cc: @catoverdrive @Dania-Abuhijleh",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9554:2859,config,config,2859,https://hail.is,https://github.com/hail-is/hail/pull/9554,1,['config'],['config']
Modifiability,"sandbox/hail [master|𝚫8?2]; snafu$ ipython ; Python 3.8.6 (default, Jan 27 2021, 15:42:20) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.21.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-e24d842d2b9a> in <module>; ----> 1 import hail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.accumulators import Accumulator; 33 from pyspark.broadcast import Broadcast, BroadcastPickleRegistry. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py in <module>; 95 import socketserver as SocketServer; 96 import threading; ---> 97 from pyspark.serializers import read_int, PickleSerializer; 98 ; 99 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/serializers.py in <module>; 69 xrange = range; 70 ; ---> 71 from pyspark import cloudpickle; 72 from pyspark.util import _exception_message; 73 . ~/sandbox/hail/venv/3.8/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:10437,sandbox,sandbox,10437,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['sandbox'],['sandbox']
Modifiability,scala:20); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:58); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:63); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:67); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:62); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:50); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:462); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:498); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:75); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:75); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:63); 	at is.hail.backend.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124:9486,adapt,adapted,9486,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124,1,['adapt'],['adapted']
Modifiability,scala:20); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:45); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:600); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:636); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:631); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:630); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(E,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:12705,adapt,adapted,12705,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['adapt'],['adapted']
Modifiability,scala:20); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:67); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:62); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:50); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:463); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:499); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:75); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:75); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:63); 	at is.hail.backend.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936:8566,adapt,adapted,8566,https://hail.is,https://github.com/hail-is/hail/issues/12936,1,['adapt'],['adapted']
Modifiability,scala:20); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:13); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:416); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:452); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:70); 	at is.hail.utils.package$.using(package.scala:646); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); 	at is.hail.utils.package$.using(package.scala:646); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); 	at is.hail.ba,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290:5772,adapt,adapted,5772,https://hail.is,https://github.com/hail-is/hail/issues/12290,1,['adapt'],['adapted']
Modifiability,scala:27); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.expr.ir.lowering.EvalRelationalLets$.execute$1(EvalRelationalLets.scala:13); 	at is.hail.expr.ir.lowering.EvalRelationalLets$.lower$1(EvalRelationalLets.scala:21); 	at is.hail.expr.ir.lowering.EvalRelationalLets$.apply(EvalRelationalLets.scala:35); 	at is.hail.expr.ir.lowering.EvalRelationalLetsPass.transform(LoweringPass.scala:168); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14529:9080,adapt,adapted,9080,https://hail.is,https://github.com/hail-is/hail/issues/14529,1,['adapt'],['adapted']
Modifiability,scala:27); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:13); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:450); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:486); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:70); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); 	at is.hail.ba,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:9391,adapt,adapted,9391,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['adapt'],['adapted']
Modifiability,scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:58); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:63); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:67); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:62); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:50); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:462); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:498); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:75); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:75); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:63); 	at is.hail.backend.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13486:4606,adapt,adapted,4606,https://hail.is,https://github.com/hail-is/hail/issues/13486,1,['adapt'],['adapted']
Modifiability,scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.expr.ir.lowering.EvalRelationalLets$.execute$1(EvalRelationalLets.scala:13); 	at is.hail.expr.ir.lowering.EvalRelationalLets$.lower$1(EvalRelationalLets.scala:21); 	at is.hail.expr.ir.lowering.EvalRelationalLets$.apply(EvalRelationalLets.scala:35); 	at is.hail.expr.ir.lowering.EvalRelationalLetsPass.transform(LoweringPass.scala:170); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14362:15842,adapt,adapted,15842,https://hail.is,https://github.com/hail-is/hail/issues/14362,1,['adapt'],['adapted']
Modifiability,scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:13); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:381); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:417); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:638); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:638); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:46); 	at is.hail.ba,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533:22268,adapt,adapted,22268,https://hail.is,https://github.com/hail-is/hail/issues/12533,1,['adapt'],['adapted']
Modifiability,scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:13); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:450); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:486); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:70); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); 	at is.hail.ba,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12531:9083,adapt,adapted,9083,https://hail.is,https://github.com/hail-is/hail/issues/12531,3,['adapt'],['adapted']
Modifiability,scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:13); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:450); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:486); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:70); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); 	at is.hail.ba,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:8876,adapt,adapted,8876,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['adapt'],['adapted']
Modifiability,scala:45); at is.hail.types.virtual.Type.ordering(Type.scala:204); at is.hail.rvd.PartitionBoundOrdering$.$anonfun$apply$1(PartitionBoundOrdering.scala:16); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286); at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); at scala.collection.TraversableLike.map(TraversableLike.scala:286); at scala.collection.TraversableLike.map$(TraversableLike.scala:279); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198); at is.hail.rvd.PartitionBoundOrdering$.apply(PartitionBoundOrdering.scala:16); at is.hail.rvd.RVDPartitioner.<init>(RVDPartitioner.scala:62); at is.hail.rvd.RVDPartitioner.<init>(RVDPartitioner.scala:33); at is.hail.rvd.RVDPartitioner.copy(RVDPartitioner.scala:225); at is.hail.expr.ir.lowering.TableStage.extendKeyPreservesPartitioning(LowerTableIR.scala:534); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2035); at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1051); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1655); at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:728); at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:1021); at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:27); at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:11); at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:91); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:27); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14249:3373,extend,extendKeyPreservesPartitioning,3373,https://hail.is,https://github.com/hail-is/hail/issues/14249,1,['extend'],['extendKeyPreservesPartitioning']
Modifiability,"se <code>#!/bin/sh</code> on windows for hook script.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2182"">#2182</a> issue by <a href=""https://github.com/hushigome-visco""><code>@​hushigome-visco</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2187"">#2187</a> PR by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; </ul>; <h1>2.16.0 - 2021-11-30</h1>; <h3>Features</h3>; <ul>; <li>add warning for regexes containing <code>[\/]</code> or <code>[/\\]</code>.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2053"">#2053</a> PR by <a href=""https://github.com/radek-sprta""><code>@​radek-sprta</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2043"">#2043</a> issue by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; <li>move hook template back to <code>bash</code> resolving shebang-portability issues.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2065"">#2065</a> PR by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; <li>add support for <code>fail_fast</code> at the individual hook level.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2097"">#2097</a> PR by <a href=""https://github.com/colens3""><code>@​colens3</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/1143"">#1143</a> issue by <a href=""https://github.com/potiuk""><code>@​potiuk</code></a>.</li>; </ul>; </li>; <li>allow passthrough of <code>GIT_CONFIG_KEY_*</code>, <code>GIT_CONFIG_VALUE_*</code>, and; <code>GIT_CONFIG_COUNT</code>.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2136"">#2136</a> PR by <a href=""https://github.com/emzeat""><code>@​emzeat</code></a>.</li>; </",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11460:11035,portab,portability,11035,https://hail.is,https://github.com/hail-is/hail/pull/11460,2,['portab'],['portability']
Modifiability,"se to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:7089,config,config-http,7089,https://hail.is,https://github.com/hail-is/hail/pull/8561,1,['config'],['config-http']
Modifiability,"se); r2_adj = r2_adj.sparsify_triangle(); r2_adj = r2_adj.checkpoint(f'{tmp}/adj', overwrite=args.overwrite). if __name__ == '__main__':; main(); ```. ### Version. 0.2.128. ### Relevant log output. ```shell; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.128-17247d8990c6; LOGGING: writing to /home/edmund/.local/src/hail/hail-20240508-1553-0.2.128-17247d8990c6.log; Traceback (most recent call last):; File ""/home/edmund/.local/share/pyenv/versions/3.9.18/lib/python3.9/runpy.py"", line 197, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/home/edmund/.local/share/pyenv/versions/3.9.18/lib/python3.9/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py"", line 39, in <module>; cli.main(); File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 430, in main; run(); File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 284, in run_file; runpy.run_path(target, run_name=""__main__""); File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 321, in run_path; return _run_module_code(code, init_globals, run_name,; File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 135, in _run_module_code; _run_code(code, mod_globals, init_globals,; File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 124, in _run_code; exec(code, run_globals); File ""/home/ed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14537:2123,adapt,adapter,2123,https://hail.is,https://github.com/hail-is/hail/issues/14537,1,['adapt'],['adapter']
Modifiability,"sed: 1; 2018-06-26 01:47:57 Hail: INFO: Number of samples in BGEN files: 487409; 2018-06-26 01:47:57 Hail: INFO: Number of variants across all BGEN files: 1255683; 2018-06-26 01:49:08 Hail: INFO: Coerced almost-sorted dataset; 2018-06-26 01:49:08 Hail: INFO: No multiallelics detected.; 2018-06-26 01:49:09 Hail: INFO: interval filter loaded 27 of 586 partitions; ```. I'll kill what I have and run a single regression, since this will take a long time.; ```; 2018-07-18 15:39:30 Hail: INFO: Number of BGEN files parsed: 1; 2018-07-18 15:39:30 Hail: INFO: Number of samples in BGEN files: 487409; 2018-07-18 15:39:30 Hail: INFO: Number of variants across all BGEN files: 1255683; 2018-07-18 15:40:37 Hail: INFO: Coerced almost-sorted dataset; 2018-07-18 15:40:39 Hail: INFO: interval filter loaded 5 of 293 partitions; 2018-07-18 15:43:13 Hail: WARN: 126215 of 487409 samples have a missing phenotype or covariate.; 2018-07-18 15:43:13 Hail: INFO: linear_regression: running on 361194 samples for 110 response variables y,; with input variable x, intercept, and 25 additional covariates...; 2018-07-18 15:44:06 Hail: WARN: 132571 of 487409 samples have a missing phenotype or covariate.; 2018-07-18 15:44:06 Hail: INFO: linear_regression: running on 354838 samples for 1 response variable y,; with input variable x, intercept, and 25 additional covariates...; 2018-07-18 15:44:59 Hail: WARN: 132781 of 487409 samples have a missing phenotype or covariate.; 2018-07-18 15:44:59 Hail: INFO: linear_regression: running on 354628 samples for 1 response variable y,; with input variable x, intercept, and 25 additional covariates...; 2018-07-18 15:45:42 Hail: WARN: 133165 of 487409 samples have a missing phenotype or covariate.; 2018-07-18 15:45:42 Hail: INFO: linear_regression: running on 354244 samples for 1 response variable y,; with input variable x, intercept, and 25 additional covariates...; 2018-07-18 15:46:57 Hail: WARN: 132601 of 487409 samples have a missing phenotype or covariate.; 2018-0",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3945#issuecomment-405979965:1106,variab,variables,1106,https://hail.is,https://github.com/hail-is/hail/pull/3945#issuecomment-405979965,2,['variab'],"['variable', 'variables']"
Modifiability,see gitter. Possible different Python semantics on windows throw a name error at `__all__.extend(genetics.__all__)`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3286#issuecomment-378015179:90,extend,extend,90,https://hail.is,https://github.com/hail-is/hail/pull/3286#issuecomment-378015179,1,['extend'],['extend']
Modifiability,"self.assertEqual is unittest style. We use pytest now, which rewrites Python assert statements to provide good failure messages. New code should use `assert`, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7611#issuecomment-559186521:61,rewrite,rewrites,61,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-559186521,1,['rewrite'],['rewrites']
Modifiability,"seqr hail search code is here: https://github.com/broadinstitute/seqr/tree/master/hail_search. Running the aiohttp service is just `python -m hail_search`. Runs on hail 0.2.126. The data you need is `gs://seqr-datasets/v03/GRCh38/SNV_INDEL/runs/manual__2023-11-07T23-31-23.149902+00-00`; For running the service, you need a `DATASETS_DIR` env variable defined, and that data should be available at `$DATASETS_DIR/GRCh38/SNV_INDEL`. Post body for a relatively quick search:; ```; {; ""genome_version"": ""GRCh38"",; ""num_results"": 100,; ""annotations"": {; ""in_frame"": [; ""inframe_insertion"",; ""inframe_deletion""; ],; ""missense"": [; ""stop_lost"",; ""initiator_codon_variant"",; ""start_lost"",; ""protein_altering_variant"",; ""missense_variant""; ],; ""nonsense"": [; ""stop_gained""; ],; ""splice_ai"": ""0.2"",; ""frameshift"": [; ""frameshift_variant""; ],; ""structural"": [],; ""extended_splice_site"": [],; ""essential_splice_site"": [; ""splice_donor_variant"",; ""splice_acceptor_variant""; ],; ""structural_consequence"": [; ""LOF"",; ""DUP_LOF"",; ""INV_SPAN"",; ""COPY_GAIN""; ]; },; ""datasetType"": ""VARIANTS"",; ""pathogenicity"": {; ""hgmd"": [; ""disease_causing""; ],; ""clinvar"": [; ""pathogenic"",; ""likely_pathogenic""; ]; },; ""dataset_type"": ""ALL"",; ""secondary_dataset_type"": null,; ""inheritance_mode"": ""de_novo"",; ""inheritance_filter"": {; ""A"": ""has_alt"",; ""N"": ""ref_ref""; },; ""sample_data"": {; ""SNV_INDEL"": [; {; ""sample_id"": ""RGP_2436_2_D1"",; ""individual_guid"": ""I0097169_rgp_2436_2"",; ""family_guid"": ""F041731_rgp_2436"",; ""project_guid"": ""R0594_rare_genomes_project_gen"",; ""affected"": ""N""; },; {; ""sample_id"": ""RGP_2436_3_D1"",; ""individual_guid"": ""I0097170_rgp_2436_3"",; ""family_guid"": ""F041731_rgp_2436"",; ""project_guid"": ""R0594_rare_genomes_project_gen"",; ""affected"": ""A""; },; {; ""sample_id"": ""RGP_2436_1_D1"",; ""individual_guid"": ""I0097168_rgp_2436_1"",; ""family_guid"": ""F041731_rgp_2436"",; ""project_guid"": ""R0594_rare_genomes_project_gen"",; ""affected"": ""N""; }; ]; },; ""sort"": ""xpos"",; ""sort_metadata"": null,; ""frequencies"": {; ""g1k"": {;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1810939501:343,variab,variable,343,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1810939501,1,['variab'],['variable']
Modifiability,"service] make user cache thread-safe; - [ ] (@tpoterba) c315fcb0b1 [query-service] bugfix: preserve globals through a shuffle; - [ ] (@catoverdrive) 912c21f709 [shuffler] log ShuffleCodecSpec anytime it is created; - [x] (@daniel-goldstein) c2495837e7 [scala-lsm] bugfix: least key may equal greatest key; - [x] (@daniel-goldstein) 5fb3db703e [services] discovered new transient error; - [x] (@daniel-goldstein) 9cd0999938 [shuffler] more assertions in ShuffleClient; - [x] (@daniel-goldstein) a71a3c9b8c [shuffler] bugfix: shuffler needs a HailContext to decode loci; - [x] (@daniel-goldstein) 41b06aeaa8 [query-service] move hail.jar earlier in Dockerfile; - [x] (@daniel-goldstein) 8df4029698 [query-service] permit pod scaling and remove cpu limit; - [ ] (@catoverdrive) 0354e1f557 [query-service] simplify socket handling; - [x] (@jigold) 6690a4decc [batch] teach JVMJob where to find the hail configuration files; - [x] (@daniel-goldstein) ae2e3d2996 [query-service] switch to services team approved logging; - [ ] (@tpoterba) b18f86e647 [query-service] query workers need a hail context; - [ ] (@daniel-goldstein, @catoverdrive) 6d5d0b68af [query-service] use a UNIX Domain Socket for Py-Scala communication; - [ ] (@daniel-goldstein, @catoverdrive) 0d42df8b08 [query-service] run tests against query service; - [x] (@jigold) f9d361e686 [query-service] aiohttp.ClientSession must be created in async code; - [ ] (@cseed) c35f2e10e3 [query-service][hail][build.yaml] address miscellaneous comments from cotton; - [x] (@daniel-goldstein) 7f1b1363e9 [query-service] use two containers sharing an empty volume; - [x] (@daniel-goldstein) 2a8f23404a [query-service] in the service, log everything to stdout; - [x] (trivial) d8104a1dc4 [query-service] do not catch CancelledError; - [x] (trivial) efcb345185 [query-service] slightly more useful error message when socket dies; - [ ] (@tpoterba) f79c4023cf [shuffler] if we have an ExecuteContext, use it; - [x] (@daniel-goldstein,fyi: @tpoterba) 259f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10100:1378,config,configuration,1378,https://hail.is,https://github.com/hail-is/hail/pull/10100,1,['config'],['configuration']
Modifiability,setFunctions$.exportVCF$extension(VariantDataset.scala:425); E at is.hail.variant.VariantDatasetFunctions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$m,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:11517,Config,Configuration,11517,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['Config'],['Configuration']
Modifiability,"sf/black/issues/2945"">#2945</a>)</li>; <li>Avoid magic-trailing-comma in single-element subscripts (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2942"">#2942</a>)</li>; </ul>; <h3>Configuration</h3>; <ul>; <li>Do not format <code>__pypackages__</code> directories by default (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2836"">#2836</a>)</li>; <li>Add support for specifying stable version with <code>--required-version</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2832"">#2832</a>).</li>; <li>Avoid crashing when the user has no homedir (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2814"">#2814</a>)</li>; <li>Avoid crashing when md5 is not available (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2905"">#2905</a>)</li>; <li>Fix handling of directory junctions on Windows (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2904"">#2904</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Update pylint config documentation (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2931"">#2931</a>)</li>; </ul>; <h3>Integrations</h3>; <ul>; <li>Move test to disable plugin in Vim/Neovim, which speeds up loading (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2896"">#2896</a>)</li>; </ul>; <h3>Output</h3>; <ul>; <li>In verbose, mode, log when <em>Black</em> is using user-level config (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2861"">#2861</a>)</li>; </ul>; <h3>Packaging</h3>; <ul>; <li>Fix Black to work with Click 8.1.0 (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2966"">#2966</a>)</li>; <li>On Python 3.11 and newer, use the standard library's <code>tomllib</code> instead of <code>tomli</code>; (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2903"">#2903</a>)</li>; <li><code>black-primer</code>, the deprecated internal devtool, has been removed and copied to a; <a href=""https://github",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11696:1761,config,config,1761,https://hail.is,https://github.com/hail-is/hail/pull/11696,2,['config'],['config']
Modifiability,"shoudn't be terribly surprising that `rand_unif` has weird behavior, but here's a case that is definitely The Wrong Thing:. ```; Python 3.6.0 |Continuum Analytics, Inc.| (default, Dec 23 2016, 13:19:00); Type 'copyright', 'credits' or 'license' for more information; IPython 7.6.1 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl. In [2]: r = hl.rand_unif(0, 1). In [3]: hl.eval(r); Out[3]: 0.5387579341676381. In [4]: hl.eval(hl.tuple([r, r])); Out[4]: (0.5387579341676381, 0.5387579341676381); ```. okay, this makes sense becuase they have the same seed:; ```; In [5]: print(hl.tuple([r, r])._ir); (MakeTuple (0 1) (ApplySeeded rand_unif 806694938962853089 Float64 (Apply toFloat64 Float64 (I32 0)) (Apply toFloat64 Float64 (I32 1))) (ApplySeeded rand_unif 806694938962853089 Float64 (Apply toFloat64 Float64 (I32 0)) (Apply toFloat64 Float64 (I32 1)))); ```. how about this:; ```; In [6]: hl.eval(hl.range(2).map(lambda x: r)); Out[6]: [0.5387579341676381, 0.9394799645512691]; ```. odd. but maybe rand_unif inside an iteration has some semantics for advancing the RNG (like an aggregation). ```; In [7]: p = 1 - r. In [8]: hl.eval(hl.range(2).map(lambda x: p)); Out[8]: [0.46124206583236194, 0.06052003544873086]; ```; ok... ```; In [9]: hl.eval((p, hl.range(2).map(lambda x: p))); Out[9]: (0.46124206583236194, [0.46124206583236194, 0.46124206583236194]); ```; wtf? . if you look in the logs, its explained by the fact that only the final IR triggers CSE:; ```; (Let __cse_1; (ApplyBinaryPrimOp Subtract; (ApplyIR toFloat64 Float64; (I32 1)); (ApplySeeded rand_unif 806694938962853089 Float64; (ApplyIR toFloat64 Float64; (I32 0)); (ApplyIR toFloat64 Float64; (I32 1)))); (MakeTuple (0 1); (Ref __cse_1); (ArrayMap __uid_5; (ArrayRange; (I32 0); (I32 2); (I32 1)); (Ref __cse_1)))); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7572:287,enhance,enhanced,287,https://hail.is,https://github.com/hail-is/hail/issues/7572,1,['enhance'],['enhanced']
Modifiability,"singleton subscriptions; e.g., <code>nptyping.Float[64]</code></li>; <li>Resolve forward references</li>; <li>Expand and better handle <code>TypeVar</code></li>; <li>Add intershpinx reference link for <code>...</code> to <code>Ellipsis</code> (as is just an alias)</li>; </ul>; <h2>1.15.3</h2>; <ul>; <li>Prevents reaching inner blocks that contains <code>if TYPE_CHECKING</code></li>; </ul>; <h2>1.15.2</h2>; <ul>; <li>Log a warning instead of crashing when a type guard import fails to resolve</li>; <li>When resolving type guard imports if the target module does not have source code (such is the case for C-extension modules) do nothing instead of crashing</li>; </ul>; <h2>1.15.1</h2>; <ul>; <li>Fix <code>fully_qualified</code> should be <code>typehints_fully_qualified</code></li>; </ul>; <h2>1.15.0</h2>; <ul>; <li>Resolve type guard imports before evaluating annotations for objects</li>; <li>Remove <code>set_type_checking_flag</code> flag as this is now done by default</li>; <li>Fix crash when the <code>inspect</code> module returns an invalid python syntax source</li>; <li>Made formatting function configurable using the option <code>typehints_formatter</code></li>; </ul>; <h2>1.14.1</h2>; <ul>; <li>Fixed <code>normalize_source_lines()</code> messing with the indentation of methods with decorators that have parameters starting; with <code>def</code>.</li>; <li>Handle <code>ValueError</code> or <code>TypeError</code> being raised when signature of an object cannot be determined</li>; <li>Fix <code>KeyError</code> being thrown when argument is not documented (e.g. <code>cls</code> argument for class methods, and <code>self</code> for; methods)</li>; </ul>; <h2>1.14.0</h2>; <ul>; <li>Added <code>typehints_defaults</code> config option allowing to automatically annotate parameter defaults.</li>; </ul>; <h2>1.13.1</h2>; <ul>; <li>Fixed <code>NewType</code> inserts a reference as first argument instead of a string</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>...",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11503:1835,config,configurable,1835,https://hail.is,https://github.com/hail-is/hail/pull/11503,2,['config'],['configurable']
Modifiability,"sion at 0x104ab3850>. In [3]: aiohttp.ClientSession(); <ipython-input-1-028690903e5f>:7: DeprecationWarning: The object should be created within an async function; oldinit(self, *args, **kwargs); Out[3]: <aiohttp.client.ClientSession at 0x104dac8b0>. In [4]: aiohttp.ClientSession(); <ipython-input-1-028690903e5f>:7: DeprecationWarning: The object should be created within an async function; oldinit(self, *args, **kwargs); Out[4]: <aiohttp.client.ClientSession at 0x104daeec0>. In [5]:. Do you really want to exit ([y]/n)? y; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x104ab3850>; source_traceback: Object created at (most recent call last):; File ""/Users/dking/miniconda3/bin/ipython"", line 8, in <module>; sys.exit(start_ipython()); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/__init__.py"", line 128, in start_ipython; return launch_new_instance(argv=argv, **kwargs); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/traitlets/config/application.py"", line 1043, in launch_instance; app.start(); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/terminal/ipapp.py"", line 318, in start; self.shell.mainloop(); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/terminal/interactiveshell.py"", line 888, in mainloop; self.interact(); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/terminal/interactiveshell.py"", line 881, in interact; self.run_cell(code, store_history=True); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3009, in run_cell; result = self._run_cell(; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3064, in _run_cell; result = runner(coro); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner; coro.send(None); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/cor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13421:2092,config,config,2092,https://hail.is,https://github.com/hail-is/hail/pull/13421,1,['config'],['config']
Modifiability,"so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tells `clang` to not error if an included file does not exist. This can be helpful if the project's `Makefile` knows how to generate these header files. In our example, `catch.hpp` can be generated by `hail/src/main/c/Makefile` by downloading it from GitHub.; ```; # rm -rf bar.h; # clang -MM foo.cpp ; foo.cpp:1:10: fatal error: 'bar.h' file not found; #include ""bar.h""; ^~~~~~~; 1 error generated.; # clang -MM foo.cpp -MG; foo.o: foo.cpp bar.h; ```. The `-MF file` argument tells `clang` to write to `file` instead of stdout. The `-MMD` argument is used if we *also want to compile the file*. The `-MM` argument defaults to adding `-E` meaning ""only run the preprocessor"".",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5331:1800,variab,variable,1800,https://hail.is,https://github.com/hail-is/hail/pull/5331,1,['variab'],['variable']
Modifiability,"specific Hail Query functionality, let's mirror that structure. Let's move REGENIE and any non-Python dependencies of it into `hailtop/batch/genetics/regenie`. Sure. > How is the Dockerfile meant to be used? As written it doesn't appear that it would work because there doesn't exist any regenie source code to COPY in. It's meant to create a Regenie docker image that we could use. It's a copy of the regenie c++ repo's dockerfile, with the removal of the ENTRYPOINT /usr/local/bin/regenie, so that I could issue a command that included an executable, which is convenient to give me the ability to check that intermediate files are actually created (wc, ls) by batch, and because that seems more idiomatic for batch. I don't think there is a published regenie image, but docker hub is down so can't double check. . You're right, I shouldn't have deleted the bulk of the repo, kept as is. Didn't want to deal with submodules. > I've made some other in-line comments in the python file. It's not clear to me how all those other files are related to the python files and I'm a bit uncomfortable adding a whole directory with a LICENSE file, especially when not all the files in the directory fall under that license (e.g. the regenie py file) and moreover the license makes claims about things linking to BGEN, which none of our code here does. The license is only contained within the folder with the licensed files. In a previous conversation with Nate/Cotton, if we use any open-source software, best to keep those files segregated, alongside their license (license must be kept alongside the code, easier to see the demarcation if in a separate folder). > We have some BGEN files for testing in the Hail src/test/resources. I already have an example provided. The example folder contains the config for that, and the regenie folder contains the example. We need an example that has a known result, and regenie's c++ repo conveniently provides that. This is what the regenie/regenie folder contains.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-668357987:2122,config,config,2122,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-668357987,2,['config'],['config']
Modifiability,"sphinx/issues/9894"">#9894</a>: linkcheck: add option <code>linkcheck_exclude_documents</code> to disable link; checking in matched documents.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9793"">#9793</a>: sphinx-build: Allow to use the parallel build feature in macOS on macOS; and Python3.8+</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10055"">#10055</a>: sphinx-build: Create directories when <code>-w</code> option given</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9993"">#9993</a>: std domain: Allow to refer an inline target (ex. ``_<code>target name```) via :rst:role:</code>ref` role</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9981"">#9981</a>: std domain: Strip value part of the option directive from general index</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9391"">#9391</a>: texinfo: improve variable in <code>samp</code> role</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9578"">#9578</a>: texinfo: Add :confval:<code>texinfo_cross_references</code> to disable cross; references for readability with standalone readers</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9822"">#9822</a> (and <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9062"">#9062</a>), add new Intersphinx role :rst:role:<code>external</code> for explict; lookup in the external projects, without resolving to the local project.</li>; </ul>; <h2>Bugs fixed</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9866"">#9866</a>: autodoc: doccomment for the imported class was ignored</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/88f9647a223c77a29153683b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11522:4380,variab,variable,4380,https://hail.is,https://github.com/hail-is/hail/pull/11522,2,['variab'],['variable']
Modifiability,"splitting in the Hail Query compiler. The first is a heuristic and greedy IR-level method splitter that generates new methods every X IR nodes, simply based on node count. However, the size of code generated by each IR can vary widely (`I32` vs `LowerBoundOnOrderedCollection` for instance), and so we have two other kinds of splitting that operate on the LIR level. The first is region splitting, which is used to split large blocks of LIR. In order to insert a split, any variables on the stack are stored in local variables before the split and loaded from those locals after the split. The second is method splitting, which is used to split large single methods. A single-exit group of blocks can be split into a separate method, and we have some machinery for replacing control flow instructions (which I will not go into here, for they are not relevant now), as well as handling local variables that are used across a method split. These shared Local variables are replaced by fields on a ""spills"" class which is allocated any time a split method is called. Spilled local `store`s are rewritten as field `store`s, and `load`s are rewritten as field `load`s. # What was the problem here?. A region split was inserted *directly between* the `I2B` instruction and the call to `OutputBuffer.write`. This meant that the result of `I2B` was stored in a local variable and read in the subsequent block. **The incorrect TypeInfo of Boolean was used for that local variable**, but this seems not to pose a problem -- both Boolean and Byte use a single slot, and so the code still works even with the wrong variable type. However, the method splitter then **generated a method split at the same point where the region was split**. This means that the local variable resulting from I2B is spilled to a class field on the spills class. Our incorrectly-Boolean local becomes an incorrectly-Boolean **field**, and this is where things go wrong -- it seems as though Boolean class fields (appropriately) trunc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:2573,variab,variables,2573,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['variab'],['variables']
Modifiability,sql migrations using the wrong config file,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7706:31,config,config,31,https://hail.is,https://github.com/hail-is/hail/pull/7706,1,['config'],['config']
Modifiability,"ssembly' not specified. Setting to GRCh37; [Stage 22:======================================================>(99 + 1) / 100]2018-03-08 02:54:37 Hail: INFO: vep: annotated 243477 variants; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-6-a229f1f9de81> in <module>(); ----> 1 clinvar_vep = hl.vep(clinvar_mt, vep_config). <decorator-gen-843> in vep(dataset, config, block_size, name, csq). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/typecheck/check.py in _typecheck(__orig_func__, *args, **kwargs); 491 def _typecheck(__orig_func__, *args, **kwargs):; 492 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=False); --> 493 return __orig_func__(*args_, **kwargs_); 494; 495 return decorator(_typecheck). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/methods/qc.py in vep(dataset, config, block_size, name, csq); 545; 546 require_row_key_variant(dataset, 'vep'); --> 547 return MatrixTable(Env.hail().methods.VEP.apply(dataset._jvds, config, 'va.`{}`'.format(name), csq, block_size)); 548; 549. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.methods.VEP$.annotate(VEP.scala:4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3099:2035,config,config,2035,https://hail.is,https://github.com/hail-is/hail/issues/3099,1,['config'],['config']
Modifiability,"sses that backslash and newline to the shell. In Make 3.81, the `\` was also required but the newline and backslash *are not passed* to the shell. In other words: in 3.81, backslash-newline is always replaced with a space and in 4.0, backslash-newline is replaced with a space *except on recipe lines in which case it is necessary to indicate the recipe continues but it is also passed literally to the shell*. The docs page you linked directly addresses our use case and suggests we put the command inside of a make variable (thus triggering normal backslash-newline rules rather than the special recipe line rules). > Sometimes you want to split a long line inside of single quotes, but you don’t want the backslash/newline to appear in the quoted content. This is often the case when passing scripts to languages such as Perl, where extraneous backslashes inside the script can change its meaning or even be a syntax error. One simple way of handling this is to place the quoted string, or even the entire command, into a make variable then use the variable in the recipe. In this situation the newline quoting rules for makefiles will be used, and the backslash/newline will be removed. If we rewrite our example above using this method:; > ; > ```; > HELLO = 'hello \; > world'; > ; > all : ; @echo $(HELLO); > ```; > ; > we will get output like this:; > ; > ```; > hello world; > ```; >; > If you like, you can also use target-specific variables (see [Target-specific Variable Values](https://www.gnu.org/software/make/manual/html_node/Target_002dspecific.html)) to obtain a tighter correspondence between the variable and the recipe that uses it. It seems to me like there are not any great choices. Putting the JSON into a Make variable seems too magical and likely to confuse a newbie editing this file. Using escaped double quotes is less legible than literal JSON. Putting the whole JSON array on one line is quite long. I guess we can go with double quotes for now. I tested on Make 3.81 a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324:1179,variab,variable,1179,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324,4,['variab'],['variable']
Modifiability,"st_write_data.py is not deterministic; - if I add/change one dataset, my commit explodes with changes to all the datasets (see above); - doctest_write_data.py has to be run by *me*, it's not run by CI. I also noticed that when you specify no `row_key` to `import_matrix_table` you get a row key called `row_id`, which is annoying. Anyway now when someone asks how to count the mutations in each gene by consequence type we can point them to the `counter` docs. ---. Adding a dataset caused a bunch of docs failure that lead me to change how we do doctesting. The changes are summarized below.; - ignore `python/.eggs`; - make `PARALLELISM` configurable in `Makefile`; - fix `make pytest` (it referenced a non-extant target); - add `make doctest` (this and `pytest` use setup.py to replicate the environment the user would have after installation, I prefer this approach because I need not manually install any dependencies, setup.py handles that, it also configures spark correctly without environment variables); - harmonize `doctest` and `pytest` parameters in `build.gradle` and `Makefile`; - clean up import order in `conftest.py` to match pylint's desired ordering; - use a `temple.TemporaryDirectory` for all doctest and test output, which is automatically cleaned up (if you want to interrogate it you can `ctrl-z` a running doctest); this allows us to not copy the entire python directory into a build directory before running pytest; - *important:* re-generate all input datasets on every run of the tests. Previously, there was a file `doctest_write_data.py` which you were supposed to run when you changed the datasets, but if Hail changes then the random datasets generated by `doctest_write_data.py` might change. This means when I came along to add a new dataset, I had to address all the test failures introduced since the last time `doctest_write_data.py`'s results were checked in. (the doctests still only take about 2 minutes); - I fixed several latent doc bugs caused by the afore",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6856:1171,config,configures,1171,https://hail.is,https://github.com/hail-is/hail/pull/6856,2,"['config', 'variab']","['configures', 'variables']"
Modifiability,stics: [2023-07-24 13:52:49.515]Container killed on request. Exit code is 137; [2023-07-24 13:52:49.517]Container exited with a non-zero exit code 137. ; [2023-07-24 13:52:49.518]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2204); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2225); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2244); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2269); 	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287:7152,adapt,adapted,7152,https://hail.is,https://github.com/hail-is/hail/issues/13287,1,['adapt'],['adapted']
Modifiability,stics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2204); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2225); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2244); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2269); 	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:5857,adapt,adapted,5857,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['adapt'],['adapted']
Modifiability,still failing:; ```gsutil cat gs://hail-ci-0-1/deploy/c28a3f9863a0\*/job-log ```; ```; + gcloud -q auth activate-service-account --key-file=/secrets/gcr-push-service-account-key.json; Activated service account credentials for: [gcr-push@broad-ctsa.iam.gserviceaccount.com]; + gcloud -q auth configure-docker; Docker configuration file updated.; + make push-batch; docker build -t batch .; Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.38/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&networkmode=default&rm=1&session=nt5ube8nzit2kbdia2afrfify&shmsize=0&t=batch&target=&ulimits=null&version=1: dial unix /var/run/docker.sock: connect: permission denied; make: *** [build-batch] Error 1; Makefile:14: recipe for target 'build-batch' failed. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4443#issuecomment-424716708:291,config,configure-docker,291,https://hail.is,https://github.com/hail-is/hail/issues/4443#issuecomment-424716708,2,['config'],"['configuration', 'configure-docker']"
Modifiability,store dataset configuration in a sensible way (not in global annotation),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/368:14,config,configuration,14,https://hail.is,https://github.com/hail-is/hail/issues/368,1,['config'],['configuration']
Modifiability,stry/client/__pycache__/docker_name_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_puller_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_pusher_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_appender_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_appender_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_puller_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_pusher_.cpython-39.pyc; /usr/local/share/google/dataproc/npd-config/docker-monitor-counter.json; /usr/local/share/google/dataproc/npd-config/docker-monitor.json; /usr/local/share/google/dataproc/npd-config/health-checker-docker.json; /usr/local/share/google/dataproc/npd-config/docker-monitor-filelog.json; /usr/local/share/google/dataproc/bdutil/fluentd/container_logging/plugin/test/Dockerfile; /usr/local/share/google/dataproc/bdutil/components/initialize/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/post-install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/activate/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/shared/docker.sh; /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/configure_docker.sh; /run/docker.sock; /tmp/dataproc/uninstall/docker-ce; /tmp/dataproc/components/uninstall/docker-ce.running; /tmp/dataproc/components/uninstall/docker-ce.done; /tmp/dataproc/components/pre-uninstall/docker-ce.running; /tmp/dataproc/components/pre-uninstall/docker-ce.done; /etc/apt/preferences.d/docker-ce.pref; /etc/apt/preferences.d/docker-ce-cli.pref; /etc/apt/sources.list.d/docker.list; /var/lib/apt/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:11955,plugin,plugin,11955,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['plugin'],['plugin']
Modifiability,"sts/releases"">requests's releases</a>.</em></p>; <blockquote>; <h2>v2.27.1</h2>; <h2>2.27.1 (2022-01-05)</h2>; <p><strong>Bugfixes</strong></p>; <ul>; <li>Fixed parsing issue that resulted in the <code>auth</code> component being; dropped from proxy URLs. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/6028"">#6028</a>)</li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/psf/requests/blob/v2.27.1/HISTORY.md#2271-2022-01-05"">https://github.com/psf/requests/blob/v2.27.1/HISTORY.md#2271-2022-01-05</a></p>; <h2>v2.27.0</h2>; <h2>2.27.0 (2022-01-03)</h2>; <p><strong>Improvements</strong></p>; <ul>; <li>; <p>Officially added support for Python 3.10. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5928"">#5928</a>)</p>; </li>; <li>; <p>Added a <code>requests.exceptions.JSONDecodeError</code> to unify JSON exceptions between; Python 2 and 3. This gets raised in the <code>response.json()</code> method, and is; backwards compatible as it inherits from previously thrown exceptions.; Can be caught from <code>requests.exceptions.RequestException</code> as well. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5856"">#5856</a>)</p>; </li>; <li>; <p>Improved error text for misnamed <code>InvalidSchema</code> and <code>MissingSchema</code>; exceptions. This is a temporary fix until exceptions can be renamed; (Schema-&gt;Scheme). (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/6017"">#6017</a>)</p>; </li>; <li>; <p>Improved proxy parsing for proxy URLs missing a scheme. This will address; recent changes to <code>urlparse</code> in Python 3.9+. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5917"">#5917</a>)</p>; </li>; </ul>; <p><strong>Bugfixes</strong></p>; <ul>; <li>; <p>Fixed defect in <code>extract_zipped_paths</code> which could result in an infinite loop; for some paths. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5851"">#5851</a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11528:1183,inherit,inherits,1183,https://hail.is,https://github.com/hail-is/hail/pull/11528,2,['inherit'],['inherits']
Modifiability,"such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDelaySeconds: 5; periodSeconds: 5; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805:1241,config,config,1241,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805,6,['config'],"['config', 'configure-liveness-readiness-startup-probes', 'configure-pod-container']"
Modifiability,"such as chr13, it worked well in Terra. Since we will convert multiple plink files to hailmatrix table using Terra platform in future, I need to figure the problem out. Any advise would be appreciated. ### Version. 0.2.127. ### Relevant log output. ```shell; 2024/01/17 20:20:25 Starting container setup.; 2024/01/17 20:20:26 Done container setup.; 2024/01/17 20:20:27 Starting localization.; 2024/01/17 20:20:34 Localization script execution started...; 2024/01/17 20:20:34 Localizing input gs://fc-5a8938eb-1299-4afc-957f-afb53ef602b9/submissions/e8747e74-47d1-4f52-acfc-1ac7f81d79ba/VUMCBed2HailMatrix/683447d9-9342-4058-bcfc-ba21422d3121/call-Bed2HailMatrix/script -> /cromwell_root/script; 2024/01/17 20:20:36 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bed -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bed; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.fam -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.fam; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bim -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bim; Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.fam...; / [0 files][ 0.0 B/910.3 KiB] / [1 files][910.3 KiB/910.3 KiB] Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.bim...; / [1 files][910.3 KiB/369.7 MiB] - - [1 files][ 51.9 MiB/369.7 MiB] \ | | [1 files][107.6 MiB/369.7 MiB] / - - [1 files][162.3 MiB/369.7 MiB] \ \ [1 files][213.9 MiB/369.7 MiB] | / / [1 files][286.6 MiB/369.7 MiB] - \ \ [1 files][342.1 MiB/369.7 MiB] |; Operation completed over 2 objects/369.7 MiB.; | [2 files][369.7 MiB/369.7 MiB] 2024/01/17 20:59:27 Localization script execution complete.; 2024/01/17 20:59:38 Done localization.; 2024/01/17 20:59:39 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint=/bin/bash hailgenetics/hail@sha256:3f22576793ce3161893aed2bd40949b1fc822d2b7e6517dc0ac993b62badaff8 /cromwell_root/script; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.81879b1c; P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:9918,sandbox,sandbox,9918,https://hail.is,https://github.com/hail-is/hail/issues/14168,1,['sandbox'],['sandbox']
Modifiability,support dbnsfp plugin in VEP,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/167:15,plugin,plugin,15,https://hail.is,https://github.com/hail-is/hail/issues/167,1,['plugin'],['plugin']
Modifiability,"t <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/343"">#343</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/7f01591fdbca66375a61d70e505a286550a1c1b1""><code>7f01591</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/814d42df9787494f01474116940782ab67da083f""><code>814d42d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/342"">#342</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/b08f9ca307ce64867070a3ca5ee5f1a6c5742069""><code>b08f9ca</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/89d6a1dda507abde79ff79b3fd95b9d013eaa02d""><code>89d6a1d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/340"">#340</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/93c70a9a9f350b24af796bb31d81182be4ac4b1f""><code>93c70a9</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/e836a7e7c3778ac34a8bd117c2ce701209097cd5""><code>e836a7e</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/339"">#339</a> from sass/pre-commit-ci-update-config</li>; <li>Additional commits viewable in <a href=""https://github.com/sass/libsass-python/compare/0.19.2...0.21.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=libsass&package-manager=pip&previous-version=0.19.2&new-version=0.21.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11508:5095,config,config,5095,https://hail.is,https://github.com/hail-is/hail/pull/11508,1,['config'],['config']
Modifiability,"t <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/405"">#405</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/20b3cdade8a199e832521d0e44a2507bc75315e0""><code>20b3cda</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/940ef2e9f9dd4143d642a29156c94d0a3133a691""><code>940ef2e</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/404"">#404</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/5f8470b48cb576f188ad7424327babb056ce152a""><code>5f8470b</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/7c76abf3a14b1606cc6053f5d95d2e0fa231b97f""><code>7c76abf</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/403"">#403</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/40be5dc3e2c326006fc7f4f1ffcc3cada877c701""><code>40be5dc</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li>Additional commits viewable in <a href=""https://github.com/sass/libsass-python/compare/0.21.0...0.22.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=libsass&package-manager=pip&previous-version=0.21.0&new-version=0.22.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12482:2874,config,config,2874,https://hail.is,https://github.com/hail-is/hail/pull/12482,1,['config'],['config']
Modifiability,"t <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/406"">#406</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/980b41f462ae07939515993781e72654b117bdce""><code>980b41f</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/cfffd417e56b7fd3aaf6034fa49083185714f6b7""><code>cfffd41</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/405"">#405</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/20b3cdade8a199e832521d0e44a2507bc75315e0""><code>20b3cda</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/940ef2e9f9dd4143d642a29156c94d0a3133a691""><code>940ef2e</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/404"">#404</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/5f8470b48cb576f188ad7424327babb056ce152a""><code>5f8470b</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/7c76abf3a14b1606cc6053f5d95d2e0fa231b97f""><code>7c76abf</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/403"">#403</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/40be5dc3e2c326006fc7f4f1ffcc3cada877c701""><code>40be5dc</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li>Additional commits viewable in <a href=""https://github.com/sass/libsass-python/compare/0.21.0...0.22.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=libsass&package-manager=pip&previous-version=0.21.0&new-version=0.22.0)](htt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12482:2426,config,config,2426,https://hail.is,https://github.com/hail-is/hail/pull/12482,1,['config'],['config']
Modifiability,"t array *must* be the JAR URL and the SHA-1 is confusing. Instead, there are now two keys in a JVM process specification:; 1. `jar_spec`, which may be either `{""type"": ""jar_url"", ""value"": ""gs://..../abc123....jar""}` or `{""type"":""git_revision"", ""value"": ""abc123...""}`.; 2. `argv`, an opaque list of strings which are passed, by the JVMEntryway, along with a few more args, to `is.hail.backend.service.Main`. The `Main` class dispatches to either `ServiceBackendSocketAPI2` or the `Worker` based on the first element of `argv`. Each class expects different contents in `argv` that suits its needs. Second, I completely eliminated the HAIL_SHA/revision from the Worker and Hail Query Java code. This was only ever used as unique name for the JAR. Instead, I just use the full JAR URL as a unique name for the JAR. If you need to defeat the cache, just create a new git commit before running `make -C query ipython`. If defeating the cache becomes a common problem, we can add a ""reload_jar"" parameter or similar to the job spec. Third, I renamed `push-jar` in `query/Makefile` to `upload-query-jar` to mirror the build.yaml step. Fourth, I embraced the use of `NAMEPSACE` in `query/Makefile` instead of relying on the minor hack that our laptop usernames match our namespace names. This does mean you need to always specify NAMESPACE when uploading a jar. Finally, a pleasant outcome of this change is the elimination of a bunch of conditional build.yaml logic in the service backend tests!. I think this will simplify the use of Hail Query by Australia et al. because I've isolated the use of hail-specific data to `query/Makefile`. If there's a way to access the relevant global-config variables from `query/Makefile`, I can also fix the `query/Makefile` to be deployment-independent. cc: @lgruen @illusional @tpoterba . [1] For our default namespace deployment, `gs://hail-query/jars/{GIT_REVISION}.jar`. In general, `{HAIL_QUERY_STORAGE_URI}{HAIL_QUERY_ACCEPTABLE_JAR_SUBFOLDER}/{GIT_REVISION}.jar`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11645:3113,config,config,3113,https://hail.is,https://github.com/hail-is/hail/pull/11645,2,"['config', 'variab']","['config', 'variables']"
Modifiability,"t clear from where this came.; - Removed use of the `subtitle` tag, which isn't actually an HTML tag?. Future work:. - Simplify our CSS. It's not possible to logically reason about our CSS. And it; interacts in bad ways with the latent RTD themes. I want a unified Hail visual; theme.; - Clean up the search-related JavaScript in nav-bottom.html and; search.html. These both seem too complicated to just make search work. ---. The thrust of this PR is to restructure Hail's website and documentation to; entirely rely on Jinja2 templates. Previously, we used a mix of Jinja2, XSLT,; and in-browser JavaScript DOM manipulation to piece together a web page. Now, all of Hail's non-service website derives from; `site/templates/base.html`. It is a Jinja2 template with four blocks: title,; meta_description, head, and content. It ensures that:; - Hail's CSS is loaded,; - the Hail icon is set,; - the fonts are loaded,; - the source code highlighter is loaded (prism.js, only used outside the docs); - the nav bar is present and configured.; The nav bar is somewhat complicated. There are two pieces. `nav-top.html`; contains the nav bar HTML elements. `nav-bottom.html` contains the JavaScript; code that hooks the search bar up to Algolia and sets the active page in the; navigation. I believe JavaScript which modifies the HTML DOM is traditionally; placed at the bottom of the `body` tag so that it is executed *after* the HTML; DOM is mostly rendered. That's why the navigation/search bar is split across two; files. I also load the `prism.js` source code highlighter at the end of the; body. All of the non-docs pages are defined by html files in `pages`. Each one of; these is a Jinja2 template which derives from the base template. `make render`; converts every template in `pages` into a real HTML file in `www`. Check out; 404.html for a simple example. Once I had the site in working order, I turned my eyes to the docs. I converted; `docs/_templates/layout.html`, the base template for our do",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9597:1951,config,configured,1951,https://hail.is,https://github.com/hail-is/hail/pull/9597,1,['config'],['configured']
Modifiability,t is.hail.rvd.RVDPartitioner.<init>(RVDPartitioner.scala:33); at is.hail.rvd.RVDPartitioner.copy(RVDPartitioner.scala:225); at is.hail.expr.ir.lowering.TableStage.extendKeyPreservesPartitioning(LowerTableIR.scala:534); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2035); at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1051); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1655); at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:728); at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:1021); at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:27); at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:11); at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:91); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:27); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); at scala.collection.mutable.ResizableArray.foreach(ResizableA,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14249:4190,rewrite,rewrite,4190,https://hail.is,https://github.com/hail-is/hail/issues/14249,1,['rewrite'],['rewrite']
Modifiability,t java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:635); 	at is.hail.asm4s.HailClassLoader.liftedTree1$1(HailClassLoader.scala:10); 	at is.hail.asm4s.HailClassLoader.loadOrDefineClass(HailClassLoader.scala:6); 	at is.hail.asm4s.ClassesBytes.$anonfun$load$1(ClassBuilder.scala:64); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); 	at is.hail.asm4s.ClassesBytes.load(ClassBuilder.scala:62); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:715); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:708); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1(Compile.scala:311); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1$adapted(Compile.scala:310); 	at is.hail.expr.ir.lowering.TableStageToRVD$.$anonfun$apply$9(RVDToTableStage.scala:106); 	at is.hail.sparkextras.ContextRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1234); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1233); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.exe,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:4436,adapt,adapted,4436,https://hail.is,https://github.com/hail-is/hail/issues/12532,2,['adapt'],['adapted']
Modifiability,t org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844); 	at org.apache.spark.u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:8442,adapt,adapted,8442,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['adapt'],['adapted']
Modifiability,"t variant"">; ##INFO=<ID=SNPEFF_IMPACT,Number=1,Type=String,Description=""Impact of the highest-impact effect resulting from the current variant [MODIFIER, LOW, MODERATE, HIGH]"">; ##INFO=<ID=SNPEFF_TRANSCRIPT_ID,Number=1,Type=String,Description=""Transcript ID for the highest-impact effect resulting from the current variant"">; ##INFO=<ID=STR,Number=0,Type=Flag,Description=""Variant is a short tandem repeat"">; ##INFO=<ID=VQSLOD,Number=1,Type=Float,Description=""Log odds ratio of being a true variant versus being false under the trained gaussian mixture model"">; ##INFO=<ID=culprit,Number=1,Type=String,Description=""The annotation which was the worst performing in the Gaussian mixture model, likely the reason why the variant was filtered out"">; ##INFO=<ID=set,Number=1,Type=String,Description=""Source VCF for the merged record in CombineVariants"">; ##OriginalSnpEffCmd=""SnpEff eff -v -onlyCoding true -c /seq/references/Homo_sapiens_assembly19/v1/snpEff/Homo_sapiens_assembly19.snpEff.config -i vcf -o vcf GRCh37.64 /seq/dax/all_1kg_exomes/v1/all_1kg_exomes.unannotated.vcf ""; ##OriginalSnpEffVersion=""2.0.5 (build 2011-12-24), by Pablo Cingolani""; ##SelectVariants=""analysis_type=SelectVariants input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false pres",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:11091,config,config,11091,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['config'],['config']
Modifiability,t$.alreadyLowered(Interpret.scala:56); 	at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$.rewrite$1(InterpretNonCompilable.scala:53); 	at is.hail.expr.ir.InterpretNonCompilable$.$anonfun$apply$1(InterpretNonCompilable.scala:25); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at scala.collection.TraversableLike.map(TraversableLike.scala:286); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279); 	at scala.collection.AbstractTraversable.map(Traversable.scala:108); 	at is.hail.expr.ir.InterpretNonCompilable$.rewriteChildren$1(InterpretNonCompilable.scala:25); 	at is.hail.expr.ir.InterpretNonCompilable$.rewrite$1(InterpretNonCompilable.scala:54); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:67); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:12); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:62); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682:10984,rewrite,rewrite,10984,https://hail.is,https://github.com/hail-is/hail/issues/10682,1,['rewrite'],['rewrite']
Modifiability,"t, self.target_id, self.name) 1134 1135 for temp_arg in temp_args: /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.py in deco(*a, **kw) 61 def deco(*a, **kw): 62 try: ---> 63 return f(*a, **kw) 64 except py4j.protocol.Py4JJavaError as e: 65 s = e.java_exception.toString() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name) 317 raise Py4JJavaError( 318 ""An error occurred while calling {0}{1}{2}.\n"". --> 319 format(target_id, ""."", name), value) 320 else: 321 raise Py4JError( Py4JJavaError: An error occurred while calling o68.apply. : org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at: org.apache.spark.SparkContext.<init>(SparkContext.scala:76) is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:84) is.hail.HailContext$.apply(HailContext.scala:164) sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) java.lang.reflect.Method.invoke(Method.java:498) py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) py4j.Gateway.invoke(Gateway.java:280) py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) py4j.commands.CallCommand.execute(CallCommand.java:79) py4j.GatewayConnection.run(GatewayConnection.java:214) java.lang.Thread.run(Thread.java:745) at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2278) at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2274) at scala.Option.foreach(Option.scala:257) at org.apac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1525:5066,config,configureAndCreateSparkContext,5066,https://hail.is,https://github.com/hail-is/hail/issues/1525,1,['config'],['configureAndCreateSparkContext']
Modifiability,"t. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:3515,config,configuration,3515,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['config'],['configuration']
Modifiability,"t.com/lepture/mistune/pull/295"">lepture/mistune#295</a></p>; <h2>Version 2.0.1</h2>; <p>Fix XSS for image link syntax.</p>; <h2>Version 2.0.0</h2>; <p>First release of Mistune v2.</p>; <h2>Version 2.0.0 RC1</h2>; <p>In this release, we have a <strong>Security Fix</strong> for harmful links.</p>; <h2>Version 2.0.0 Alpha 1</h2>; <p>This is the first release of v2. An alpha version for users to have a preview of the new mistune.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/lepture/mistune/blob/master/docs/changes.rst"">mistune's changelog</a>.</em></p>; <blockquote>; <h2>Changelog</h2>; <p>Here is the full history of mistune v2.</p>; <p>Version 2.0.4</p>; <pre><code>; Released on Jul 15, 2022; <ul>; <li>Fix <code>url</code> plugin in <code>&amp;lt;a&amp;gt;</code> tag</li>; <li>Fix <code>*</code> formatting</li>; </ul>; <p>Version 2.0.3; </code></pre></p>; <p>Released on Jun 27, 2022</p>; <ul>; <li>Fix <code>table</code> plugin</li>; <li>Security fix for CVE-2022-34749</li>; </ul>; <p>Version 2.0.2</p>; <pre><code>; Released on Jan 14, 2022; <p>Fix <code>escape_url</code></p>; <p>Version 2.0.1; </code></pre></p>; <p>Released on Dec 30, 2021</p>; <p>XSS fix for image link syntax.</p>; <p>Version 2.0.0</p>; <pre><code>; Released on Dec 5, 2021; <p>This is the first non-alpha release of mistune v2.</p>; <p>Version 2.0.0rc1; </code></pre></p>; <p>Released on Feb 16, 2021</p>; <p>Version 2.0.0a6</p>; <pre><code>; &lt;/tr&gt;&lt;/table&gt; ; </code></pre>; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/lepture/mistune/commit/3f422f1e84edae0f39756c45be453ecde534b755""><code>3f422f1</code></a> Version bump 2.0.3</li>; <li><a href=""https://github.com/lepture/mistune/commit/a6d43215132fe4f3d93f8d7e90ba83b16a0838b2""><code>a6d4321</code></a> Fix asteris emphasis regex CVE-2022-34749</li>; <li><a href=""https://github.com/lepture/m",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12064:1350,plugin,plugin,1350,https://hail.is,https://github.com/hail-is/hail/pull/12064,1,['plugin'],['plugin']
Modifiability,"t.com/lepture/mistune/pull/295"">lepture/mistune#295</a></p>; <h2>Version 2.0.1</h2>; <p>Fix XSS for image link syntax.</p>; <h2>Version 2.0.0</h2>; <p>First release of Mistune v2.</p>; <h2>Version 2.0.0 RC1</h2>; <p>In this release, we have a <strong>Security Fix</strong> for harmful links.</p>; <h2>Version 2.0.0 Alpha 1</h2>; <p>This is the first release of v2. An alpha version for users to have a preview of the new mistune.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/lepture/mistune/blob/master/docs/changes.rst"">mistune's changelog</a>.</em></p>; <blockquote>; <h2>Changelog</h2>; <p>Here is the full history of mistune v2.</p>; <p>Version 2.0.4</p>; <pre><code>; Released on Jul 15, 2022; <ul>; <li>Fix <code>url</code> plugin in <code>&amp;lt;a&amp;gt;</code> tag</li>; <li>Fix <code>*</code> formatting</li>; </ul>; <p>Version 2.0.3; </code></pre></p>; <p>Released on Jun 27, 2022</p>; <ul>; <li>Fix <code>table</code> plugin</li>; <li>Security fix for CVE-2022-34749</li>; </ul>; <p>Version 2.0.2</p>; <pre><code>; Released on Jan 14, 2022; <p>Fix <code>escape_url</code></p>; <p>Version 2.0.1; </code></pre></p>; <p>Released on Dec 30, 2021</p>; <p>XSS fix for image link syntax.</p>; <p>Version 2.0.0</p>; <pre><code>; Released on Dec 5, 2021; <p>This is the first non-alpha release of mistune v2.</p>; <p>Version 2.0.0rc1; </code></pre></p>; <p>Released on Feb 16, 2021</p>; <p>Version 2.0.0a6</p>; <pre><code>; &lt;/tr&gt;&lt;/table&gt; ; </code></pre>; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/lepture/mistune/commit/b92a5febd4da3d7097a3d2b8d7cac6f5d57ea20c""><code>b92a5fe</code></a> Version bump 2.0.4</li>; <li><a href=""https://github.com/lepture/mistune/commit/98a1c0afc51d4be719cb17401a35e62f46206915""><code>98a1c0a</code></a> Fix url plugin render, <a href=""https://github-redirect.dependabot.com/lepture/mistune/is",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12066:1350,plugin,plugin,1350,https://hail.is,https://github.com/hail-is/hail/pull/12066,2,['plugin'],['plugin']
Modifiability,"t.html; 4. Structure, aforementioned; 5. Path to relatively performant desktop and mobile applications, via [Electron](https://getstream.io/blog/takeaways-on-building-a-react-based-app-with-electron/). [Visual Studio Code](https://github.com/Microsoft/vscode) and [Slack](https://slack.engineering/growing-pains-migrating-slacks-desktop-app-to-browserview-2759690d9c7b) are good examples. Facebook Messenger written in React Native, which we have an even more straightforward path to.; 6. Low cognitive cost (relative to Angular, others. React is just a view layer, and has a tiny API. I've develop a large application in AngularJS, and have spent a bit of time with Angular2+. There is no comparison: Angular takes months to know well, React days at worst. Also, by not buying into a full framework, we achieve modularity: If we end up finding React too slow, even with [planned 2019 improvements](https://reactjs.org/blog/2018/11/27/react-16-roadmap.html), there are plenty of others view layers we can migrate to, without gutting our entire app. Next makes this somewhat trickier, but since it's mostly a light wrapper around Webpack, there is essentially no Next-specific code (just a bit in pages/_app.js and pages/_document.js). Migrating from Next won't be an issue, we would just lose some of the tooling benefits. 8. Typescript: static typing, syntax more familiar to OO-language devs (interfaces, types, classes). 9. Really amazing debug, tooling. React debug tools natively included in Chrome for instance. First class support for JSX (React-wrapped HTML) in popular IDE's, most obviously Visual Studio Code. ## Tech TL;DR; Mainly React. React should take about a day to learn well enough to make contributions. Guide: https://reactjs.org/docs/introducing-jsx.html. ```jsx; # Renders Hello World; # Biggest annoyance (may go away in 2019) is that ""class"" is not a valid tag (reserved by React); export default function SomePage() {; const name = 'Alex'. # Renders ""Hello Alex""; return (; <",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:4916,layers,layers,4916,https://hail.is,https://github.com/hail-is/hail/pull/4931,1,['layers'],['layers']
Modifiability,"tHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:7311,config,configuration,7311,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['config'],['configuration']
Modifiability,tIntervalFilters.scala:249); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:4380,Rewrite,RewriteBottomUp,4380,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"t_errors; return await retry_transient_errors_with_debug_string('', 0, f, *args, **kwargs); File ""/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/hailtop/utils/utils.py"", line 819, in retry_transient_errors_with_debug_string; return await f(*args, **kwargs); File ""/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/hail/backend/service_backend.py"", line 517, in _read_output; raise reconstructed_error.maybe_user_error(ir); hail.utils.java.FatalError: HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht. Java stack trace:; is.hail.utils.HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht; at __C5681Compiled.__m5684begin_group_0(Emit.scala); at __C5681Compiled.__m5683begin_group_0(Emit.scala); at __C5681Compiled.apply(Emit.scala); at is.hail.backend.service.ServiceBackend.$anonfun$execute$1(ServiceBackend.scala:305); at is.hail.backend.service.ServiceBackend.$anonfun$execute$1$adapted(ServiceBackend.scala:305); at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:140); at is.hail.utils.package$.using(package.scala:657); at is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:140); at is.hail.backend.service.ServiceBackend.execute(ServiceBackend.scala:305); at is.hail.backend.service.ServiceBackend.execute(ServiceBackend.scala:333); at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$parseInputToCommandThunk$15(ServiceBackend.scala:718); at is.hail.backend.service.ServiceBackendSocketAPI2.withIRFunctionsReadFromInput(ServiceBackend.scala:812); at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$parseInputToCommandThunk$14(ServiceBackend.scala:716); at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$parseInputToCommandThunk$5(ServiceBackend.scala:673); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); at is.hail.utils.package$.using(package.scala:657); at is.hail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13809:4232,adapt,adapted,4232,https://hail.is,https://github.com/hail-is/hail/issues/13809,1,['adapt'],['adapted']
Modifiability,"te `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. 7. BLAS and LAPACK need to be thread-local. 8. The LSM-tree used by the Shuffler needs to permit multiple values for the same key. There was; some subtlety here around fine-grained locking. Unfortunately, the LSM doesn't expose the right; operations (putIfAbsent) to make this easy, so I had to use a concurrent hash map of locks. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key. - Create a test query-gsa-key in test and dev namespaces. - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`. - The `query` user was missing from bootstrap-create-accounts. - Remove the auto-scaling and remove the k8s grace period. Neither of these really works right. We; will live without the auto-scaling for now. I have to fix the grace period thing before we let; users run real pipelines.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10390:3723,config,config,3723,https://hail.is,https://github.com/hail-is/hail/pull/10390,1,['config'],['config']
Modifiability,"te(); r.getNewRegion()`. As a demonstration of how this should work, this PR does convert `EmitStream.{toArray, write}` to use `eltRegion` correctly, which are used in the consumer nodes `ToArray`, `ArraySort`, `ToSet`, `ToDict`, and `GroupByKey`. Going forward, the plan is to convert the rest of the consumer and producer nodes, before converting the transformer nodes (which both consume and produce streams), as correctness can only be tested on pipelines in which all nodes have been converted. ### Semantics. A stream producer is passed two regions from its consumer: `outerRegion` and `eltRegion`. The node being emitted does not own either region, so may not free/clear them. The only thing a producer may do with either region is to put data in them, by writing directly to them or by giving/sharing ownership of a producer-owned region to them. Consumers' contract:; * The lifetime of `outerRegion` must contain the lifetime of the producer stream, i.e. `outerRegion` must be valid before the producer's `setup0` is called, and must still be valid when the producer's `close0` is called. Thus a producer may use `outerRegion` to store state that persists between elements, or it may use a region it owns itself.; * When the producer's `pull` is called, `eltRegion` must be valid (it does not need to be valid for setup/close). Producers' contract:; * When the consumer's `push` is called, the pushed value must be owned by either `outerRegion` or `eltRegion` (really `eltRegion`; using `outerRegion` is correct but unnecessarily extends the lifetime of the value). This can be accomplished by writing directly to the consumer's region, or by giving/sharing ownership of a producer-owned region to it. Thus the consumer may assume the pushed value is valid until it frees/clears `eltRegion`. If the consumer needs the value to live longer, it must deep-copy the value to another region. These allow us to reason about (and, in principle, to prove) the correctness of each node independently.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9106:2517,extend,extends,2517,https://hail.is,https://github.com/hail-is/hail/pull/9106,1,['extend'],['extends']
Modifiability,"ted dependencies (we have a lot!). See [here](https://mill-build.com/mill/Intro_to_Mill.html) and [here](https://mill-build.com/mill/Builtin_Commands.html) for more details. ## IntelliJ setup. To import from scratch:; * delete any `.idea` directories in the hail root, or `hail/` subdirectory (you could try skipping this, but you're on your own); * in the `hail/` subdirectory, you can also delete `.classpath`, `.gradle`, `.project`, `.settings/`, `build/` (we still use this for a few things, but most of it is dead gradle output, and it's safe to delete it all and start clean), `build.gradle`, `gradle/`, `gradlew`, `gradlew.bat`, `pgradle`, `settings.gradle`; * run `mill mill.bsp.BSP/install` to generate the `.bsp` config directory (bsp is the Build Server Protocol); * In IntelliJ, go to File->Open, and choose the hail root directory; * When the project is open, go to File->Project Structure; * in the Project pane, set an sdk (8 or 11), and set the language level to 8; * in the Modules pane, delete the existing root module, click the plus sign -> Import Module, choose the `hail/` subdirectory, and choose ""Import module from external model"" and `BSP`; * you should see a progress bar at the bottom as it imports the project; * when it's done, quit and reopen IntelliJ. There should now be a bsp icon (two bars with two arrows between them) on the right, where the gradle elephant used to be. Just like before, sometimes you'll need to click the ""reload"" icon in there if things get wonky.; * if it says ""scalafmt configuration detected"", go ahead and enable the formatter. ## Metals setup. * delete any `.metals` directories; * open the hail repo in VSCode (even if you won't use VSCode, this seems to be the best way to get metals set up initially); * it should ask you to import a Mill build; * when that finishes, at the bottom it should say it's connected to a Bloop build server. In general, I think using Mill as the BSP directly will work best, but I don't have much experience t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14147:3777,config,config,3777,https://hail.is,https://github.com/hail-is/hail/pull/14147,1,['config'],['config']
Modifiability,terator.scala:1431); 	at is.hail.asm4s.ModuleBuilder.classesBytes(ClassBuilder.scala:152); 	at is.hail.expr.ir.EmitClassBuilder.resultWithIndex(EmitClassBuilder.scala:706); 	at is.hail.expr.ir.WrappedEmitClassBuilder.resultWithIndex(EmitClassBuilder.scala:174); 	at is.hail.expr.ir.WrappedEmitClassBuilder.resultWithIndex$(EmitClassBuilder.scala:174); 	at is.hail.expr.ir.EmitFunctionBuilder.resultWithIndex(EmitClassBuilder.scala:1078); 	at is.hail.expr.ir.Emit.$anonfun$emitI$238(Emit.scala:2400); 	at is.hail.expr.ir.IEmitCodeGen.map(Emit.scala:336); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:2341); 	at is.hail.expr.ir.Emit.emitI$1(Emit.scala:630); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$26(Emit.scala:748); 	at is.hail.expr.ir.TableTextFinalizer.writeMetadata(TableWriter.scala:552); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:748); 	at is.hail.expr.ir.Emit.emitVoid$1(Emit.scala:627); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$5(Emit.scala:644); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$5$adapted(Emit.scala:644); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$4(Emit.scala:644); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$4$adapted(Emit.scala:643); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:1011); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$3(Emit.scala:643); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$3$adapted(Emit.scala:641); 	at scala.collection.Iterator.foreach(Iterator.scala:943); 	at scala.collection.Iterator.foreach$(Iterator.scala:943); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:641); 	at is.hail,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12531:6243,adapt,adapted,6243,https://hail.is,https://github.com/hail-is/hail/issues/12531,1,['adapt'],['adapted']
Modifiability,"ternal signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.u",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:5337,adapt,adapted,5337,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['adapt'],['adapted']
Modifiability,"tes below</strong>)</p>; <h2>Deprecations</h2>; <ul>; <li>; <p><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9488"">#9488</a>: If custom subclasses of nodes like <code>pytest.Item</code>{.interpreted-text role=&quot;class&quot;} override the; <code>__init__</code> method, they should take <code>**kwargs</code>. See; <code>uncooperative-constructors-deprecated</code>{.interpreted-text role=&quot;ref&quot;} for details.</p>; <p>Note that a deprection warning is only emitted when there is a conflict in the; arguments pytest expected to pass. This deprecation was already part of pytest; 7.0.0rc1 but wasn't documented.</p>; </li>; </ul>; <h2>Bug Fixes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9355"">#9355</a>: Fixed error message prints function decorators when using assert in Python 3.8 and above.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9396"">#9396</a>: Ensure <code>pytest.Config.inifile</code>{.interpreted-text role=&quot;attr&quot;} is available during the <code>pytest_cmdline_main &lt;_pytest.hookspec.pytest_cmdline_main&gt;</code>{.interpreted-text role=&quot;func&quot;} hook (regression during <code>7.0.0rc1</code>).</li>; </ul>; <h2>Improved Documentation</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9404"">#9404</a>: Added extra documentation on alternatives to common misuses of [pytest.warns(None)]{.title-ref} ahead of its deprecation.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9505"">#9505</a>: Clarify where the configuration files are located. To avoid confusions documentation mentions; that configuration file is located in the root of the repository.</li>; </ul>; <h2>Trivial/Internal Changes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9521"">#9521</a>: Add test coverage to assertion rewrite path.</li>; </ul>; <!-- raw HTML o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11516:2872,Config,Config,2872,https://hail.is,https://github.com/hail-is/hail/pull/11516,3,['Config'],['Config']
Modifiability,"tes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""1c6dbf20333a""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:4b4139c73fe9be3bee6c2895aa74059e157eb861d2bdac7d2304ba44b5421f88"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""labels"":map[""app"":""batch""] ""name"":""batch""] ""spec"":map[""ports"":[map[""port"":'P' ""protocol"":""TCP"" ""targetPort"":'\u1388']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; make: *** [deploy-batch] Error 1; Makefile:45: recipe for target 'deploy-batch' failed; ```; [deploy.log](https://github.com/hail-is/hail/files/2504429/deploy.log). Service accounts:; ```; error: the server doesn't have a resource type ""service-accounts""; # kubectl get serviceaccounts ; NAME SECRETS AGE; batch-svc 1 9h; default 1 113d; # kubectl get serviceaccounts -n batch-pods; NAME SECRETS AGE; default 1 7d; deploy-svc 1 1d; test-svc 1 1d; # kubectl get serviceaccounts -n test ; NAME SECRETS AGE; default 1 1d; ```. Apparently this is caused by a lack of permissions",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:4669,config,configuration,4669,https://hail.is,https://github.com/hail-is/hail/issues/4609,1,['config'],['configuration']
Modifiability,"tes/kubernetes/pull/106163"">kubernetes/kubernetes#106163</a>, <a href=""https://github.com/aojea""><code>@​aojea</code></a>) [SIG API Machinery, Apps, Architecture, Auth, Autoscaling, CLI, Cloud Provider, Contributor Experience, Instrumentation, Network, Node, Release, Scalability, Scheduling, Storage, Testing and Windows]</li>; <li>If a conflict occurs when creating an object with <code>generateName</code>, the server now returns an &quot;AlreadyExists&quot; error with a retry option. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104699"">kubernetes/kubernetes#104699</a>, <a href=""https://github.com/vincepri""><code>@​vincepri</code></a>)</li>; <li>Implement support for recovering from volume expansion failures (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/106154"">kubernetes/kubernetes#106154</a>, <a href=""https://github.com/gnufied""><code>@​gnufied</code></a>) [SIG API Machinery, Apps and Storage]</li>; <li>In kubelet, log verbosity and flush frequency can also be configured via the configuration file and not just via command line flags. In other commands (kube-apiserver, kube-controller-manager), the flags are listed in the &quot;Logs flags&quot; group and not under &quot;Global&quot; or &quot;Misc&quot;. The type for <code>-vmodule</code> was made a bit more descriptive (<code>pattern=N,...</code> instead of <code>moduleSpec</code>). (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/106090"">kubernetes/kubernetes#106090</a>, <a href=""https://github.com/pohly""><code>@​pohly</code></a>) [SIG API Machinery, Architecture, CLI, Cluster Lifecycle, Instrumentation, Node and Scheduling]</li>; <li>Introduce <code>OS</code> field in the PodSpec (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104693"">kubernetes/kubernetes#104693</a>, <a href=""https://github.com/ravisantoshgudimetla""><code>@​ravisantoshgudimetla</code></a>)</li>; <li>Introduce <code>v1beta3</code> API",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:4912,config,configured,4912,https://hail.is,https://github.com/hail-is/hail/pull/11957,2,['config'],"['configuration', 'configured']"
Modifiability,"tested this out in my dev namespace with [this PR](https://github.com/hail-ci-test/ci-test-fhkslcwtu6ij/pull/1) in the associated test repo, which didn't merge until it was approved and had the `ci-test` commit status passing, while the `hail-ci-azure` commit status has failed (the test repo configuration requires one approval and just the `ci-test` commit status to be passing)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14676#issuecomment-2349266606:293,config,configuration,293,https://hail.is,https://github.com/hail-is/hail/pull/14676#issuecomment-2349266606,1,['config'],['configuration']
Modifiability,"the `@benchmark` decorator).; By convention, benchmarks are python tests whose names are prefixed by `benchmark_` and are located in files with the same prefix.; Nothing enforces this, however, so you could name your benchmarks `test_*` and put them in files named `test_*.py`.; Benchmarks may import and use any test code or utilities defined in `test/`.; The results of each benchmark are outputted as json lines (`.jsonl`) to the file specified by the `--output` pytest arg or stdout. The folder structure should be familiar, resembling our `test/` directory.; I believe this is flexible enough to add `hailtop` benchmarks should we so wish:; ```; pytest.ini - hoisted from `test/` to include benchmark marks; benchmark/; - conftest.py for custom pytest command line args ; - hail/; - confest.py for custom plugin that runs hail benchmarks; - benchmark_*.py hail query benchmark code; - tools/; - shared utilites, including the `@benchmark`; ```; Supporting pytest fixtures required writing a custom plugin to run benchmarks, as using off-the-shelf; solutions like `pytest-benchmark` would forbid method level fixtures like `tmp_path` etc.; The plugin is designed to run ""macro-benchmarks"" (ie long-running tests) and fully supports pytest parameterisation.; For each benchmark, the plugin initialises hail and then repeats (for a number of iterations defined by the pytest mark); acquiring fixtures, timing invocation and tearing-down fixtures, finally stopping hail. It is therefore unsuitable for; microbenchmarks, for which we currenly have none in python. If we add them we'd need to tweak this so support them.; Perhaps an inner loop or something. The process of submitting benchmarks to batch is greatly simplified as the old `Makefile` infrastructure for ; building wheels and docker images etc has been replaced with the script `benchmark_in_batch.py`.; Benchmark images are now based off the `hail-dev` image built in CI (or via the `hail-dev-image` make target). ; Furthermore, you can ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14565:1400,plugin,plugin,1400,https://hail.is,https://github.com/hail-is/hail/pull/14565,1,['plugin'],['plugin']
Modifiability,the configuration changes we made mean that it's perfectly natural to handle this PR!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/805#issuecomment-248415712:4,config,configuration,4,https://hail.is,https://github.com/hail-is/hail/pull/805#issuecomment-248415712,1,['config'],['configuration']
Modifiability,the former `ascending` and `onKey` parameters are just rewrites of the comparison expression.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5084:55,rewrite,rewrites,55,https://hail.is,https://github.com/hail-is/hail/pull/5084,1,['rewrite'],['rewrites']
Modifiability,"the gcloud command write to `stderr` which gets interpreted as errors in the logs and causes [a lot of noise](https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aseverity%3DERROR%0Aresource.labels.namespace_name%3D%22default%22%0Aresource.labels.container_name%3D%22image-fetcher%22;timeRange=PT3H?project=hail-vdc&folder=true&organizationId=548622027621&query=%0A). . Added `gcr.io` because without it `configure-docker` would configure all the registries it can find, which is unnecessary. Also removed `-x` from the site script since each line of that goes to error logs and none of it is helpful. If errors occur they will show up in some other form.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9991:440,config,configure-docker,440,https://hail.is,https://github.com/hail-is/hail/pull/9991,2,['config'],"['configure', 'configure-docker']"
Modifiability,"the number of used bits is a (statically known) constant. We use this to ensure the number of used bits is known statically.; 	; Types:; - missingness; - treat as a type constructor `optional<T>`, i.e. base types don't encode missingness. Emits a single bit in the encoding. Can invert this bit to control whether missing values come first or last in the ordering. If missing, nothing is emitted after.; - sort-order; - treat reversing the default ordering as a type constructor `reverse<T>`; - simply inverts the encoding bitwise; - primitive types; - same as in datafusion, encoding has same size as original type; - signed integers - flip the sign bit; - floating point numbers - if sign bit is set, invert all bits, otherwise only flip the sign bit; - arrays; - before each element and after last element, emit continuation bit (0 if no more elements); - pad before each element. This prevents a variable number of missing bits packing into a byte; - strings and byte-arrays; - simply use null-terminated strings (being careful to do this in a unicode-safe way); - structs; - simply concatenate element encodings. safe because codes are prefix-free; - key structs; - support variable length ""interval endpoints""; - e.g. for a key type `struct<t1, t2>`, the interval `[{a}, {a, b})` contains all keys with first field `a` and second field less than `b`. We break it into two ""interval endpoints"", `({a}, -1)` and `({a, b}, -1)`, which consist of a struct value which is a prefix of the key struct type, and a ""sign"". In this case, both endpoints ""lean left"".; - needed for working with partitioners at runtime; - like an array with fixed but heterogenous types and a max length; - before each element and after last element, emit two continuation bits; - `00` - end of key, leans left (less than all longer keys with this prefix); - `01` - continue, or after last key field of actual key value (not interval endpoint); - unambiguous because key value can't terminate early, and can't continue past",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14396:2036,variab,variable,2036,https://hail.is,https://github.com/hail-is/hail/issues/14396,1,['variab'],['variable']
Modifiability,the parallel header reading doesn't change the config settings.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6282:47,config,config,47,https://hail.is,https://github.com/hail-is/hail/issues/6282,1,['config'],['config']
Modifiability,"the script may be clearer and easier to extend if written in python, too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5910#issuecomment-484684296:40,extend,extend,40,https://hail.is,https://github.com/hail-is/hail/pull/5910#issuecomment-484684296,2,['extend'],['extend']
Modifiability,the two rewrites have made this much more powerful and understandable,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5245#issuecomment-464144563:8,rewrite,rewrites,8,https://hail.is,https://github.com/hail-is/hail/pull/5245#issuecomment-464144563,1,['rewrite'],['rewrites']
Modifiability,the uri got dropped in refactoring on PR that just went in #5686. Reverted the change and added a test to cluster sanity check.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5709:23,refactor,refactoring,23,https://hail.is,https://github.com/hail-is/hail/pull/5709,1,['refactor'],['refactoring']
Modifiability,"there's also a small amount of refactoring that was necessary to send TableIRSuite.testRangeCount through the lowered, jvm-emitted path to test the CollectDistributedArray implementation. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6087:31,refactor,refactoring,31,https://hail.is,https://github.com/hail-is/hail/pull/6087,1,['refactor'],['refactoring']
Modifiability,these are the last of the changes from #6480; really this should be several distinct PRs and if I have a chance to refactor the tests I'll split it apart. notes to follow in the morning.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6518:115,refactor,refactor,115,https://hail.is,https://github.com/hail-is/hail/pull/6518,1,['refactor'],['refactor']
Modifiability,"think helped to reduce the number of places that are aware of the notion of a default_region. It's really now just isolated to the `InstanceCollectionManager`, since that's the piece of code that's making the decision ""use the default region when the cluster is small"". I didn't quite like the pattern of retrieving a default from a `LocationMonitor` just to give it right back to the location monitor in the next line. I think this way the `LocationMonitor` API is much simpler, and we can actually remove its `default_location` method entirely as I believe it is no longer used. I can do that in a follow-up PR if you like this approach. One other thing is I wanted to articulate the distinction between the ""region CI needs its jobs in"" and the ""default region that batch will spin up workers in for small clusters"". While they are in practice the same, I found that treating them both as the ""default_region"" tied the logic around jobs for CI closely with internal Batch decisions and made it more confusing for me to reason about. I tried to separate out these two concepts so that in the future when jobs support region-specific scheduling it will be easier to excise the CI-specific code from the Batch scheduler. Another thing that I realized during this process is that Azure has regions and zones just like GCP, though they may differ slightly since we don't need to specify a zone for a VM and such. I am fine with using the term ""location"" to mean ""where we scheduled the VM, either zone or region depending on the cloud provider"", but I would also like to follow up with a sweep that makes this language more precise where possible. For example, the `possible_cloud_locations` function is really just `possible_cloud_regions`, and we could even go so far as mandating a `region` field in the global config instead of having `azure_location` and `gcp_region`, which are synonymous even though named differently. It also leads me to wonder why we only schedule in a single region in Azure?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12078#issuecomment-1207095240:1837,config,config,1837,https://hail.is,https://github.com/hail-is/hail/pull/12078#issuecomment-1207095240,2,['config'],['config']
Modifiability,"this code](https://github.com/broadinstitute/gnomad_methods/blob/0d2e71f93d5cfceebfad2ab538e47d8457d57d6d/gnomad/sample_qc/sex.py#L18-L46):. ```python; male = karyotype_expr == xy_karyotype_str; female = karyotype_expr == xx_karyotype_str; x_nonpar = locus_expr.in_x_nonpar(); y_par = locus_expr.in_y_par(); y_nonpar = locus_expr.in_y_nonpar(); return (; hl.case(missing_false=True); .when(female & (y_par | y_nonpar), hl.null(hl.tcall)); .when(male & (x_nonpar | y_nonpar) & gt_expr.is_het(), hl.null(hl.tcall)); .when(male & (x_nonpar | y_nonpar), hl.call(gt_expr[0], phased=False)); .default(gt_expr); ); ```; A single partition is taking a very long time to compute. Manual sampling of stack traces via `jstack` or the Spark UI reveals we spend a lot of time in computing the inPar predicates:; ```; app//is.hail.utils.Interval.contains(Interval.scala:67); app//is.hail.variant.ReferenceGenome.$anonfun$inPar$1(ReferenceGenome.scala:298); app//is.hail.variant.ReferenceGenome.$anonfun$inPar$1$adapted(ReferenceGenome.scala:298); app//is.hail.variant.ReferenceGenome$Lambda$924/0x00000008009b2840.apply(Unknown Source); app//scala.collection.IndexedSeqOptimized.prefixLengthImpl(IndexedSeqOptimized.scala:41); app//scala.collection.IndexedSeqOptimized.exists(IndexedSeqOptimized.scala:49); app//scala.collection.IndexedSeqOptimized.exists$(IndexedSeqOptimized.scala:49); app//scala.collection.mutable.ArrayOps$ofRef.exists(ArrayOps.scala:198); app//is.hail.variant.ReferenceGenome.inPar(ReferenceGenome.scala:298); app//is.hail.variant.ReferenceGenome.inYPar(ReferenceGenome.scala:302)__C9622collect_distributed_array_table_native_writer.__m10668inYPar(Unknown Source)__C9622collect_distributed_array_table_native_writer.__m10656split_Let(Unknown Source)__C9622collect_distributed_array_table_native_writer.__m10638split_ToArray_region3_65(Unknown Source)__C9622collect_distributed_array_table_native_writer.__m10638split_ToArray(Unknown Source)__C9622collect_distributed_array_table_native_writer.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13862:1027,adapt,adapted,1027,https://hail.is,https://github.com/hail-is/hail/issues/13862,1,['adapt'],['adapted']
Modifiability,this is necessary for rewrite rules to preserve the values generated by seeded functions correctly. I believe this is the last piece for #4017. (fixes #4017),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4165:22,rewrite,rewrite,22,https://hail.is,https://github.com/hail-is/hail/pull/4165,1,['rewrite'],['rewrite']
Modifiability,"this is the very first part of an effort to make build.yaml steps that should ultimately work in any cloud deployment cloud-agnostic from the perspective of build.yaml. In this case, it removes the gcp project environment variable from the build.yaml step and instead has bootstrap_create_accounts.py read it from a `GCPConfig`. This `GCPConfig` is constructed from fields of the global config, but the actual application code doesn't care about which secret it's coming from.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10957:222,variab,variable,222,https://hail.is,https://github.com/hail-is/hail/pull/10957,2,"['config', 'variab']","['config', 'variable']"
Modifiability,"this release. People with a &quot;+&quot; by; their names contributed a patch for the first time.</p>; <ul>; <li>Bas van Beek</li>; <li>Charles Harris</li>; <li>Matthew Barber</li>; <li>Matti Picus</li>; <li>Ralf Gommers</li>; <li>Ross Barnowski</li>; <li>Sebastian Berg</li>; <li>Sicheng Zeng +</li>; </ul>; <h2>Pull requests merged</h2>; <p>A total of 13 pull requests were merged for this release.</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22368"">#22368</a>: BUG: Add <code>__array_api_version__</code> to <code>numpy.array_api</code> namespace</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22370"">#22370</a>: MAINT: update sde toolkit to 9.0, fix download link</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22382"">#22382</a>: BLD: use macos-11 image on azure, macos-1015 is deprecated</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22383"">#22383</a>: MAINT: random: remove <code>get_info</code> from &quot;extending with Cython&quot;...</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22384"">#22384</a>: BUG: Fix complex vector dot with more than NPY_CBLAS_CHUNK elements</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22387"">#22387</a>: REV: Loosen <code>lookfor</code>'s import try/except again</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22388"">#22388</a>: TYP,ENH: Mark <code>numpy.typing</code> protocols as runtime checkable</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22389"">#22389</a>: TYP,MAINT: Change more overloads to play nice with pyright</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22390"">#22390</a>: TST,TYP: Bump mypy to 0.981</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22391"">#22391</a>: DOC: Update delimiter param description.</li>; <li><a href=""https://github-redirect.dep",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12441:1897,extend,extending,1897,https://hail.is,https://github.com/hail-is/hail/pull/12441,1,['extend'],['extending']
Modifiability,"this the exception (attached the log):. ``` ; File ""/tmp/7f8f775e-2ec3-40ee-ad5b-3e4df5649682/annotate_and_generate_scores_cloud.py"", line 24, in <module>; .vep(config='/vep/vep-gcloud.properties', root='va.vep', force=True); File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/dataset.py"", line 3095, in vep; File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/context.py"", line 81, in run_command; File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/java.py"", line 5, in jarray; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_collections.py"", line 228, in __setitem__; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_collections.py"", line 211, in __set_item; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 323, in get_return_value; py4j.protocol.Py4JError: An error occurred while calling None.None. Trace:; java.lang.IllegalArgumentException: array element type mismatch; at java.lang.reflect.Array.set(Native Method); at py4j.commands.ArrayCommand.setArray(ArrayCommand.java:141); at py4j.commands.ArrayCommand.execute(ArrayCommand.java:94); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745) ; ```. [hail.txt](https://github.com/hail-is/hail/files/725407/hail.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1284:161,config,config,161,https://hail.is,https://github.com/hail-is/hail/issues/1284,1,['config'],['config']
Modifiability,"thlib-1.3.0 rsa-4.7.2 scipy-1.6.1 six-1.15.0 tabulate-0.8.3 tornado-6.1 tqdm-4.42.1 traitlets-5.0.5 typing-extensions-3.7.4.3 urllib3-1.25.11 wcwidth-0.2.5 wrapt-1.12.1 yarl-1.6.3; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]; snafu$ ipython ; Python 3.8.6 (default, Jan 27 2021, 15:42:20) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.21.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-e24d842d2b9a> in <module>; ----> 1 import hail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.accumulators import Accumulator; 33 from pyspark.broadcast import Broadcast, BroadcastPickleRegistry. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py in <module>; 95 import socketserver as SocketServer; 96 import threading; ---> 97 from pyspark.serializers import read_int, PickleSerializer; 98 ; 99 . ~/sandbox/hail/venv/3.8/lib/python3.8/s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:10246,sandbox,sandbox,10246,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['sandbox'],['sandbox']
Modifiability,"thub.com/axios/axios/commit/82c94555917834770bd1389fc0b4cd9ba35ec3fe""><code>82c9455</code></a> Create SECURITY.md (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3981"">#3981</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/5b457116e31db0e88fede6c428e969e87f290929""><code>5b45711</code></a> Security fix for ReDoS (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3980"">#3980</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/5bc9ea24dda14e74def0b8ae9cdb3fa1a0c77773""><code>5bc9ea2</code></a> Update ECOSYSTEM.md (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3817"">#3817</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/e72813a385c32e4c3eeaeb4fcc4437dd124bbbcf""><code>e72813a</code></a> Fixing README.md (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3818"">#3818</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/e10a0270e988a641ba0f01509c4c3ba657afe5a5""><code>e10a027</code></a> Fix README typo under Request Config (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3825"">#3825</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/e091491127893a476b0223ab72f788c3b30fc082""><code>e091491</code></a> Update README.md (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3936"">#3936</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/b42fbad57b093bb7214991161c5355bd46b864d0""><code>b42fbad</code></a> Removed un-needed bracket</li>; <li><a href=""https://github.com/axios/axios/commit/520c8dccdef92cccbe51ea7cd96ad464c6401914""><code>520c8dc</code></a> Updating CI status badge (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3953"">#3953</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/axios/axios/compare/v0.21.1...v0.21.2"">compare view</a></li>; </ul>; </details>; <details>; <summary>Maintainer changes</summary>; <p>This version was pushed to npm by <a href=""https://www.n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11080:12667,Config,Config,12667,https://hail.is,https://github.com/hail-is/hail/pull/11080,2,['Config'],['Config']
Modifiability,"till; accessible with <code>import importlib;metadata.metadata('pylint')</code>.</p>; </li>; <li>; <p>COPYING has been renamed to LICENSE for standardization.</p>; </li>; <li>; <p>Fix false-positive <code>used-before-assignment</code> in function returns.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/4301"">#4301</a></p>; </li>; <li>; <p>Updated <code>astroid</code> to 2.5.3</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/PyCQA/pylint/blob/main/ChangeLog"">pylint's changelog</a>.</em></p>; <blockquote>; <h1>What's New in Pylint 2.12.2?</h1>; <p>Release date: 2021-11-25</p>; <ul>; <li>; <p>Fixed a false positive for <code>unused-import</code> where everything; was not analyzed properly inside typing guards.</p>; </li>; <li>; <p>Fixed a false-positive regression for <code>used-before-assignment</code> for; typed variables in the body of class methods that reference the same class</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5342"">#5342</a></p>; </li>; <li>; <p>Specified that the <code>ignore-paths</code> option considers &quot;&quot; to represent a; windows directory delimiter instead of a regular expression escape; character.</p>; </li>; <li>; <p>Fixed a crash with the <code>ignore-paths</code> option when invoking the option; via the command line.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5437"">#5437</a></p>; </li>; <li>; <p>Fixed handling of Sphinx-style parameter docstrings with asterisks. These; should be escaped with by prepending a &quot;&quot;.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5406"">#5406</a></p>; </li>; <li>; <p>Add <code>endLine</code> and <code>endColumn</code> keys to output of <code>JSONReporter</code>.</p>; <p>Closes <a href=""https://github-redirect.dependab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11461:3649,variab,variables,3649,https://hail.is,https://github.com/hail-is/hail/pull/11461,2,['variab'],['variables']
Modifiability,"ting decisions dynamically circumvents this limitation. However, this prevents usage of NGINX [upstream](http://nginx.org/en/docs/http/ngx_http_upstream_module.html) blocks that provide connection pooling, at least in the community edition, and as a result the gateways will create and terminate a TCP connection per http request. This likely causes minor delays on the front-end through gateway, but this hampers performance greatly in job scheduling. The batch driver is forced to establish a new TCP connection and do an SSL handshake with the internal-gateway multiple times per job, which is expensive and slow. We currently have to dedicate a 2-core NGINX sidecar for the batch-driver just to terminate TLS with internal-gateway and free up cycles in the batch-driver python process. By using proper persistent connections, we can reduce the TLS overhead to single-digit percents of a core. This leads to the first goal of this transition: configure our load balancers to know the full cluster configuration at any point in time so they can properly maintain connection pools with upstream services. However, this is not the only problem. Each ""upstream"" Service in Kubernetes may consist of multiple underlying pods but Kubernetes Services as we use them don't provide proper load-balancing when mixed with persistent connections. When we declare a Service for say, batch in default, Kubernetes adds a DNS record for `batch.default` that resolves to a single IP pointing at kube-proxy. When a new TCP connection is established with kube-proxy for that IP, it rolls the dice (using `iptables`) and assigns that connection to a particular pod to which it will forward all subsequent packets. From the load-balancer's perspective, there is only one IP address, and only one place to open connections. The load-balancer doesn't have the information to actually load-balance once we have a functioning connection pool. This can lead to really unbalanced scenarios when preemptible pods come and go. ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:2239,config,configure,2239,https://hail.is,https://github.com/hail-is/hail/pull/12095,2,['config'],"['configuration', 'configure']"
Modifiability,tionTimer$.time(ExecutionTimer.scala:52); E 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.withExecuteContext$1(ServiceBackend.scala:631); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:693); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6(ServiceBackend.scala:459); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6$adapted(ServiceBackend.scala:458); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5(ServiceBackend.scala:458); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:458); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:456); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:456); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:456); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor90.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:6713,adapt,adapted,6713,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['adapt'],['adapted']
Modifiability,tionTimer$.time(ExecutionTimer.scala:52); E 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.withExecuteContext$1(ServiceBackend.scala:634); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:697); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6(ServiceBackend.scala:462); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6$adapted(ServiceBackend.scala:461); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5(ServiceBackend.scala:461); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:134); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:460); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:459); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:134); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13074:6427,adapt,adapted,6427,https://hail.is,https://github.com/hail-is/hail/issues/13074,2,['adapt'],['adapted']
Modifiability,tionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:74); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:19); 	at is.hail.expr.ir.lowering.LowerDistributedSort$.distributedSort(LowerDistributedSort.scala:163); 	at is.hail.backend.service.ServiceBackend.lowerDistributedSort(ServiceBackend.scala:356); 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:100); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:23); 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$4(RewriteBottomUp.scala:26); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:36); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:20); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:157); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringPass.scala:151); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:6,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:8040,Rewrite,RewriteBottomUp,8040,https://hail.is,https://github.com/hail-is/hail/issues/12983,4,['Rewrite'],['RewriteBottomUp']
Modifiability,tionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:74); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:19); 	at is.hail.expr.ir.lowering.LowerDistributedSort$.distributedSort(LowerDistributedSort.scala:163); 	at is.hail.backend.service.ServiceBackend.lowerDistributedSort(ServiceBackend.scala:356); 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:100); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:23); 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$4(RewriteBottomUp.scala:26); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:36); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:20); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:157); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringPass.scala:151); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); 	at scala.collection.mutable.ResizableArray.foreach(Res,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:8017,Rewrite,RewriteBottomUp,8017,https://hail.is,https://github.com/hail-is/hail/issues/12983,4,['Rewrite'],['RewriteBottomUp']
Modifiability,"tl command line argument parsing code. While the interface remains largely the same, a few changes were made to how options are handled. We first introduce a bit of terminology. In a shell command invocation like `$ cmd a -o c`, `a`, `-o` and `c` are called parameters. `a` and `c` which do not start with dashes, are called arguments. `-o`, which starts with a dash, is an option. This PR makes the following changes:. - For dataproc commands taking extra gcloud parameters, all parameters after a double-dash (--) are passed to gcloud.; - The actual rule is slightly more complicated, but I think the above rule is the right take away. In detail, extra parameters are passed to gcloud. Unknown options (starting with a dash) before `--` are reported as an error. So arguments (not options) before `--` and all parameters after are passed to gcloud. ; - Short options don't need a `=` when specifying a value. It is now `-p2`, not `-p=2`.; - While I was making breaking changes, I changed `dataproc submit` `--gcloud_configuration` to `--gcloud-configuration`. I am happy to undo this one.; - Group arguments must go before the next command. Write `hailctl dataproc --beta start ...` not `hailctl dataproc start --beta ...`, which is an error since `start` has no option `--beta`. This PR rewrites argument parsing to use click instead of argparse: https://click.palletsprojects.com/en/7.x/. Things you need to know about click:; - A group is a group of commands or subgroups, like `hailctl dataproc`, `hailctl batch`, etc. Groups are defined like this:; ; ```; @hailctl.group(; help=""Manage the Hail Batch service.""); def batch():; pass; ```; - A command in a group is defined like this:. ```; @batch.command(; help=""Get a particular batch's info.""); @click.argument('batch_id', type=int); @click.option('--output-format', '-o',; type=click.Choice(['yaml', 'json']),; default='yaml', show_default=True,; help=""Specify output format"",); def get(batch_id, output_format):; ...; ```. The command decor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842:1072,config,configuration,1072,https://hail.is,https://github.com/hail-is/hail/pull/9842,1,['config'],['configuration']
Modifiability,"to be included in the dev docs:. the devserver uses the `default_namespace` configured via `hailctl dev config`, so make sure to change that to your dev namespace or the default namespace as desired. whatever service you're trying to hack on the UI of needs to be `pip install`ed, as well as `web_common` and `gear`; for example, `pip install -e web_common gear batch`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13632#issuecomment-1783089092:76,config,configured,76,https://hail.is,https://github.com/hail-is/hail/pull/13632#issuecomment-1783089092,2,['config'],"['config', 'configured']"
Modifiability,"to become C++11-compliant. Systems using the earlier libstdc++; (which also typically have g++-4.8.x/4.9x) have the old-ABI not-fully-compliant std::string; (and std::list, but I've gone 20 years without ever using that). Later versions of libstdc++ have *both* flavors of std::string, but use namespaces to allow them; to coexist (but not to be interchangeable, so interfaces between old-ABI and new-ABI are; problematic). https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html. ""In the GCC 5.1 release libstdc++ introduced a new library ABI that includes new implementations of std::string and std::list. These changes were necessary to conform to the 2011 C++ standard which forbids Copy-On-Write strings and requires lists to keep track of their size. In order to maintain backwards compatibility for existing code linked to libstdc++ the library's soname has not changed and the old implementations are still supported in parallel with the new ones. This is achieved by defining the new implementations in an inline namespace so they have different names for linkage purposes, e.g. the new version of std::list<int> is actually defined as std::__cxx11::list<int>. Because the symbols for the new implementations have different names the definitions for both versions can be present in the same library."". OSX doesn't have this ABI-compatibility issue because for several years it has been using; libc++ as the default library, and libc++ is a post-C++11 rewrite-from-scratch implementation; of the required standard-library functionality. My understanding is that it is perfectly feasible to mix and match different g++/clang++compiler; versions and different libstdc++ versions. It just doesn't happen very much because both the; g++ version and the libstdc++ are chosen at the same time, early in building a Linux distribution,; and then are frozen throughout the release's lifetime to ensure interoperability of binaries. So debian8's version of g++ and libstdc++ doesn't change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424797612:1941,rewrite,rewrite-from-scratch,1941,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424797612,1,['rewrite'],['rewrite-from-scratch']
Modifiability,"to complete before it starts the next iteration of the scheduler. This leaves the scheduler vulnerable to problematic workers or workers that happen to be preempted during the scheduling process. So, the driver sets a [2 second timeout](https://github.com/hail-is/hail/blob/b27737f67bf9e69f1abed2fec07fc7c921790ef8/batch/batch/driver/job.py#L585) on the call to `/api/v1alpha/batches/jobs/create`. Additionally, this general design means that in the event of a request timeout or transient error, Batch cannot guarantee that there is always at most one concurrent running attempt for a given job. This ends up being a fine (and intentional) concession in practice because the idempotent design of preemptible jobs tends to cover this scenario, but it is regardless wasted compute and cost to users. Nevertheless, we strive to minimize cases where we might halt the scheduling loop or double-schedule work, and one way to do that in the current design is to minimize the variance in latency of `/api/v1alpha/batches/jobs/create`. The largest source of this latency is the request to blob storage. While GCS and ABS are relatively fast and highly available, Batch in Azure Terra requires first obtaining SAS tokens from the Terra control plane, which can introduce much higher and more variable latency. There have also been occurrences in the past of corrupted or deleted specs, which introduce unexpected failure modes that should error the job but instead disrupt the scheduling loop. Many of these problems would be mitigated by moving the read from object storage outside of the `/api/v1alpha/batches/jobs/create` endpoint. The endpoint should push this read into the asynchronous task that ultimately runs the job and therefore return its acknowledgement to the driver faster. If the worker encounters errors later on while reading the spec, those should result in `error`ing the job instead of raising a 500 in the scheduling request. ### Version. 0.2.129. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14456:2229,variab,variable,2229,https://hail.is,https://github.com/hail-is/hail/issues/14456,1,['variab'],['variable']
Modifiability,to make further refactoring easier.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10857:16,refactor,refactoring,16,https://hail.is,https://github.com/hail-is/hail/pull/10857,1,['refactor'],['refactoring']
Modifiability,"to v0.9.26 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2196"">#2196</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4f8bb658e9ff3cba5e745acae13ec4094a1a48d5"">4f8bb65</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.26.0...v2.26.1"">2.26.1</a> (2023-08-14)</h2>; <h3>Bug Fixes</h3>; <ul>; <li>Make use of ImmutableMap.Builder#buildOrThrow graceful (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2159"">#2159</a>) (<a href=""https://github.com/googleapis/java-storage/commit/e9746f856e9204c1c0ec62f19e6f71ff8a0b9750"">e9746f8</a>)</li>; <li>Update gRPC writeAndClose to only set finish_write on the last message (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2163"">#2163</a>) (<a href=""https://github.com/googleapis/java-storage/commit/95df758d6753005226556177e68a3e9c630c789b"">95df758</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.24 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2158"">#2158</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4f5682a4f6d6d5372a2d382ae3e47dace490ca0d"">4f5682a</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.25.0...v2.26.0"">2.26.0</a> (2023-08-03)</h2>; <h3>Features</h3>; <ul>; <li>Implement BufferToDiskThenUpload BlobWriteSessionConfig (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2139"">#2139</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4dad2d5c3a81eda7190ad4f95316471e7fa30f66"">4dad2d5</a>)</li>; <li>Introduce new BlobWriteSession (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2123"">#2123</a>) (<a href=""https://github.com/googleapis/java-storage/commit/e0191b518e50a49fae0691894b50f0c5f33fc6af"">e0191b5</a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</su",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13624:11567,plugin,plugin,11567,https://hail.is,https://github.com/hail-is/hail/pull/13624,1,['plugin'],['plugin']
Modifiability,"top/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/config/cli.py; +++ b/hail/python/hailtop/hailctl/dev/config/cli.py; @@ -1,6 +1,6 @@; import os; import json; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; ; def init_parser(parser):; @@ -35,6 +35,6 @@ def main(args):; }; ; config_file = os.environ.get(; - 'HAIL_D",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3672,config,config,3672,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['config'],['config']
Modifiability,"top_level references are references to things that should be in the EvalContext. top_level=False are references intermediate variables. Inside the view_join_x stuff, we constructed top_level=False references to things like `row` and `va`, so these threw errors if used inside aggregators. I'll add a test for this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3055#issuecomment-369985720:125,variab,variables,125,https://hail.is,https://github.com/hail-is/hail/pull/3055#issuecomment-369985720,1,['variab'],['variables']
Modifiability,"tps://redirect.github.com/python-pillow/Pillow/issues/7242"">#7242</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Limit size even if one dimension is zero in decompression bomb check <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7235"">#7235</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Restored 32-bit support <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7234"">#7234</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Removed deleted file from codecov.yml and increased coverage threshold <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7232"">#7232</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Removed support for 32-bit <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7228"">#7228</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use --config-settings instead of deprecated --global-option <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7171"">#7171</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Better C integer definitions <a href=""https://redirect.github.com/python-pillow/Pillow/issues/6645"">#6645</a> [<a href=""https://github.com/Yay295""><code>@​Yay295</code></a>]</li>; <li>Fixed finding dependencies on Cygwin <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7175"">#7175</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Improved checks in font_render <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7218"">#7218</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Change <code>grabclipboard()</code> to use PNG compression on macOS <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7219"">#7219</a> [<a href=""https://github.com/abey79""><code>@​abey79</code></a>]</li>; <l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13321:2681,config,config-settings,2681,https://hail.is,https://github.com/hail-is/hail/pull/13321,1,['config'],['config-settings']
Modifiability,"tps://redirect.github.com/python-pillow/Pillow/issues/7883"">#7883</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Improve speed of loading QOI images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7925"">#7925</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Added RGB to I;16N conversion <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7920"">#7920</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Add --report argument to <strong>main</strong>.py to omit supported formats <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7818"">#7818</a> [<a href=""https://github.com/nulano""><code>@​nulano</code></a>]</li>; <li>Added RGB to I;16, I;16L and I;16B conversion <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7918"">#7918</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fix editable installation with custom build backend and configuration options <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7658"">#7658</a> [<a href=""https://github.com/nulano""><code>@​nulano</code></a>]</li>; <li>Fix putdata() for I;16N on big-endian <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7209"">#7209</a> [<a href=""https://github.com/Yay295""><code>@​Yay295</code></a>]</li>; <li>Determine MPO size from markers, not EXIF data <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7884"">#7884</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Improved conversion from RGB to RGBa, LA and La <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7888"">#7888</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Support FITS images with GZIP_1 compression <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7894"">#7894</a> [<a href=""https://github.com/radarhere""><code>@​radarhere",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14439:1990,config,configuration,1990,https://hail.is,https://github.com/hail-is/hail/pull/14439,3,['config'],['configuration']
Modifiability,"tps://www.github.com/googleapis/python-logging/commit/e3a1eba74dd8b67bcc73a78f784189ef2a9927c2"">e3a1eba</a>)</li>; <li>use structured logging on GCF with python 3.7 (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/434"">#434</a>) (<a href=""https://www.github.com/googleapis/python-logging/commit/5055919f70c82b38de6d1fa7f1df6006865a857b"">5055919</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/python-logging/blob/main/CHANGELOG.md"">google-cloud-logging's changelog</a>.</em></p>; <blockquote>; <h2><a href=""https://github.com/googleapis/python-logging/compare/v2.7.0...v3.0.0"">3.0.0</a> (2022-01-27)</h2>; <h3>⚠ BREAKING CHANGES</h3>; <ul>; <li>make logging API more friendly to use (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/422"">#422</a>)</li>; <li>api consistency between HTTP and Gapic layers (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/375"">#375</a>)</li>; <li>support string-encoded json (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/339"">#339</a>)</li>; <li>Infer default resource in logger (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/315"">#315</a>)</li>; <li>support json logs (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/316"">#316</a>)</li>; <li>deprecate AppEngineHandler and ContainerEngineHandler (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/310"">#310</a>)</li>; </ul>; <h3>Features</h3>; <ul>; <li>add api key support (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/472"">#472</a>) (<a href=""https://github.com/googleapis/python-logging/commit/81ca8c616acb988be1fbecfc2a0b1a5b39280149"">81ca8c6</a>)</li>; <li>add json_fields ext",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11574:6073,layers,layers,6073,https://hail.is,https://github.com/hail-is/hail/pull/11574,1,['layers'],['layers']
Modifiability,"tqdm.notebook and pkg_resources take .55s and .37s to import, respectively. humanize imports pkg_resources which is why that is moved to. Moving these out of the top level improve cold-start time from 1.5s to .5s, and never even need to be imported for invocations such as `hailctl config list`, whose runtime is entirely import time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11210:282,config,config,282,https://hail.is,https://github.com/hail-is/hail/pull/11210,1,['config'],['config']
Modifiability,"ts/issues/5924"">#5924</a>)</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/psf/requests/blob/main/HISTORY.md"">requests's changelog</a>.</em></p>; <blockquote>; <h2>2.27.1 (2022-01-05)</h2>; <p><strong>Bugfixes</strong></p>; <ul>; <li>Fixed parsing issue that resulted in the <code>auth</code> component being; dropped from proxy URLs. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/6028"">#6028</a>)</li>; </ul>; <h2>2.27.0 (2022-01-03)</h2>; <p><strong>Improvements</strong></p>; <ul>; <li>; <p>Officially added support for Python 3.10. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5928"">#5928</a>)</p>; </li>; <li>; <p>Added a <code>requests.exceptions.JSONDecodeError</code> to unify JSON exceptions between; Python 2 and 3. This gets raised in the <code>response.json()</code> method, and is; backwards compatible as it inherits from previously thrown exceptions.; Can be caught from <code>requests.exceptions.RequestException</code> as well. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5856"">#5856</a>)</p>; </li>; <li>; <p>Improved error text for misnamed <code>InvalidSchema</code> and <code>MissingSchema</code>; exceptions. This is a temporary fix until exceptions can be renamed; (Schema-&gt;Scheme). (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/6017"">#6017</a>)</p>; </li>; <li>; <p>Improved proxy parsing for proxy URLs missing a scheme. This will address; recent changes to <code>urlparse</code> in Python 3.9+. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5917"">#5917</a>)</p>; </li>; </ul>; <p><strong>Bugfixes</strong></p>; <ul>; <li>; <p>Fixed defect in <code>extract_zipped_paths</code> which could result in an infinite loop; for some paths. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5851"">#5851</a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11528:4294,inherit,inherits,4294,https://hail.is,https://github.com/hail-is/hail/pull/11528,2,['inherit'],['inherits']
Modifiability,"turn_value(; File ""/opt/homebrew/lib/python3.10/site-packages/py4j/protocol.py"", line 326, in get_return_value; raise Py4JJavaError(; py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.backend.spark.SparkBackend.apply.; : java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x4d740d85) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed module @0x4d740d85; 	at org.apache.spark.storage.StorageUtils$.<init>(StorageUtils.scala:213); 	at org.apache.spark.storage.StorageUtils$.<clinit>(StorageUtils.scala); 	at org.apache.spark.storage.BlockManagerMasterEndpoint.<init>(BlockManagerMasterEndpoint.scala:109); 	at org.apache.spark.SparkEnv$.$anonfun$create$9(SparkEnv.scala:371); 	at org.apache.spark.SparkEnv$.registerOrLookupEndpoint$1(SparkEnv.scala:311); 	at org.apache.spark.SparkEnv$.create(SparkEnv.scala:359); 	at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:189); 	at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:277); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:458); 	at is.hail.backend.spark.SparkBackend$.configureAndCreateSparkContext(SparkBackend.scala:148); 	at is.hail.backend.spark.SparkBackend$.apply(SparkBackend.scala:230); 	at is.hail.backend.spark.SparkBackend.apply(SparkBackend.scala); 	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104); 	at java.base/java.lang.reflect.Method.invoke(Method.java:578); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:1589); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12630:2831,config,configureAndCreateSparkContext,2831,https://hail.is,https://github.com/hail-is/hail/issues/12630,1,['config'],['configureAndCreateSparkContext']
Modifiability,"type; (namely, <code>module</code>, <code>keyword</code>, <code>operator</code>, <code>object</code>, <code>exception</code>,; <code>statement</code>, and <code>builtin</code>) in the :rst:dir:<code>index</code> directive, and; set the removal version to Sphinx 9. Patch by Adam Turner.</li>; </ul>; <h2>Features added</h2>; <ul>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11415"">#11415</a>: Add a checksum to JavaScript and CSS asset URIs included within; generated HTML, using the CRC32 algorithm.</li>; <li>:meth:<code>~sphinx.application.Sphinx.require_sphinx</code> now allows the version; requirement to be specified as <code>(major, minor)</code>.</li>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11011"">#11011</a>: Allow configuring a line-length limit for object signatures, via; :confval:<code>maximum_signature_line_length</code> and the domain-specific variants.; If the length of the signature (in characters) is greater than the configured; limit, each parameter in the signature will be split to its own logical line.; This behaviour may also be controlled by options on object description; directives, for example :rst:dir:<code>py:function:single-line-parameter-list</code>.; Patch by Thomas Louf, Adam Turner, and Jean-François B.</li>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/10983"">#10983</a>: Support for multiline copyright statements in the footer block.; Patch by Stefanie Molin</li>; <li><code>sphinx.util.display.status_iterator</code> now clears the current line; with ANSI control codes, rather than overprinting with space characters.</li>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11431"">#11431</a>: linkcheck: Treat SSL failures as broken links.; Patch by Bénédikt Tran</li>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11157"">#11157</a>: Keep the <code>translated</code> attribute on translated nodes.</li>; <li><a href=""https://redirect.github.com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13295:2567,config,configured,2567,https://hail.is,https://github.com/hail-is/hail/pull/13295,1,['config'],['configured']
Modifiability,"ub-redirect.dependabot.com/psf/black/issues/3168"">#3168</a>)</li>; <li>When using <code>--skip-magic-trailing-comma</code> or <code>-C</code>, trailing commas are stripped from subscript expressions with more than 1 element (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3209"">#3209</a>)</li>; <li>Implicitly concatenated strings inside a list, set, or tuple are now wrapped inside parentheses (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3162"">#3162</a>)</li>; <li>Fix a string merging/split issue when a comment is present in the middle of implicitly concatenated strings on its own line (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3227"">#3227</a>)</li>; </ul>; <h3><em>Blackd</em></h3>; <ul>; <li><code>blackd</code> now supports enabling the preview style via the <code>X-Preview</code> header (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3217"">#3217</a>)</li>; </ul>; <h3>Configuration</h3>; <ul>; <li>Black now uses the presence of debug f-strings to detect target version (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3215"">#3215</a>)</li>; <li>Fix misdetection of project root and verbose logging of sources in cases involving <code>--stdin-filename</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3216"">#3216</a>)</li>; <li>Immediate <code>.gitignore</code> files in source directories given on the command line are now also respected, previously only <code>.gitignore</code> files in the project root and automatically discovered directories were respected (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3237"">#3237</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Recommend using BlackConnect in IntelliJ IDEs (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3150"">#3150</a>)</li>; </ul>; <h3>Integrations</h3>; <ul>; <li>Vim plugin: prefix messages with <code>Black: </code> so it's clear they come from Black (<a href=""ht",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12174:2841,Config,Configuration,2841,https://hail.is,https://github.com/hail-is/hail/pull/12174,1,['Config'],['Configuration']
Modifiability,"ucket containing the VEP cache data is still `aus-sydney`, meaning that the VEP data is not copied into the dataproc cluster, and when trying to run VEP I get the error `No cache found for homo_sapiens, version 95`. ### Version. 0.2.130. ### Relevant log output. ```shell; FatalError: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:1731,Plugin,Plugins,1731,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,"ugh, I'm going to have to rethink this or add `internal_ip` to the global config in `build.py`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10796#issuecomment-905875522:74,config,config,74,https://hail.is,https://github.com/hail-is/hail/pull/10796#issuecomment-905875522,1,['config'],['config']
Modifiability,uleBuilder.classesBytes(ClassBuilder.scala:152); 	at is.hail.expr.ir.EmitClassBuilder.resultWithIndex(EmitClassBuilder.scala:660); 	at is.hail.expr.ir.WrappedEmitClassBuilder.resultWithIndex(EmitClassBuilder.scala:155); 	at is.hail.expr.ir.WrappedEmitClassBuilder.resultWithIndex$(EmitClassBuilder.scala:155); 	at is.hail.expr.ir.EmitFunctionBuilder.resultWithIndex(EmitClassBuilder.scala:1052); 	at is.hail.expr.ir.Emit.$anonfun$emitI$225(Emit.scala:2315); 	at is.hail.expr.ir.IEmitCodeGen.map(Emit.scala:320); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:2256); 	at is.hail.expr.ir.Emit.emitI$3(Emit.scala:2458); 	at is.hail.expr.ir.Emit.$anonfun$emit$22(Emit.scala:2546); 	at is.hail.expr.ir.EmitCode$.fromI(Emit.scala:415); 	at is.hail.expr.ir.Emit.emit(Emit.scala:2545); 	at is.hail.expr.ir.Emit.emit$1(Emit.scala:591); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:624); 	at is.hail.expr.ir.Emit.$anonfun$emitVoidInSeparateMethod$1(Emit.scala:549); 	at is.hail.expr.ir.Emit.$anonfun$emitVoidInSeparateMethod$1$adapted(Emit.scala:547); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:985); 	at is.hail.expr.ir.Emit.emitVoidInSeparateMethod(Emit.scala:547); 	at is.hail.expr.ir.Emit.emitInSeparateMethod(Emit.scala:571); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:760); 	at is.hail.expr.ir.Emit.emitI$1(Emit.scala:600); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$26(Emit.scala:715); 	at is.hail.expr.ir.RelationalWriter.writeMetadata(TableWriter.scala:341); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:715); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3(Emit.scala:70); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3$adapted(Emit.scala:68); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWith,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533:19744,adapt,adapted,19744,https://hail.is,https://github.com/hail-is/hail/issues/12533,1,['adapt'],['adapted']
Modifiability,"un(ServerImpl.java:817); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:201); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:560); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:15864,Plugin,Plugins,15864,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,unction$(Backend.scala:122); E 	at is.hail.backend.local.LocalBackend.lookupOrCompileCachedFunction(LocalBackend.scala:73); E 	at is.hail.expr.ir.Compile$.apply(Compile.scala:39); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$5(CompileAndEvaluate.scala:66); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:66); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:19); E 	at is.hail.expr.ir.lowering.LowerDistributedSort$.distributedSort(LowerDistributedSort.scala:158); E 	at is.hail.backend.local.LocalBackend.lowerDistributedSort(LocalBackend.scala:331); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:67); E 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:14); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:167); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(Lowering,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:9926,Rewrite,RewriteBottomUp,9926,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"unts distinctlyKeyed firstKey; lastKey)]; }; !37 = ToArray(!s21); !38 = WriteMetadata(!37) [""{\""name\"":\""TableSpecWriter\"",\""path\"":\""/tmp/foo.ht\"",\""typ\"":{\""rowType\"":\""Struct{locus:Locus(GRCh38),alleles:Array[String],data:Array[Struct{}]}\"",\""key\"":[\""locus\"",\""alleles\""],\""globalType\"":\""Struct{new_globals:Array[Struct{}]}\""},\""rowRelPath\"":\""rows\"",\""globalRelPath\"":\""globals\"",\""refRelPath\"":\""references\"",\""log\"":true}""]; !39 = Begin(!34, !36, !38); WriteMetadata(!39) [""{\""name\"":\""RelationalWriter\"",\""path\"":\""/tmp/foo.ht\"",\""overwrite\"":true,\""maybeRefs\"":{\""references\"":[\""GRCh38\""]}}""]. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:23); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:23); 	at is.hail.utils.package$.fatal(package.scala:89); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:17); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:29); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:205); 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:249); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$2(LocalBackend.scala:314); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1(LocalBackend.scala:309); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1$adapted(LocalBackend.scala:308); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext$.$anonfun$sco",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14245:15954,adapt,adapted,15954,https://hail.is,https://github.com/hail-is/hail/issues/14245,1,['adapt'],['adapted']
Modifiability,update deploy script to generate some cloudtools configs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4248:49,config,configs,49,https://hail.is,https://github.com/hail-is/hail/pull/4248,1,['config'],['configs']
Modifiability,update query on batch tmp config flag,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13258:26,config,config,26,https://hail.is,https://github.com/hail-is/hail/pull/13258,1,['config'],['config']
Modifiability,upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 0; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 		at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 		at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:297); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:279); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:297); 		at is.hail.io.fs.FSPositionedOutputStream.write(FS.scala:219); 		at java.io.DataOutputStream.write(DataOutputStream.java:107); 		at is.hail.fs.FSSuite.$anonfun$testSeekMoreThanMaxInt$1(FSSuite.scala:329); 		at is.hail.fs.FSSuite.$anonfun$testSeekMoreThanMaxInt$1$adapted(FSSuite.scala:323); 		at is.hail.utils.package$.using(package.scala:635); 		... 26 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:9019,adapt,adapted,9019,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['adapt'],['adapted']
Modifiability,"ure storage. Basic summary of the changes:; 	- Update `AzureAsyncFS` url parsing function to look for and separate out a SAS-token-like query string. Note: made fairly specific to SAS tokens - generic detection of query string syntax interferes with glob support and '?' characters in file names; 	- Added `generate_sas_token` convenience function to `AzureAsyncFS`. Adds new `azure-mgmt-storage` package requirement.; 	- Updated `AzureAsyncFS` to use `(account, credential)` tuple as internal `BlobServiceClient` cache key; 	- Updated `AzureAsyncFSURL` and `AzureFileListEntry` to track the token separately from the name, and extend the base classes to allow returning url with or without a token ; 	- Update `RouterFS.ls` function and associated `listfiles` function to allow for trailing query strings during path traversal ; 	- Change to existing behavior: `LocalAsyncFSURL.__str__`no longer returns 'file:' prefix. Done to make `str()` output be appropriate for input to `fs` functions across all subclasses; 	- Updated `inter_cloud/test_fs.py` to generically use query-string-friendly file path building functions; - Updated InputResource to not include the SAS token as part of the destination file name . `test_fs.py` has been updated to respect the new model, where it is no longer safe to extend URLs by just appending new segments with + ""/"" because there may be a query string. But actually running those tests for the SAS case will require some new test variables to allow the test code to generate SAS tokens (`build.yaml/test_hail_python_fs`): ; ```; export HAIL_TEST_AZURE_ACCOUNT=hailtest; export HAIL_TEST_AZURE_CONTAINER=hail-test-4nxei; # Required for SAS testing on Azure; export HAIL_TEST_AZURE_RESGRP=hailms02; export HAIL_TEST_AZURE_SUBID=12ab51c6-da79-4a99-8dec-3d2decc97343; ```; So the SAS case is disabled for now (`test_fs.py`):; ```; @pytest.fixture(params=['file', 'gs', 's3', 'hail-az', 'router/file', 'router/gs', 'router/s3', 'router/hail-az']) # 'sas/hail-az'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12877:1415,extend,extend,1415,https://hail.is,https://github.com/hail-is/hail/pull/12877,2,"['extend', 'variab']","['extend', 'variables']"
Modifiability,"url configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:4515,config,configuration,4515,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['config'],['configuration']
Modifiability,"us-east4/us-west1/us-west2/us-west3/us-west4]: us-east1; WARNING: remote temporary directory ""gs://hail-batch-jigold-oxmmp/bar/foo"" is not located in the selected compute region for Batch jobs ""us-east1"".; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp/bar/foo; batch/regions=us-east1; batch/backend=service; query/backend=batch; WARNING: Initialized Hail with warnings! The currently specified configuration will result in additional ingress and egress fees when using Hail Batch.; ```. Existing multiregional bucket:. ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-jigold-test-multi-regional; WARNING: remote temporary directory gs://hail-jigold-test-multi-regional is multi-regional. Using this bucket with the Batch Service will incur addtional ingress and egress fees.; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-jigold-test-multi-regional? [y/n]: y; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-jigold-test-multi-regional.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-east1; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-jigold-test-multi-regional; batch/regions=us-east1; batch/backend=service; query/backend=batch; WARNING: Initialized Hail with warnings! The currently specified configuration will result in additional ingress and egress fees when using Hail Batch.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:6658,CONFIG,CONFIGURATION,6658,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,2,"['CONFIG', 'config']","['CONFIGURATION', 'configuration']"
Modifiability,"use <code>#!/bin/sh</code> on windows for hook script.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2182"">#2182</a> issue by <a href=""https://github.com/hushigome-visco""><code>@​hushigome-visco</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2187"">#2187</a> PR by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; </ul>; <h2>pre-commit v2.16.0</h2>; <h3>Features</h3>; <ul>; <li>add warning for regexes containing <code>[\/]</code> or <code>[/\\]</code>.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2053"">#2053</a> PR by <a href=""https://github.com/radek-sprta""><code>@​radek-sprta</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2043"">#2043</a> issue by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; <li>move hook template back to <code>bash</code> resolving shebang-portability issues.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2065"">#2065</a> PR by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; <li>add support for <code>fail_fast</code> at the individual hook level.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2097"">#2097</a> PR by <a href=""https://github.com/colens3""><code>@​colens3</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/1143"">#1143</a> issue by <a href=""https://github.com/potiuk""><code>@​potiuk</code></a>.</li>; </ul>; </li>; <li>allow passthrough of <code>GIT_CONFIG_KEY_*</code>, <code>GIT_CONFIG_VALUE_*</code>, and <code>GIT_CONFIG_COUNT</code>.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2136"">#2136</a> PR by <a href=""https://github.com/emzeat""><code>@​emzeat</code></a>.</li>; </u",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11460:4553,portab,portability,4553,https://hail.is,https://github.com/hail-is/hail/pull/11460,2,['portab'],['portability']
Modifiability,"use-overlay, we might need to modify the VM image to include fuse-overlay. ```; + set +x; Using GOOGLE_APPLICATION_CREDENTIALS; + export TMPDIR=/io/; + TMPDIR=/io/; + retry buildah build -t us-docker.pkg.dev/hail-vdc/hail/git-make-bash:test-deploy-j6d7pph9mlzf -f /Dockerfile --cache-from us-docker.pkg.dev/hail-vdc/hail/cache --cache-to us-docker.pkg.dev/hail-vdc/hail/cache --layers /io; + buildah build -t us-docker.pkg.dev/hail-vdc/hail/git-make-bash:test-deploy-j6d7pph9mlzf -f /Dockerfile --cache-from us-docker.pkg.dev/hail-vdc/hail/cache --cache-to us-docker.pkg.dev/hail-vdc/hail/cache --layers /io; STEP 1/2: FROM us-docker.pkg.dev/hail-vdc/hail/ubuntu:20.04; Trying to pull us-docker.pkg.dev/hail-vdc/hail/ubuntu:20.04...; Getting image source signatures; Copying blob sha256:ca1778b6935686ad781c27472c4668fc61ec3aeb85494f72deb1921892b9d39e; Copying config sha256:88bd6891718934e63638d9ca0ecee018e69b638270fe04990a310e5c78ab4a92; Writing manifest to image destination; Storing signatures; time=\""2023-05-26T14:52:12Z\"" level=error msg=\""Unmounting /var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/merged: invalid argument\""; Error: mounting new container: mounting build container \""45e0ed631d22b6e1de7945266efcf0b802aa3b919d6b6ebd529ded6fedc11cf9\"": creating overlay mount to /var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/merged, mount_data=\""lowerdir=/var/lib/containers/storage/overlay/l/ZCKOX3GV2VWHWT4DMPLYJGMJWL,upperdir=/var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/diff,workdir=/var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/work,nodev,fsync=0,volatile\"": using mount program /usr/bin/fuse-overlayfs: unknown argument ignored: lazytime; fuse: device not found, try 'modprobe fuse' first; fuse-overlayfs: cannot mount: No such file or directory; : exit status 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13103#issuecomment-1564774692:1039,config,config,1039,https://hail.is,https://github.com/hail-is/hail/pull/13103#issuecomment-1564774692,1,['config'],['config']
Modifiability,"users. At worst, this would require another `hailctl auth login`. This is the patch I _wanted_ to write. ```patch; From aef878903d9249b542522082cba705eaf26d728a Mon Sep 17 00:00:00 2001; From: Christopher Vittal <christopher.vittal@gmail.com>; Date: Wed, 25 Sep 2019 14:55:42 -0400; Subject: [PATCH] [hailctl] Move default location for hail config directory; MIME-Version: 1.0; Content-Type: text/plain; charset=UTF-8; Content-Transfer-Encoding: 8bit. Now we try, in order:; $XDG_CONFIG_HOME/hail; ~/.config/hail. The XDG Base Directory Specification[1] is a freedesktop spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__ini",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:1123,config,config,1123,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['config'],['config']
Modifiability,"ustomPlugin and DescribeConnector responses to return errors from asynchronous resource creation.</li>; </ul>; <h1>1.21.9</h1>; <ul>; <li>api-change:<code>finspace-data</code>: [<code>botocore</code>] Add new APIs for managing Users and Permission Groups.</li>; <li>api-change:<code>amplify</code>: [<code>botocore</code>] Add repositoryCloneMethod field for hosting an Amplify app. This field shows what authorization method is used to clone the repo: SSH, TOKEN, or SIGV4.</li>; <li>api-change:<code>fsx</code>: [<code>botocore</code>] This release adds support for the following FSx for OpenZFS features: snapshot lifecycle transition messages, force flag for deleting file systems with child resources, LZ4 data compression, custom record sizes, and unsetting volume quotas and reservations.</li>; <li>api-change:<code>fis</code>: [<code>botocore</code>] This release adds logging support for AWS Fault Injection Simulator experiments. Experiment templates can now be configured to send experiment activity logs to Amazon CloudWatch Logs or to an S3 bucket.</li>; <li>api-change:<code>route53-recovery-cluster</code>: [<code>botocore</code>] This release adds a new API option to enable overriding safety rules to allow routing control state updates.</li>; <li>api-change:<code>amplifyuibuilder</code>: [<code>botocore</code>] We are adding the ability to configure workflows and actions for components.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/boto3/commit/67b84e02c185294c54a8e49510d4cb962e89cee2""><code>67b84e0</code></a> Merge branch 'release-1.21.13'</li>; <li><a href=""https://github.com/boto/boto3/commit/99acd545b20fe30ffa2f589a674c5a7ad74c266b""><code>99acd54</code></a> Bumping version to 1.21.13</li>; <li><a href=""https://github.com/boto/boto3/commit/83a8f662655bada44d442df7f33cb20d71ead257""><code>83a8f66</code></a> Add changelog entries from botocore",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11504:5007,config,configured,5007,https://hail.is,https://github.com/hail-is/hail/pull/11504,1,['config'],['configured']
Modifiability,"ustomPlugin and DescribeConnector responses to return errors from asynchronous resource creation.</li>; </ul>; <h1>1.21.9</h1>; <ul>; <li>api-change:<code>finspace-data</code>: [<code>botocore</code>] Add new APIs for managing Users and Permission Groups.</li>; <li>api-change:<code>amplify</code>: [<code>botocore</code>] Add repositoryCloneMethod field for hosting an Amplify app. This field shows what authorization method is used to clone the repo: SSH, TOKEN, or SIGV4.</li>; <li>api-change:<code>fsx</code>: [<code>botocore</code>] This release adds support for the following FSx for OpenZFS features: snapshot lifecycle transition messages, force flag for deleting file systems with child resources, LZ4 data compression, custom record sizes, and unsetting volume quotas and reservations.</li>; <li>api-change:<code>fis</code>: [<code>botocore</code>] This release adds logging support for AWS Fault Injection Simulator experiments. Experiment templates can now be configured to send experiment activity logs to Amazon CloudWatch Logs or to an S3 bucket.</li>; <li>api-change:<code>route53-recovery-cluster</code>: [<code>botocore</code>] This release adds a new API option to enable overriding safety rules to allow routing control state updates.</li>; <li>api-change:<code>amplifyuibuilder</code>: [<code>botocore</code>] We are adding the ability to configure workflows and actions for components.</li>; <li>api-change:<code>athena</code>: [<code>botocore</code>] This release adds support for updating an existing named query.</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] This release adds support for new AMI property 'lastLaunchedTime'</li>; <li>api-change:<code>servicecatalog-appregistry</code>: [<code>botocore</code>] AppRegistry is deprecating Application and Attribute-Group Name update feature. In this release, we are marking the name attributes for Update APIs as deprecated to give a heads up to our customers.</li>; </ul>; <h1>1.21.8</h1>; <ul>; <li>api-chan",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11486:3560,config,configured,3560,https://hail.is,https://github.com/hail-is/hail/pull/11486,1,['config'],['configured']
Modifiability,"ut. ```shell; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.128-17247d8990c6; LOGGING: writing to /home/edmund/.local/src/hail/hail-20240508-1553-0.2.128-17247d8990c6.log; Traceback (most recent call last):; File ""/home/edmund/.local/share/pyenv/versions/3.9.18/lib/python3.9/runpy.py"", line 197, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/home/edmund/.local/share/pyenv/versions/3.9.18/lib/python3.9/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py"", line 39, in <module>; cli.main(); File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 430, in main; run(); File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 284, in run_file; runpy.run_path(target, run_name=""__main__""); File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 321, in run_path; return _run_module_code(code, init_globals, run_name,; File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 135, in _run_module_code; _run_code(code, mod_globals, init_globals,; File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 124, in _run_code; exec(code, run_globals); File ""/home/edmund/.local/src/hail/test.py"", line 34, in <module>; main(); File ""/home/edmund/.local/src/hail/test.py"", line 28, in main; r2_adj = r2_adj.checkpoint(f'{tmp}/adj', overwrite=args.overwrite); Fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14537:2317,adapt,adapter,2317,https://hail.is,https://github.com/hail-is/hail/issues/14537,1,['adapt'],['adapter']
Modifiability,util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.lang.RuntimeException: Stream is already closed.; E 	at com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:79); E 	at com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:89); E 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.$anonfun$call$1(ServiceBackend.scala:122); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E ; E ; E ; E Hail version: 0.2.115-f6017673dbb6; E Error summary: RuntimeException: Stream is already closed. /usr/local/lib/python3.8/dist-packages/hail/backend/service_backend.py:477: FatalError; ------------------------------ Captured log call ----,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:22448,adapt,adapted,22448,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['adapt'],['adapted']
Modifiability,util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.lang.RuntimeException: Stream is already closed.; E 	at com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:79); E 	at com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:89); E 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.$anonfun$call$1(ServiceBackend.scala:122); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E ; E ; E ; E Hail version: 0.2.115-f6017673dbb6; E Error summary: RuntimeException: Stream is already closed.; ```. ### Version. 0.2.115-f6017673dbb6. ### Relevant log output. ```shell; ________________________________ test_spectra_4 _________________,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:8830,adapt,adapted,8830,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['adapt'],['adapted']
Modifiability,utionTimer$.time(ExecutionTimer.scala:52); E 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.withExecuteContext$1(ServiceBackend.scala:535); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:602); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$7(ServiceBackend.scala:433); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$7$adapted(ServiceBackend.scala:432); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6(ServiceBackend.scala:432); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:77); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5(ServiceBackend.scala:432); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5$adapted(ServiceBackend.scala:430); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:430); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:77); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:430); E 	at is.hail.backend.service.Main$.main(Main.scala:33); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:3180,adapt,adapted,3180,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['adapt'],['adapted']
Modifiability,"v oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaults. File paths should be determined by the location, which is loaded from HAIL_DEPLOY_CONFIG_FILE. If that isn't set, I look in ~/.hail/deploy_config.json, and if that doesn't exist, use external/default. All other configuration files are determined by the location: the tokens file is in ~/.hail/tokens.json for external, in /user-tokens/tokens.json for k8s, etc. What remains:; - what a `hailctl dev config` to set the (local) deploy config for switching between default and dev namespaces.; - salt session IDs in the database; - dev oauth2 key; - add `dev deploy` service override option so we can use production/default auth with services deployed in dev namespaces. These are all pretty easy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:1983,config,configuration,1983,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251,4,['config'],"['config', 'configuration']"
Modifiability,"v/pytest/commit/6ca733e8f19fa5c4271bf3e5bb295c8b62757e4a""><code>6ca733e</code></a> Enable testing with Python 3.11 (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9511"">#9511</a>)</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/ac37b1b1139eaa71b3bcb16b630abfc0223241ef""><code>ac37b1b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9671"">#9671</a> from nicoddemus/backport-9668</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/c891e402ac28f20dd3d018dc25f1ea1a273997be""><code>c891e40</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9672"">#9672</a> from nicoddemus/backport-9669</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/e2753a2b8b55de73adcc992036d0dc52facdbab9""><code>e2753a2</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9669"">#9669</a> from hugovk/ci-only-update-plugin-list-for-upstream</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/b5a154c1d961dbc19a3c00d798de2f27aaa5ace5""><code>b5a154c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9668"">#9668</a> from hugovk/test-me-latest-3.10</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/0fae45bb6e4ecf177afdfa3bf03738813ec7b913""><code>0fae45b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9660"">#9660</a> from pytest-dev/backport-9646-to-7.0.x</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/37d434f5fcb5f80188b3d5b8f22d418dc191b955""><code>37d434f</code></a> [7.0.x] Delay warning about collector/item diamond inheritance</li>; <li>Additional commits viewable in <a href=""https://github.com/pytest-dev/pytest/compare/6.2.5...7.0.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11516:5643,plugin,plugin-list-for-upstream,5643,https://hail.is,https://github.com/hail-is/hail/pull/11516,3,['plugin'],['plugin-list-for-upstream']
Modifiability,"va.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:16173,Plugin,Plugins,16173,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,va.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.e,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:19443,adapt,adapted,19443,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['adapt'],['adapted']
Modifiability,"valid importing of <code>importlib.readers</code> in Python 3.9.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9610"">#9610</a>: Restore [UnitTestFunction.obj]{.title-ref} to return unbound rather than bound method.; Fixes a crash during a failed teardown in unittest TestCases with non-default [__init__]{.title-ref}.; Regressed in pytest 7.0.0.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9636"">#9636</a>: The <code>pythonpath</code> plugin was renamed to <code>python_path</code>. This avoids a conflict with the <code>pytest-pythonpath</code> plugin.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9642"">#9642</a>: Fix running tests by id with <code>::</code> in the parametrize portion.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9643"">#9643</a>: Delay issuing a <code>~pytest.PytestWarning</code>{.interpreted-text role=&quot;class&quot;} about diamond inheritance involving <code>~pytest.Item</code>{.interpreted-text role=&quot;class&quot;} and; <code>~pytest.Collector</code>{.interpreted-text role=&quot;class&quot;} so it can be filtered using <code>standard warning filters &lt;warnings&gt;</code>{.interpreted-text role=&quot;ref&quot;}.</li>; </ul>; <h2>7.0.0</h2>; <h1>pytest 7.0.0 (2022-02-03)</h1>; <p>(<strong>Please see the full set of changes for this release also in the 7.0.0rc1 notes below</strong>)</p>; <h2>Deprecations</h2>; <ul>; <li>; <p><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9488"">#9488</a>: If custom subclasses of nodes like <code>pytest.Item</code>{.interpreted-text role=&quot;class&quot;} override the; <code>__init__</code> method, they should take <code>**kwargs</code>. See; <code>uncooperative-constructors-deprecated</code>{.interpreted-text role=&quot;ref&quot;} for details.</p>; <p>Note that a deprection warning is only emitted when there is a conflict in the; arguments p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11516:1431,inherit,inheritance,1431,https://hail.is,https://github.com/hail-is/hail/pull/11516,3,['inherit'],['inheritance']
Modifiability,"vals), cond2)`. A common example is when `cond` can be completely captured by the interval filter, i.e. `cond` is equivant to `intervals.contains(key)`, in which case we can take `cond2 = True`, and the `TableFilter` can be optimized away. This all happens in the function; ```scala; def extractPartitionFilters(ctx: ExecuteContext, cond: IR, ref: Ref, key: IndexedSeq[String]): Option[(IR, IndexedSeq[Interval])] = {; if (key.isEmpty) None; else {; val extract = new ExtractIntervalFilters(ctx, ref.typ.asInstanceOf[TStruct].typeAfterSelectNames(key)); val trueSet = extract.analyze(cond, ref.name); if (trueSet == extract.KeySetLattice.top); None; else {; val rw = extract.Rewrites(mutable.Set.empty, mutable.Set.empty); extract.analyze(cond, ref.name, Some(rw), trueSet); Some((extract.rewrite(cond, rw), trueSet)); }; }; }; ```; `trueSet` is the set of intervals which contains all rows where `cond` is true. This set is passed back into `analyze` in a second pass, which asks it to rewrite `cond` to something equivalent, under the assumption that all keys are contained in `trueSet`. The abstraction of runtime values tracks two types of information:; * Is this value a reference to / copy of one of the key fields of this row? We need to know this to be able to recognize comparisons with key values, which we want to extract to interval filters.; * For boolean values (including, ultimately, the filter predicate itself), we track three sets of intervals of the key type: overapproximations of when the bool is true, false, and missing. Overapproximation here means, for example, if the boolean evaluates to true in some row with key `k`, then `k` must be contained in the ""true"" set of intervals. But it's completely fine if the set of intervals contains keys of rows where the bool is not true. In particular, a boolean about which we know nothing (e.g. it's just some non-key boolean field in the dataset) is represented by an abstract boolean value where all three sets are the set of all ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13355:2533,rewrite,rewrite,2533,https://hail.is,https://github.com/hail-is/hail/pull/13355,1,['rewrite'],['rewrite']
Modifiability,"vc9f</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/740380c59ca2a7c2dceca19e5dba99f6b7060e62""><code>740380c</code></a> Bump cryptography from 41.0.3 to 41.0.4 (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3131"">#3131</a>)</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/d9f85a749488188c286cd50606d159874db94d5f""><code>d9f85a7</code></a> Release 2.0.5</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/d41f4122966f7f4f5f92001ad518e5d9dafcc886""><code>d41f412</code></a> Undeprecate pyOpenSSL module (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3127"">#3127</a>)</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/b6c04cb3e62ef5a0e4947d037c12fb3ca79e024a""><code>b6c04cb</code></a> Fix a link to &quot;absolute URI&quot; definition (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3128"">#3128</a>)</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/af7c78fa30f5a4e265911371d0c59b6baeddca0f""><code>af7c78f</code></a> refactor: change double conditional to one (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3118"">#3118</a>)</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/34c13c8e68df6f89890ba08b9fc4fbf87ed21669""><code>34c13c8</code></a> Refer to current internet standards in docs on proxies (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3124"">#3124</a>)</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/a3e94f218cd8297db73302eadae235f0c832a809""><code>a3e94f2</code></a> Fix a name of an attribute in docs (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3125"">#3125</a>)</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/da69d4f4f95bc7ef9307fc8e0499c2121f1e4791""><code>da69d4f</code></a> Fix docs build (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3123"">#3123</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/urllib3/urllib3/compare/1.26.16...2.0.6"">compare view</a></li>; </ul>; </de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13768:13140,refactor,refactor,13140,https://hail.is,https://github.com/hail-is/hail/pull/13768,2,['refactor'],['refactor']
Modifiability,"veLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/FS.o -o lib/linux-x86-64/libhail.so; cp -p -f lib/linux-x86-64/libboot.so lib/linux-x86-64/libhail.so ../../../prebuilt/lib/linux-x86-64/; make[1]: Leaving directory `/mnt/tmp/hail/hail/src/main/c'; ./gradlew shadowJar -Dscala.version=2.12.15 -Dspark.version=3.3.2 -Delasticsearch.major-version=7; Downloading https://services.gradle.org/distributions/gradle-8.3-bin.zip; ............10%............20%.............30%............40%.............50%............60%.............70%............80%.............90%............100%. Welcome to Gradle 8.3!. Here are the highlights of this release:; - Faster Java compilation; - Reduced memory usage; - Support for running on Java 20. For more details see https://docs.gradle.org/8.3/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; WARNING: Hail primarily tested with Spark 3.3.0, use other versions at your own risk. > Task :shadedazure:compileJava NO-SOURCE; > Task :shadedazure:processResources NO-SOURCE; > Task :shadedazure:classes UP-TO-DATE; > Task :shadedazure:shadowJar; > Task :compileJava NO-SOURCE; > Task :compileScala; > Task :processResources; > Task :classes; > Task :shadowJar. BUILD SUCCESSFUL in 4m 20s; 4 actionable tasks: 4 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; WARNING: The wheel packa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:15919,Config,Configure,15919,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['Config'],['Configure']
Modifiability,"ved the code). - `.strip()` the GitHub token in case there are newlines. - print the SHA being deployed in the log statement. - add `hail-ci-build.sh` to CI, which just invokes `make test-in-cluster`(which in turn runs `test-in-cluster.sh`. - `test-in-cluster.sh` copies the secrets for testing to the expected locations and exposes the pod in which it is running with an internal service, recent changes to `site` [redirect sub URLs of ci.test.is to services named using this scheme](https://github.com/hail-is/hail/blob/master/site/hail.nginx.conf#L38-L41). GitHub uses these URLs to send updates to the CI under test about the watched repositories. - `test-locally.sh` now installs `../batch` into the currently running `pip` before testing (NB: if you edit batch and run the tests without committing the changes you've made to batch, this will pass tests but fail when pushed to a PR!). - `test-locally.sh` activates the `hail-ci` conda environment itself because it was not being propagated from the `Makefile`. I don't know why, but this is a simple fix. - `test-locally.sh` starts the ci after the repository is created. CI will print error messages if a watched repository doesn't exist. - `test/test-ci.py` now uses access tokens for all interaction with GitHub, previously it relied on the latent privileges that I and Cotton had in our environments. - `test/test-ci.py` uses a temporary, but not automatically deleted, directory when the environment variable `IN_CLUSTER` is set to `true` (to which it is set by `test-in-cluster.sh`). I noticed that, when running in a batch job pod, if an error occurred, `pytest` failed to print any error information and instead failed because the current working directory no longer existed. I found very little information on Google about this. It seems safe to not clean up temporary directories created in the batch job pod because pods are ephemeral. cc: @cseed. Assigning to @tpoterba since he has the most context on this stuff other than Cotton.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4474:1929,variab,variable,1929,https://hail.is,https://github.com/hail-is/hail/pull/4474,1,['variab'],['variable']
Modifiability,"ver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:817); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:201); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:560); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:15675,Plugin,Plugins,15675,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,['Plugin'],['Plugins']
Modifiability,"ver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:80); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:848); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:817); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:201); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:560); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:15441,plugin,plugin,15441,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,"['Plugin', 'plugin']","['Plugins', 'plugin']"
Modifiability,"wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very worried ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:1002,layers,layers,1002,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358,2,['layers'],['layers']
Modifiability,"we hardcode the scala version to 2.11 at the moment:. https://github.com/hail-is/hail/blob/9ef5ebb362e70633a8e29b81768ad92f2853e6cb/hail/build.gradle#L37-L38. so this is pretty much to be expected. We'll make this flexible when Google Dataproc releases a GA image using 2.12, but could certainly document it before then.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8009#issuecomment-580354782:214,flexible,flexible,214,https://hail.is,https://github.com/hail-is/hail/issues/8009#issuecomment-580354782,1,['flexible'],['flexible']
Modifiability,"we now have our dev test environment running with hail 0.2.126 and this query took ~92 seconds. So faster, but we do still need some performance enhancements. @ehigham let me know what would be helpful for you to get you started on this effort",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1795785654:145,enhance,enhancements,145,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1795785654,1,['enhance'],['enhancements']
Modifiability,"we recommend that users check that; their own code does not use deprecated SciPy functionality (to do so,; run your code with <code>python -Wd</code> and check for <code>DeprecationWarning</code> s).; Our development attention will now shift to bug-fix releases on the; 1.8.x branch, and on adding new features on the master branch.</p>; <p>This release requires Python <code>3.8+</code> and <code>NumPy 1.17.3</code> or greater.</p>; <p>For running on PyPy, PyPy3 <code>6.0+</code> is required.</p>; <h1>Highlights of this release</h1>; <ul>; <li>A sparse array API has been added for early testing and feedback; this; work is ongoing, and users should expect minor API refinements over; the next few releases.</li>; <li>The sparse SVD library PROPACK is now vendored with SciPy, and an interface; is exposed via <code>scipy.sparse.svds</code> with <code>solver='PROPACK'</code>. It is currently; default-off due to potential issues on Windows that we aim to; resolve in the next release, but can be optionally enabled at runtime for; friendly testing with an environment variable setting of <code>USE_PROPACK=1</code>.</li>; <li>A new <code>scipy.stats.sampling</code> submodule that leverages the <code>UNU.RAN</code> C; library to sample from arbitrary univariate non-uniform continuous and; discrete distributions</li>; <li>All namespaces that were private but happened to miss underscores in; their names have been deprecated.</li>; </ul>; <h1>New features</h1>; <h1><code>scipy.fft</code> improvements</h1>; <p>Added an <code>orthogonalize=None</code> parameter to the real transforms in <code>scipy.fft</code>; which controls whether the modified definition of DCT/DST is used without; changing the overall scaling.</p>; <p><code>scipy.fft</code> backend registration is now smoother, operating with a single</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/scipy/scipy/commit/b5d8",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11538:1793,variab,variable,1793,https://hail.is,https://github.com/hail-is/hail/pull/11538,1,['variab'],['variable']
Modifiability,"we should extend the valid locus range, I think. For now you can import with `reference_genome=None`, which will import the chr/pos as a string/int instead of a Hail locus type.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6339#issuecomment-501674970:10,extend,extend,10,https://hail.is,https://github.com/hail-is/hail/issues/6339#issuecomment-501674970,1,['extend'],['extend']
Modifiability,"with `eq`. That way `freshName` becomes just `new Name()`, with stronger guarantees that the new name doesn't occur anywhere in the current IR, without needing to maintain global state as we do now.; * get rid of `NormalizeNames`, instead enforcing the global uniqueness of names as a basic invariant of the IR (typecheck could also check this invariant); * keep a string in the `Name`, but no longer require it to be unique. Instead it's just a suggestion for how to show the name in printouts, adding a uniqueifying suffix as needed. With `NormalizeNames` gone, this would let us preserve meaningful variable names further in the lowering pipeline.; * possibly keep other state in the `Name`, for example to allow a more efficient implementation of environments, similar to the `mark` state on `BaseIR`. This is obviously a large change, but there are only a few conceptual pieces (appologies for not managing to separate these out):; * attempt to minimize the number of locations in which the `Name` constructor is called, to make future refactorings easier; * add `freshName()`, which just wraps `genUID()`, returning a `Name`; * convert IR construction to use the convenience methods in `ir.package`, which take scala lambdas to represent blocks with bound variables, instead of manually creating new variable names; * replace uses of the magic constant variable names (`row`, `va`, `sa`, `g`, `global`) with constants (`TableIR.{rowName, globalName}`, `MatrixIR.{rowName, colName, entryName, globalName}`); * the above changes modified the names we use for bound variables in many places. That shouldn't matter, but it cought a couple bugs where it did.; * `NormalizeNames` optionally allows the IR to contain free variables. But it didn't do anything to ensure the newly generated variable names are distinct from any contained free variables. Thus it was possible to rename a bound variable to mistakenly capture a contained free variable. I've fixed that.; * `SimplifySuite` compared simplifi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14547:1372,refactor,refactorings,1372,https://hail.is,https://github.com/hail-is/hail/pull/14547,1,['refactor'],['refactorings']
Modifiability,"without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as follows:. 1. Envoy-based gateways and internal-gateways will load their routing configuration from a Kubernetes ConfigMap, which they watch for changes and reconcile their configuration when the ConfigMap changes. The ConfigMap can be populated with a manual deploy and is populated from the beginning with production routes (i.e. batch.hail.is gets routed to batch.default); 2. When running CI, CI will regularly update the ConfigMap with additional routes based on which internal namespaces (dev and PR) are currently active. This requires relatively small changes to CI to track active namespaces but overall is a pretty small change. Note that this does not introduce a dependency on CI to support production traffic, only development traffic.; 3. Deployments that run more than 1 replica (but really can be all of them) are run behind Headless Services, which expose the underlying pod IPs so Envoy can handle load-balancing instead of kube-proxy. This allows Envoy to make smart load-balancing decisions and correctly enforce rate-limiting when using connection pools. The namespace tracking in CI in Point 2 is possible before we make any changes to our networking, so that comes first in #12093. Point 3 is taken care of in #12094, and the rest of Point 2 and Point 1, everything to do with Envoy, is in this PR. ### Additional QoL improvements; - Envoy by default exposes Prometheus metrics that we can use to easily monitor things like rate-limiting, request failures and durations; - Since all Envoy configuration is in the configmap, we don't need to build any images. I suppose we could have done this with NGINX, so this isn't something to fault NGINX for. Just another small win buried in these changes. cc @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:5664,config,configuration,5664,https://hail.is,https://github.com/hail-is/hail/pull/12095,2,['config'],"['configmap', 'configuration']"
Modifiability,"wtf. It passes in azure, passes on my laptop under a number of configurations. passes in google on everything but QoB. What's going on",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12588#issuecomment-1397706166:63,config,configurations,63,https://hail.is,https://github.com/hail-is/hail/pull/12588#issuecomment-1397706166,1,['config'],['configurations']
Modifiability,"xed</h3>; <ul>; <li>Fixed Text.expand_tabs assertion error</li>; </ul>; <h2>v13.5.1</h2>; <p>Very minor update to URL highlighting</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/Textualize/rich/blob/master/CHANGELOG.md"">rich's changelog</a>.</em></p>; <blockquote>; <h2>[13.7.1] - 2023-02-28</h2>; <h3>Fixed</h3>; <ul>; <li>Updated the widths of some characters <a href=""https://redirect.github.com/Textualize/rich/pull/3289"">Textualize/rich#3289</a></li>; </ul>; <h2>[13.7.0] - 2023-11-15</h2>; <h3>Added</h3>; <ul>; <li>Adds missing parameters to Panel.fit <a href=""https://redirect.github.com/Textualize/rich/issues/3142"">Textualize/rich#3142</a></li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Some text goes missing during wrapping when it contains double width characters <a href=""https://redirect.github.com/Textualize/rich/issues/3176"">Textualize/rich#3176</a></li>; <li>Ensure font is correctly inherited in exported HTML <a href=""https://redirect.github.com/Textualize/rich/issues/3104"">Textualize/rich#3104</a></li>; <li>Fixed typing for <code>FloatPrompt</code>.</li>; </ul>; <h2>[13.6.0] - 2023-09-30</h2>; <h3>Added</h3>; <ul>; <li>Added Python 3.12 to classifiers.</li>; </ul>; <h2>[13.5.3] - 2023-09-17</h2>; <h3>Fixed</h3>; <ul>; <li>Markdown table rendering issue with inline styles and links <a href=""https://redirect.github.com/Textualize/rich/issues/3115"">Textualize/rich#3115</a></li>; <li>Fix Markdown code blocks on a light background <a href=""https://redirect.github.com/Textualize/rich/issues/3123"">Textualize/rich#3123</a></li>; </ul>; <h2>[13.5.2] - 2023-08-01</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs assertion error</li>; </ul>; <h2>[13.5.1] - 2023-07-31</h2>; <h3>Fixed</h3>; <ul>; <li>Fix tilde character (<code>~</code>) not included in link regex when printing to console <a href=""https://redirect.github.com/Textualize/rich/issues/3057",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14376:2847,inherit,inherited,2847,https://hail.is,https://github.com/hail-is/hail/pull/14376,2,['inherit'],['inherited']
Modifiability,xpr.ir.EmitFunctionBuilder.emitWithBuilder(EmitClassBuilder.scala:1489); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:121); 	at is.hail.expr.ir.Compile$.$anonfun$apply$4(Compile.scala:90); 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction(Backend.scala:279); 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction$(Backend.scala:274); 	at is.hail.backend.spark.SparkBackend.lookupOrCompileCachedFunction(SparkBackend.scala:298); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:50); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$5(CompileAndEvaluate.scala:66); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:66); 	at is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:28); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14362:14908,rewrite,rewrite,14908,https://hail.is,https://github.com/hail-is/hail/issues/14362,1,['rewrite'],['rewrite']
Modifiability,xpr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:205); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:201); 	at scala.Option.flatMap(Option.scala:171); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:201); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:151); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractPartitionFilters(ExtractIntervalFilters.scala:249); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:3858,Rewrite,RewriteBottomUp,3858,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"xt of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/subscriptions ws://localhost:8111/app/subscriptions. ProxyPass / http://localhost:8111/ connectiontimeout=240 timeout=1200; ProxyPassReverse / http://localhost:81",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1768,config,configuration,1768,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170,1,['config'],['configuration']
Modifiability,"y as HTTP instead of HTTPS could appear even when an HTTPS proxy wasn't configured.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h1>1.26.12 (2022-08-22)</h1>; <ul>; <li>Deprecated the <code>urllib3[secure]</code> extra and the <code>urllib3.contrib.pyopenssl</code> module.; Both will be removed in v2.x. See this <code>GitHub issue &lt;https://github.com/urllib3/urllib3/issues/2680&gt;</code>_; for justification and info on how to migrate.</li>; </ul>; <h1>1.26.11 (2022-07-25)</h1>; <ul>; <li>Fixed an issue where reading more than 2 GiB in a call to <code>HTTPResponse.read</code> would; raise an <code>OverflowError</code> on Python 3.9 and earlier.</li>; </ul>; <h1>1.26.10 (2022-07-07)</h1>; <ul>; <li>Removed support for Python 3.5</li>; <li>Fixed an issue where a <code>ProxyError</code> recommending configuring the proxy as HTTP; instead of HTTPS could appear even when an HTTPS proxy wasn't configured.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/a5b29ac1025f9bb30f2c9b756f3b171389c2c039""><code>a5b29ac</code></a> Add outputs.hashes to build action</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/a0b22f820639e6212994ba147f76b60d88185792""><code>a0b22f8</code></a> Release 1.26.12</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/13f11172648e880bb4a385fc4425420cd2534516""><code>13f1117</code></a> [1.26] Add SLSA generic generator to publish workflow</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/f95b9640604e7dd70d50d81f68fd14a36c082841""><code>f95b964</code></a> Add deprecation warnings for pyOpenSSL and the [secure] extra</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/aa3def7d242525e6e854991247c4b68583d15135""><code>aa3def7</code></a> Release 1.26.11</li>; <li",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12140:2781,config,configuring,2781,https://hail.is,https://github.com/hail-is/hail/pull/12140,2,['config'],"['configured', 'configuring']"
Modifiability,"y iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); > raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; E hail.utils.java.FatalError: AssertionError: assertion failed; E ; E Java stack trace:; E java.lang.AssertionError: assertion failed; E 	at scala.Predef$.assert(Predef.scala:208); E 	at is.hail.expr.ir.BlockMatrixMap.execute(BlockMatrixIR.scala:269); E 	at is.hail.expr.ir.BlockMatrixMap2.execute(BlockMatrixIR.scala:393); E 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:875); E 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:59); E 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:20); E 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); E 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:53); E 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); E 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.Index",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229:3409,rewrite,rewrite,3409,https://hail.is,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229,1,['rewrite'],['rewrite']
Modifiability,"y list of the variant start positions. This is a fair bit of data. Chromosome 1 has about 250 million bases, so in the worst case this is 250 * 8 million bytes = 2 GB. It occurs to me that this is actually way to much data to load on the master node in general (since I just try to open the indexes for every file). I should switch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with fatals, so as not to allocate strings~ Moved to #3771. - ~We no longer copy the genotype data into a buffer in the block reader. This was forcing the `fastKeys` to do an unnecessary data copy~ Moved to #3783 (with some substantial refactoring so it doesn't look much like this PR anymore). - ~I changed the contract on BgenRecord to require that `getValue` is called to ""consume"" the record before the next record is taken~ Irrelevant thanks to #3783 's refactoring. - ~`getValue(null)` just skips bytes (no copy, no decompression)~ Irrelevant thanks to #3783 's refactoring. - ~I added `RegionValueBuilder.unsafeAdvance` which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work.~ Moved to #3773. - ~I use `RegionValueBuilder.unsafeAdvance` to make loading a BGEN without entry fields very fast.~ Rolled into #3783. - ~I fixed `Table.index` to not trigger a partition key info gathering~ Moved to #3774. I had to ship the arrays of filtered variant indices to the workers somehow, so I shipped them as base64 encoded arrays of bytes. It's pretty groady (and that's why I added the commons-codec library). I don't know how else to initialize record readers with hadoop. Generally, I think the BGEN loading code could use a clean up, and I haven't done that here, if anything I've made it more complicated. I a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3727:2455,refactor,refactoring,2455,https://hail.is,https://github.com/hail-is/hail/pull/3727,1,['refactor'],['refactoring']
Modifiability,y$6(BackendUtils.scala:52) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.utils.package$.using(package.scala:635) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:51) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$12(Worker.scala:167) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]; 	at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$11(Worker.scala:166) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$11$adapted(Worker.scala:164) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.utils.package$.using(package.scala:635) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.main(Worker.scala:164) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Main$.main(Main.scala:14) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Main.main(Main.scala) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	... 11 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553:6041,adapt,adapted,6041,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553,2,['adapt'],['adapted']
Modifiability,y'; adding 'hailtop/aiotools/fs/stream.py'; adding 'hailtop/auth/__init__.py'; adding 'hailtop/auth/auth.py'; adding 'hailtop/auth/flow.py'; adding 'hailtop/auth/sql_config.py'; adding 'hailtop/auth/tokens.py'; adding 'hailtop/batch/__init__.py'; adding 'hailtop/batch/backend.py'; adding 'hailtop/batch/batch.py'; adding 'hailtop/batch/batch_pool_executor.py'; adding 'hailtop/batch/conftest.py'; adding 'hailtop/batch/docker.py'; adding 'hailtop/batch/exceptions.py'; adding 'hailtop/batch/globals.py'; adding 'hailtop/batch/hail_genetics_images.py'; adding 'hailtop/batch/job.py'; adding 'hailtop/batch/resource.py'; adding 'hailtop/batch/utils.py'; adding 'hailtop/batch_client/__init__.py'; adding 'hailtop/batch_client/aioclient.py'; adding 'hailtop/batch_client/client.py'; adding 'hailtop/batch_client/globals.py'; adding 'hailtop/batch_client/parse.py'; adding 'hailtop/batch_client/types.py'; adding 'hailtop/cleanup_gcr/__init__.py'; adding 'hailtop/cleanup_gcr/__main__.py'; adding 'hailtop/config/__init__.py'; adding 'hailtop/config/deploy_config.py'; adding 'hailtop/config/user_config.py'; adding 'hailtop/config/variables.py'; adding 'hailtop/fs/__init__.py'; adding 'hailtop/fs/fs.py'; adding 'hailtop/fs/fs_utils.py'; adding 'hailtop/fs/router_fs.py'; adding 'hailtop/fs/stat_result.py'; adding 'hailtop/hailctl/__init__.py'; adding 'hailtop/hailctl/__main__.py'; adding 'hailtop/hailctl/deploy.yaml'; adding 'hailtop/hailctl/describe.py'; adding 'hailtop/hailctl/auth/__init__.py'; adding 'hailtop/hailctl/auth/cli.py'; adding 'hailtop/hailctl/auth/create_user.py'; adding 'hailtop/hailctl/auth/delete_user.py'; adding 'hailtop/hailctl/auth/login.py'; adding 'hailtop/hailctl/batch/__init__.py'; adding 'hailtop/hailctl/batch/batch_cli_utils.py'; adding 'hailtop/hailctl/batch/cli.py'; adding 'hailtop/hailctl/batch/initialize.py'; adding 'hailtop/hailctl/batch/list_batches.py'; adding 'hailtop/hailctl/batch/submit.py'; adding 'hailtop/hailctl/batch/utils.py'; adding 'hailtop/ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:26353,config,config,26353,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['config'],['config']
Modifiability,y'; adding 'hailtop/auth/__init__.py'; adding 'hailtop/auth/auth.py'; adding 'hailtop/auth/flow.py'; adding 'hailtop/auth/sql_config.py'; adding 'hailtop/auth/tokens.py'; adding 'hailtop/batch/__init__.py'; adding 'hailtop/batch/backend.py'; adding 'hailtop/batch/batch.py'; adding 'hailtop/batch/batch_pool_executor.py'; adding 'hailtop/batch/conftest.py'; adding 'hailtop/batch/docker.py'; adding 'hailtop/batch/exceptions.py'; adding 'hailtop/batch/globals.py'; adding 'hailtop/batch/hail_genetics_images.py'; adding 'hailtop/batch/job.py'; adding 'hailtop/batch/resource.py'; adding 'hailtop/batch/utils.py'; adding 'hailtop/batch_client/__init__.py'; adding 'hailtop/batch_client/aioclient.py'; adding 'hailtop/batch_client/client.py'; adding 'hailtop/batch_client/globals.py'; adding 'hailtop/batch_client/parse.py'; adding 'hailtop/batch_client/types.py'; adding 'hailtop/cleanup_gcr/__init__.py'; adding 'hailtop/cleanup_gcr/__main__.py'; adding 'hailtop/config/__init__.py'; adding 'hailtop/config/deploy_config.py'; adding 'hailtop/config/user_config.py'; adding 'hailtop/config/variables.py'; adding 'hailtop/fs/__init__.py'; adding 'hailtop/fs/fs.py'; adding 'hailtop/fs/fs_utils.py'; adding 'hailtop/fs/router_fs.py'; adding 'hailtop/fs/stat_result.py'; adding 'hailtop/hailctl/__init__.py'; adding 'hailtop/hailctl/__main__.py'; adding 'hailtop/hailctl/deploy.yaml'; adding 'hailtop/hailctl/describe.py'; adding 'hailtop/hailctl/auth/__init__.py'; adding 'hailtop/hailctl/auth/cli.py'; adding 'hailtop/hailctl/auth/create_user.py'; adding 'hailtop/hailctl/auth/delete_user.py'; adding 'hailtop/hailctl/auth/login.py'; adding 'hailtop/hailctl/batch/__init__.py'; adding 'hailtop/hailctl/batch/batch_cli_utils.py'; adding 'hailtop/hailctl/batch/cli.py'; adding 'hailtop/hailctl/batch/initialize.py'; adding 'hailtop/hailctl/batch/list_batches.py'; adding 'hailtop/hailctl/batch/submit.py'; adding 'hailtop/hailctl/batch/utils.py'; adding 'hailtop/hailctl/batch/billing/__init__.py'; addin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:26390,config,config,26390,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['config'],['config']
Modifiability,"y/commit/38f1e30e8137ccc1aad6a4f113eb4360c6206539""><code>38f1e30</code></a> Update version to 0.942</li>; <li><a href=""https://github.com/python/mypy/commit/1c836685da13f11287ae6d6931c04337f881ec40""><code>1c83668</code></a> Let overload item have a wider return type than implementation (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12435"">#12435</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/67088e558dc24a2c6c231db542a367923dfdc049""><code>67088e5</code></a> Pin the version of bugbear (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12436"">#12436</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/367b29d4aac16fc7493abffe2df0d8f477c23923""><code>367b29d</code></a> Make order of processing the builtins SCC predictable (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12431"">#12431</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/f81b228e66d8a95cc39247f189e7be7e894f7f92""><code>f81b228</code></a> Fix inheritance false positives with dataclasses/attrs (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12411"">#12411</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/7e09c2a100209072429e290d2f7b9b8007b8629c""><code>7e09c2a</code></a> Support overriding dunder attributes in Enum subclass (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12138"">#12138</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/837543efb616b14e2f800db6962d216621dee4d7""><code>837543e</code></a> Fix crash in match statement if class name is undefined (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12417"">#12417</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/6606dbe98d09170d3ad810bc791a16d99ceb2281""><code>6606dbe</code></a> Allow non-final <strong>match_args</strong> and overriding (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12415"">#12415</a>)</li>; <li><a href=""https://github.com/python/mypy/commit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11667:1159,inherit,inheritance,1159,https://hail.is,https://github.com/hail-is/hail/pull/11667,2,['inherit'],['inheritance']
Modifiability,"y_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deplo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2699,config,config,2699,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['config'],['config']
Modifiability,"yload); 216 path = action_routes[action]; 217 port = self._backend_server_port; → 218 resp = self._requests_session.post(f’http://localhost:{port}{path}', data=data); 219 if resp.status_code >= 400:; 220 error_json = orjson.loads(resp.content). File ~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:637, in Session.post(self, url, data, json, **kwargs); 626 def post(self, url, data=None, json=None, **kwargs):; 627 r""""“Sends a POST request. Returns :class:Response object.; 628; 629 :param url: URL for the new :class:Request object.; (…); 634 :rtype: requests.Response; 635 “””; → 637 return self.request(“POST”, url, data=data, json=json, **kwargs). File ~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:589, in Session.request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json); 584 send_kwargs = {; 585 “timeout”: timeout,; 586 “allow_redirects”: allow_redirects,; 587 }; 588 send_kwargs.update(settings); → 589 resp = self.send(prep, **send_kwargs); 591 return resp. File ~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:703, in Session.send(self, request, **kwargs); 700 start = preferred_clock(); 702 # Send the request; → 703 r = adapter.send(request, **kwargs); 705 # Total elapsed time of the request (approximately); 706 elapsed = preferred_clock() - start. File ~/Library/Python/3.9/lib/python/site-packages/requests/adapters.py:501, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies); 486 resp = conn.urlopen(; 487 method=request.method,; 488 url=url,; (…); 497 chunked=chunked,; 498 ); 500 except (ProtocolError, OSError) as err:; → 501 raise ConnectionError(err, request=request); 503 except MaxRetryError as e:; 504 if isinstance(e.reason, ConnectTimeoutError):; 505 # TODO: Remove this in 3.0.0: see #2811. ConnectionError: (‘Connection aborted.’, RemoteDisconnected(‘Remote end closed connection without response’)); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14557:2579,adapt,adapter,2579,https://hail.is,https://github.com/hail-is/hail/issues/14557,2,['adapt'],"['adapter', 'adapters']"
Modifiability,you'll notice that BroadcastValue requires the type but doesn't do anything with it -- this is to make it trivial to extend it to expose something like `broadcastRegionValue`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3130#issuecomment-372654665:117,extend,extend,117,https://hail.is,https://github.com/hail-is/hail/pull/3130#issuecomment-372654665,1,['extend'],['extend']
Modifiability,"ype$lzycompute(TextTableReader.scala:347); at is.hail.expr.ir.TextTableReader.fullType(TextTableReader.scala:347); at is.hail.expr.ir.IRParser$$anonfun$table_ir_1$2.apply(Parser.scala:1231); at is.hail.expr.ir.IRParser$$anonfun$table_ir_1$2.apply(Parser.scala:1231); at scala.Option.getOrElse(Option.scala:121); at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:1231); at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:1205); at is.hail.expr.ir.IRParser$$anonfun$parse_table_ir$2.apply(Parser.scala:1675); at is.hail.expr.ir.IRParser$$anonfun$parse_table_ir$2.apply(Parser.scala:1675); at is.hail.expr.ir.IRParser$.parse(Parser.scala:1664); at is.hail.expr.ir.IRParser$.parse_table_ir(Parser.scala:1675); at is.hail.expr.ir.IRParser$.parse_table_ir(Parser.scala:1674); at is.hail.expr.ir.IRParser.parse_table_ir(Parser.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.34-2684f0214a05; Error summary: NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;; ```. This is caused by a backwards incompatible change introduced in Google's Hadoop connector in version 2.1.0 https://github.com/GoogleCloudDataproc/hadoop-connectors/issues/323. As of 2.1.0 Google's Hadoop connector relies on Hadoop 2.8.3. Unfortunately, there are no Spark releases with Hadoop 2.8.3 yet.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8343:7655,Config,Configuration,7655,https://hail.is,https://github.com/hail-is/hail/issues/8343,1,['Config'],['Configuration']
Modifiability,"ypehints_fully_qualified</code></li>; </ul>; <h2>1.15.0</h2>; <ul>; <li>Resolve type guard imports before evaluating annotations for objects</li>; <li>Remove <code>set_type_checking_flag</code> flag as this is now done by default</li>; <li>Fix crash when the <code>inspect</code> module returns an invalid python syntax source</li>; <li>Made formatting function configurable using the option <code>typehints_formatter</code></li>; </ul>; <h2>1.14.1</h2>; <ul>; <li>Fixed <code>normalize_source_lines()</code> messing with the indentation of methods with decorators that have parameters starting; with <code>def</code>.</li>; <li>Handle <code>ValueError</code> or <code>TypeError</code> being raised when signature of an object cannot be determined</li>; <li>Fix <code>KeyError</code> being thrown when argument is not documented (e.g. <code>cls</code> argument for class methods, and <code>self</code> for; methods)</li>; </ul>; <h2>1.14.0</h2>; <ul>; <li>Added <code>typehints_defaults</code> config option allowing to automatically annotate parameter defaults.</li>; </ul>; <h2>1.13.1</h2>; <ul>; <li>Fixed <code>NewType</code> inserts a reference as first argument instead of a string</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/commit/1ef84886873b80ff62ed1ea76e111dd9e96dbf18""><code>1ef8488</code></a> Release 1.17.0</li>; <li><a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/commit/aa345ca0475dbe8f4da7f4c7c56832c8d8e6884a""><code>aa345ca</code></a> Add <code>typehints_use_rtype</code> option (<a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/issues/218"">#218</a>)</li>; <li><a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/commit/a022d1a430db886decf2221b712bc3bd881f5e86""><code>a022d1a</code></a> inspect.getsource can raise TypeError (<a href=""https://github-redirect.dependabot.c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11503:2467,config,config,2467,https://hail.is,https://github.com/hail-is/hail/pull/11503,2,['config'],['config']
Modifiability,"ys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2562,config,config,2562,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['config'],['config']
Modifiability,"yter/jupyter_client/compare/v7.3.2...37ca37d865db260e7da6fa85339be450d6fd3c3c"">Full Changelog</a>)</p>; <h3>Bugs fixed</h3>; <ul>; <li>Add local-provisioner entry point to pyproject.toml Fixes <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/issues/800"">#800</a> <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/801"">#801</a> (<a href=""https://github.com/utkonos""><code>@​utkonos</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2022-06-06&amp;to=2022-06-07&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Autkonos+updated%3A2022-06-06..2022-06-07&amp;type=Issues""><code>@​utkonos</code></a></p>; <h2>7.3.2</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v7.3.1...c81771416d9e09e0e92be799f3e8549d0db57e43"">Full Changelog</a>)</p>; <h3>Enhancements made</h3>; <ul>; <li>Correct <code>Any</code> type annotations. <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/791"">#791</a> (<a href=""https://github.com/joouha""><code>@​joouha</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>[pre-commit.ci] pre-commit autoupdate <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/792"">#792</a> (<a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a>)</li>; <li>Use hatch backend <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/789"">#789</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>[pre-commit.ci] pre-commit autoupdate <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/788"">#788</a> (<a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a>)</li>; <li>Use flit build backend <a href=""https://github-redirect.dependabot.com/jupyter/jupy",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12110:6283,Enhance,Enhancements,6283,https://hail.is,https://github.com/hail-is/hail/pull/12110,1,['Enhance'],['Enhancements']
Modifiability,"ython-jsonschema/jsonschema/compare/v4.10.2...v4.10.3"">https://github.com/python-jsonschema/jsonschema/compare/v4.10.2...v4.10.3</a></p>; <h2>v4.10.2</h2>; <ul>; <li>Fix a second place where subclasses may have added attrs attributes (<a href=""https://github-redirect.dependabot.com/python-jsonschema/jsonschema/issues/982"">#982</a>).</li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/python-jsonschema/jsonschema/compare/v4.10.1...v4.10.2"">https://github.com/python-jsonschema/jsonschema/compare/v4.10.1...v4.10.2</a></p>; <h2>v4.10.1</h2>; <ul>; <li>Fix Validator.evolve (and APIs like <code>iter_errors</code> which call it) for cases; where the validator class has been subclassed. Doing so wasn't intended to be; public API, but given it didn't warn or raise an error it's of course; understandable. The next release however will make it warn (and a future one; will make it error). If you need help migrating usage of inheriting from a; validator class feel free to open a discussion and I'll try to give some; guidance (<a href=""https://github-redirect.dependabot.com/python-jsonschema/jsonschema/issues/982"">#982</a>).</li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/python-jsonschema/jsonschema/compare/v4.10.0...v4.10.1"">https://github.com/python-jsonschema/jsonschema/compare/v4.10.0...v4.10.1</a></p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/python-jsonschema/jsonschema/blob/main/CHANGELOG.rst"">jsonschema's changelog</a>.</em></p>; <blockquote>; <h1>v4.15.0</h1>; <ul>; <li>A specific API Reference page is now present in the documentation.</li>; <li><code>$ref</code> on earlier drafts (specifically draft 7 and 6) has been &quot;fixed&quot; to; follow the specified behavior when present alongside a sibling <code>$id</code>.; Specifically the ID is now properly ignored, and references are resolved; a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12163:3456,inherit,inheriting,3456,https://hail.is,https://github.com/hail-is/hail/pull/12163,1,['inherit'],['inheriting']
Modifiability,"ze check was missing (Adam Kaczmarek); Fixed: GITHUB-2709: Testnames not working together with suites in suite (Martin Aldrin); Fixed: GITHUB-2704: IHookable and IConfigurable callback discrepancy (Krishnan Mahadevan); Fixed: GITHUB-2637: Upgrade to JDK11 as the minimum JDK requirements (Krishnan Mahadevan); Fixed: GITHUB-2734: Keep the initial order of listeners (Andrei Solntsev); Fixed: GITHUB-2359: Testng <a href=""https://github.com/BeforeGroups""><code>@​BeforeGroups</code></a> is running in parallel with testcases in the group (Anton Velma); Fixed: Possible StringIndexOutOfBoundsException in XmlReporter (Anton Velma); Fixed: GITHUB-2754: <a href=""https://github.com/AfterGroups""><code>@​AfterGroups</code></a> is executed for each &quot;finished&quot; group when it has multiple groups defined (Anton Velma)</p>; <p>7.5; Fixed: GITHUB-2701: Bump gradle version to 7.3.3 to support java17 build (ZhangJian He); Fixed: GITHUB-2646: Streamline Logging Across TestNG (Krishnan Mahadevan); Fixed: GITHUB-2658: Inheritance + dependsOnMethods (Krishnan Mahadevan)</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/cbeust/testng/commit/b94395dea479308ea3fe825269730b960f44d805""><code>b94395d</code></a> Bump version to 7.7.1 for release</li>; <li><a href=""https://github.com/cbeust/testng/commit/89dc5845fcb46c26af187e50ea907a7382d06e72""><code>89dc584</code></a> Streamline overloaded assertion methods for Groovy</li>; <li><a href=""https://github.com/cbeust/testng/commit/5ac0021d14f7eb00804fe235aaefc5c2fbce57d1""><code>5ac0021</code></a> Adding release notes</li>; <li><a href=""https://github.com/cbeust/testng/commit/c0e1e772f1fc0ab2142f3a4114a2b8cfe60fa7e1""><code>c0e1e77</code></a> Adjust version reference in deprecation msgs.</li>; <li><a href=""https://github.com/cbeust/testng/commit/011527d9bf0f91a40539f5e5467cc106888810d9""><code>011527d</code></a> Bump version to 7.7.0 for r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:14593,Inherit,Inheritance,14593,https://hail.is,https://github.com/hail-is/hail/pull/12665,1,['Inherit'],['Inheritance']
Modifiability,"~Stacked on #10905~. This PR refactors; * BinarySearch; * BLAS/LAPACK wrapper methods; * CodeBuilder.assign (the SSettable overload); * SSettable.store - Most uses now call with an SValue, but the SCode overload is still used in SCode subclasses.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10906:29,refactor,refactors,29,https://hail.is,https://github.com/hail-is/hail/pull/10906,1,['refactor'],['refactors']
Modifiability,"~Stacked on #10906~. This PR refactors `MethodBuilder.invoke` and `EmitCodeBuilder.invoke(S)Code` to take/return values. * `invoke` now takes a `CodeBuilderLike`. It is used in places where there is only access to a `CodeBuilder` (not an `EmitCodeBuilder`), so I had to use the generic interface, and had to move a couple methods on `EmitCodeBuilder` to `CodeBuilderLike`. I have never understood this Emit/non-Emit split; would be a great simplification if we could collapse it.; * This change pushed some (S)Code->(S)Value refactorings inside some aggregator implementations, which generate and invoke internal methods.; * I had to keep a version of `MethodBuilder.invoke` that doesn't take a CodeBuilder, for use in `ThisLazyFieldRef.get`. Will have to think more about how this should work when Code is (mostly) gone. Maybe lazy fields should not subclass Value, and to access a lazy field requires an explicit `load(cb: CodeBuilder): Value[T]`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10907:29,refactor,refactors,29,https://hail.is,https://github.com/hail-is/hail/pull/10907,2,['refactor'],"['refactorings', 'refactors']"
Modifiability,"~Stacked on #14404~. This set of changes started out as refactoring and trying to better understand `extract.scala`. But the bigger change ended up being getting rid of AggLet in favor of allowing bindings in `Let` to be agg/scan bindings. So each binding in a `Let` now has a binding scope, encoded using the pre-existing `Scope` enum. I also renamed `Let` to `Block`. While I think that's a better name for what `Let` has become, and what I hope it will further become in the future, I initially tried to keep the name `Let`. I changed it so that I could keep `Let.apply` the way it is, specialized to bindings all in the eval scope. We can change all the uses of `Let.apply` seperately, I didn't want to pollute this already large pr with that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14402:56,refactor,refactoring,56,https://hail.is,https://github.com/hail-is/hail/pull/14402,1,['refactor'],['refactoring']
Modifiability,"~Stacked on #8917~. Adds `zipPartitions`, `extendKeyPreservesPartitioning`, `orderedJoin`, and `alignAndZipPartitions` methods to `TableStage`, trying to mirror the `RVD` implementation, especially concerning the partitioning logic. Adds lowering case for `TableJoin`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8921:43,extend,extendKeyPreservesPartitioning,43,https://hail.is,https://github.com/hail-is/hail/pull/8921,1,['extend'],['extendKeyPreservesPartitioning']
Modifiability,~more flexible~ more better,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3625#issuecomment-390296315:6,flexible,flexible,6,https://hail.is,https://github.com/hail-is/hail/pull/3625#issuecomment-390296315,1,['flexible'],['flexible']
Modifiability,"~~Stacked on #8140~~. This PR adds support for the new streams in the emitter for ArrayFilter, ArrayZip, ArrayFlatMap, If, and Let. It also changes the semantics of `Stream` slightly: Before, a producer had to close itself before calling EOS on its consumer. Now, the consumer is responsible for closing the producer after the producer calls EOS. This makes flatMap slightly more complicated, but it allows a significant simplification to zip. To implement the different modes of `ArrayZip`, I added `extendNA` and `take`. `extendNA` turns a stream into an infinite stream, where values after the end are missing, to allow the consumer to decide what to do when the stream ends. `take` is then needed to end infinite streams. Both use `COption` to encode whether the child stream has ended / whether to end the child stream. I still intend to convert uses of `COption` to use `CodeConditional` after lowering is unblocked.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8156:501,extend,extendNA,501,https://hail.is,https://github.com/hail-is/hail/pull/8156,2,['extend'],['extendNA']
Modifiability,"~~Stacked on #9320~~. Use the framework introduced in #9320 to make the IR parser stack safe. This touches a lot of lines, but it is a completely mechanical refactoring. I did some preliminary benchmarking by timing the parse of the IR in `test_ld_prune`. (I chose that test fairly arbitrarily, and we can probably find better test cases, or generate synthetic large IRs.) Running a loop parsing the same IR repeatedly, with 10 burn-in rounds, and 60 timed rounds, I got the following results:; * On main; ```; quartiles = [4.55816, 5.209579, 5.647135]; avg = 5.332496950000001, std = 1.13761574944604; ```; * Using `StackSafe`, without the optimization in `repUntil`; ```; quantiles = [4.610798, 5.09793, 7.159075]; avg = 5.7519015000000016, std = 1.612397075594852; ```; * Using `StackSafe, with the optimization which reuses the `cont` closure, instead of allocating a new one for each token.; ```; quantiles = [4.466849, 4.826873, 5.719238]; avg = 5.2787357833333335, std = 1.272006325411254; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9332:157,refactor,refactoring,157,https://hail.is,https://github.com/hail-is/hail/pull/9332,1,['refactor'],['refactoring']
Modifiability,"~~Stride for a dimension should be calculated based off the length * stride of the previous innermost dimension with length > 1. I was remembering to do that for the stride but just taking the length of the adjacent dimension.~~. ~~Length-1 dimensions have stride 0 so broadcasting happens implicitly without the need for checks while indexing into the ndarray.~~. Broadcasting was previously handled implicitly by having 0 stride for dimensions of length 1. This doesn't actually work in all scenarios and conflates the concept of a multidimensional index with the underlying representation. Since we already compute the shapes of all intermediate NDArrays before doing any iteration, we can identify broadcasting ""statically"". Instead, loop variables associated with a braodcast are replaced with 0 when indexing into the backing array. This method is more robust and I've already seen works with slicing, which will follow this PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6154:743,variab,variables,743,https://hail.is,https://github.com/hail-is/hail/pull/6154,1,['variab'],['variables']
Modifiability,"─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮; │ * name TEXT [default: None] [required] │; │ * script TEXT [default: None] [required] │; │ arguments [ARGUMENTS]... You should use -- if you want to pass option-like arguments through. [default: None] │; ╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯; ╭─ Options ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮; │ --files TEXT Comma-separated list of files to add to the working directory of the Hail application. │; │ --pyfiles TEXT Comma-separated list of files (or directories with python files) to add to the PYTHONPATH. │; │ --properties -p TEXT Extra Spark properties to set. [default: None] │; │ --gcloud_configuration TEXT Google Cloud configuration to submit job (defaults to currently set configuration). [default: None] │; │ --dry-run --no-dry-run Print gcloud dataproc command, but don't run it. [default: no-dry-run] │; │ --region TEXT Compute region for the cluster. [default: None] │; │ --help Show this message and exit. │; ╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13447#issuecomment-1681403012:6418,config,configuration,6418,https://hail.is,https://github.com/hail-is/hail/pull/13447#issuecomment-1681403012,2,['config'],['configuration']
Modifiability,"──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮; │ --files TEXT Files or directories to add to the working directory of the job. [default: None] │; │ --name TEXT The name of the batch. │; │ --image-name TEXT Name of Docker image for the job (default: hailgenetics/hail) [default: None] │; │ --output -o [text|yaml|json] [default: text] │; │ --help Show this message and exit. │; ╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯. (base) dking@wm28c-761 hail % ; (base) dking@wm28c-761 hail % hailctl hdinsight submit --help ; ; Usage: hailctl hdinsight submit [OPTIONS] NAME STORAGE_ACCOUNT HTTP_PASSWORD ; SCRIPT [ARGUMENTS]... ; ; Submit a job to an HDInsight cluster configured for Hail. ; If you wish to pass option-like arguments you should use ""--"". For example: ; ; $ hailctl hdinsight submit name account password script.py --image-name docker.io/image my_script.py -- some-argument --animal dog ; ; ╭─ Arguments ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮; │ * name TEXT [default: None] [required] │; │ * storage_account TEXT Storage account in which the cluster's container exists. [default: None] [required] │; │ * http_password TEXT Web password for the cluster [default: None] [required] │; │ * script TEXT Path to script. [default: None] [required] │; │ arguments [ARGUMENTS]... You should use -- if you want to pass option-like arguments through. [default: None] │; ╰───────────────",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13447#issuecomment-1681403012:2440,config,configured,2440,https://hail.is,https://github.com/hail-is/hail/pull/13447#issuecomment-1681403012,1,['config'],['configured']
Modifiability,"（1）yum info atlas-devel; root yum.repos.d $ yum info atlas-devel; Loaded plugins: fastestmirror, langpacks; base | 3.6 kB 00:00:00 ; extras | 3.4 kB 00:00:00 ; updates | 3.4 kB 00:00:00 ; (1/4): base/7/x86_64/group_gz | 155 kB 00:00:00 ; (2/4): extras/7/x86_64/primary_db | 160 kB 00:00:00 ; (3/4): base/7/x86_64/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-239729893:73,plugin,plugins,73,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893,1,['plugin'],['plugins']
Performance,	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:458); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:456); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:456); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:456); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor90.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.lang.RuntimeException: Stream is already closed.; E 	at com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:79); E 	at com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:89); E 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:7539,concurren,concurrent,7539,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['concurren'],['concurrent']
Performance,	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:460); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:459); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:134); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.net.SocketTimeoutException: connect timed out; E 	at java.net.PlainSocketImpl.socketConnect(Native Method); E 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); E 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); E 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); E 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); E 	at java.net.Socket.connect(Socket.java:607); E 	at is.hail.relocated.org,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13074:7253,concurren,concurrent,7253,https://hail.is,https://github.com/hail-is/hail/issues/13074,1,['concurren'],['concurrent']
Performance,	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at java.util.concurrent.AbstractExecutorService.doInvokeAny(AbstractExecutorService.java:193); 	at java.util.concurrent.AbstractExecutorService.invokeAny(AbstractExecutorService.java:215); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.concurrentGlobInternal(GoogleHadoopFileSystemBase.java:1282); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1261); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1229); 	at is.hail.io.fs.HadoopFS.listStatus(HadoopFS.scala:104); 	at is.hail.utils.Py4jUtils$class.ls(Py4jUtils.scala:55); 	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.r,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:3158,concurren,concurrent,3158,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrent']
Performance,	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.Lowe,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:6079,Optimiz,Optimize,6079,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.34-914bd8a10ca2; Error summary: HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:118134,concurren,concurrent,118134,https://hail.is,https://github.com/hail-is/hail/issues/8469,2,['concurren'],['concurrent']
Performance," 	at is.hail.backend.Backend.execute(Backend.scala:77); 	at is.hail.backend.Backend.executeJSON(Backend.scala:96); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). org.apache.spark.SparkException: Job aborted due to stage failure: Task 40 in stage 7.0 failed 20 times, most recent failure: Lost task 40.19 in stage 7.0 (TID 3171, seqr-loading-cluster-sw-z91p.c.seqr-project.internal, executor 14): is.hail.utils.HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:74); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:210); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:974); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:967); 	at is.hail.utils.FlipbookIterator$$anon$5.<init>(FlipbookIterator.scala:176); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:174); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:145); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:967); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:963); 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(KeyedRVD.scala:147); 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:51226,load,loading-cluster-sw-,51226,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['load'],['loading-cluster-sw-']
Performance, 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5(ServiceBackend.scala:432); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5$adapted(ServiceBackend.scala:430); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:430); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:77); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:430); E 	at is.hail.backend.service.Main$.main(Main.scala:33); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.concurrent.TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.handleTimeout(FluxTimeout.java:294); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.doTimeout(FluxTimeout.java:279); E 	at reactor.core.publisher.FluxTimeout$TimeoutTimeoutSubscriber.onNext(FluxTimeout.java:418); E 	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onNext,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:4005,concurren,concurrent,4005,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['concurren'],['concurrent']
Performance, 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$15(BackendUtils.scala:90); 	at is.hail.backend.service.Worker$.$anonfun$main$9(Worker.scala:172); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.backend.service.Worker$.$anonfun$main$8(Worker.scala:171); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.service.Worker$.main(Worker.scala:169); 	at is.hail.backend.service.Main$.main(Main.scala:14); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.lang.NullPointerException: null; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionPutTask.call(JsonResumableSessionPutTask.java:201); 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.lambda$put$0(JsonResumableSession.java:81); 	at is.hail.relocated.com.google.cloud.storage.Retrying.lambda$run$0(Retrying.java:102); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13937:5390,concurren,concurrent,5390,https://hail.is,https://github.com/hail-is/hail/issues/13937,1,['concurren'],['concurrent']
Performance, 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); 	at java.util.zip.DeflaterOutputStream.close(DeflaterOutputStream.java:241); 	at is.hail.utils.package$.using(package.scala:669); 	at is.hail.io.index.IndexWriterUtils.writeMetadata(IndexWriter.scala:253); 	at __C511collect_distributed_array_table_native_writer.apply_region3_328(Unknown Source); 	at __C511collect_distributed_array_table_native_writer.apply(Unknown Source); 	at __C511collect_distributed_array_table_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:87); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86); 	at is.hail.backend.service.Worker$.$anonfun$main$9(Worker.scala:198); 	at is.hail.services.package$.retryTransientErrors(package.scala:187); 	at is.hail.backend.service.Worker$.$anonfun$main$8(Worker.scala:197); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.service.Worker$.main(Worker.scala:195); 	at is.hail.backend.service.Main$.main(Main.scala:9); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14430:2243,concurren,concurrent,2243,https://hail.is,https://github.com/hail-is/hail/pull/14430,6,['concurren'],['concurrent']
Performance, 	at java.lang.Thread.run(Thread.java:748). java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at org.apache.hadoop.fs.Path.initialize(Path.java:205); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHadoopFileSystemBase.java:1279); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.net.URI.checkPath(URI.java:1823); 	at java.net.URI.<init>(URI.java:745); 	at org.apache.hadoop.fs.Path.initialize(Path.java:202); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHadoopFileSystemBase.java:1279); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:5572,concurren,concurrent,5572,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrent']
Performance, 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:834). is.hail.utils.HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:5710,Load,LoadPlink,5710,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance, 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3901:6780,concurren,concurrent,6780,https://hail.is,https://github.com/hail-is/hail/issues/3901,2,['concurren'],['concurrent']
Performance, 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3465:8406,concurren,concurrent,8406,https://hail.is,https://github.com/hail-is/hail/issues/3465,5,['concurren'],['concurrent']
Performance, 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:3887,concurren,concurrent,3887,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,2,['concurren'],['concurrent']
Performance, 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:357); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:471); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4114:3337,concurren,concurrent,3337,https://hail.is,https://github.com/hail-is/hail/issues/4114,1,['concurren'],['concurrent']
Performance, 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1015); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:357); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:471); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4055:5388,concurren,concurrent,5388,https://hail.is,https://github.com/hail-is/hail/issues/4055,1,['concurren'],['concurrent']
Performance," ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and sidecar requests; Broadly, there are three strategies: browser caching, server caching, and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-hitching UI interactions, at some cost to the speed with which 100,000 elements can be appended to a page. ; * Some consider this to be more reliable: https://localvoid.github.io/uibench/; * Here React performs many times better than vanilla JS for some operations.; * I should probably figure out exactly why. In practice, React in 2019 will likely be the best performing UI solution available, with of course the exception of some very well optimized JS. This is because of React Fiber's time slice mode, wh",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:14902,perform,performance-cost-of-server-side-rendered-react-node-js,14902,https://hail.is,https://github.com/hail-is/hail/pull/5162,2,['perform'],"['performance', 'performance-cost-of-server-side-rendered-react-node-js']"
Performance," ### What went wrong (all error messages here, including the full java stack trace): HailException: optimization changed type!; before: Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],rsid:String,qual:Float64,filters:Set[String],info:Struct{ABHet:Float64,ABHom:Float64,AC:Array[Int32],AF:Array[Float64],AN:Int32,AS_BaseQRankSum:Array[Float64],AS_FS:Array[Float64],AS_InbreedingCoeff:Array[Float64],AS_InsertSizeRankSum:Array[Float64],AS_MQ:Array[Float64],AS_MQRankSum:Array[Float64],AS_QD:Float64,AS_ReadPosRankSum:Array[Float64],AS_SOR:Array[Float64],BaseQRankSum:Float64,DP:Int32,DS:Boolean,ExcessHet:Float64,FS:Float64,HRun:Int32,HaplotypeScore:Float64,InbreedingCoeff:Float64,LikelihoodRankSum:Float64,MLEAC:Array[Int32],MLEAF:Array[Float64],MQ:Float64,MQ0:Int32,MQRankSum:Float64,OND:Float64,QD:Float64,RPA:Array[Int32],RU:String,ReadPosRankSum:Float64,ReverseComplementedAlleles:Boolean,SOR:Float64,STR:Boolean,SwappedAlleles:Boolean},a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{AB:Float64,AD:Array[+Int32],DP:Int32,GQ:Int32,GT:Call,MQ0:Int32,PL:Array[+Int32],SB:Array[+Int32]}}; after: Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],rsid:String,qual:Float64,filters:Set[String],info:Struct{ABHet:Float64,ABHom:Float64,AC:Array[Int32],AF:Array[Float64],AN:Int32,AS_BaseQRankSum:Array[Float64],AS_FS:Array[Float64],AS_InbreedingCoeff:Array[Float64],AS_InsertSizeRankSum:Array[Float64],AS_MQ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4827:479,optimiz,optimization,479,https://hail.is,https://github.com/hail-is/hail/issues/4827,1,['optimiz'],['optimization']
Performance, %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.496ms self 0.040ms children 0.456ms %children 91.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.228ms self 0.228ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.064ms self 0.064ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:207211,Optimiz,Optimize,207211,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance, %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.503ms self 0.061ms children 0.443ms %children 87.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.219ms self 0.219ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:215232,Optimiz,Optimize,215232,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance, %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.460ms self 0.066ms children 1.394ms %children 95.49%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.654ms self 0.654ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.072ms self 0.072ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:194607,Optimiz,Optimize,194607,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance," (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthli",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:6698,cache,cached,6698,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance," (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:3221,Load,LoadMatrix,3221,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance," (most recent call last):; File ""<ipython-input-7-ccfd397235b8>"", line 6, in raises; raise ValueError(message); ValueError: unretrieved case; ```. 5. And here is an example of ""retrieving"" the exception (by calling `Task.exception`). Notice we do not get the ""Task exception was never retrieved"" message. ```; In [8]: async def foo():; ...: async def raises(message):; ...: try:; ...: await asyncio.sleep(100); ...: finally:; ...: raise ValueError(message); ...:; ...: t = asyncio.create_task(raises('retrieved case')); ...: await asyncio.sleep(0) # let the other task run for a moment; ...: t.cancel(); ...: await asyncio.wait([t]); ...: print((t, t.done(), t.cancelled(), t.exception())); ...:; ...: asyncio.run(foo()); ...:; (<Task finished name='Task-599' coro=<foo.<locals>.raises() done, defined at <ipython-input-8-5fa396151822>:2> exception=ValueError('retrieved case')>, True, False, ValueError('retrieved case')); ```. ---. One more thing of interest, I confirmed that the ExitStack throws the first exception it encountered *after* performing all callbacks. ```ipython; In [6]: with ExitStack() as exit:; ...: def foo(i):; ...: print(str(i)); ...: raise ValueError(i); ...: exit.callback(foo, 1); ...: exit.callback(foo, 2); 2; 1; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); Cell In[6], line 1; ----> 1 with ExitStack() as exit:; 2 def foo(i):; 3 print(str(i)). File ~/miniconda3/lib/python3.10/contextlib.py:576, in ExitStack.__exit__(self, *exc_details); 572 try:; 573 # bare ""raise exc_details[1]"" replaces our carefully; 574 # set-up context; 575 fixed_ctx = exc_details[1].__context__; --> 576 raise exc_details[1]; 577 except BaseException:; 578 exc_details[1].__context__ = fixed_ctx. File ~/miniconda3/lib/python3.10/contextlib.py:561, in ExitStack.__exit__(self, *exc_details); 559 assert is_sync; 560 try:; --> 561 if cb(*exc_details):; 562 suppressed_exc = True; 563 pending_raise = False. File ~/min",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13876:4837,perform,performing,4837,https://hail.is,https://github.com/hail-is/hail/pull/13876,1,['perform'],['performing']
Performance," **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 890 exprs, named_exprs, self._row_indices,; 891 protect_keys=True); --> 892 return self._select('Table.select', value_struct=hl.struct(**row)); 893 ; 894 @typecheck_method(exprs=oneof(str, Expression)). ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/table.py in _select(self, caller, key_struct, value_struct, do_process_joins); 433 ; 434 if do_process_joins:; --> 435 base, cleanup = self._process_joins(row); 436 analyze(caller, row, self._row_indices); 437 else:. ~/hail/python/hail/table.py in _process_joins(self, *exprs); 1484 def broadcast_f(left, data, jt):; 1485 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1486 return process_joins(self, exprs, broadcast_f); 1487 ; 1488 def cache(self):. ~/hail/python/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 364 data = hail.Struct(**{b.uid: b.value for b in broadcasts}); 365 data_json = t._to_json(data); --> 366 left = broadcast_f(left, data_json, t._jtype); 367 ; 368 def cleanup(table):. ~/hail/python/hail/table.py in broadcast_f(left, data, jt); 1483 def _process_joins(self, *exprs):; 1484 def broadcast_f(left, data, jt):; -> 1485 return Table(left._jt.annotateGlobalJSON(data, jt)); 1486 return process_joins(self, exprs, broadcast_f); 1487 . ~/apache/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. ~/hail/python/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3785:3791,cache,cache,3791,https://hail.is,https://github.com/hail-is/hail/issues/3785,1,['cache'],['cache']
Performance," - unable to resolve class reference is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; Exception in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1210,load,loadClass,1210,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460,1,['load'],['loadClass']
Performance," -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 3.975ms, total 407.299ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- LoweringTransformation : 77.664ms, total 485.244ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:2312,Optimiz,Optimize,2312,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance," -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 3.975ms, total 407.299ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- LoweringTransformation : 77.664ms, total 485.244ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 1.780ms, total 487.281ms, tagged coverage 0.0; ...; ...; ...; 2019-11-06 18:44:11 roo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:2451,Optimiz,Optimize,2451,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance, --gvcf-gq-bands 17 --gvcf-gq-bands 18 --gvcf-gq-bands 19 --gvcf-gq-bands 20 --gvcf-gq-bands 21 --gvcf-gq-bands 22 --gvcf-gq-bands 23 --gvcf-gq-bands 24 --gvcf-gq-bands 25 --gvcf-gq-bands 26 --gvcf-gq-bands 27 --gvcf-gq-bands 28 --gvcf-gq-bands 29 --gvcf-gq-bands 30 --gvcf-gq-bands 31 --gvcf-gq-bands 32 --gvcf-gq-bands 33 --gvcf-gq-bands 34 --gvcf-gq-bands 35 --gvcf-gq-bands 36 --gvcf-gq-bands 37 --gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47 --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-gq-bands 57 --gvcf-gq-bands 58 --gvcf-gq-bands 59 --gvcf-gq-bands 60 --gvcf-gq-bands 70 --gvcf-gq-bands 80 --gvcf-gq-bands 90 --gvcf-gq-bands 99 --indel-size-to-eliminate-in-ref-model 10 --use-alleles-trigger false --disable-optimizations false --just-determine-active-regions false --dont-genotype false --max-mnp-distance 0 --dont-trim-active-regions false --max-disc-ar-extension 25 --max-gga-ar-extension 300 --padding-around-indels 150 --padding-around-snps 20 --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 --recover-dangling-heads false --do-not-recover-dangling-branches false --min-dangling-branch-length 4 --consensus false --max-num-haplotypes-in-population 128 --error-correct-kmers false --min-pruning 2 --debug-graph-transformations false --kmer-length-for-read-error-correction 25 --min-observations-for-kmer-to-be-solid 20 --likelihood-calculation-engine PairHMM --base-quality-score-threshold 18 --pair-hmm-gap-continuation-penalty 10 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --native-pair-hmm-threads 4 --native-pair-hmm-use-double-precision fal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:2226,optimiz,optimizations,2226,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['optimiz'],['optimizations']
Performance, 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.092ms self 0.092ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:176100,Optimiz,Optimize,176100,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance, 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.201ms self 0.201ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:32783,Optimiz,Optimize,32783,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance, 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.140ms self 0.140ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:170983,Optimiz,OptimizePass,170983,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance, 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.187ms self 0.187ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.031ms self 0.031ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:27666,Optimiz,OptimizePass,27666,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance, 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.032ms self 0.028ms children 0.003ms %children 10.95%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.092ms self 0.092ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:174928,Optimiz,OptimizePass,174928,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance, 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.050ms self 0.041ms children 0.010ms %children 19.41%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.201ms self 0.201ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:31611,Optimiz,OptimizePass,31611,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance, 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.496ms self 0.040ms children 0.456ms %children 91.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.228ms self 0.228ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.064ms self 0.064ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.Compile,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:207182,Optimiz,OptimizePass,207182,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance, 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.503ms self 0.061ms children 0.443ms %children 87.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.219ms self 0.219ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.Compile,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:215203,Optimiz,OptimizePass,215203,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance, 0.013ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.460ms self 0.066ms children 1.394ms %children 95.49%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.654ms self 0.654ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.072ms self 0.072ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.Compile,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:194578,Optimiz,OptimizePass,194578,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance, 10: pool-2-thread-2; 2023-09-27 16:44:22.623 WorkerTimer$: INFO: executeFunction took 71843.446957 ms.; 2023-09-27 16:44:22.623 GoogleStorageFS$: INFO: createNoCompression: gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g=/result.7028; 2023-09-27 16:44:22.668 GoogleStorageFS$: INFO: close: gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g=/result.7028; 2023-09-27 16:44:33.788 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/1-day/o?name=parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g%3D/result.7028&uploadType=resumable&upload_id=ADPycdv7y3A6GjTh6Kv7vrUu2ap2Kv0peLVWsVTAghIs7RCZk9X3fI1BDkeHag1cd9g3etP2sS4f13bN6iJPU_sbnRnyRE91VPtjUpuYLPqmOq13; 	|> content,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:6862,concurren,concurrent,6862,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['concurren'],['concurrent']
Performance," 140.241861 ms; 2018-10-09 15:04:36 Executor: INFO: Finished task 0.0 in stage 0.0 (TID 0). 1015 bytes result sent to driver; 2018-10-09 15:04:36 TaskSetManager: INFO: Finished task 0.0 in stage 0.0 (TID 0) in 395 ms on localhost (executor driver) (1/1); 2018-10-09 15:04:36 TaskSchedulerImpl: INFO: Removed TaskSet 0.0, whose tasks have all completed, from pool ; 2018-10-09 15:04:36 DAGScheduler: INFO: ResultStage 0 (collect at utils.scala:44) finished in 0.412 s; 2018-10-09 15:04:36 DAGScheduler: INFO: Job 0 finished: collect at utils.scala:44, took 0.679005 s; 2018-10-09 15:04:36 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:36 SparkSqlParser: INFO: Parsing command: table8508c46074; 2018-10-09 15:04:36 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:36 SparkSqlParser: INFO: Parsing command: CACHE TABLE `table8508c46074`; 2018-10-09 15:04:36 SparkSqlParser: INFO: Parsing command: `table8508c46074`; 2018-10-09 15:04:37 CodeGenerator: INFO: Code generated in 15.850234 ms; 2018-10-09 15:04:37 CodeGenerator: INFO: Code generated in 9.347112 ms; 2018-10-09 15:04:37 SparkContext: INFO: Starting job: sql at NativeMethodAccessorImpl.java:0; 2018-10-09 15:04:37 DAGScheduler: INFO: Registering RDD 12 (sql at NativeMethodAccessorImpl.java:0); 2018-10-09 15:04:37 DAGScheduler: INFO: Got job 1 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions; 2018-10-09 15:04:37 DAGScheduler: INFO: Final stage: ResultStage 2 (sql at NativeMethodAccessorImpl.java:0); 2018-10-09 15:04:37 DAGScheduler: INFO: Parents of final stage: List(ShuffleMapStage 1); 2018-10-09 15:04:37 DAGScheduler: INFO: Missing parents: List(ShuffleMapStage 1); 2018-10-09 15:04:37 DAGScheduler: INFO: Submitting ShuffleMapStage 1 (MapPartitionsRDD[12] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents; 2018-10-09 15:04:37 MemoryStore: INFO: Block",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:18283,CACHE,CACHE,18283,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['CACHE'],['CACHE']
Performance," 1; CHECKCAST [Ljava/lang/Object;; ALOAD 2; CHECKCAST scala/collection/mutable/ArrayBuffer; INVOKESPECIAL is/hail/codegen/generated/C0.apply ([Ljava/lang/Object;Lscala/collection/mutable/ArrayBuffer;)Ljava/lang/Object;; ARETURN; MAXSTACK = 3; MAXLOCALS = 3; }; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-297>"", line 2, in filter_genotypes; File ""/tmp/spark-0721abd3-c72d-4439-a655-c09fddad864c/userFiles-7c41df44-c5b2-44b1-924e-8f73a9aa8148/hail.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: ClassNotFoundException: is.hail.asm4s.AsmFunction2. Java stack trace:; java.lang.NoClassDefFoundError: is/hail/asm4s/AsmFunction2; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.lang.ClassLoader.defineClass(ClassLoader.java:642); at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:254); at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:250); at is.hail.asm4s.package$.loadClass(package.scala:261); at is.hail.asm4s.FunctionBuilder$$anon$2.apply(FunctionBuilder.scala:218); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:80); at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:53); at is.hail.expr.Parser$$anonfun$evalTypedExpr$1.apply(Parser.scala:71); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:324); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:321); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:10710,load,loadOrDefineClass,10710,https://hail.is,https://github.com/hail-is/hail/issues/2966,1,['load'],['loadOrDefineClass']
Performance," 233s, 233s; linreg with 10 PCs, 8 cores: 23s, 24s, 24s; pca, 8 cores: 179s. hail command:; ~/hail/build/install/hail/bin/hail read -i ~/data/profile75.vds linreg -c ~/data/profile75.cov -f ~/data/profile.fam -o ~/data/profile75.linreg. read: 1570.888546; linreg: 58496.508588. plink vcf command to create bed/bim/fam:; ./plink --vcf ~/data/profile75.vcf.bgz; - rename plink.bed/bim/fam to profile75.bed/bim/fam; - create covar with FID column by doubling first column of cov file (use cut and paste in bash). plink linreg command:; time ./plink --bfile profile75 --double-id --pheno ~/data/profile.pheno --allow-no-sex --covar ~/data/profile75.covar --linear --out ~/data/profile75.plink. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/profile75.plink.log.; Options in effect:; --allow-no-sex; --bfile profile75; --covar /Users/Jon/data/profile75.covar; --double-id; --linear; --out /Users/Jon/data/profile75.plink; --pheno /Users/Jon/data/profile.pheno. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 74885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/profile75.plink.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you may want; to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.914041.; 74885 variants and 2535 people pass filters and QC.; Phenotype data is quantitative.; Writing linear model association results to; /Users/Jon/data/profile75.plink.assoc.linear ... done. real 0m38.837s; user 0m38.609s; sys 0m0.187s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/61:1286,load,loaded,1286,https://hail.is,https://github.com/hail-is/hail/pull/61,3,['load'],['loaded']
Performance," 2619 ; 2620 Examples; (...); 2628 Number of rows, number of cols.; 2629 """"""; 2630 count_ir = ir.MatrixCount(self._mir); -> 2631 return Env.backend().execute(count_ir). File ~/miniconda3/lib/python3.10/site-packages/hail/backend/backend.py:180, in Backend.execute(self, ir, timed); 178 result, timings = self._rpc(ActionTag.EXECUTE, payload); 179 except FatalError as e:; --> 180 raise e.maybe_user_error(ir) from None; 181 if ir.typ == tvoid:; 182 value = None. File ~/miniconda3/lib/python3.10/site-packages/hail/backend/backend.py:178, in Backend.execute(self, ir, timed); 176 payload = ExecutePayload(self._render_ir(ir), '{""name"":""StreamBufferSpec""}', timed); 177 try:; --> 178 result, timings = self._rpc(ActionTag.EXECUTE, payload); 179 except FatalError as e:; 180 raise e.maybe_user_error(ir) from None. File ~/miniconda3/lib/python3.10/site-packages/hail/backend/py4j_backend.py:214, in Py4JBackend._rpc(self, action, payload); 212 if resp.status_code >= 400:; 213 error_json = orjson.loads(resp.content); --> 214 raise fatal_error_from_java_error_triplet(error_json['short'], error_json['expanded'], error_json['error_id']); 215 return resp.content, resp.headers.get('X-Hail-Timings', ''). FatalError: FileNotFoundException: File not found: gs://danking/chr*.vcf. Java stack trace:; java.io.FileNotFoundException: File not found: gs://danking/chr*.vcf; 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:984); 	at is.hail.io.fs.HadoopFS.fileListEntry(HadoopFS.scala:175); 	at is.hail.io.fs.HadoopFS.fileListEntry(HadoopFS.scala:87); 	at is.hail.io.fs.FS.fileListEntry(FS.scala:417); 	at is.hail.io.fs.FS.fileListEntry$(FS.scala:417); 	at is.hail.io.fs.HadoopFS.fileListEntry(HadoopFS.scala:87); 	at is.hail.expr.ir.analyses.SemanticHash$.getFileHash(SemanticHash.scala:373); 	at is.hail.expr.ir.analyses.SemanticHash$.$anonfun$encode$18(SemanticHash.scala:198); 	at scala.collection.immutable.List.foreach(List.scala:431); 	at is.hai",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13915:3968,load,loads,3968,https://hail.is,https://github.com/hail-is/hail/issues/13915,1,['load'],['loads']
Performance," 3.6. The `hail-ubuntu` image explicitly installs Python 3.7. I'm happy to drop linting for Python 3.6 from build.yaml if compilers team is OK with that (ask Tim). We are already using Ubuntu 20.04 for our services and tests. See below for details, but `hail-ubuntu` is based on 20.04. We explicitly install JDK 8 in the `base` image. ---. This PR doesn't change the hail-ubuntu image which is the basis for nearly all our images. `DOCKER_ROOT_IMAGE` seems to have been introduced [here](https://github.com/hail-is/hail/pull/9660). That's my bad for not flagging in the review that this isn't actually a ""root image"". `DOCKER_ROOT_IMAGE` is just some Linux image with the standard utils. It was introduced as a distinct global concept when Docker Hub began enforcing rate limits (so we needed all our test images to live in GCR). If you look at the occurrences of `DOCKER_ROOT_IMAGE` or `docker_root_image` you'll find it's almost exclusively used in the tests except for one occurrence in `build-batch-worker-image-startup-gcp.sh`. We could just remove that line. That line is an attempt to keep a relatively recent version of the ubuntu image cached on the worker VM so that we can save some time when pulling the worker Docker image. In practice, the ubuntu image is extraordinarily tiny and quickly becomes out of date (because we rarely rebuild the VM). hail-ubuntu uses a timestamped ubuntu 20.04 tag: `ubuntu:focal-20201106`. I did this because we kept getting screwed by new ubuntu images getting released which were incompatible with us. We would only find out later when we changed the hail-ubuntu Dockerfile and triggered a refetch of the latest image at the `ubuntu:18.04` tag which included the breaking changes. You're correct that this is technical debt of ours. DOCKER_ROOT_IMAGE should really be SOME_LINUX_IMAGE and we shouldn't bother trying to fetch it in the worker image startup script. You're fighting the good fight, but, man, keeping a growing software project clean is hard.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11046#issuecomment-965578805:1355,cache,cached,1355,https://hail.is,https://github.com/hail-is/hail/pull/11046#issuecomment-965578805,1,['cache'],['cached']
Performance, 7.0.0. Specify --resolver=backtracking to silence this warning.; + cat python/pinned-requirements.txt; + sed /#/d; + sed /#/d; + cat /tmp/tmp.YoVBQEw8XF; + diff /tmp/tmp.WRSKGgGEB8 /tmp/tmp.C8ggaXDHDt; sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$' | tr '\n' '\0' | xargs -0 python3 -m pip install -U; Defaulting to user installation because normal site-packages is not writeable; Collecting aiodns==2.0.0; Using cached aiodns-2.0.0-py2.py3-none-any.whl (4.8 kB); Collecting aiohttp==3.8.5; Using cached aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB); Collecting aiosignal==1.3.1; Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB); Collecting async-timeout==4.0.3; Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:32538,cache,cached,32538,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance," </li>; <li>; <p>Determine MPO size from markers, not EXIF data <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7884"">#7884</a>; [radarhere]</p>; </li>; <li>; <p>Improved conversion from RGB to RGBa, LA and La <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7888"">#7888</a>; [radarhere]</p>; </li>; <li>; <p>Support FITS images with GZIP_1 compression <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7894"">#7894</a>; [radarhere]</p>; </li>; <li>; <p>Use I;16 mode for 9-bit JPEG 2000 images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7900"">#7900</a>; [scaramallion, radarhere]</p>; </li>; <li>; <p>Raise ValueError if kmeans is negative <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7891"">#7891</a>; [radarhere]</p>; </li>; <li>; <p>Remove TIFF tag OSUBFILETYPE when saving using libtiff <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7893"">#7893</a>; [radarhere]</p>; </li>; <li>; <p>Raise ValueError for negative values when loading P1-P3 PPM images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7882"">#7882</a>; [radarhere]</p>; </li>; <li>; <p>Added reading of JPEG2000 palettes <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7870"">#7870</a>; [radarhere]</p>; </li>; <li>; <p>Added alpha_quality argument when saving WebP images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7872"">#7872</a>; [radarhere]</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/python-pillow/Pillow/commit/5c89d88eee199ba53f64581ea39b6a1bc52feb1a""><code>5c89d88</code></a> 10.3.0 version bump</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/63cbfcfdea2d163ec93bae8d283fcfe4b73b5dc7""><code>63cbfcf</code></a> Update CHANGES.rst [ci skip]</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/2776126aa9",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14439:12637,load,loading,12637,https://hail.is,https://github.com/hail-is/hail/pull/14439,3,['load'],['loading']
Performance," <https://github.com/hail-is/hail/pull/3973#discussion_r207422997>:; >; > > +}; > +; > +std::string strip_suffix(const std::string& s, const char* suffix) {; > + size_t len = s.length();; > + size_t n = strlen(suffix);; > + if ((n > len) || (strncmp(&s[len-n], suffix, n) != 0)) return s;; > + return std::string(s, 0, len-n);; > +}; > +; > +std::string get_cxx_name() {; > + char* p = ::getenv(""CXX"");; > + if (p) return std::string(p);; > + // We prefer clang because it has faster compile; > + auto s = run_shell_get_first_line(""which clang"");; > + if (strstr(s.c_str(), ""clang"")) return s;; > + s = run_shell_get_first_line(""which g++"");; >; > I'm lazy and I want to save my thinking for stuff where I can really; > provide value (like building a whole stage code generator, or designing a; > new format to speed up the decoder!) If there are standards, I want to use; > them so I don't have to think unless I have a strong reason to think they; > won't work for me. It also makes communicating with others easier who are; > expecting the same standards. So I'd just kick out a Makefile and call make; > on it like I would for any project (and maybe print out some helpful info; > when it fails like the make command, the path to the Makefile and maybe the; > environment). I didn't realize it would be such a hard sell because I; > thought I was proposing the easy path which was the least work. I like the; > easy path! And if it means we can get this in and start building on it, all; > the better.; >; > The cached Makefile isn't actually reused. ... It's occasionally useful to; > run it by hand for debugging.; >; > Ah, thanks for clarifying. Definitely.; >; > —; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/3973#discussion_r207422997>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AJzExsJ8Hk-GuvDywoKPy8DF_SSBqgG-ks5uM66RgaJpZM4VbZpP>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410127709:3008,cache,cached,3008,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410127709,1,['cache'],['cached']
Performance," <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:2136,perform,performance,2136,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['perform'],['performance']
Performance," <ul>; <li>Setting or accessing <code>json_encoder</code> or <code>json_decoder</code> raises a; deprecation warning. :issue:<code>4732</code></li>; </ul>; <h2>Version 2.2.0</h2>; <p>Released 2022-08-01</p>; <ul>; <li>; <p>Remove previously deprecated code. :pr:<code>4667</code></p>; <ul>; <li>Old names for some <code>send_file</code> parameters have been removed.; <code>download_name</code> replaces <code>attachment_filename</code>, <code>max_age</code>; replaces <code>cache_timeout</code>, and <code>etag</code> replaces <code>add_etags</code>.; Additionally, <code>path</code> replaces <code>filename</code> in; <code>send_from_directory</code>.</li>; <li>The <code>RequestContext.g</code> property returning <code>AppContext.g</code> is; removed.</li>; </ul>; </li>; <li>; <p>Update Werkzeug dependency to &gt;= 2.2.</p>; </li>; <li>; <p>The app and request contexts are managed using Python context vars; directly rather than Werkzeug's <code>LocalStack</code>. This should result; in better performance and memory use. :pr:<code>4682</code></p>; <ul>; <li>Extension maintainers, be aware that <code>_app_ctx_stack.top</code>; and <code>_request_ctx_stack.top</code> are deprecated. Store data on; <code>g</code> instead using a unique prefix, like; <code>g._extension_name_attr</code>.</li>; </ul>; </li>; <li>; <p>The <code>FLASK_ENV</code> environment variable and <code>app.env</code> attribute are; deprecated, removing the distinction between development and debug; mode. Debug mode should be controlled directly using the <code>--debug</code>; option or <code>app.run(debug=True)</code>. :issue:<code>4714</code></p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/flask/commit/a1c478bc93d3dc018a6e7a1ba3cf5409553c9df3""><code>a1c478b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/flask/issues/4755"">#4755</a> from",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12206:6606,perform,performance,6606,https://hail.is,https://github.com/hail-is/hail/pull/12206,1,['perform'],['performance']
Performance," == 'T', 'A',; hl.cond(base == 'C', 'G',; hl.cond(base == 'G', 'C', base)))). def main(pheno):; # load ldpred sumstats file; sumstats = hl.import_table('gs://ukbb_prs/sumstats/UKB_'+pheno+'_LDPred.txt', delimiter='\s+', impute=True, min_partitions=100). # create locus and alleles columns and key by locus; sumstats = (sumstats.annotate(locus=hl.parse_locus(sumstats.chrom[6:] + "":"" + hl.str(sumstats.pos)),; alleles=[sumstats.nt1,sumstats.nt2]); .key_by('locus')). # write the sumstats table; sumstats.write('gs://ukbb_prs/sumstats/temp.kt', overwrite=True). # read the sumstats table; sumstats = hl.read_table('gs://ukbb_prs/sumstats/temp.kt'). # remove leading zeros from contigs; contigs = {'0{}'.format(x):str(x) for x in range(1, 10)}. # import bgen(s); mt = hl.methods.import_bgen('gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{22}_v3.bgen',; ['dosage'],; sample_file='gs://phenotype_31063/ukb31063.imputed_v3.autosomes.sample',; contig_recoding=contigs). # load scoring sample; sampleids = hl.import_table('gs://ukb31063-mega-gwas/hail-0.1/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'). # merge sumstats on bgen matrixtable; mt = mt.annotate_rows(ss=sumstats[mt.locus]). # handle strand/sign flips -- score in terms of alt allele; mt = mt.annotate_rows(beta = hl.case(); .when(((mt.alleles[0] == mt.ss.nt1) &; (mt.alleles[1] == mt.ss.nt2)) |; ((flip_text(mt.alleles[0]) == mt.ss.nt1) &; (flip_text(mt.alleles[1]) == mt.ss.nt2)),; (-1*mt.ss.ldpred_inf_beta)); .when(((mt.alleles[0] == mt.ss.nt2) &; (mt.alleles[1] == mt.ss.nt1)) |; ((flip_text(mt.alleles[0]) == mt.ss.nt2) &; (flip_text(mt.alleles[1]) == mt.ss.nt1)),; mt.ss.ldpred_inf_beta); .or_missing()). # filter bgen matrixtable down to only SNPs with betas; mt = mt.filter_rows(hl.is_defined(mt.beta)). # filter bgen matrixtable to only include people in scoring sample; mt = mt.filter_cols(hl.is_defined(sampleids[mt.s])). # score sample; mt = mt.annotate_cols(prs=hl.agg.sum(mt.beta * mt.dosage)). # wr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:1290,load,load,1290,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,1,['load'],['load']
Performance," ACCEPTED); 2019-01-22 13:11:36 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:36 YarnSchedulerBackend$YarnSchedulerEndpoint: INFO: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 2019-01-22 13:11:36 YarnClientSchedulerBackend: INFO: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> scc-hsn1.scc.bu.edu, PROXY_URI_BASES -> https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0174), /proxy/application_1542127286896_0174; 2019-01-22 13:11:36 JettyUtils: INFO: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 2019-01-22 13:11:37 Client: INFO: Application report for application_1542127286896_0174 (state: RUNNING); 2019-01-22 13:11:37 Client: INFO:; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: 192.168.18.203; ApplicationMaster RPC port: 0; queue: default; start time: 1548180691687; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0174/; user: farrell; 2019-01-22 13:11:37 YarnClientSchedulerBackend: INFO: Application application_1542127286896_0174 has started running.; 2019-01-22 13:11:37 Utils: INFO: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44817.; 2019-01-22 13:11:37 NettyBlockTransferService: INFO: Server created on 10.48.225.55:44817; 2019-01-22 13:11:37 BlockManager: INFO: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 2019-01-22 13:11:37 BlockManagerMaster: INFO: Registering BlockManager BlockManagerId(driver, 10.48.225.55, 44817, None); 2019-01-22 13:11:37 BlockManagerMasterEndpoint: INFO: Registering block manager 10.48.225.55:44817 with 2.5 GB RAM, BlockManagerId(driver, 10.48.225.55, 44817, None); 2019-01-22 13:11:37 BlockManagerMaster: INFO: Registered BlockManager BlockManagerId(driver, 10.4",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:18891,queue,queue,18891,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['queue'],['queue']
Performance," BlockManagerId(1, bw2-sw-dp3j.c.seqr-project.internal, 43693, None); 2019-07-14 20:55:04 BlockManagerMaster: INFO: Removed 1 successfully in removeExecutor; 2019-07-14 20:55:04 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 1 at RPC address 10.128.0.126:36052, but got no response. Marking as slave lost.; java.io.IOException: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:357); at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:334); at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507); at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481); at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420); at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122); at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.uti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:2302,concurren,concurrent,2302,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['concurren'],['concurrent']
Performance," ClientSecretCredential.get_token succeeded; 2023-09-06T21:45:25 INFO batch_client.aioclient aioclient.py:809:_submit created batch 191; submit job bunches ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 1/1 0:00:00 0:00:00; 2023-09-06T21:47:17 WARNING hailtop.utils utils.py:842:retry_transient_errors_with_debug_string A transient error occured. We will automatically retry. Do not be alarmed. We have thus far seen 2 transient errors (next delay: 3.794s). The most recent error was <class 'asyncio.exceptions.TimeoutError'> . . +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++. ~~~~~~~~~~~~~~~~~~~~~ Stack of asyncio_0 (140387515627072) ~~~~~~~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++; FAILED; _________________ test_always_run_job_private_instance_cancel __________________. client = <hailtop.batch_client.client.BatchClient object at 0x7fae899806a0>. def test_always_run_job_private_instance_cancel(client: BatchClient):; b = create_batch(client); resources = {'machine_type': smallest_machine_type()}; j = b.create_job(DOCKER_ROOT_IMAGE, ['true'], resources=resources, always_run=True); b.submit(); b.cancel(); > status = j.wait(). io/test/test_batch.py:1487: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; usr/local/lib/python3.9/dist-packages/hailtop/batch_client/client.py:84: in wait; return async_to_blocking(self._async_job.wait()); usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py:156: in async_to_blocking; return loop.run_until_complete(task); usr/lib/python3.9/a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13582:1635,concurren,concurrent,1635,https://hail.is,https://github.com/hail-is/hail/issues/13582,1,['concurren'],['concurrent']
Performance, Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB); Collecting typer==0.9.0; Using cached typer-0.9.0-py3-none-any.whl (45 kB); Collecting typing-extensions==4.7.1; Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB); Collecting tzdata==2023.3; Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB); Collecting urllib3==1.26.16; Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB); Collecting uvloop==0.17.0; Using cached uvloop-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB); Collecting wrapt==1.15.0; Using cached wrapt-1.15.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB); Collecting xyzservices==2023.7.0; Using cached xyzservices-2023.7.0-py3-none-any.whl (5,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:40069,cache,cached,40069,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance," Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB); Collecting typer==0.9.0; Using cached typer-0.9.0-py3-none-any.whl (45 kB); Collecting typing-extensions==4.7.1; Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB); Collecting tzdata==2023.3; Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB); Collecting urllib3==1.26.16; Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB); Collecting uvloop==0.17.0; Using cached uvloop-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB); Collecting wrapt==1.15.0; Using cached wrapt-1.15.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB); Collecting xyzservices==2023.7.0; Using cached xyzservices-2023.7.0-py3-none-any.whl (56 kB); Collecting yarl==1.9.2; Using cached yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB); Building wheels for collected packages: avro; Building wheel for avro (pyproject.toml): started; Building wheel for avro (pyproject.toml): finished with status 'done'; Created wheel for avro: filename=avro-1.11.2-py2.py3-none-any.whl size=119738 sha256=d7f238f86de270b449b018590930a06270766887328bdb51066eccff2cd696a6; Stored in directory: /home/hadoop/.cache/pip/wheels/e3/a2/1e/5c1be0865f4170a89de34e0a798f32f674a7eaf63a93272c7f; Successfully built avro; Installing collected packages: sortedcontainers, pytz, py4j, commonmark, azure-common, xyzservices, wrapt, uvloop, urllib3, tzdata, typing-extensions, tornado, tenacity, tabulate, six, regex, pyyaml, python-json-logger, pyjwt, pygments, pycparser, pyasn1, protobuf, portalocker, pillow, packaging, orjs; on, oauthlib, numpy, nest-asyncio, multidict, markupsafe, jmespath, idna, huma",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:41027,cache,cached,41027,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance," ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 3.975ms, total 407.299ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- LoweringTransformation : 77.664ms, total 485.244ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 1.780ms, total 487.281ms, tagged coverage 0.0; ...; ...; ...; 2019-11-06 18:44:11 root: INFO: Timer: aggregate:; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'Verify' (8): 8.109ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'ConvertToSafeValue' (2): 12.828ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'InitializeCompiledFunction' (2): 14.536ms; 2019-11-06 18",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:2746,Optimiz,Optimize,2746,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance," I learned early that the issue was somehow related to method splitting, but it was far more devious than I expected, and the code in LIR method splitting is completely correct. The particular case I was debugging had expected behavior when trying to write the missing bits of four fields A,B,C,D, where A and B were missing and C and D were present. These should have written the byte. 1<<0 | 1<<1 | 0<<2 | 0<<3; ==> b00000011; ==> 3. But instead wrote the byte `b00000001 or 1`, incorrectly leading readers to try to read field B when it was missing (and not written). This is due to the load-bearing and incorrect type of an I2B instruction generated [here](https://github.com/hail-is/hail/blob/8bd9b7b2224b77372a72f02f2b13806267892a35/hail/src/main/scala/is/hail/types/encoded/EBaseStruct.scala#L107). I2B is an instruction that truncates an integer to a byte, and it is used in various places in code generation but primarily encoding missing bits in arrays and structs. . I2B loads a byte to the stack, not a boolean. TypeInfos are mostly non-structural since they rarely influence the bytecode generated. Here is an exception, and that's where the method splitter comes in. Method splitting exists in the Hail compiler because not only does the JVM have limits on how large methods can be, but also the JIT compiler handles small methods much more effectively than large methods (and so splitting a large method into two small ones can make an order of magnitude or more in performance difference). We have three forms of method splitting in the Hail Query compiler. The first is a heuristic and greedy IR-level method splitter that generates new methods every X IR nodes, simply based on node count. However, the size of code generated by each IR can vary widely (`I32` vs `LowerBoundOnOrderedCollection` for instance), and so we have two other kinds of splitting that operate on the LIR level. The first is region splitting, which is used to split large blocks of LIR. In order to insert a s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:1062,load,loads,1062,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['load'],['loads']
Performance, JVMEntryway: INFO: 8: gs://cpg-bioheart-hail/batch-tmp/tmp/hail/sRjJqvkZ3l9nmKuUErfNZv/jHpWQ6lemx/out; 2024-11-05 02:43:37.202 JVMEntryway: INFO: Yielding control to the QoB Job.; 2024-11-05 02:43:37.206 ServiceBackendAPI$: INFO: BatchClient allocated.; 2024-11-05 02:43:37.207 ServiceBackendAPI$: INFO: BatchConfig parsed.; 2024-11-05 02:43:37.209 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2024-11-05 02:43:37.783 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]; 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]; 	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) [jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]; 	at java.lang.Thread.run(Thread.java:829) [?:?]; Caused by: com.fasterxml.jackson.core.exc.StreamConstraintsException: String length (20013488) exceeds the maximum length (20000000); 	at com.fasterxml.jackson.core.StreamReadConstraints.validateStringLength(StreamReadConstraints.java:324) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.ReadConstrainedTextBuffer.validateStringLength(ReadConstrainedTextBuffer.java:27) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.TextBuffer.finishCurren,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14749:2683,concurren,concurrent,2683,https://hail.is,https://github.com/hail-is/hail/issues/14749,1,['concurren'],['concurrent']
Performance," MatrixTable, GroupedMatrixTable # noqa: E402. /opt/conda/lib/python3.7/site-packages/hail/table.py in <module>; 4 import numpy as np; ----> 5 import pyspark; 6 from typing import Optional, Dict, Callable, Sequence. ModuleNotFoundError: No module named 'pyspark'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); /tmp/ipykernel_233/4275665471.py in <module>; ----> 1 combined_pandas = pd.read_pickle(gwas_pandas_file). /opt/conda/lib/python3.7/site-packages/pandas/io/pickle.py in read_pickle(filepath_or_buffer, compression, storage_options); 220 # ""No module named 'pandas.core.sparse.series'""; 221 # ""Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib""; --> 222 return pc.load(handles.handle, encoding=None); 223 except UnicodeDecodeError:; 224 # e.g. can occur for files written in py27; see GH#28645 and GH#31988. /opt/conda/lib/python3.7/site-packages/pandas/compat/pickle_compat.py in load(fh, encoding, is_verbose); 272 up.is_verbose = is_verbose; 273 ; --> 274 return up.load(); 275 except (ValueError, TypeError):; 276 raise. /opt/conda/lib/python3.7/pickle.py in load(self); 1086 raise EOFError; 1087 assert isinstance(key, bytes_types); -> 1088 dispatch[key[0]](self); 1089 except _Stop as stopinst:; 1090 return stopinst.value. /opt/conda/lib/python3.7/pickle.py in load_stack_global(self); 1383 if type(name) is not str or type(module) is not str:; 1384 raise UnpicklingError(""STACK_GLOBAL requires str""); -> 1385 self.append(self.find_class(module, name)); 1386 dispatch[STACK_GLOBAL[0]] = load_stack_global; 1387 . /opt/conda/lib/python3.7/site-packages/pandas/compat/pickle_compat.py in find_class(self, module, name); 204 key = (module, name); 205 module, name = _class_locations_map.get(key, key); --> 206 return super().find_class(module, name); 207 ; 208 . /opt/conda/lib/python3.7/pickle.py in find_class(self, module, name); 1424 elif module in _compat_pickle.IMPORT_MAPPING:; 1425 modu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14004:2928,load,load,2928,https://hail.is,https://github.com/hail-is/hail/issues/14004,1,['load'],['load']
Performance, Size Trials Mean (ms) Std Dev (ms) Median (ms) 90th % (ms); ========= ========= ====== ========= ============ =========== ===========; Delete 0 B 5 43.1 6.4 40.9 50.9 ; Delete 1 KiB 5 44.2 12.7 42.5 58.1 ; Delete 100 KiB 5 44.7 10.4 42.8 56.3 ; Delete 1 MiB 5 41.5 3.7 40.2 45.7 ; Download 0 B 5 74.6 7.9 73.2 84.0 ; Download 1 KiB 5 84.3 15.9 80.6 103.4 ; Download 100 KiB 5 81.9 16.0 82.7 99.6 ; Download 1 MiB 5 90.6 6.5 94.5 96.8 ; Metadata 0 B 5 23.6 2.7 23.6 26.3 ; Metadata 1 KiB 5 25.5 2.1 26.9 27.4 ; Metadata 100 KiB 5 26.2 3.6 27.3 29.9 ; Metadata 1 MiB 5 24.0 3.7 23.3 28.4 ; Upload 0 B 5 98.1 16.6 95.5 117.9 ; Upload 1 KiB 5 116.7 21.8 115.5 142.1 ; Upload 100 KiB 5 116.5 17.8 115.1 135.1 ; Upload 1 MiB 5 168.2 18.5 179.6 185.6 . ------------------------------------------------------------------------------; Write Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Write throughput: 977.7 Mibit/s.; Parallelism strategy: both. ------------------------------------------------------------------------------; Read Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Read throughput: 1.11 Gibit/s.; Parallelism strategy: both. ------------------------------------------------------------------------------; System Information ; ------------------------------------------------------------------------------; IP Address: ; 172.21.46.11; Temporary Directory: ; /tmp; Bucket URI: ; gs://hail-jigold/; gsutil Version: ; 5.24; boto Version: ; 2.49.0; Measurement time: ; 2023-06-05 03:25:16 PM ; Running on GCE: ; True; GCE Instance:; 	; Bucket location: ; US-CENTRAL1; Bucket storage class: ; REGIONAL; Google Server: ; ; Google Server IP Addresses: ; 142.250.128.128; 142.251.6.128; 108.177.112.128; 74.125.124.128; 172.217.212.128; 172.217.214.128; 172.253.119.128; 108.177.1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597:1998,throughput,throughput,1998,https://hail.is,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597,1,['throughput'],['throughput']
Performance," Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4810 bytes); 2018-10-09 14:46:41 Executor: INFO: Running task 0.0 in stage 0.0 (TID 0); 2018-10-09 14:46:41 Executor: INFO: Fetching spark://10.32.119.167:61636/jars/sparklyr-2.2-2.11.jar with timestamp 1539121597559; 2018-10-09 14:46:41 TransportClientFactory: INFO: Successfully created connection to /10.32.119.167:61636 after 11 ms (0 ms spent in bootstraps); 2018-10-09 14:46:41 Utils: INFO: Fetching spark://10.32.119.167:61636/jars/sparklyr-2.2-2.11.jar to /private/var/folders/w4/9k0my8pd6113d61pq05fvqlr0000gn/T/spark-02128b51-f37e-4798-84bb-d3e3819e51be/userFiles-7053b92c-9117-46b6-8c52-752fee2701e9/fetchFileTemp5171194947676284646.tmp; 2018-10-09 14:46:41 Executor: INFO: Adding file:/private/var/folders/w4/9k0my8pd6113d61pq05fvqlr0000gn/T/spark-02128b51-f37e-4798-84bb-d3e3819e51be/userFiles-7053b92c-9117-46b6-8c52-752fee2701e9/sparklyr-2.2-2.11.jar to class loader; 2018-10-09 14:46:41 CodeGenerator: INFO: Code generated in 142.013327 ms; 2018-10-09 14:46:41 Executor: INFO: Finished task 0.0 in stage 0.0 (TID 0). 972 bytes result sent to driver; 2018-10-09 14:46:41 TaskSetManager: INFO: Finished task 0.0 in stage 0.0 (TID 0) in 406 ms on localhost (executor driver) (1/1); 2018-10-09 14:46:41 TaskSchedulerImpl: INFO: Removed TaskSet 0.0, whose tasks have all completed, from pool ; 2018-10-09 14:46:41 DAGScheduler: INFO: ResultStage 0 (collect at utils.scala:44) finished in 0.420 s; 2018-10-09 14:46:41 DAGScheduler: INFO: Job 0 finished: collect at utils.scala:44, took 0.669318 s; 2018-10-09 14:46:41 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:41 SparkSqlParser: INFO: Parsing command: table7e606a8b83f4; 2018-10-09 14:46:41 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:41 SparkSqlParser: INFO: Parsing command: CACHE TABLE `tabl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:34788,load,loader,34788,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['load'],['loader']
Performance," True]); ; self.assertEqual(hl.eval(hl.len([0, 1, 4, 6])), 4); ; > self.assertTrue(math.isnan(hl.eval(hl.mean(hl.empty_array(hl.tint))))). test/hail/expr/test_expr.py:2240: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-540>:2: in eval; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:189: in eval; return eval_timed(expression)[0]; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-538>:2: in eval_timed; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:155: in eval_timed; return Env.backend().execute(expression._ir, True); hail/backend/backend.py:109: in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in parse; ir_map); /miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py:1257: in __call__; answer, self.gateway_client, self.target_id, self.name); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. args = ('xro1961', <py4j.java_gateway.GatewayClient object at 0x7ffa62e9e390>, 'z:is.hail.expr.ir.IRParser', 'parse_value_ir'), kwargs = {}; pyspark = <module 'pyspark' from '/miniconda3/lib/python3.7/site-packages/pyspark/__init__.py'>, s = 'java.lang.RuntimeException: typ: inference failure: \n(MakeArray Array[Int32])'; tpl = JavaObject id=o1962, deepest = 'NoSuchElementException: next on empty iterator'; full = 'java.lang.RuntimeException: typ: inference fai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:1820,load,loads,1820,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930,1,['load'],['loads']
Performance," Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(sc); hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-0320a61; Error summary: HailException: arguments refer to no files; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321215583:2520,Load,LoadVCF,2520,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321215583,2,['Load'],['LoadVCF']
Performance," When we declare a Service for say, batch in default, Kubernetes adds a DNS record for `batch.default` that resolves to a single IP pointing at kube-proxy. When a new TCP connection is established with kube-proxy for that IP, it rolls the dice (using `iptables`) and assigns that connection to a particular pod to which it will forward all subsequent packets. From the load-balancer's perspective, there is only one IP address, and only one place to open connections. The load-balancer doesn't have the information to actually load-balance once we have a functioning connection pool. This can lead to really unbalanced scenarios when preemptible pods come and go. This leads to our second goal: instead of routing all requests through kube-proxy, use Kubernetes Headless Services to expose all pod IPs underlying a Service so that our proxies can properly load-balance across persistent connections. ## Solution. This PR addresses the two goals outlined above and does so through using Envoy, a load-balancer/proxy that is well-suited to this sort of highly-dynamic cluster configuration. Envoy does not have the constraint that all upstream services must be available at start-time, and has a very convenient API for updating the cluster configuration without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as follows:. 1. Envoy-based gateways and internal-gateways will load their routing configuration from a Kubernetes ConfigMap, which they watch for changes and reconcile their configuration when the ConfigMap changes. The ConfigMap can be populated with a manual deploy and is populated from the beginning with production routes (i.e. batch.hail.is gets routed to batch.default); 2. When running CI, CI will regularly update the ConfigMap with additional routes based on which inte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:3625,load,load-balancer,3625,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['load'],['load-balancer']
Performance," Z; 31027 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31028 25782 (LdcX 0 I))); 31029 25783 (InsnX ISHL; 31030 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2348null Z; 31031 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31032 25785 (LdcX 1 I))); 31033 25786 (InsnX ISHL; 31034 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2350null Z; 31035 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31036 25788 (LdcX 2 I))); 31037 25789 (InsnX ISHL; 31038 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2352null Z; 31039 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31040 25791 (LdcX 3 I))))); 31041 (ReturnX). # Elsewhere, this split method is called, then the resulting field is loaded and written to the output buffer. 11325 (MethodStmtX INVOKEVIRTUAL __C1527collect_distributed_array.__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616_region0_0 (L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)V; 11326 (LoadX arg:0 L__C1527collect_distributed_array;); 11327 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 11328 25772 (MethodStmtX INVOKEINTERFACE is/hail/io/OutputBuffer.writeByte (B)Vinterface; 11329 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2354null Lis/hail/io/OutputBuffer;; 11330 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 11331 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2355__l2315split_large_block Z; 11332 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:5629,load,loaded,5629,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['load'],['loaded']
Performance," Zain (1) +</li>; </ul>; <p>A total of 14 people contributed to this release.; People with a &quot;+&quot; by their names contributed a patch for the first time.; This list of names is automatically generated, and may not be fully complete.</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/scipy/scipy/commit/656076ca6b490f587e9bd9c4cd10cb259a687c5b""><code>656076c</code></a> MAINT: wheel push 1.9.2 [wheel build]</li>; <li><a href=""https://github.com/scipy/scipy/commit/ad0d0f907010fbc8b66cdbe8ce0af2683881a309""><code>ad0d0f9</code></a> REL: set 1.9.2 released [wheel build]</li>; <li><a href=""https://github.com/scipy/scipy/commit/d9ad9801323653a2015b4d3e80d6d3ea93b6c021""><code>d9ad980</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17150"">#17150</a> from tylerjereddy/treddy_scipy_192_more_backports</li>; <li><a href=""https://github.com/scipy/scipy/commit/6b098c25223e224ff44101f86bbc86efecffe1d9""><code>6b098c2</code></a> TST: optimize.milp: remove problematic timeout/iteration test</li>; <li><a href=""https://github.com/scipy/scipy/commit/24dce9760b87934f1be046ec817c758b0f3952dc""><code>24dce97</code></a> DOC: stats.pearsonr: typo in coeffic<em>i</em>ent (<a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17153"">#17153</a>)</li>; <li><a href=""https://github.com/scipy/scipy/commit/a6ba7cad3b54c35d2ccb55c595691689004742c1""><code>a6ba7ca</code></a> MAINT: misc 1.9.2 updates</li>; <li><a href=""https://github.com/scipy/scipy/commit/ed9760e60a28b8f13e5644494033e2dab9aafbcd""><code>ed9760e</code></a> MAINT: stats.pearson3: fix ppf for negative skew (<a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17055"">#17055</a>)</li>; <li><a href=""https://github.com/scipy/scipy/commit/6fb67007dd7105755057f3379fb7ef423eae524e""><code>6fb6700</code></a> FIX: optimize.milp: return feasible solution if available on timeout/node lim...</li>; <li><a href=""https",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12352:1931,optimiz,optimize,1931,https://hail.is,https://github.com/hail-is/hail/pull/12352,1,['optimiz'],['optimize']
Performance," `/ / /; /_/ /_/\_,_/_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181114-1827-0.2-721af83bc30a.log; Exception in thread ""dispatcher-event-loop-8"" Exception in thread ""refresh progress"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(Heart",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780:1854,load,loadClass,1854,https://hail.is,https://github.com/hail-is/hail/issues/4780,1,['load'],['loadClass']
Performance," ```; tgp = hl.import_vcf('gs://genomics-public-data/1000-genomes-phase-3/vcf-20150220/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf'); tgp.describe(); tgp.rows().show(); ```; Getting:; ```; hail.utils.java.FatalError: NoSuchElementException: key not found: GT. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 104, pca-w-1.c.daly-ibd.internal, executor 2): is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:1059,Load,LoadVCF,1059,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance," a 35K cohort. The VCF format of chr1 is 2.4T.; > ; > Heh. So, yes, ""project"" VCFs grow super-linearly in the number of samples. I (and others) are currently pushing very hard for the VCF spec to support two sparse representations: ""local alleles"" ([samtools/hts-specs#434](https://github.com/samtools/hts-specs/pull/434)) and ""reference blocks"" ([samtools/hts-specs#435](https://github.com/samtools/hts-specs/pull/435)). When using these two sparse representations, you should be able to store 35,000 whole genomes in ~10TiB of GZIP-compressed VCF.; > ; > What is your calling pipeline? Do you generate GVCFs? If yes, I strongly recommend you use the [VDS Combiner](https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html#hail.vds.combiner.VariantDatasetCombiner) to produce a [VDS](https://hail.is/docs/0.2/vds/index.html). You can read more details in [this recent preprint we wrote](https://www.biorxiv.org/content/10.1101/2024.01.09.574205v1.full.pdf), but a VDS of 35,000 whole genomes should be a few terabytes. I'd guess 4 TiB, but it depends on your reference block granularity. I strongly recommend using size 10 GQ buckets. Looks like VDS is a better solution than HailMatrix. However, we got the joint call result as vcf alreay. Can VDS Combiner read joint call VCF and then save it as VDS format? I cannot find any example to transfer VCF to VDS. Thanks. > ; > > I don't know the Kryo JAR. I tested on both docker images hailgenetics/hail:0.2.126-py3.11 and hailgenetics/hail:0.2.127-py3.11.; > ; > Those should use Kryo 4.0.2. OK. My conclusion is that Kryo still has a bug preventing the serialization of very large objects. This becomes a limitation in Hail: we cannot support PLINK files with tens of millions of variants. Our community is largely transitioning to GVCFs and VDS, so I doubt we'll improve our PLINK1 importer to support such large PLINK1 files. That said, PRs are always welcome if loading such large PLINK1 files is a hard requirement for you all.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168#issuecomment-1937459344:1945,load,loading,1945,https://hail.is,https://github.com/hail-is/hail/issues/14168#issuecomment-1937459344,1,['load'],['loading']
Performance," a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add terraform rules for the query service account. It already existed, but it",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:1533,load,load,1533,https://hail.is,https://github.com/hail-is/hail/pull/10279,1,['load'],['load']
Performance," a diagram of the current and proposed scenarios that I hope helps:. ### Normal services (current `main`); 1. gateway receives a request destined for `batch.hail.is`; 2. gateway intends to forward this request to `batch.default:443`; 3. gateway makes a DNS request to resolve `batch.default`. gateway receives IP `A.A.A.A` which is the cluster IP of the batch Kubernetes Service; 4. gateway forwards the request to `A.A.A.A:443`; 5. The Kubernetes Service (really kube-proxy) receives the request, selects a pod with IP `X.X.X.X` and forwards the request to `X.X.X.X:5000`. ### Proposed headless service approach; 1. gateway receives a request destined for `batch.hail.is`; 2. gateway intends to forward this request to `batch.default:443`; 3. gateway makes a DNS request to resolve `batch.default`. gateway receives multiple DNS records back saying that `batch.default` corresponds to the IP addresses `X.X.X.X`, `Y.Y.Y.Y`, and `Z.Z.Z.Z` (assuming there are 3 pods in the deployment).; 4. gateway gets its pick out of the pods (this is really important and is why envoy needs all the IPs to properly load balance!) and decides to forward the request directly to pod `X.X.X.X:443`. So in the second scenario, it is necessary that the pod itself be listening on 443 because that is where gateway is going to send the request. It is not exactly a permissions issue, but upon writing this I am now realizing that by doing so we *require* that the service pods like `auth` and `batch` be running as root in order to bind on port 443. I think the port specified in the `Service` yaml is actually useless now. So two actionable options are:. 1. Remove the useless `port` field on the `Service` yaml for auth, batch, etc.; 2. Keep all of our services on unprivileged ports (5000) and have gateway forward traffic to `batch.default:5000` instead of `batch.default:443`. Keeping our services on port 5000 could allow us to run those services as non-root users. I guess k8s has them running as root by default…",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12094#issuecomment-1283052287:1135,load,load,1135,https://hail.is,https://github.com/hail-is/hail/pull/12094#issuecomment-1283052287,1,['load'],['load']
Performance," a near drop-in for Flask, and is the most popular afaik library built around asyncio. It has 2x as many stars as aiohttp, slightly more forks. The original motivation for considering aiohttp: ; https://magic.io/blog/uvloop-blazing-fast-python-networking. In short, one of the creators of asyncio discusses uvloop performance relative to other libraries. They key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference stand",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:1458,perform,performance,1458,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['perform'],['performance']
Performance," able to implement read, write, and; matrix-multiply for DNDArray!. In addition, to the arguable hacks above, a couple pain points remain:; 1. I do not know how to rename keys in Python without triggering shuffles. If I; write `key_by(x=t.y, y=t.x)`, Hail implements this as; `TableKeyBy(TableMapRows(TableKeyBy(Array(), ...)`. The inner key by throws; the keys away so that they can be modified with TableMapRows. Unfortunately,; this completely defeats my attempts to avoid shuffles. I avoid this issue by; not using fixed names for the x and y block coordinates (their names are; stored in `x_field` and `y_field`).; 2. Hail lacks `ndarray_sum`. Instead, I convert from ndarray to array so that I; can use `array_sum`. Unfortunately, this operation seems to completely; dominate all of my time. It takes about 10x as much time as the matrix; multiplies take. I do not understand this. I should be reading the entries in; column-major order. Performance; -----------. ```; In [1]: %%time; ...: import hail as hl; ...: mt = hl.balding_nichols_model(n_populations=2,; ...: n_variants=10000,; ...: n_samples=10000,; ...: n_partitions=100); ...: mt = mt.select_entries(gt = hl.float(mt.GT.n_alt_alleles())); ...: da = hl.experimental.dnd.array(mt, 'gt'); ...: da.write('/tmp/in.da', overwrite=True); In [3]: %%time; ...: bm = hl.linalg.BlockMatrix.from_entry_expr(mt.gt); In [5]: %%time; ...: (bm @ bm.T).write('/tmp/foo.bm', overwrite=True); In [7]: %%time; ...: import hail as hl; ...: da = hl.experimental.dnd.read('/tmp/in.da'); ...: (da @ da.T).write('/tmp/out.da', overwrite=True); ```. Block matrix performed the matrix multiply in 19.3s. DNDArray performed the; matrix multiply in 37.6s. Block Matrix:; ![Screen Shot 2020-05-26 at 1 37 51 PM](https://user-images.githubusercontent.com/106194/82932367-54630200-9f56-11ea-86f4-94726c36d727.png). DNDArray:; ![Screen Shot 2020-05-26 at 1 37 08 PM](https://user-images.githubusercontent.com/106194/82932387-5927b600-9f56-11ea-820e-97d1eb443d8f.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8864:2644,perform,performed,2644,https://hail.is,https://github.com/hail-is/hail/pull/8864,2,['perform'],['performed']
Performance," already died by this point. ```; {; ""batch_id"": 1,; ""job_id"": 19,; ""name"": ""18"",; ""state"": ""Error"",; ""exit_code"": null,; ""duration"": 10408,; ""msec_mcpu"": 1040800,; ""cost"": ""$0.0000"",; ""status"": {; ""worker"": ""batch-worker-dking-16py5"",; ""batch_id"": 1,; ""job_id"": 19,; ""attempt_id"": ""5cs0mg"",; ""user"": ""dking"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1580760856472,; ""finish_time"": 1580760856486,; ""duration"": 14; },; ""creating"": {; ""start_time"": 1580760856486,; ""finish_time"": 1580760856629,; ""duration"": 143; },; ""runtime"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; },; ""starting"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 281, in run\n await docker_call_retry(self.container.start)\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 87, in docker_call_retry\n return await f(*args, **kwargs)\n File \""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py\"", line 188, in start\n data=kwargs\n File \""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py\"", line 166, in _query\n json.loads(what.decode('utf8')))\naiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown')\n""; }; },; ""start_time"": 1580760856630,; ""end_time"": 1580760867038; },; ""spec"": {; ""command"": [; ""/bin/bash"",; ""-c"",; ""set -e; mkdir -p /io/pipeline/pipeline-3dea50d54013/__TASK__18/; /bin/true""; ],; ""image"": ""ubuntu:18.04"",; ""job_id"": 19,; ""mount_docker_socket"": false,; ""resources"": {; ""cpu"": ""0.001"",; ""memory"": ""375M""; },; ""secrets"": [; {; ""namespace"": ""dking"",; ""name"": ""dking-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""env"": []; },; ""attributes"": {; ""name"": ""18""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8029:2208,load,loads,2208,https://hail.is,https://github.com/hail-is/hail/issues/8029,1,['load'],['loads']
Performance," amino acid for the highest-impact effect resulting from the current variant (in HGVS style)"">; ##INFO=<ID=SNPEFF_CODON_CHANGE,Number=1,Type=String,Description=""Old/New codon for the highest-impact effect resulting from the current variant"">; ##INFO=<ID=SNPEFF_EFFECT,Number=1,Type=String,Description=""The highest-impact effect resulting from the current variant (or one of the highest-impact effects, if there is a tie)"">; ##INFO=<ID=SNPEFF_EXON_ID,Number=1,Type=String,Description=""Exon ID for the highest-impact effect resulting from the current variant"">; ##INFO=<ID=SNPEFF_FUNCTIONAL_CLASS,Number=1,Type=String,Description=""Functional class of the highest-impact effect resulting from the current variant: [NONE, SILENT, MISSENSE, NONSENSE]"">; ##INFO=<ID=SNPEFF_GENE_BIOTYPE,Number=1,Type=String,Description=""Gene biotype for the highest-impact effect resulting from the current variant"">; ##INFO=<ID=SNPEFF_GENE_NAME,Number=1,Type=String,Description=""Gene name for the highest-impact effect resulting from the current variant"">; ##INFO=<ID=SNPEFF_IMPACT,Number=1,Type=String,Description=""Impact of the highest-impact effect resulting from the current variant [MODIFIER, LOW, MODERATE, HIGH]"">; ##INFO=<ID=SNPEFF_TRANSCRIPT_ID,Number=1,Type=String,Description=""Transcript ID for the highest-impact effect resulting from the current variant"">; ##INFO=<ID=STR,Number=0,Type=Flag,Description=""Variant is a short tandem repeat"">; ##INFO=<ID=VQSLOD,Number=1,Type=Float,Description=""Log odds ratio of being a true variant versus being false under the trained gaussian mixture model"">; ##INFO=<ID=culprit,Number=1,Type=String,Description=""The annotation which was the worst performing in the Gaussian mixture model, likely the reason why the variant was filtered out"">; ##INFO=<ID=set,Number=1,Type=String,Description=""Source VCF for the merged record in CombineVariants"">; ##OriginalSnpEffCmd=""SnpEff eff -v -onlyCoding true -c /seq/references/Homo_sapiens_assembly19/v1/snpEff/Homo_sapiens_assembly19.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:10755,perform,performing,10755,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['perform'],['performing']
Performance, an error (I un-escaped the string here):. ```; JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	... 7 more; Caused by: is.hail.backend.service.EndOfInputException; 	at is.hail.backend.service.ServiceBackendSocketAPI2.read(ServiceBackend.scala:497); 	at is.hail.backend.service.ServiceBackendSocketAPI2.readInt(ServiceBackend.scala:510); 	at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:561); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6(ServiceBackend.scala:462); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6$adapted(ServiceBackend.scala:461); 	at is.hail.utils.package$.usi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13160:4654,concurren,concurrent,4654,https://hail.is,https://github.com/hail-is/hail/pull/13160,1,['concurren'],['concurrent']
Performance," and as a result the gateways will create and terminate a TCP connection per http request. This likely causes minor delays on the front-end through gateway, but this hampers performance greatly in job scheduling. The batch driver is forced to establish a new TCP connection and do an SSL handshake with the internal-gateway multiple times per job, which is expensive and slow. We currently have to dedicate a 2-core NGINX sidecar for the batch-driver just to terminate TLS with internal-gateway and free up cycles in the batch-driver python process. By using proper persistent connections, we can reduce the TLS overhead to single-digit percents of a core. This leads to the first goal of this transition: configure our load balancers to know the full cluster configuration at any point in time so they can properly maintain connection pools with upstream services. However, this is not the only problem. Each ""upstream"" Service in Kubernetes may consist of multiple underlying pods but Kubernetes Services as we use them don't provide proper load-balancing when mixed with persistent connections. When we declare a Service for say, batch in default, Kubernetes adds a DNS record for `batch.default` that resolves to a single IP pointing at kube-proxy. When a new TCP connection is established with kube-proxy for that IP, it rolls the dice (using `iptables`) and assigns that connection to a particular pod to which it will forward all subsequent packets. From the load-balancer's perspective, there is only one IP address, and only one place to open connections. The load-balancer doesn't have the information to actually load-balance once we have a functioning connection pool. This can lead to really unbalanced scenarios when preemptible pods come and go. This leads to our second goal: instead of routing all requests through kube-proxy, use Kubernetes Headless Services to expose all pod IPs underlying a Service so that our proxies can properly load-balance across persistent connections. ## S",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:2576,load,load-balancing,2576,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['load'],['load-balancing']
Performance," and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-hitching UI interactions, at some cost to the speed with which 100,000 elements can be appended to a page. ; * Some consider this to be more reliable: https://localvoid.github.io/uibench/; * Here React performs many times better than vanilla JS for some operations.; * I should probably figure out exactly why. In practice, React in 2019 will likely be the best performing UI solution available, with of course the exception of some very well optimized JS. This is because of React Fiber's time slice mode, which will effectively allow UI operations, like user input, [to preempt other operations](https://reactjs.org/blog/2018/03/01/sneak-peek-beyond-react-16.html). ### How React works; React Fiber, the new reconciling/scheduling algorithm: https://github.com/acdlite/react-fiber-architecture",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:15438,tune,tuned,15438,https://hail.is,https://github.com/hail-is/hail/pull/5162,4,"['optimiz', 'perform', 'tune']","['optimized', 'performing', 'performs', 'tuned']"
Performance," applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.1.3; SparkUI available at http://192.168.248.80:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.100-2ea2615a797a; LOGGING: writing to /; --------------------------------------------------------------------------; mt.filter_rows(mt.locus.position==2867101).count_rows(); ```; ### Expected ; Return a count of rows with that condition. ### Error ; ```; FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:208); at is.hail.expr.ir.LoweredTableReader$.makeCoercer(TableIR.scala:135); at is.hail.expr.ir.GenericTableValue.getLTVCoercer(GenericTableValue.scala:137); at is.hail.expr.ir.GenericTableValue.toTableStage(GenericTableValue.scala:162); at is.hail.io.vcf.MatrixVCFReader.lower(LoadVCF.scala:1798); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:717); at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:697); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:903); at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:467); at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:472); at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:73); at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:18); at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:77); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:27); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:53); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpre",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12280:2405,Load,LoadVCF,2405,https://hail.is,https://github.com/hail-is/hail/issues/12280,1,['Load'],['LoadVCF']
Performance," at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:13270,Load,LoadVCF,13270,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance," at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.124-87398e1b514e; Error summary: HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht; ```; </details>. The code is simple and clearly is running against a path that does not already exist:; ```; if not hl.hadoop_exists(get_aou_util_path('mt_sample_qc')):; print('Run sample qc MT.....'); mt = hl.read_matrix_table(ACAF_MT_PATH); mt = mt.filter_rows(mt.locus.in_autosome()); # mt = mt.filter_rows(mt.locus.contig == 'chr1'); ht = hl.sample_qc(mt, name='mt_sample_qc'); ht.write(get_aou_util_path('mt_sample_qc'), overwrite=args.overwrite); ```. Job log: https://batch.hail.is/batches/8058522/jobs/171029. <details>; <summary>The last TableIR logged</summary>. ```; 2023-10-13 02:14:44.213 : INFO: after optimize: darrayLowerer, after LowerAndExecuteShuffles: IR size 232: . !ht = TableRead [Table{global:Struct{},key:[locus,alleles],row:Struct{locus:Locus(GRCh38),alleles:Array[String],filters:Set[String],a_index:Int32,was_split:Boolean,variant_qc:Struct{gq_stats:Struct{mean:Float64,stdev:Float64,min:Float64,max:Float64},call_rate:Float64,n_called:Int64,n_not_called:Int64,n_filtered:Int64,n_het:Int64,n_non_ref:Int64,het_freq_hwe:Float64,p_value_hwe:Float64,p_value_excess_het:Float64},info:Struct{AC:Array[Int32],AF:Array[Float64],AN:Int32,homozygote_count:Array[Int32]},`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:Array[Struct{GT:Call,GQ:Int32,RGQ:Int32,FT:String,AD:Array[Int32]}]}}, False, (TableNativeZippedReader gs://prod-drc-broad/aou-wgs-delta-small_callsets_gq0/v7.1/acaf_threshold_v7.1/splitMT/delta_basis_without_ext_aian_prod_gq0_3regions.acaf_threshold.split.mt/rows gs://prod-drc-broad/aou-wgs-delta-small_callsets_gq0/v7.1/acaf_threshold_v7.1/splitMT/delta_basis_without_ext_aian_prod_gq0_3regions.acaf_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13809:7722,optimiz,optimize,7722,https://hail.is,https://github.com/hail-is/hail/issues/13809,1,['optimiz'],['optimize']
Performance, at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:807); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:796); at is.hail.utils.richUtils.RichHadoopConfiguration$.is$hail$utils$richUtils$RichHadoopConfiguration$$create$extension(RichHadoopConfiguration.scala:24); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:296); at is.hail.io.reference.FASTAReader$$anonfun$setup$1.apply(FASTAReader.scala:45); at is.hail.io.reference.FASTAReader$$anonfun$setup$1.apply(FASTAReader.scala:44); at is.hail.utils.package$.using(package.scala:587); at is.hail.utils.richUtils.RichHadoopConfiguration$.readFile$extension(RichHadoopConfiguration.scala:293); at is.hail.io.reference.FASTAReader$.setup(FASTAReader.scala:44); at is.hail.io.reference.FASTAReader$$anonfun$getLocalFastaFileName$1.apply(FASTAReader.scala:30); at is.hail.io.reference.FASTAReader$$anonfun$getLocalFastaFileName$1.apply(FASTAReader.scala:30); at scala.collection.concurrent.TrieMap.getOrElseUpdate(TrieMap.scala:901); at is.hail.io.reference.FASTAReader$.getLocalFastaFileName(FASTAReader.scala:30); at is.hail.io.reference.SerializableReferenceSequenceFile.value$lzycompute(FASTAReader.scala:18); at is.hail.io.reference.SerializableReferenceSequenceFile.value(FASTAReader.scala:17); at is.hail.io.reference.FASTAReader.<init>(FASTAReader.scala:77); at is.hail.variant.ReferenceGenome.addSequenceFromReader(ReferenceGenome.scala:354); at is.hail.codegen.generated.C2.method2(Unknown Source); at is.hail.codegen.generated.C2.apply(Unknown Source); at is.hail.codegen.generated.C2.apply(Unknown Source); at is.hail.expr.ir.TableFilter$$anonfun$execute$3.apply(TableIR.scala:286); at is.hail.expr.ir.TableFilter$$anonfun$execute$3.apply(TableIR.scala:286); at is.hail.expr.ir.TableValue$$anonfun$6.apply(TableValue.scala:54); at is.hail.expr.ir.TableValue$$anonfun$6.apply(TableValue.scala:54); at is.hail.rvd.RVD$$anonfun$filterWithContext$1$$anonfun$apply$9.apply(RVD.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5371:1604,concurren,concurrent,1604,https://hail.is,https://github.com/hail-is/hail/issues/5371,1,['concurren'],['concurrent']
Performance, azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB); Collecting dill==0.3.7; Using cached dill-0.3.7-py3-none-any.whl (115 kB); Collecting frozenlist==1.4.0; Using cached frozenlist-1.4.0-cp39-,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:33940,cache,cached,33940,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance," backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. A fresh class loader for; each Hail version allows the classes to co-exist in the same JVM. We cache jars on the local; filesystem. ---. `javac` compiles Java files to JVM Bytecode. JVM Bytecode is normally stored in `class` files. A JAR; file is, essentially, a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corres",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:1103,load,loadClass,1103,https://hail.is,https://github.com/hail-is/hail/pull/10279,2,['load'],"['loadClass', 'loading']"
Performance," bb bf`, is converted by Java into the UTF-16 BOM, `fe ff`. This is apparently [a well known Java bug](https://stackoverflow.com/questions/1835430/byte-order-mark-screws-up-file-reading-in-java)? This looks pretty annoying to fix in Scala/Java because we'd have to muck around with Spark's `hadoopFile` infrastructure to figure out where it is actually reading from a file. ```; # hexdump /tmp/bar; 0000000 ef bb bf 73 61 6d 70 6c 65 5f 69 64 0a 66 6f 6f; 0000010 0a ; 0000011; # ipython; import hail asPython 3.7.3 (default, Mar 27 2019, 09:23:15) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl ; hl.import_; In [2]: t = hl.import_table('/tmp/bar') ; ...: t.describe() ; ...: t = t.key_by('sample_id') ; Initializing Spark and Hail with default parameters...; using hail jar at /usr/local/lib/python3.7/site-packages/hail/hail-all-spark.jar; 19/06/13 14:08:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.1; SparkUI available at http://wm06b-953.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.14-5cb00c115421; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20190613-1408-0.2.14-5cb00c115421.log; 2019-06-13 14:08:15 Hail: INFO: Reading table with no type imputation; Loading column '?sample_id' as type 'str' (type not specified). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; '﻿sample_id': str ; ----------------------------------------; Key: []; ----------------------------------------; --------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6342:1186,load,load,1186,https://hail.is,https://github.com/hail-is/hail/issues/6342,1,['load'],['load']
Performance," because this PR addresses questions of state and will take care of this. ```python; self.exit_code = pod.status.container_statuses[0].state.terminated.exit_code; ```. We should probably do something like . ```python; self.exit_code = max(status.state.terminated.exit_code for status in pod.status.container_statuses); ```; although I also see that in update_job_with_pod we effectively restrict to a single container. I'm not sure why this limit exists, but if needed, should probably occur during creation. In the upcoming PR, which moves state to MySQL 5.7+, and a different server model (async), I think it would be neat to represent meta-state (across all containers, and potentially the job subgraph whose first node is the inspected job) as:. ```go; const (; 	Cancelled = -3; 	Initialized = -2; 	Created = -1; ); ```. with values >=0 being the maximum of the linux error codes, 0-255, of the subgraph. Simple queries. Alternative is to use NULL when not completed, but when used in a client would require a null check, or potentially have surprising side effects (i.e where the default value is 0). We could also use a separate, text-based status field, but I will store a queryable JSON field containing the full status as well. In a similar vein, we have some state race conditions. For instance:. ```python; self.pod_template = kube.client.V1Pod(; metadata=kube.client.V1ObjectMeta(generate_name='job-{}-'.format(self.id),; labels={; 'app': 'batch-job',; 'hail.is/batch-instance': instance_id,; 'uuid': uuid.uuid4().hex; }),; spec=pod_spec). self._pod_name = None; self.exit_code = None. self._state = 'Created'; log.info('created job {}'.format(self.id)). self._create_pod(); ```. Here, every time pod creation fails, _state will be misaligned, and will have potential side effects (say in get_log). One solution could be to validate and rewind state in _create_pod. In any case, I will do my best to address state questions in the upcoming PR, and will close this when / if we approve it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5118:1358,race condition,race conditions,1358,https://hail.is,https://github.com/hail-is/hail/issues/5118,1,['race condition'],['race conditions']
Performance, build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; U,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:33744,cache,cachetools,33744,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cachetools']
Performance," by; their names contributed a patch for the first time.</p>; <ul>; <li>Bas van Beek</li>; <li>Charles Harris</li>; <li>Khem Raj +</li>; <li>Mark Harfouche</li>; <li>Matti Picus</li>; <li>Panagiotis Zestanakis +</li>; <li>Peter Hawkins</li>; <li>Pradipta Ghosh</li>; <li>Ross Barnowski</li>; <li>Sayed Adel</li>; <li>Sebastian Berg</li>; <li>Syam Gadde +</li>; <li>dmbelov +</li>; <li>pkubaj +</li>; </ul>; <h2>Pull requests merged</h2>; <p>A total of 17 pull requests were merged for this release.</p>; <ul>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22965"">#22965</a>: MAINT: Update python 3.11-dev to 3.11.</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22966"">#22966</a>: DOC: Remove dangling deprecation warning</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22967"">#22967</a>: ENH: Detect CPU features on FreeBSD/powerpc64*</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22968"">#22968</a>: BUG: np.loadtxt cannot load text file with quoted fields separated...</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22969"">#22969</a>: TST: Add fixture to avoid issue with randomizing test order.</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22970"">#22970</a>: BUG: Fix fill violating read-only flag. (<a href=""https://redirect.github.com/numpy/numpy/issues/22959"">#22959</a>)</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22971"">#22971</a>: MAINT: Add additional information to missing scalar AttributeError</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22972"">#22972</a>: MAINT: Move export for scipy arm64 helper into main module</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22976"">#22976</a>: BUG, SIMD: Fix spurious invalid exception for sin/cos on arm64/clang</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22989"">#22989</a>: BUG: Ensure correct loop order in sin, cos, and arctan2</li>; <li><a href=""https://redirect.github",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12898:1537,load,loadtxt,1537,https://hail.is,https://github.com/hail-is/hail/pull/12898,2,['load'],"['load', 'loadtxt']"
Performance," cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:4762,cache,cached,4762,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance," cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5578,cache,cached,5578,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance, cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB); Collecting dill==0.3.7; Using cached dill-0.3.7-py3-none-any.whl (115 kB); Collecting frozenlist==1.4.0; Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB); Collecting google-api-core==2.11.1; Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.wh,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:34914,cache,cached,34914,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance, children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.06%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:183394,Optimiz,OptimizePass,183394,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance, children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.19%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.023ms self 0.023ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:190419,Optimiz,OptimizePass,190419,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance, children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.038ms self 0.038ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.038ms self 0.029ms children 0.009ms %children 22.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.189ms self 0.189ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:17129,Optimiz,OptimizePass,17129,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance, children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.044ms self 0.035ms children 0.009ms %children 20.05%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.193ms self 0.193ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:10145,Optimiz,OptimizePass,10145,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance, children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.526ms self 0.526ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.660ms self 0.648ms children 0.012ms %children 1.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 2.298ms self 2.298ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:5854,Optimiz,OptimizePass,5854,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance, children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.660ms self 0.648ms children 0.012ms %children 1.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 2.298ms self 2.298ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.050ms self 0.050ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Loweri,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:6236,Optimiz,Optimize,6236,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance," contain the value constructor (e.g allocate). ## Core Methods. ```scala; def allocate(region: Region, length: Int): Long = ...; def allocate(region: Code[Region], length: Code[Int]): Code[Long] = ...; ```. - Allocate the memory needed for an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: Code[Long], length: Code[Int], setMissing: Boolean = false): Code[Unit] = ...; ```. - Initialize an allocated array by setting its elements to present or missing. ```scala; def isElementMissing(arrayAddress: Long, elementIndex: Int): Boolean= ...; def isElementMissing(arrayAddress: Long, elementIndex: Code[Int]): Code[Boolean] = ...; ```. - Does the element at the given index exist. ```scala; def loadLength(arrayAddress: Long): Int = ...; def loadLength(arrayAddress: Code[Long]): Code[Int] = ...; ```. - Gets the array length, will not exceed 2^31. ```scala; def loadElement(arrayAddress: Long, elementIndex: Int): Long = ...; def loadElement(arrayAddress: Code[Long], elementIndex: Code[Int]): Code[Long] = ...; ```. - Gets the address of the element at the given index.; - For pointer types loads the address at the offset into arrayAddress, otherwise returns that address. ## <a name=""parray""></a> PCanonicalArray. A growable array that is accessed by a pointer. ### Structure. Starting at `arrayAddress`:. [`4-byte length`, `n/8 byte missigness data`, `n * elementByteSize byte element data`]. # <a name=""parray""></a> PSet. An abstract class for immutable ordered collections where all elements are unique. ## Core Methods. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalSet. A PCanonicalArray-backed implementation of PSet. # <a name=""parray""></a> PDict. An abstract class for immutable unordered collections of key-value pairs. All keys must have one PType, and all values must have one (possibly differe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:4973,load,loadElement,4973,https://hail.is,https://github.com/hail-is/hail/issues/7988,1,['load'],['loadElement']
Performance," deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Itera",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:3175,Load,LoadMatrix,3175,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance," determine which concrete file system to use. For example, a; router fs can `open` both `gs://danking/abc` and `s3a://danking/abc`. Each Hail Query Python Backend is associated with one file system class. This PR associates the; ServiceBackend with `RouterFS`, enabling `hl.current_backend().fs.open`, `hl.hadoop_open`, etc. to; read from S3, GCS, ABS, and the local file system. We should deprecate `hail.utils.hadoop_utils`; because it is not Hadoop-specific. We should instead advertise the class-based `hail.fs` or create a; new function-based interface (e.g. `hl.fs.open(...)`. # Test Clean-up. The Hail Query local and spark tests should now work in Azure. I moved all the `hail.fs` and; `hailtop.aiotools.fs` tests into two build.yaml steps: `test_hail_python_fs` and; `test_hail_scala_fs`. These tests are exhaustive: they test every file system: S3, ABS, and GCS. The only file system tests that remain in the Hail Query tests are the tests of; `hail.utils.hadoop_utils`. The hadoop tests are not exhaustive: they only test the *current* file; system. In Azure, they test ABS. In Google, they test GCS. I have not decided yet if we should enable the hail python tests in Azure. It seems mostly wasteful. # Local Cache. I added a local cache directory. It defaults to `$XDG_CONFIG_HOME/hail/cache` or; `~/.config/hail/cache` if `XDG_CONFIG_HOME` is not set. I store Python reference genome metadata; here. # Batch Attributes. The ServiceBackend `batch_attributes` attribute specifies the attributes for any batch created by; the ServiceBackend. I modified the tests so that the test function name is use the ""name"" of the; batch. When a Hail Query driver job executes `parallelizeAndComputeWithIndex`, it uses its name with; a unique suffix as the name of the batch of worker jobs. # Changes to Hail Scala. I think the only changes outside of `is.hail.backend.service` and `is.hail.services` are:. 1. More retries in GoogleStorageFS. 2. Make MatrixSpecHelper and TableSpecHelper serializable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11194:6236,Cache,Cache,6236,https://hail.is,https://github.com/hail-is/hail/pull/11194,4,"['Cache', 'cache']","['Cache', 'cache']"
Performance," did:. ```py; import hail as hl; mt = hl.balding_nichols_model(3, 100, 100); mt.aggregate_entries(hl.agg.mean(mt.GT.n_alt_alleles())); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ERROR: dlopen(""/tmp/libhail6105307987842221044.so""): /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); 	at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); 	at is.hail.annotations.Region.<init>(Region.scala:34); 	at is.hail.annotations.Region$.apply(Region.scala:16); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1771); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1558); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixKeyRowsBy.execute(MatrixIR.scala:1317); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapRows.execute(MatrixIR.scala:1352); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixKeyRowsBy.execute(MatrixIR.scala:1317); 	at is.hail.expr.ir.MatrixM",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733:1403,load,load,1403,https://hail.is,https://github.com/hail-is/hail/issues/4733,1,['load'],['load']
Performance," directly to your last comment. > We have a difference of opinion about the risks. I think I'd say we have a difference of opinion about the importance of the risks. I'm well aware of the potential pitfalls you list there, and more. I just don't think they're a very big deal. I'm also aware of a shit ton of things that are vastly more important than what we're arguing about and we're not talking about those. Let's talk about goals for the project and the landscape of technical risk in our next 1:1. This is assuming we're controlling the compiler in the packaged distribution and on the cloud, we're testing representative user pipelines against gcc and clang, so the scenario you're imagining is either a Hail developer or someone who is sophisticated enough to maintain a Spark cluster (1000x worse configuration nonsense than we're arguing about here, I promise) who is either (1) running old or obscure compiler, or (2) ran into a bug that had test coverage. You're worrying about (1)? What's the worst that will happen, seriously? We'll get a bug report? Let's make sure the compiler version is in the log. > A couple of years ago; > g++ take 40-60 seconds to compile; > fairly heavily templated cod. Can we avoid heavily (or even moderately) templated code? I'm already nervous long-term about the latency of the C++ compiler overhead and if I'm being honest would prefer to generate LLVM IR directly into memory. We should ship whatever compiler is best on the cloud and in the download package. That already covers a vast majority of our users. If clang is the clear winner, we can make that clear in the documentation and maybe warn about gcc it on startup. > But that becomes a problem in itself if we want the shipped compiler to work on a variety of OS'es. Variety isn't a requirement. We don't need to make this hard for ourselves. Let's have two versions: OSX and a recent linux. If we're getting a lot of requests/questions/issues about older versions of linux, we can reevaluate.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414:1334,latency,latency,1334,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414,2,['latency'],['latency']
Performance," fix is to use `array_elements_required=False`. ```; hl.import_vcf(..., array_elements_required=False); ```. ---. ### What happened?. https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/checkpoint.20with.20missing.20fields. ```; is.hail.utils.HailException: gs://jn-vcf-cleanup-central1/McCarroll-Macosko-UM1-BICAN-Express-WGS-2023-0626/McCarroll-Macosko-UM1-BICAN-Express-WGS-2023-0626.vcf.gz:offset 1344376382: error while parsing line; chr1	10403	.	ACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC	A,ACCCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC	.	LowQual	AC=1,1;AF=0.250,0.250;AN=4;AS_QUALapprox=0|23|45;AS_VQSLOD=.,.;AS_YNG=.,.;QUALapprox=45	GT:AD:GQ:RGQ	./.	0/1:23,7,0:20:23	./.	./.	./.	0/2:6,0,4:35:45	./.	./.	./.	./.	./.	./.	./.	./.	./.	./.	./.	./. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:21); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:21); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1934); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1922); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C2005collect_distributed_array_matrix_native_writer.apply_region1_27(Unknown Source); 	at __C2005collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at __C2005collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:52); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:51); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:751); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13346:1052,Load,LoadVCF,1052,https://hail.is,https://github.com/hail-is/hail/issues/13346,1,['Load'],['LoadVCF']
Performance," for; commonly used functions, improvements to F2PY, and better documentation.</p>; <p>The Python versions supported in this release are 3.8-3.10, Python 3.7; has been dropped. Note that 32 bit wheels are only provided for Python; 3.8 and 3.9 on Windows, all other wheels are 64 bits on account of; Ubuntu, Fedora, and other Linux distributions dropping 32 bit support.; All 64 bit wheels are also linked with 64 bit integer OpenBLAS, which should fix; the occasional problems encountered by folks using truly huge arrays.</p>; <h2>Expired deprecations</h2>; <h3>Deprecated numeric style dtype strings have been removed</h3>; <p>Using the strings <code>&quot;Bytes0&quot;</code>, <code>&quot;Datetime64&quot;</code>, <code>&quot;Str0&quot;</code>, <code>&quot;Uint32&quot;</code>,; and <code>&quot;Uint64&quot;</code> as a dtype will now raise a <code>TypeError</code>.</p>; <p>(<a href=""https://redirect.github.com/numpy/numpy/pull/19539"">gh-19539</a>)</p>; <h3>Expired deprecations for <code>loads</code>, <code>ndfromtxt</code>, and <code>mafromtxt</code> in npyio</h3>; <p><code>numpy.loads</code> was deprecated in v1.15, with the recommendation that; users use <code>pickle.loads</code> instead. <code>ndfromtxt</code> and <code>mafromtxt</code> were both; deprecated in v1.17 - users should use <code>numpy.genfromtxt</code> instead with; the appropriate value for the <code>usemask</code> parameter.</p>; <p>(<a href=""https://redirect.github.com/numpy/numpy/pull/19615"">gh-19615</a>)</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/numpy/numpy/commit/4adc87dff15a247e417d50f10cc4def8e1c17a03""><code>4adc87d</code></a> Merge pull request <a href=""https://redirect.github.com/numpy/numpy/issues/20685"">#20685</a> from charris/prepare-for-1.22.0-release</li>; <li><a href=""https://github.com/numpy/numpy/commit/fd66547557f57c430d41be2fc0764f74a62e8ccf""><code>fd66547</code></a> REL: P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12809:2329,load,loads,2329,https://hail.is,https://github.com/hail-is/hail/pull/12809,2,['load'],['loads']
Performance," gcr.io/hail-vdc/query:tfkm2kev7zcf'). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.7/site-packages/batch/worker/worker.py"", line 372, in run; await self.ensure_image_is_pulled(); File ""/usr/local/lib/python3.7/site-packages/batch/worker/worker.py"", line 363, in ensure_image_is_pulled; docker.images.pull, self.image); File ""/usr/local/lib/python3.7/site-packages/batch/worker/worker.py"", line 111, in wrapper; return await asyncio.wait_for(f(*args, **kwargs), timeout); File ""/usr/local/lib/python3.7/asyncio/tasks.py"", line 442, in wait_for; return fut.result(); File ""/usr/local/lib/python3.7/asyncio/futures.py"", line 181, in result; raise self._exception; File ""/usr/local/lib/python3.7/asyncio/tasks.py"", line 249, in __step; result = coro.send(None); File ""/usr/local/lib/python3.7/site-packages/aiodocker/images.py"", line 104, in _handle_list; async with cm as response:; File ""/usr/local/lib/python3.7/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.7/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(500, ""unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication""); ```. [1] The Docker specifications are confusing. After reading; [v1](https://github.com/moby/moby/blob/master/image/spec/v1.md) and; [v1.2](https://github.com/moby/moby/blob/master/image/spec/v1.2.md), it seems that the ""repository""; is everything before the last colon and the ""image name suffix"" is everything after the last; colon. For example, in `server:8080/abc/def:123`, the repository is `server:8080/abc/def` and the; ""image name suffix"" is `123`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9902:4010,load,loads,4010,https://hail.is,https://github.com/hail-is/hail/pull/9902,2,"['load', 'perform']","['loads', 'perform']"
Performance," get_return_value(; 1322 answer, self.gateway_client, self.target_id, self.name); 1324 for temp_arg in temp_args:; 1325 temp_arg._detach(). File ~/miniconda3/lib/python3.10/site-packages/hail/backend/py4j_backend.py:35, in handle_java_exception.<locals>.deco(*args, **kwargs); 33 tpl = Env.jutils().handleForPython(e.java_exception); 34 deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 35 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 36 except pyspark.sql.utils.CapturedException as e:; 37 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 38 'Hail version: %s\n'; 39 'Error summary: %s' % (e.desc, e.stackTrace, hail.__version__, e.desc)) from None. FatalError: ClassCastException: class is.hail.types.physical.stypes.concrete.SIndexablePointer cannot be cast to class is.hail.types.physical.stypes.concrete.SJavaArrayString (is.hail.types.physical.stypes.concrete.SIndexablePointer and is.hail.types.physical.stypes.concrete.SJavaArrayString are in unnamed module of loader 'app'). Java stack trace:; java.lang.ClassCastException: class is.hail.types.physical.stypes.concrete.SIndexablePointer cannot be cast to class is.hail.types.physical.stypes.concrete.SJavaArrayString (is.hail.types.physical.stypes.concrete.SIndexablePointer and is.hail.types.physical.stypes.concrete.SJavaArrayString are in unnamed module of loader 'app'); 	at is.hail.expr.ir.functions.RegistryFunctions.unwrapReturn(Functions.scala:364); 	at is.hail.expr.ir.Emit.$anonfun$emitI$85(Emit.scala:1173); 	at is.hail.expr.ir.IEmitCodeGen.map(Emit.scala:352); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:1153); 	at is.hail.expr.ir.streams.EmitStream$.is$hail$expr$ir$streams$EmitStream$$emit$1(EmitStream.scala:148); 	at is.hail.expr.ir.streams.EmitStream$.produce(EmitStream.scala:321); 	at is.hail.expr.ir.Emit.emitStream$2(Emit.scala:821); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:1177); 	at is.hail.expr.ir.Emit.$anonfun$emitSplitMethod$1(Emit.scala:607); 	at is.hail.expr.ir.Emit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13633:4120,load,loader,4120,https://hail.is,https://github.com/hail-is/hail/issues/13633,1,['load'],['loader']
Performance," hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:2018,cache,cached,2018,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance," href=""https://redirect.github.com/python-pillow/Pillow/issues/7823"">#7823</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Allow writing IFDRational to UNDEFINED tag <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7840"">#7840</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fix logged tag name when loading Exif data <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7842"">#7842</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use maximum frame size in IHDR chunk when saving APNG images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7821"">#7821</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Prevent opening P TGA images without a palette <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7797"">#7797</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use palette when loading ICO images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7798"">#7798</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use consistent arguments for load_read and load_seek <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7713"">#7713</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Turn off nullability warnings for macOS SDK <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7827"">#7827</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fix shift-sign issue in Convert.c <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7838"">#7838</a> [<a href=""https://github.com/r-barnes""><code>@​r-barnes</code></a>]</li>; <li>winbuild: Refactor dependency versions into constants <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7843"">#7843</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14439:6848,load,loading,6848,https://hail.is,https://github.com/hail-is/hail/pull/14439,3,['load'],['loading']
Performance," httpx.ClientSession, access_token: str, *, oauth2_client: dict) -> Optional[str]:; + oauth2_client_audience = oauth2_client['installed']['client_id']; + try:; + userinfo = await retry_transient_errors(; + session.get_read_json,; + 'https://www.googleapis.com/oauth2/v3/tokeninfo',; + params={'access_token': access_token},; + ); + if userinfo['aud'] != oauth2_client_audience and userinfo['aud'] != userinfo['sub']:; + return None; +; + email = userinfo['email']; + if email.endswith('iam.gserviceaccount.com'):; + return userinfo['sub']; + # We don't currently track user's unique GCP IAM ID (sub) in the database, just their email.; + return email; + except httpx.ClientResponseError as e:; + if e.status in (400, 401):; + return None; + raise; +; +; +class AadJwk(TypedDict):; + kid: str; + x5c: List[str]; +; ; class AzureFlow(Flow):; + _aad_keys: Optional[List[AadJwk]] = None; +; def __init__(self, credentials_file: str):; with open(credentials_file, encoding='utf-8') as f:; data = json.loads(f.read()); ; tenant_id = data['tenant']; authority = f'https://login.microsoftonline.com/{tenant_id}'; - client = msal.ConfidentialClientApplication(data['appId'], data['password'], authority); -; - self._client = client; + self._client = msal.ConfidentialClientApplication(data['appId'], data['password'], authority); self._tenant_id = tenant_id; ; def initiate_flow(self, redirect_uri: str) -> dict:; @@ -107,10 +170,31 @@ class AzureFlow(Flow):; ; return FlowResult(token['id_token_claims']['oid'], token['id_token_claims']['preferred_username'], token); ; -; -def get_flow_client(credentials_file: str) -> Flow:; - cloud = get_global_config()['cloud']; - if cloud == 'azure':; - return AzureFlow(credentials_file); - assert cloud == 'gcp'; - return GoogleFlow(credentials_file); + @staticmethod; + def perform_installed_app_login_flow(oauth2_client: Dict[str, Any]) -> Dict[str, Any]:; + tenant_id = oauth2_client['tenant']; + authority = f'https://login.microsoftonline.com/{tenant_id}'; + app",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13131#issuecomment-1668558329:4693,load,loads,4693,https://hail.is,https://github.com/hail-is/hail/pull/13131#issuecomment-1668558329,1,['load'],['loads']
Performance," idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5392,cache,cached,5392,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance," image that was previously associated with it. That image is still in the registry though. I thought that was sufficient for the cache to work. (I was wrong, see below!). AFAICT, this change doesn't prevent PR tests from pushing to the `cache` tag. This change just makes the tests run by the CI-in-the-PR not overwrite the cache. Every image build for a PR (which is tested by default namespace CI) will still overwrite the cache tag. AFAICT, this; ```; --import-cache type=registry,ref=gcr.io/hail-vdc/foo; ```; Will use as a cache source the `latest` tag in the `gcr.io/hail-vdc/foo` repository. It is *not* sufficient for an image to be present in the repository and untagged or with a different tag from `latest`. In particular, every push to the `cache` tag prevents us from using other images even though they are in the registry! For example, I pushed two images to `cache`:. ```; (base) # gcloud container images list-tags gcr.io/hail-vdc/dktest; DIGEST TAGS TIMESTAMP; fb551d9bdb94 2022-06-10T14:16:39; afb4c5ad2d7b cache,latest 2022-06-10T14:15:55; ```. If I rebuild [1] the most recently pushed image with; ```; --import-cache type=registry,ref=gcr.io/hail-vdc/dktest:cache; ```; it succeeds in getting the cache. If I rebuild the other image with the same import-cache, it does not see that the (untagged) image is already there! . ---. This all suggests that all our attempts at image caching are failing terribly. Options:; 1. Only deploy builds push to a `:cache` tag, everyone uses that tag.; 2. List all the tags in the repository and include them all as --cache-from's (this doesn't actually work: https://github.com/moby/moby/issues/34715#issuecomment-425933774); 3. Push a tag for each git SHA and then include as --cache-from's the last ten git SHAs on this branch, the most recent common commit with main (i.e. `git merge-base origin/main this-branch`), maybe the current main, and maybe the PR number?; 4. Write our own OCI image builder so we can write our own OCI image cach",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800:1122,cache,cache,1122,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800,2,['cache'],['cache']
Performance," in <module>; 4 import numpy as np; ----> 5 import pyspark; 6 from typing import Optional, Dict, Callable, Sequence. ModuleNotFoundError: No module named 'pyspark'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); /tmp/ipykernel_233/4275665471.py in <module>; ----> 1 combined_pandas = pd.read_pickle(gwas_pandas_file). /opt/conda/lib/python3.7/site-packages/pandas/io/pickle.py in read_pickle(filepath_or_buffer, compression, storage_options); 220 # ""No module named 'pandas.core.sparse.series'""; 221 # ""Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib""; --> 222 return pc.load(handles.handle, encoding=None); 223 except UnicodeDecodeError:; 224 # e.g. can occur for files written in py27; see GH#28645 and GH#31988. /opt/conda/lib/python3.7/site-packages/pandas/compat/pickle_compat.py in load(fh, encoding, is_verbose); 272 up.is_verbose = is_verbose; 273 ; --> 274 return up.load(); 275 except (ValueError, TypeError):; 276 raise. /opt/conda/lib/python3.7/pickle.py in load(self); 1086 raise EOFError; 1087 assert isinstance(key, bytes_types); -> 1088 dispatch[key[0]](self); 1089 except _Stop as stopinst:; 1090 return stopinst.value. /opt/conda/lib/python3.7/pickle.py in load_stack_global(self); 1383 if type(name) is not str or type(module) is not str:; 1384 raise UnpicklingError(""STACK_GLOBAL requires str""); -> 1385 self.append(self.find_class(module, name)); 1386 dispatch[STACK_GLOBAL[0]] = load_stack_global; 1387 . /opt/conda/lib/python3.7/site-packages/pandas/compat/pickle_compat.py in find_class(self, module, name); 204 key = (module, name); 205 module, name = _class_locations_map.get(key, key); --> 206 return super().find_class(module, name); 207 ; 208 . /opt/conda/lib/python3.7/pickle.py in find_class(self, module, name); 1424 elif module in _compat_pickle.IMPORT_MAPPING:; 1425 module = _compat_pickle.IMPORT_MAPPING[module]; -> 1426 __import__(module, level=0); 1427 if self.proto",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14004:3016,load,load,3016,https://hail.is,https://github.com/hail-is/hail/issues/14004,1,['load'],['load']
Performance," in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:12716,load,load,12716,https://hail.is,https://github.com/hail-is/hail/pull/8561,1,['load'],['load']
Performance," in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.ClassCastException: java.lang.String cannot be cast to scala.collection.immutable.Set; at org.broadinstitute.hail.driver.ExportVCF$$anonfun$6.apply(ExportVCF.scala:185); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$6.apply(ExportVCF.scala:185); at scala.Option.map(Option.scala:145); at org.broadinstitute.hail.driver.ExportVCF$.org$broadinstitute$hail$driver$ExportVCF$$appendRow$1(ExportVCF.scala:185); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$run$1$$anonfun$apply$11.apply(ExportVCF.scala:278); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$run$1$$anonfun$apply$11.apply(ExportVCF.scala:276); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1109); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/785:2084,concurren,concurrent,2084,https://hail.is,https://github.com/hail-is/hail/issues/785,2,['concurren'],['concurrent']
Performance, is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply total 20.227ms self 0.484ms children 19.744ms %children 97.61%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.NormalizeNames.apply total 0.154ms self 0.154ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply total 4.179ms self 1.415ms children 2.764ms %children 66.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 1.490ms self 0.008ms children 1.482ms %children 99.45%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.013ms self 0.013ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.460ms self 0.066ms children 1.394ms %children 95.49%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.ex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:193215,Optimiz,OptimizePass,193215,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance, is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.140ms self 0.140ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:171012,Optimiz,Optimize,171012,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance, is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.187ms self 0.187ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.031ms self 0.031ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:27695,Optimiz,Optimize,27695,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance, is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.032ms self 0.028ms children 0.003ms %children 10.95%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.092ms self 0.092ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.loweri,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:174957,Optimiz,Optimize,174957,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance, is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.050ms self 0.041ms children 0.010ms %children 19.41%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.201ms self 0.201ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.loweri,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:31640,Optimiz,Optimize,31640,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance, java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.base/java.lang.reflect.Method.invoke(Method.java:566); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182); 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106); 	at java.base/java.lang.Thread.run(Thread.java:829). java.lang.ClassCastException: class is.hail.types.virtual.TStruct cannot be cast to class is.hail.types.virtual.TIterable (is.hail.types.virtual.TStruct and is.hail.types.virtual.TIterable are in unnamed module of loader 'app'); 	at is.hail.expr.ir.InferType$.apply(InferType.scala:115); 	at is.hail.expr.ir.IR.typ(IR.scala:36); 	at is.hail.expr.ir.IR.typ$(IR.scala:33); 	at is.hail.expr.ir.ToStream.typ(IR.scala:300); 	at is.hail.expr.ir.IRParser$.$anonfun$ir_value_expr_1$81(Parser.scala:1111); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_value_ir$1(Parser.scala:2157); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:2153); 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:2157); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_value_ir$2(SparkBackend.scala:691); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); 	at is.hail.utils.package$.usin,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13699:6069,load,loader,6069,https://hail.is,https://github.com/hail-is/hail/issues/13699,1,['load'],['loader']
Performance," kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:1455,cache,cached,1455,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance, mktemp; + pinned_no_comments=/tmp/tmp.WRSKGgGEB8; ++ mktemp; + new_pinned_no_comments=/tmp/tmp.C8ggaXDHDt; + PATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/aws/puppet/bin/:/home/hadoop/.local/bin:/home/hadoop/.local/bin; + pip-compile --quiet python/requirements.txt python/pinned-requirements.txt --output-file=/tmp/tmp.YoVBQEw8XF; WARNING: the legacy dependency resolver is deprecated and will be removed in future versions of pip-tools. The default resolver will be changed to 'backtracking' in pip-tools 7.0.0. Specify --resolver=backtracking to silence this warning.; + cat python/pinned-requirements.txt; + sed /#/d; + sed /#/d; + cat /tmp/tmp.YoVBQEw8XF; + diff /tmp/tmp.WRSKGgGEB8 /tmp/tmp.C8ggaXDHDt; sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$' | tr '\n' '\0' | xargs -0 python3 -m pip install -U; Defaulting to user installation because normal site-packages is not writeable; Collecting aiodns==2.0.0; Using cached aiodns-2.0.0-py2.py3-none-any.whl (4.8 kB); Collecting aiohttp==3.8.5; Using cached aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB); Collecting aiosignal==1.3.1; Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB); Collecting async-timeout==4.0.3; Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Colle,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:31994,cache,cached,31994,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance, msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached pandas-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB); Collecting parsimonious==0.10.0; Using cached parsimonious-0.10.0-py3-none-any.whl (48 kB); Collecting pillow==10.0.0; Using cached Pillow-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB); Collecting plotly==5.16.1; Using cached plotly-5.16.1-py2.py3-none-any.whl (15.6 MB); Collecting portalocker==2.7.0; Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB); Collecting protobuf==3.20.2; Using cached protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:37747,cache,cached,37747,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance," new methods every X IR nodes, simply based on node count. However, the size of code generated by each IR can vary widely (`I32` vs `LowerBoundOnOrderedCollection` for instance), and so we have two other kinds of splitting that operate on the LIR level. The first is region splitting, which is used to split large blocks of LIR. In order to insert a split, any variables on the stack are stored in local variables before the split and loaded from those locals after the split. The second is method splitting, which is used to split large single methods. A single-exit group of blocks can be split into a separate method, and we have some machinery for replacing control flow instructions (which I will not go into here, for they are not relevant now), as well as handling local variables that are used across a method split. These shared Local variables are replaced by fields on a ""spills"" class which is allocated any time a split method is called. Spilled local `store`s are rewritten as field `store`s, and `load`s are rewritten as field `load`s. # What was the problem here?. A region split was inserted *directly between* the `I2B` instruction and the call to `OutputBuffer.write`. This meant that the result of `I2B` was stored in a local variable and read in the subsequent block. **The incorrect TypeInfo of Boolean was used for that local variable**, but this seems not to pose a problem -- both Boolean and Byte use a single slot, and so the code still works even with the wrong variable type. However, the method splitter then **generated a method split at the same point where the region was split**. This means that the local variable resulting from I2B is spilled to a class field on the spills class. Our incorrectly-Boolean local becomes an incorrectly-Boolean **field**, and this is where things go wrong -- it seems as though Boolean class fields (appropriately) truncate on store and load a single bit. Our value of `3` was stored as a class Boolean, and came out `1`. The fact th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:2741,load,load,2741,https://hail.is,https://github.com/hail-is/hail/pull/11328,2,['load'],['load']
Performance," org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, com2, executor 1): java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:178); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readB",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:1857,concurren,concurrent,1857,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['concurren'],['concurrent']
Performance," org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:5865,Load,LoadVCF,5865,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance, portalocker==2.7.0; Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB); Collecting protobuf==3.20.2; Using cached protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB);,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:38922,cache,cached,38922,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance," push to a `:cache` tag, everyone uses that tag.; 2. List all the tags in the repository and include them all as --cache-from's (this doesn't actually work: https://github.com/moby/moby/issues/34715#issuecomment-425933774); 3. Push a tag for each git SHA and then include as --cache-from's the last ten git SHAs on this branch, the most recent common commit with main (i.e. `git merge-base origin/main this-branch`), maybe the current main, and maybe the PR number?; 4. Write our own OCI image builder so we can write our own OCI image cache that actually works the way it ought to (everything in the registry is considered fair game for the cache). It seems like 3 is actually a decent solution that should enable lots of caching.; 1. The last ten SHAs on the branch should speed up repeated builds when you're fixing little bugs.; 2. The most recent common commit with main should avoid rebuilds unless the packages changed.; 3. I suspect the current main is actually not helpful (either 2 will work or 3 wouldn't help).; 4. Pushing to something like `cache-11907` would allow force pushes to still access the last build's images. What do you think of the #3 proposal? . ---. [1]: I had two files:; ```; # cat sleep/Dockerfile; FROM ubuntu:18.04; RUN sleep 10; # cat touch/Dockerfile; FROM ubuntu:18.04; RUN touch foo; ```; To build I use this command (slightly different syntax from the buildctl syntax, but, AFAIK, uses the same backend):; ```; docker buildx \ ; build \; DIRECTORY_NAME_HERE \; --output 'type=image,""name=gcr.io/hail-vdc/dktest,gcr.io/hail-vdc/dktest:cache"",push=true' \; --cache-to type=inline \; --cache-from type=registry,ref=gcr.io/hail-vdc/dktest; ```; Before every build I clear the _local_ cache with:; ```; docker system prune -a; ```; I can clear the remote cache with:; ```; gcloud container images list-tags gcr.io/hail-vdc/dktest --format=""get(digest)"" \; | awk '{print ""gcr.io/hail-vdc/dktest@"" $1}' \; | xargs gcloud container images delete --force-delete-tags; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800:2610,cache,cache-,2610,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800,12,['cache'],"['cache', 'cache-', 'cache-from', 'cache-to']"
Performance," region, cloud); 107 raise ValueError(f'Region {repr(region)} not available for dataset'; 108 f' {repr(name)} on cloud platform {repr(cloud)}.\n'; 109 f'Available regions: {regions}.'); 111 path = [dataset['url'][cloud][region]; 112 for dataset in datasets[name]['versions']; 113 if all([dataset['version'] == version,; 114 dataset['reference_genome'] == reference_genome])]; --> 115 assert len(path) == 1; 116 path = path[0]; 117 if path.startswith('s3://'):. AssertionError: ; ```. I'm a new Hail user and don't have the full context here, but it seems like there are at least three problems:. 1. An assert failed in production code, which indicates either the presence of a bug or an incorrect use of assert (e.g. using assert to check for value errors).; 2. The assert has no corresponding error message, so the user learns that something has gone wrong but can't easily tell what.; 3. The assert is bare. Bare asserts can get optimized out of code in ways that are difficult to foresee in advance, and are generally deprecated in favor of the `if error_condition: raise AssertionError(...)` pattern (see: https://discuss.python.org/t/stop-ignoring-asserts-when-running-in-optimized-mode/13132). **The Big Picture**. The bare assert pattern is used over 3k times in Hail. To be fair, many of these usages occur in test directories, where they're fine. But they also occur in application code, and often in the dangerous form `assert(expr1, expr2)` which will never fail (because a tuple with two falsy elements is truthy in python). These asserts are never actually getting checked. . Fixing all of them would be a heavy lift. One compromise solution might be to add a bare assert rule to the linter (e.g. https://pypi.org/project/flake8-assert-msg/). This would prevent the introduction of further bare asserts to the codebase, and encourage authors to clean up existing bare asserts on files they touch. The `assert` keyword is an unfortunate language wart that makes it very easy for developer",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12952:1439,optimiz,optimized,1439,https://hail.is,https://github.com/hail-is/hail/issues/12952,1,['optimiz'],['optimized']
Performance," rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:6363,cache,cached,6363,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance," run_until_complete; self.run_forever(); /usr/lib/python3.7/asyncio/base_events.py:541: in run_forever; self._run_once(); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <_UnixSelectorEventLoop running=False closed=False debug=False>. def _run_once(self):; """"""Run one full iteration of the event loop.; ; This calls all currently ready callbacks, polls for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); se",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10705:2244,queue,queue,2244,https://hail.is,https://github.com/hail-is/hail/pull/10705,1,['queue'],['queue']
Performance," run_until_complete; self.run_forever(); /usr/lib/python3.9/asyncio/base_events.py:601: in run_forever; self._run_once(); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <_UnixSelectorEventLoop running=False closed=False debug=False>. def _run_once(self):; """"""Run one full iteration of the event loop.; ; This calls all currently ready callbacks, polls for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callback",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13997:3155,queue,queue,3155,https://hail.is,https://github.com/hail-is/hail/issues/13997,1,['queue'],['queue']
Performance, scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). java.net.SocketException: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6e,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:17369,concurren,concurrent,17369,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['concurren'],['concurrent']
Performance, scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.net.SocketException: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Driver stack,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:8071,concurren,concurrent,8071,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['concurren'],['concurrent']
Performance, scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217503,concurren,concurrent,217503,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance, scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216788,concurren,concurrent,216788,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance, scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218210,concurren,concurrent,218210,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance," scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB); Collecting typer==0.9.0; Using cached typer-0.9.0-py3-none-any.whl (45 kB); Collecting typing-extensions==4.7.1; Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB); Collecting tzdata==2023.3; Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB); Collecting urllib3==1.26.16; Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB); Collecting uvloop==0.17.0; Using cached uvloop-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB); Collecting wrapt==1.15.0; Using cached wrapt-1.15.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB); Collecting xyzservices==2023.7.0; Using cached xyzservices-2023.7.0-py3-none-any.whl (56 kB); Collecting yarl==1.9.2; Using cached yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB); Building wheels for collected packages: avro; Building wheel for avro (pyproject.toml): started; Building wheel for avro (pyproject.toml): finished with status 'done'; Created wheel for avro: filename=avro-1.11.2-py2.py3-none-any.whl size=119738 sha256=d7f238f86de270b449b018590930a06270766887328bdb51066eccff2cd696a6; Stored in directory: /home/hadoop/.cache/pip/wheels/e3/a2/1e/5c1be0865f4170a89de34e0a798f32f674a7eaf63a93272c7f; Successfully built avro; Installing collected packages: sortedcontainers, pytz, py4j, commonmark, azure-common, xyzser",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:40742,cache,cached,40742,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance," secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_URL='https://localhost/api/graphql'; ```. The .env for the gateway; ```; AUTH0_WEB_KEY_SET_URL=https://hail.auth0.com/.well-known/jwks.json; AUTH0_AUDIENCE=hail; AUTH0_DOMAIN=https://hail.auth0.com/. AUTH0_MANAGEMENT_API_CLIENT=eqDY6HWOKd6MzC8kWCFaZUAoZgNUHypA; AUTH0_MANAGEMENT_API_SECRET=<I'll give this>; AUTH0_MANAGEMENT_API_TOKEN_URL=https://hail.auth0.com/oauth/token; AUTH0_MANAGEMENT_API_URL=https://hail.auth0.com/api/v2/users; AUTH0_MANAGEMENT_API_AUDIENCE=https://hail.auth0.com/api/v2/; ```. Organization of the web app is simple. There is a pages directory. Routes match the folder structure. Pages that don't need to maintain their own state look a lot like HTML wrapped in a function:. ```js; export default function() { <div>Hello World </div> }; ```. or in JS ES6 form:; ```js; export default () => <div>Hello World</div>; ``` . Performance is excellent. SSR should run about as fast as jinja2 (will be getting faster in 2019). Client side interactions are obviously far more performant. Bundle sizes are on a downward trajectory; react + react-dom is about as big as jQuery today, and reducing that is a focus on facebook in 2019. There are alternatives to react-dom that are under 10kb, but in practice 20kb is nothing to worry about, especially when initial load doesn't require it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:3612,Perform,Performance,3612,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935,5,"['Perform', 'load', 'perform']","['Performance', 'load', 'performant']"
Performance," six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB); Collecting typer==0.9.0; Using cached typer-0.9.0-py3-none-any.whl (45 kB); Collecting typing-extensions==4.7.1; Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB); Collecting tzdata==2023.3; Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB); Collecting urllib3==1.26.16; Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB); Collecting uvloop==0.17.0; Using cached uvloop-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB); Collecting wrapt==1.15.0; Using cached wrapt-1.15.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB); Collecting xyzservices==2023.7.0; Using cached xyzservices-2023.7.0-py3-none-any.whl (56 kB); Collecting yarl==1.9.2; Using cached yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB); Building wheels for collected packages: avro; Building wheel for avro (pyproject.toml): started; Building wheel for avro (pyproject.toml): finished with status 'done'; Created wheel for avro: filename=avro-1.11.2-py2.py3-none-any.whl size=119738 sha256=d7f238f86de270b449b018590930a06270766887328bdb51066eccff2cd696a6; Stored in directory: /home/hadoop/.cache/pip/wheels/e3/a2/1e/5c1be0865f4170a89de34e0a798f32f674a7eaf63a93272c7f; Successfully built avro; Installing collected packages: sortedcontainers, pytz, py4j, commonmark, azure-common, xyzservices, wrapt, uvloop, urllib3, tzdata, typing-extensions, tornado, tenacity, tabulate, six, regex, pyyaml, python-json-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:40862,cache,cached,40862,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance," speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trac",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1377,Load,Loaded,1377,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893,3,"['Load', 'cache']","['Loaded', 'Loading', 'cached']"
Performance," stage 1.0 (TID 11, localhost, executor driver): com.esotericsoftware.kryo.KryoException: sun.reflect.generics.reflectiveObjects.NotImplementedException; Serialization trace:; m (is.hail.annotations.aggregators.KeyedRegionValueAggregator); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:315); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:386); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: sun.reflect.generics.reflectiveObjects.NotImplementedException; 	at is.hail.annotations.UnKryoSerializable$class.write(UnsafeRow.scala:15); 	at is.hail.annotations.UnsafeRow.write(UnsafeRow.scala:141); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:505); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:503); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:106); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serialize",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4215:1391,concurren,concurrent,1391,https://hail.is,https://github.com/hail-is/hail/issues/4215,1,['concurren'],['concurrent']
Performance, storage client from service account key; 2024-11-05 02:43:37.783 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]; 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]; 	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) [jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]; 	at java.lang.Thread.run(Thread.java:829) [?:?]; Caused by: com.fasterxml.jackson.core.exc.StreamConstraintsException: String length (20013488) exceeds the maximum length (20000000); 	at com.fasterxml.jackson.core.StreamReadConstraints.validateStringLength(StreamReadConstraints.java:324) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.ReadConstrainedTextBuffer.validateStringLength(ReadConstrainedTextBuffer.java:27) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.TextBuffer.finishCurrentSegment(TextBuffer.java:939) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._finishString2(UTF8StreamJsonParser.java:2584) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._finishAndReturnString(UTF8StreamJsonParser.java:2560) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.json.UTF8St,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14749:3076,concurren,concurrent,3076,https://hail.is,https://github.com/hail-is/hail/issues/14749,1,['concurren'],['concurrent']
Performance," supported in this release are 3.8-3.10, Python 3.7; has been dropped. Note that 32 bit wheels are only provided for Python; 3.8 and 3.9 on Windows, all other wheels are 64 bits on account of; Ubuntu, Fedora, and other Linux distributions dropping 32 bit support.; All 64 bit wheels are also linked with 64 bit integer OpenBLAS, which should fix; the occasional problems encountered by folks using truly huge arrays.</p>; <h2>Expired deprecations</h2>; <h3>Deprecated numeric style dtype strings have been removed</h3>; <p>Using the strings <code>&quot;Bytes0&quot;</code>, <code>&quot;Datetime64&quot;</code>, <code>&quot;Str0&quot;</code>, <code>&quot;Uint32&quot;</code>,; and <code>&quot;Uint64&quot;</code> as a dtype will now raise a <code>TypeError</code>.</p>; <p>(<a href=""https://redirect.github.com/numpy/numpy/pull/19539"">gh-19539</a>)</p>; <h3>Expired deprecations for <code>loads</code>, <code>ndfromtxt</code>, and <code>mafromtxt</code> in npyio</h3>; <p><code>numpy.loads</code> was deprecated in v1.15, with the recommendation that; users use <code>pickle.loads</code> instead. <code>ndfromtxt</code> and <code>mafromtxt</code> were both; deprecated in v1.17 - users should use <code>numpy.genfromtxt</code> instead with; the appropriate value for the <code>usemask</code> parameter.</p>; <p>(<a href=""https://redirect.github.com/numpy/numpy/pull/19615"">gh-19615</a>)</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/numpy/numpy/commit/4adc87dff15a247e417d50f10cc4def8e1c17a03""><code>4adc87d</code></a> Merge pull request <a href=""https://redirect.github.com/numpy/numpy/issues/20685"">#20685</a> from charris/prepare-for-1.22.0-release</li>; <li><a href=""https://github.com/numpy/numpy/commit/fd66547557f57c430d41be2fc0764f74a62e8ccf""><code>fd66547</code></a> REL: Prepare for the NumPy 1.22.0 release.</li>; <li><a href=""https://github.com/numpy/numpy/commit/125304b035e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12809:2424,load,loads,2424,https://hail.is,https://github.com/hail-is/hail/pull/12809,2,['load'],['loads']
Performance," the container, install the jdk, then run jstack on one of the hung JVMs.; ```; # jstack 1433; ...; ""pool-1-thread-1"" #18 prio=5 os_prio=0 tid=0x00007f50c4f23000 nid=0x82c waiting on condition [0x00007f5084eeb000]; java.lang.Thread.State: WAITING (parking); 	at sun.misc.Unsafe.park(Native Method); 	- parking to wait for <0x00000000e8ddaea0> (a scala.concurrent.impl.Promise$CompletionLatch); 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); 	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242); 	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258); 	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263); 	at scala.concurrent.Await$.$anonfun$result$1(package.scala:220); 	at scala.concurrent.Await$$$Lambda$2201/1092639564.apply(Unknown Source); 	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:57); 	at scala.concurrent.Await$.result(package.scala:146); 	at is.hail.backend.service.ServiceBackend.parallelizeAndComputeWithIndex(ServiceBackend.scala:145); ...; ```. This is the line that waits to upload the compiled code for the workers to Google Cloud Storage. The other threads appear to be waiting on the memory service:; ```; ""pool-2-thread-2"" #27 prio=5 os_prio=0 tid=0x00007f5028ad9000 nid=0x88d waiting on condition [0x00007f50274fc000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method); 	at is.hail.services.package$.sleepAndBackoff(package.scala:32); 	at is.hail.services.package$.retryTransientErrors(package.scala:86); 	at is.hail.services.Requester.request",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:1023,concurren,concurrent,1023,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['concurren'],['concurrent']
Performance," the latter. You can do even better than `PureComponent`. Use a regular `Component`, and specific a `shouldComponentUpdate() { }` method in that component. Within that method, write whatever checks needed, so that when a prop, or state changes, you return `true`, otherwise `false`. When true, the component will re-render. However, this allows you to react in a more fine-grained way, i.e instead of checking reference, check for the update of a specific property, or don't react to that object changing at all. Behind the scenes, PureComponent is in effect implementing a shouldComponentUpdate that checks reference equality (prevProp !== currentProp). References: ; 1. https://reactjs.org/docs/react-api.html#reactpurecomponent. #### Memo stateless components; Stateless/functional components (i.e those than don't extend React.Component or React.PureComponent, i.e `(props) => <div>Hello {props.name}</div>`), can be memoized. As in a typical memoized function, given one set of input (props), the result is cached, and the cached result is returned for n + 1 calls. References; 1. https://reactjs.org/docs/react-api.html#reactmemo; 2. https://scotch.io/tutorials/react-166-reactmemo-for-functional-components-rendering-control. ### Typescript; 2. https://reactjs.org/docs/react-api.html#reactmemo. #### And React Component prop definitions; https://levelup.gitconnected.com/ultimate-react-component-patterns-with-typescript-2-8-82990c516935. ### NextJS; https://nextjs.org/docs/; Next has 4 deviations from normal react:; 1) _app.js: Can be omitted. Wraps all other components. Is useful for global functions, because it is not reloaded when you change pages. Good place to place a header component, a footer, global data stores, or handle page transitions.; it has this shape:; ```js; <Container>; <Header />; <Component {...pageProps} />; <Footer />; </Container>; ```. 2) _document.js: Optional. Rendered only on the server, exactly one time. Wraps _app. Good place to define external resourc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:10631,cache,cached,10631,https://hail.is,https://github.com/hail-is/hail/pull/5162,2,['cache'],['cached']
Performance," the values are stored in the array and the names are stored in a separate dictionary mapping names to their indices; 2. a `dict<str, T>`, the name-value pairs are stored as dictionary key-value pairs; 3. a `struct{name1: T, name2: T, ... nameN: T}`, the name-value pairs are stored as field name, field string pairs. The third option is the most space efficient: the type stores the field names and there is no bookkeeping overhead per-set-of-named-values. The first two options repeat the field names for each occurrence (in particular, consider a Table field or MatrixTable entry-indexed field). The first two options needlessly encode the length (which is statically known). The third option is the most access-time-efficient: the offset of any named-value is known at hail compile time. The first two options require a logarithmic search of hail's dictionary tree representation. The third option is more user-friendly for accessing: `x.name`. The first is the least user-friendly: `x[indices[""name""]]`. The first and third options are the most cache-friendly for homogenous operations. The first uses `ArrayExpression.map`, so code size is `O(CODE)`. The third option's code size is `O(CODE * #VALUES)` because structs have no `map`-like primitive. The third option is also not user-friendly for homogenous operations (the user must repeat the code for each name-value pair). The third is the most self-documenting option. The number of fields and their names are visible in `ds.describe()`. The first is the next best because the dictionary is likely a global field that can be viewed with `x.indices.show()`. ---. ## Phase 1; Implement a new virtual type `tstaticdict<T, name1, name2, ..., nameN>` who's physical type is `PStruct` with N fields. These changes span Scala and Python. Implement `map` and `__getitem__` on `StaticDictExpression`s. `map` is implemented by code duplication. ## Phase 2; Implement a new physical type that implements homogenous operations without code duplication.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6881:1171,cache,cache-friendly,1171,https://hail.is,https://github.com/hail-is/hail/issues/6881,1,['cache'],['cache-friendly']
Performance," then data must be sorted by the key and the ordering of rows with equivalent keys is undefined. In this case, it is safe to `order_by(*t.key)`. # User Expectations. There is a subtle difference in how we treat zero-length keyed (ZLK) objects and non-zero-length keyed objects. We try to preserve the data ordering of ZLK objects. In particular, users would be pretty surprised if `import_table(..., key=[])` shuffled the rows. Additionally, users expect (and we document) that `mt.key_cols_by().cols()` does not shuffle the rows. In contrast, we treat a non-zero-length keyed object as if any ordering beyond that defined by the key is irrelevant. Is this surprising to a user? Suppose they had a text file ordered by `family id, sample id`, they might reasonably expect that `import_table(..., key=['family id'])` does not reorder the rows within a family. Hail doesn't guarantee this even though we do make pains to not reorder the same data imported by `import_table(..., key=[])`. # Ordering and the Optimizer. Currently, the Hail optimizer does not remove a `choose_cols` that precedes a `order_by()`. Nor does it remove an `order_by(x, ...)` that precedes an `order_by()`. It does, however, remove a `key_by(x, ...)` that precedes an `order_by()` or a `key_by()`. ```; In [47]: import hail as hl ; ...: mt = hl.utils.range_matrix_table(3,3) ; ...: mt = mt.key_cols_by().choose_cols([1,2,0]) ; ...: mt.cols().order_by().show() ; ...: ; +---------+; | col_idx |; +---------+; | int32 |; +---------+; | 1 |; | 2 |; | 0 |; +---------+. In [48]: t = hl.utils.range_table(3) ; ...: t = t.order_by(-t.idx) ; ...: t = t.order_by().show() ; +-------+; | idx |; +-------+; | int32 |; +-------+; | 2 |; | 1 |; | 0 |; +-------+. In [49]: t = hl.utils.range_table(3) ; ...: t = t.key_by(x=-t.idx) ; ...: t = t.order_by().show() ; +-------+-------+; | idx | x |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 0 |; | 1 | -1 |; | 2 | -2 |; +-------+-------+. In [50]: t = hl.utils.range_table",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6929:4939,Optimiz,Optimizer,4939,https://hail.is,https://github.com/hail-is/hail/issues/6929,1,['Optimiz'],['Optimizer']
Performance," this:; 1. For runImage steps, you can only copy out-of or copy into `/io` (the reasoning is a bit complicated and somewhat historical).; 2. For buildImage steps, you can copy out-of or copy into `/`; 3. the `to` of an `output` specifies a file path in a ""filesystem"" that another step can access if it `dependsOn` the outputting step; 4. the `from` of an `input` specifies a file path in the aforementioned ""filesystem""; the filesystem contains all `outputs` from steps in the inputting step's `dependsOn` clause. We also have a `docker/Makefile` which is an emergency manual build system. I update that so that `hail_version` appears in the root of the docker context. The `service-base` uses the entire repository as its docker context, so I place hail_version at the root of the repository. I moved the `version` function from `hailtop.hailctl` into `hailtop`. It seems broadly useful and isn't specific to hailctl in anyway. Your concern about loading from pkg_resources repeated seems well-founded, so I went ahead and loaded the hail_version at package import time. This seems likely to ensure we learn about a missing hail_version file as early as possible (presumably at service start-time). This also means all hailtop installs need a hail_version file. I only found two other places that use hailtop. One of them was a completely unused Dockerfile. I deleted that (`Dockerfile.hailtop`). The other was Dockerfile.ci-test, which I updated to copy the hail_version file just like service-base. https://github.com/hail-is/hail/compare/main...danking:add-version-endpoint. Do you want to cherry-pick that change onto your add-version-endpoint branch?. One more change request: can we remove the try-catch? I don't expect any errors in that call and I tend to avoid revealing anything about errors to our users. Aiohttp will log the error and the stack trace if you let it rise all the way. Sorry for all the complication! Our build system is the second service we built and is clearly showing i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401:1745,load,loading,1745,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401,4,['load'],"['loaded', 'loading']"
Performance," to establish the interface and behavior. I expect several follow-on PRs:. - Revise the original copy interface proposal and add to dev-docs.; - ~~Parallelizes the transfers concurrently with async and across multiple threads.~~; - ~~After parallizing, copy will involve a lot of paralellism. Throwing an exception on the first error will be very non-deterministic. Instead, copy will return a report that collects all the errors that were encountered in the course of copying, and summarizes how many files/bytes were copied.~~; - Use multi-process parallelism; - Avoid overwriting the destination if it exists and has a matching checksum (or size).; - ~~Introduce multi-part transfers~~; - add a post-pass for Google Storage to detect file-and-directory errors.; - Adds support for S3.; - Add `hailctl cp ...` (PR); - Use copy in Batch. After this goes in, these can mostly be developed in parallel. A few principles guided the implementation of copy: perform the minimal number of system calls or API requests per copy, and only do error checking when it doesn't involve additional FS operations. For example, it is too expensive to exhaustively check if we're creating a path that is a file and a directory in Google Storage. I considered doing additional and exhaustive checking for the actual copy arguments. For example, currently, `cp -T /path/to/file /path/to/dir` will not generate an error on Google Storage. In the end, I decided to go with the current behavior and I will add an option to do a postpass to check for file-and-dir paths. To achieve this, for each transfer, I simultaneously stat the destination (if needd) to determine if it is a file, directory or doesn't exist. For each source, I simultaneously try to copy it as a file and a directory. When copying each source, we don't need to know the type of the destination until after we've stat'ed the source, so stat'ing the sources and destinations are all overlapping. This avoids dependencies where I have to e.g. stat the i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9822:1077,perform,perform,1077,https://hail.is,https://github.com/hail-is/hail/pull/9822,1,['perform'],['perform']
Performance," to stage failure: Task 22 in stage 5.0 failed 20 times, most recent failure: Lost task 22.19 in stage 5.0 (TID 133, seqr-pipeline-cluster-grch38-w-1.c.seqr-project.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFina",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822:4484,concurren,concurrent,4484,https://hail.is,https://github.com/hail-is/hail/issues/1822,1,['concurren'],['concurrent']
Performance," top level, the analysis takes a boolean typed IR `cond` in an environment where there is a reference to some `key`, and produces a set `intervals`, such that `cond` is equivalent to `cond & intervals.contains(key)` (in other words `cond` implies `intervals.contains(key)`, or `intervals` contains all rows where `cond` is true). This means for instance it is safe to replace `TableFilter(t, cond)` with `TableFilter(TableFilterIntervals(t, intervals), cond)`. Then in a second pass it rewrites `cond` to `cond2`, such that `cond & (intervals.contains(key))` is equivalent to `cond2 & intervals.contains(key)` (in other words `cond` implies `cond2`, and `cond2 & intervals.contains(key)` implies `cond`). This means it is safe to replace the `TableFilter(t, cond)` with `TableFilter(TableFilterIntervals(t, intervals), cond2)`. A common example is when `cond` can be completely captured by the interval filter, i.e. `cond` is equivant to `intervals.contains(key)`, in which case we can take `cond2 = True`, and the `TableFilter` can be optimized away. This all happens in the function; ```scala; def extractPartitionFilters(ctx: ExecuteContext, cond: IR, ref: Ref, key: IndexedSeq[String]): Option[(IR, IndexedSeq[Interval])] = {; if (key.isEmpty) None; else {; val extract = new ExtractIntervalFilters(ctx, ref.typ.asInstanceOf[TStruct].typeAfterSelectNames(key)); val trueSet = extract.analyze(cond, ref.name); if (trueSet == extract.KeySetLattice.top); None; else {; val rw = extract.Rewrites(mutable.Set.empty, mutable.Set.empty); extract.analyze(cond, ref.name, Some(rw), trueSet); Some((extract.rewrite(cond, rw), trueSet)); }; }; }; ```; `trueSet` is the set of intervals which contains all rows where `cond` is true. This set is passed back into `analyze` in a second pass, which asks it to rewrite `cond` to something equivalent, under the assumption that all keys are contained in `trueSet`. The abstraction of runtime values tracks two types of information:; * Is this value a reference to",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13355:1770,optimiz,optimized,1770,https://hail.is,https://github.com/hail-is/hail/pull/13355,1,['optimiz'],['optimized']
Performance," uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:1701,cache,cached,1701,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance," use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/09 19:16:02 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 19:16:02 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); /hail/test/BRCA1.raw_indel.vcf MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2; >>> ; ```. ----------------; When I executed the command in local mode , there seems to hava some result:; ```; [root@tele-1 ~]# python; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> from hail import *; >>> hc = HailContext();; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/opt/NfsDir/UserDir/wanghn/BRCA1.raw_indel.vcf').write('/opt/NfsDir/UserDir/wanghn/BRCA1.raw_indel_1.vds'); hail: info: No multiallelics detected.; hail: info: Coerced unsorted dataset; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; ```; ------------------------; How can I check if my spark configuration meet the requirement of the hail?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321228506:2514,load,load,2514,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321228506,1,['load'],['load']
Performance," which prevents reading and allocation of LID and RSID (also improved python-type-checking for `row_fields` and `entry_fields`)~ Moved to #3779 and #3778. - ~fixed table-table joins to _not_ always coerce (thus computing partition keys of) the right-hand table~ Moved to #3723 . - ~added a check that prevents globals and sample annotations copying when they're not used in the body of a MatrixMapCols~ Moved to #3751. - ~fixed a bug in `IndexBTree` wherein if the number of elements was a multiple of 1024, an unnecessary 1024 elements were added to the end of the index file (which I believe breaks the reading process which expects the number of bytes to correspond to the size of the tree)~ Moved to #3750. - ~added `IndexBTree2` which is just an in-memory list of the variant start positions. This is a fair bit of data. Chromosome 1 has about 250 million bases, so in the worst case this is 250 * 8 million bytes = 2 GB. It occurs to me that this is actually way to much data to load on the master node in general (since I just try to open the indexes for every file). I should switch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with fatals, so as not to allocate strings~ Moved to #3771. - ~We no longer copy the genotype data into a buffer in the block reader. This was forcing the `fastKeys` to do an unnecessary data copy~ Moved to #3783 (with some substantial refactoring so it doesn't look much like this PR anymore). - ~I changed the contract on BgenRecord to require that `getValue` is called to ""consume"" the record before the next record is taken~ Irrelevant thanks to #3783 's refactoring. - ~`getValue(null)` just skips bytes (no copy, no decompression)~ Irrelevant thanks to #3783 's refactoring. - ~I added `RegionValueBuilder.unsafeAdvan",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3727:1604,load,load,1604,https://hail.is,https://github.com/hail-is/hail/pull/3727,1,['load'],['load']
Performance," with equivalent keys is undefined. In this case, it is safe to `order_by(*t.key)`. # User Expectations. There is a subtle difference in how we treat zero-length keyed (ZLK) objects and non-zero-length keyed objects. We try to preserve the data ordering of ZLK objects. In particular, users would be pretty surprised if `import_table(..., key=[])` shuffled the rows. Additionally, users expect (and we document) that `mt.key_cols_by().cols()` does not shuffle the rows. In contrast, we treat a non-zero-length keyed object as if any ordering beyond that defined by the key is irrelevant. Is this surprising to a user? Suppose they had a text file ordered by `family id, sample id`, they might reasonably expect that `import_table(..., key=['family id'])` does not reorder the rows within a family. Hail doesn't guarantee this even though we do make pains to not reorder the same data imported by `import_table(..., key=[])`. # Ordering and the Optimizer. Currently, the Hail optimizer does not remove a `choose_cols` that precedes a `order_by()`. Nor does it remove an `order_by(x, ...)` that precedes an `order_by()`. It does, however, remove a `key_by(x, ...)` that precedes an `order_by()` or a `key_by()`. ```; In [47]: import hail as hl ; ...: mt = hl.utils.range_matrix_table(3,3) ; ...: mt = mt.key_cols_by().choose_cols([1,2,0]) ; ...: mt.cols().order_by().show() ; ...: ; +---------+; | col_idx |; +---------+; | int32 |; +---------+; | 1 |; | 2 |; | 0 |; +---------+. In [48]: t = hl.utils.range_table(3) ; ...: t = t.order_by(-t.idx) ; ...: t = t.order_by().show() ; +-------+; | idx |; +-------+; | int32 |; +-------+; | 2 |; | 1 |; | 0 |; +-------+. In [49]: t = hl.utils.range_table(3) ; ...: t = t.key_by(x=-t.idx) ; ...: t = t.order_by().show() ; +-------+-------+; | idx | x |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 0 |; | 1 | -1 |; | 2 | -2 |; +-------+-------+. In [50]: t = hl.utils.range_table(3) ; ...: t = t.key_by(x=-t.idx) ; ...: t = t.key_by().show(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6929:4970,optimiz,optimizer,4970,https://hail.is,https://github.com/hail-is/hail/issues/6929,1,['optimiz'],['optimizer']
Performance," yarl, typer, scipy, rsa, rich, requests, python-dateutil, pyasn1-modules, plotly, parsimonious; , jproperties, jinja2, janus, isodate, googleapis-common-protos, google-resumable-media, deprecated, contourpy, cffi, aiosignal, requests-oauthlib, pycares, pandas, google-auth, cryptography, botocore, azure-core, aiohttp, s3transfer, msrest, google-auth-oauthlib, google-api-core, bokeh, azure-storage; -blob, azure-mgmt-core, aiodns, msal, google-cloud-core, boto3, azure-mgmt-storage, msal-extensions, google-cloud-storage, azure-identity; Attempting uninstall: packaging; Found existing installation: packaging 23.2; Uninstalling packaging-23.2:; Successfully uninstalled packaging-23.2; Successfully installed aiodns-2.0.0 aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 asyncinit-0.2.4 attrs-23.1.0 avro-1.11.2 azure-common-1.1.28 azure-core-1.29.3 azure-identity-1.14.0 azure-mgmt-core-1.4.0 azure-mgmt-storage-20.1.0 azure-storage-blob-12.17.0 bokeh-3.2.2 boto3-1.28.41 botocore-1.31.; 41 cachetools-5.3.1 certifi-2023.7.22 cffi-1.15.1 charset-normalizer-3.2.0 commonmark-0.9.1 contourpy-1.1.0 cryptography-41.0.3 decorator-4.4.2 deprecated-1.2.14 dill-0.3.7 frozenlist-1.4.0 google-api-core-2.11.1 google-auth-2.22.0 google-auth-oauthlib-0.8.0 google-cloud-core-2.3.3 google-cloud-storag; e-2.10.0 google-crc32c-1.5.0 google-resumable-media-2.5.0 googleapis-common-protos-1.60.0 humanize-1.1.0 idna-3.4 isodate-0.6.1 janus-1.0.0 jinja2-3.1.2 jmespath-1.0.1 jproperties-2.1.1 markupsafe-2.1.3 msal-1.23.0 msal-extensions-1.0.0 msrest-0.7.1 multidict-6.0.4 nest-asyncio-1.5.7 numpy-1.25.2 oaut; hlib-3.2.2 orjson-3.9.5 packaging-23.1 pandas-2.1.0 parsimonious-0.10.0 pillow-10.0.0 plotly-5.16.1 portalocker-2.7.0 protobuf-3.20.2 py4j-0.10.9.5 pyasn1-0.5.0 pyasn1-modules-0.3.0 pycares-4.3.0 pycparser-2.21 pygments-2.16.1 pyjwt-2.8.0 python-dateutil-2.8.2 python-json-logger-2.0.7 pytz-2023.3.post; 1 pyyaml-6.0.1 regex-2023.8.8 requests-2.31.0 requests-oauthlib-1.3.1 rich-12.6.0 rsa-4.9 s3tra",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:43160,cache,cachetools-,43160,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cachetools-']
Performance," ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Worker$.main(Worker.scala:164) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Main$.main(Main.scala:14) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Main.main(Main.scala) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 		at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 		at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 		at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; 	Caused by: com.google.api.client.http.HttpResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 's",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:25086,concurren,concurrent,25086,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,"""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 53, in _read_gs_file; content = f.download_as_string(); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 697, in download_as_string; self.download_to_file(string_buffer, client=client, start=start, end=end); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 638, in download_to_file; _raise_from_invalid_response(exc); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 2034, in _raise_from_invalid_response; raise exceptions.from_http_status(response.status_code, message, response=response); google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/download/storage/v1/b/hail-batch2-nru9x/o/cd50b95a89914efb897965a5e982a29d%2F1%2F1%2Fsetup%2Fjob.log?alt=media: ('Request failed with status code', 403, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>); ```. - The first error is caused by the driver object is only available on the batch2-driver instance. Now the front end sends a request to the driver to get the logs if the job is running. I purposefully left the driver and front end calling the same function in case the job terminates before the driver handles the request from the front end. - Unified the Instance_ID between the front_end and the driver. I thought this was causing the second bug where gcs was unauthorized, but Tim was running in the default namespace, so they had the same instance id. - I checked and we're loading the same gsa-key in both the front end and driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7412:6420,load,loading,6420,https://hail.is,https://github.com/hail-is/hail/pull/7412,1,['load'],['loading']
Performance,"""analysis_type=ApplyRecalibration input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false input=[(RodBinding name=input source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.unfiltered.vcf)] recal_file=(RodBinding name=recal_file source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.recal) tranches_file=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.tranches out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub ts_filter_level=98.5 ignore_filter=null mode=SNP filter_mismatching_base_and_quals=false""; ##CombineVariants=""analysis_type=Combine",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:821,perform,performanceLog,821,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['perform'],['performanceLog']
Performance,"# --- DRAFT ---. # People. @hail-is/tensors . # Scope of Document; Python API, types, IR, optimizer, compiler. # Notation. e[[x/v]] means substitute occurrences of the variable v with the expression x in; the expression e. f[[ e ]] is just application of the function `f` but with the advantage that it doesn't look like any python or Scala syntax (so it's obviously referring to the meta-language rather than the languages we're building here). I'm going to consistently use ""distributed"" to talk about BlockMatrix-y things and ""small"" to refer to things that live in the ""value"" IR. # DistributedTensorIR. Some thoughts on TensorIR (fruits of discussion among the group):. TensorIR ::= TensorLiteral(); | TensorContract(TensorIR, TensorIR, Int, Int, body: IR); | TensorMap2(TensorIR, TensorIR, body: IR); | TensorMap(TensorIR, body: IR); | TensorSelectGlobals(TensorIR, body: IR). In the body of TensorContract and TensorMap2, four refs are free: `l`, `r`, `i`, and `j`. In the body of TensorMap, three refs are free: `e`, `i`, `j`. In the body of TensorContract, all four refs are aggregables. In the TensorMap and TensorMap2, they are scalar values. No aggregations are allowed in the body of TensorSelectGlobals. It is just `SparkContext.broadcast`. ## From Python. C[[ u @ v ]] := TensorContract(; C[[ u ]],; C[[ v ]],; 1,; 0,; hl.agg.sum(l * r)). C[[ u + v ]] := TensorMap2(; C[[ u ]],; C[[ v ]],; l + r). C[[ u + 1 ]] := TensorMap(; C[[ u ]],; e + I32(1)). C[[ u + hl.ndarray(...) ]] := TensorMap(; TensorSelectGlobals(; C[[ u ]],; uuid1,; C[[ hl.ndarray(...) ]]); e + NDArrayIndex(GetField(""globals"", uuid1), i, j)). ## Transformations. This representation admits elegant transformations:. TensorMap2(TensorMap(u, x), v, body); <=>; TensorMap2(u, v, Let(uuid1, x[l/e], body[Ref(uuid1)/l])). TensorMap(TensorMap(u, x), y); <=>; TensorMap(u, Let(uuid1, x, y[Ref(uuid1)/e])). TensorContraction(TensorMap(u, x), v, body); <=>; TensorContraction(u, v, Let(uuid1, x[l/e], body[Ref(uuid1)/l])). the ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5195:90,optimiz,optimizer,90,https://hail.is,https://github.com/hail-is/hail/issues/5195,1,['optimiz'],['optimizer']
Performance,"# Current situation. In several places, we call a method `emitNDArrayStandardStrides`, which has the effect of calling `emitDeforestedNDArray`, since that is known to always emit things in column major order. However, the downside of this is that we were doing unnecessary copies of the data, even when it was already in column major order, by constructing an `NDArrayEmitter` that just emitted an NDArray and then looked up values from it:. ```; case _ =>; val ndt = emit(x); val ndAddress = mb.genFieldThisRef[Long](); val setup = (ndAddress := ndt.value[Long]); val xP = x.pType.asInstanceOf[PNDArray]. val shapeAddress = new Value[Long] {; def get: Code[Long] = xP.shape.load(ndAddress); }; val shapeTuple = new CodePTuple(xP.shape.pType, shapeAddress). val shapeArray = (0 until xP.shape.pType.nFields).map(i => shapeTuple.apply[Long](i)). new NDArrayEmitter[C](nDims, shapeArray,; xP.shape.pType, xP.elementType, setup, ndt.setup, ndt.m) {; override def outputElement(elemMB: EmitMethodBuilder[C], idxVars: IndexedSeq[Value[Long]]): Code[_] =; xP.loadElementToIRIntermediate(idxVars, ndAddress, elemMB); }; ```. # New Situation. We now have `emitNDArrayColumnMajorStrides`, which calls `emit` on an ndarray, checks if the emitted thing is column major, and only does a copy if it needs to. This uses new `LinalgCodeUtils` methods `checkColumnMajor` and `createColumnMajorCode`. Everything else in `LinalgCodeUtils` was unused / old style and I removed them.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9428:675,load,load,675,https://hail.is,https://github.com/hail-is/hail/pull/9428,2,['load'],"['load', 'loadElementToIRIntermediate']"
Performance,# EDIT: See end of thread for new approach. The issue is that `use OpenBLAS` puts `libopenblas.so` on `LD_LIBRARY_PATH`. `netlib-java` links against `libblas.so` which is expected to contain the CBLAS symbols. It appears that Debian-style distributions use `update-alternatives` to symlink `libblas.so` to a library of the user's choice. Broad's UGER cluster does not provide such `update-alternatives` functionality. There exists two fixes:; - create a symlink to `libopenblas.so` named `libblas.so` and put it on the LD_LIBRARY_PATH; - use `LD_PRELOAD` to forcibly load `libopenblas.so`. The two solutions look like:. 1.; ```; mkdir ~/lib; ln -s /broad/software/free/Linux/redhat_7_x86_64/pkgs/openblas_0.2.20/lib/libopenblas.so ~/lib/libblas.so.3; export LD_LIBARRY_PATH=~/lib:$LD_LIBRARY_PATH; ```. 2.; ```; export LD_PRELOAD=/broad/software/free/Linux/redhat_7_x86_64/pkgs/openblas_0.2.20/lib/libopenblas.so; ```. Clearly neither of these are ideal. I recommend users place the lines from option 1 in an rc file.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5559#issuecomment-472953523:567,load,load,567,https://hail.is,https://github.com/hail-is/hail/issues/5559#issuecomment-472953523,1,['load'],['load']
Performance,"# The disappearing bit; ; This is one of the sneakiest bugs I've ever worked on. I learned early that the issue was somehow related to method splitting, but it was far more devious than I expected, and the code in LIR method splitting is completely correct. The particular case I was debugging had expected behavior when trying to write the missing bits of four fields A,B,C,D, where A and B were missing and C and D were present. These should have written the byte. 1<<0 | 1<<1 | 0<<2 | 0<<3; ==> b00000011; ==> 3. But instead wrote the byte `b00000001 or 1`, incorrectly leading readers to try to read field B when it was missing (and not written). This is due to the load-bearing and incorrect type of an I2B instruction generated [here](https://github.com/hail-is/hail/blob/8bd9b7b2224b77372a72f02f2b13806267892a35/hail/src/main/scala/is/hail/types/encoded/EBaseStruct.scala#L107). I2B is an instruction that truncates an integer to a byte, and it is used in various places in code generation but primarily encoding missing bits in arrays and structs. . I2B loads a byte to the stack, not a boolean. TypeInfos are mostly non-structural since they rarely influence the bytecode generated. Here is an exception, and that's where the method splitter comes in. Method splitting exists in the Hail compiler because not only does the JVM have limits on how large methods can be, but also the JIT compiler handles small methods much more effectively than large methods (and so splitting a large method into two small ones can make an order of magnitude or more in performance difference). We have three forms of method splitting in the Hail Query compiler. The first is a heuristic and greedy IR-level method splitter that generates new methods every X IR nodes, simply based on node count. However, the size of code generated by each IR can vary widely (`I32` vs `LowerBoundOnOrderedCollection` for instance), and so we have two other kinds of splitting that operate on the LIR level. The first is regio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:670,load,load-bearing,670,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['load'],['load-bearing']
Performance,"## Basic philosophy; https://developers.google.com/web/fundamentals/performance/prpl-pattern/. Hybrid server-side client-side rendered application, with eager pre-loading of resources. First page visited is server rendered. All client-side code asynchronously fetched, using service workers if available. HTML is ""hydrated""/bound by React, and from then on has the responsiveness of a client-side application. This is what we have in the current pull request. Initial view / first page ready in ~10ms, DOMContentLoaded in ~60-120ms (excluding network latency). ## Why not static/HTML web?; In practice: there is no such thing. Even document-centric sites often need dynamic templates, and will therefore use PHP, Python, NodeJS, Go, Rust, etc. These only work on a server, and only serve interpolated, static documents. Any interactive elements require Javascript. As soon as you need Javascript, the choice becomes Vanilla JS, JQuery, or something more structured. Vanilla JS requires a lot of boilerplate (verbose event binding, DOM modification, needs polyfills since browser incompatibilities). JQuery makes this easier, but is 1) very slow, 2) provides no structure. Vanilla JS and JQuery tend to devolve to soup of global state-modifying code, with a lot of time spent on figuring out how to update values in DOM elements. . React/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:68,perform,performance,68,https://hail.is,https://github.com/hail-is/hail/pull/4931,3,"['latency', 'load', 'perform']","['latency', 'loading', 'performance']"
Performance,"### Description. In this pull request, I add a function to perform a Cochran-Mantel-Haenszel statistical test for association. This pull request closes #13481. ### Testing. I add unit tests. Since I have not used R before (the [associated GitHub issue](https://github.com/hail-is/hail/issues/13481) suggests using R to create test cases), I created the unit tests from examples that I found on the internet. I linked these sources in the code for the unit tests. I built the documentation locally and inspected it to confirm that it matches my expectations. I am having trouble testing the docstring examples locally. When I run `make -C hail doctest-query`, the tests error due to a checksum exception. ### Discussion. ~I have not added an example to the documentation that uses a matrix table yet. (This is an acceptance criteria in #13481.) I wanted to get some advice about the best way to do this. I think ideally, the example would have a binary phenotype, an allele to test for association, and some stratifying variable. I tried to search through the existing code to find suitable example matrix tables in the docstrings, but I didn't find anything promising. I would appreciate help here.~. Update: thanks to @patrick-schultz's recommendation, I have added an example using a matrix table.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14255:59,perform,perform,59,https://hail.is,https://github.com/hail-is/hail/pull/14255,1,['perform'],['perform']
Performance,"### Description. Today our APIs are ""documented"" only through the list of endpoint handlers in implementation code ([example](https://github.com/hail-is/hail/blob/main/batch/batch/front_end/front_end.py#L239)). We can and should:; - Create OpenAPI documentation for our APIs (maybe per-service, maybe once in the gateway?); - Host swagger page/pages for exploring and testing out APIs . ### Security Impact. High. ### Security Impact Description. ""None"" for the creation of documentation, since we do not believe that documenting our APIs is inherently risky. ""High"" for hosting a new functional component on our web endpoints. Mitigating factor: swagger pages are loaded as static html with no need (or ability) to interact with other functional components, except through the same public APIs as are already accessible. ### Appsec Signoff. - [ ] Reviewed and approved",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14725:665,load,loaded,665,https://hail.is,https://github.com/hail-is/hail/issues/14725,1,['load'],['loaded']
Performance,"### Hail version:; `08224c6ab`; ### What you did:; Tried to load a plink dataset that was split by chromosome.; ```; files = [(; f'gs://fc-9a7c5487-04c9-4182-b3ec-13de7f6b409b/genotype/ukb_cal_chr{i}_v2.bed',; f'gs://fc-9a7c5487-04c9-4182-b3ec-13de7f6b409b/genotype/ukb_snp_chr{i}_v2.bim'; ) for i in range(1,23)]; mts = [hl.import_plink(bed=f[0],bim=f[1],fam=""gs://phenotype_31063/ukb31063.fam"") for f in files]; mt = mts[0].union_rows(*mts[1:]); ```; ### What went wrong (all error messages here, including the full java stack trace):; It loaded each plink file serially rather than in parallel, thus wasting many cores of my cluster (and my time).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3975:60,load,load,60,https://hail.is,https://github.com/hail-is/hail/issues/3975,2,['load'],"['load', 'loaded']"
Performance,"### Hail version:; `784ab2796878`; ### What you did:; ```; In [1]: import hail as hl; ...: ; ...: t1kg = hl.balding_nichols_model(3, 100, 100); ...: print(t1kg.describe()); ...: t1kg = t1kg_sm.repartition(500) ; ...: t1kg = t1kg._filter_partitions([1]); ...: t1kg = hl.split_multi(t1kg); ...: t1kg._force_count_rows(); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; FatalError: HailException: optimization changed type!; before: Matrix{global:Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean}},col_key:[sample_idx],col:Struct{sample_idx:Int32,pop:Int32},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],ancestral_af:Float64,af:Array[Float64],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{GT:Call}}; after: Matrix{global:Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean}},col_key:[sample_idx],col:Struct{sample_idx:Int32,pop:Int32},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],ancestral_af:Float64,af:Array[Float64],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{GT:Call}}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4527:441,optimiz,optimization,441,https://hail.is,https://github.com/hail-is/hail/issues/4527,1,['optimiz'],['optimization']
Performance,"### Hail version:; ```; 0.2.7-14ce9228174e; ```; ### What you did:; ```; mt = hl.import_bgen('... a bgen file with ~500k samples ...', entry_fields=['GT']); mt = mt.select_rows().select_cols().select_entries('GT'); mt.count(); ```; becomes; ```; 2019-01-08 18:19:48 root: INFO: optimize: before:; (TableCount; (TableMapRows; (TableMapGlobals; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (CastTableToMatrix `the entries! [877f12a8827e18f61222c6c8c5fb04a8]` __cols (s); (TableMapRows; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va)))))); (InsertFields; (Ref row); (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (Let g; (ArrayRef; (Ref global))); (SelectFields (locus alleles); (Ref row)))); 2019-01-08 18:19:48 root: INFO: optimize: after:; (TableCount; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (CastTableToMatrix `the entries! [877f12a8827e18f61222c6c8c5fb04a8]` __cols (s); (TableMapRows; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (loc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5100:278,optimiz,optimize,278,https://hail.is,https://github.com/hail-is/hail/issues/5100,1,['optimiz'],['optimize']
Performance,"### What happened?. - When a job creates a log file in excess of about half a GB, loading the job page can cause the batch front-end pod to crash as it loads the log file into memory and interpolates it directly into the job page. It should instead provide an endpoint to stream the job log and include that as an iframe or something in the job page. - When a job creates a log file in excess of 2GiB the batch worker can get into a bad state as it fails to upload the log. Again, it does not stream the log file, instead loading the whole log into memory as `bytes` and tries to upload that, but asyncio ssl has a limit of max-int sized non-streaming payloads. ### Version. 0.2.112. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12852:82,load,loading,82,https://hail.is,https://github.com/hail-is/hail/issues/12852,3,['load'],"['loading', 'loads']"
Performance,"### What happened?. A simple `hl.init()` fails, that used to work. Maybe an error with Spark, not an expert. ### Version. 0.2.108. ### Relevant log output. ```shell; ~ » python3; Python 3.10.9 (main, Dec 15 2022, 17:11:09) [Clang 14.0.0 (clang-1400.0.29.202)] on darwin; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import hail as hl; >>> hl.init(); 2023-01-27 17:15:28.940 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-1758>"", line 2, in init; File ""/opt/homebrew/lib/python3.10/site-packages/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/homebrew/lib/python3.10/site-packages/hail/context.py"", line 345, in init; return init_spark(; File ""<decorator-gen-1760>"", line 2, in init_spark; File ""/opt/homebrew/lib/python3.10/site-packages/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/homebrew/lib/python3.10/site-packages/hail/context.py"", line 424, in init_spark; backend = SparkBackend(; File ""/opt/homebrew/lib/python3.10/site-packages/hail/backend/spark_backend.py"", line 188, in __init__; self._jbackend = hail_package.backend.spark.SparkBackend.apply(; File ""/opt/homebrew/lib/python3.10/site-packages/py4j/java_gateway.py"", line 1304, in __call__; return_value = get_return_value(; File ""/opt/homebrew/lib/python3.10/site-packages/py4j/protocol.py"", line 326, in get_return_value; raise Py4JJavaError(; py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.backend.spark.SparkBackend.apply.; : java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x4d740d85) cannot access class sun.n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12630:442,load,load,442,https://hail.is,https://github.com/hail-is/hail/issues/12630,1,['load'],['load']
Performance,"### What happened?. Batch does not guarantee that there is always at most 1 running attempt for a job at any given time. While rare, this double scheduling can sometimes happen so there is a background task that checks the database for ""orphaned"" attempts -- attempts that are running but are not noted as the current attempt for the relevant job -- and stops them to reduce wasted spend. This query that polls the database for attempts to remove does a needless scan of the instances table. I'll describe below the process by which I discovered the inefficiency:. 1. GCP Cloud SQL has a nice feature Query Insights, in which reports latencies and rows scanned by popular queries. For singular queries it can show a graph of the query and indicate bottlenecks. The below query is currently scanning over a million rows of the instances table is taking on average 642.37 ms:. https://github.com/hail-is/hail/blob/091e6612752010880a130cf4010897e87ea2a864/batch/batch/driver/canceller.py#L373-L382. as shown here from Query Insights:; ![Screenshot 2024-04-11 at 10 31 17 AM](https://github.com/hail-is/hail/assets/24440116/d807b383-7825-4ff5-ad04-6869f0402dd0). 2. The thick edge on the instances scan indicates that the where condition for instances is not using an index. We can verify this by explaining the query against the DB:; ```; > kssh admin-pod; > mysql; mysql> use batch;; mysql> EXPLAIN SELECT attempts.*; -> FROM attempts; -> INNER JOIN jobs ON attempts.batch_id = jobs.batch_id AND attempts.job_id = jobs.job_id; -> LEFT JOIN instances ON attempts.instance_name = instances.name; -> WHERE attempts.start_time IS NOT NULL; -> AND attempts.end_time IS NULL; -> AND ((jobs.state != 'Running' AND jobs.state != 'Creating') OR jobs.attempt_id != attempts.attempt_id); -> AND instances.`state` = 'active'; -> ORDER BY attempts.start_time ASC; -> LIMIT 300\G;. *************************** 1. row ***************************; id: 1; select_type: SIMPLE; table: instances; partitions: NULL; type: A",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14460:748,bottleneck,bottlenecks,748,https://hail.is,https://github.com/hail-is/hail/issues/14460,1,['bottleneck'],['bottlenecks']
Performance,"### What happened?. Batch now has the ability to offer a metadata server endpoint in the network namespaces of a `DockerJob`. We should add this functionality to `JVMJob`s so that QoB jobs can use the google default credential flow instead of relying on a GSA key file in the job container. While the implementation here could be trivial, we should make sure to load test it properly as QoB jobs can be very short (100s of ms). The simplest route would be to create/close a metadata server in the `JVMContainer` at the start/end of every job. If this incurs a penalty, since these `JVMContainer`s are long-lived we can run a long-lived metadata server and swap out the underlying credentials when user jobs start/stop. ### Version. 0.2.130. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14487:362,load,load,362,https://hail.is,https://github.com/hail-is/hail/issues/14487,1,['load'],['load']
Performance,"### What happened?. Below is a high level overview of how the batch driver communicates scheduled jobs to worker nodes. Scheduling loop on the driver:; 1. Select N ready jobs from the database to schedule on available workers; 2. Compute placement of a subset of the jobs in available slots in the worker pool; 3. Concurrently call `/api/v1alpha/batches/jobs/create` on available workers for each placed job. If/when the request completes successfully, the job is marked as scheduled.; 4. Once all requests complete, goto 1. On the worker, what happens inside `/api/v1alpha/batches/jobs/create`:; 1. Read metadata describing the job to schedule from the request body; 2. Using that information, load the full job spec from blob storage; 3. Spawn a task to run the job asynchronously; 4. Respond to the driver with a 200. The key point relevant to this issue is that the driver currently must wait for all the requests to workers in an iteration to complete before it starts the next iteration of the scheduler. This leaves the scheduler vulnerable to problematic workers or workers that happen to be preempted during the scheduling process. So, the driver sets a [2 second timeout](https://github.com/hail-is/hail/blob/b27737f67bf9e69f1abed2fec07fc7c921790ef8/batch/batch/driver/job.py#L585) on the call to `/api/v1alpha/batches/jobs/create`. Additionally, this general design means that in the event of a request timeout or transient error, Batch cannot guarantee that there is always at most one concurrent running attempt for a given job. This ends up being a fine (and intentional) concession in practice because the idempotent design of preemptible jobs tends to cover this scenario, but it is regardless wasted compute and cost to users. Nevertheless, we strive to minimize cases where we might halt the scheduling loop or double-schedule work, and one way to do that in the current design is to minimize the variance in latency of `/api/v1alpha/batches/jobs/create`. The largest source of this ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14456:314,Concurren,Concurrently,314,https://hail.is,https://github.com/hail-is/hail/issues/14456,2,"['Concurren', 'load']","['Concurrently', 'load']"
Performance,"### What happened?. Ben submitted a pipeline where the first 85% of jobs run in us-central1 while the last 15% run in us-east1. The autoscaler only looks at the head of the job queue and then sorts the result set to figure out the regions to spin up instances in. The scheduler looks at the entire job queue and then sorts the result set to figure out the regions to spin up instances in. The sort order placed us-east1 before us-central1. Concretely, the autoscaler is spinning up instances in us-central1 only while the scheduler is trying to schedule jobs in us-east1. See also: https://github.com/hail-is/hail/pull/13268. ### Version. 0.2.118. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13269:177,queue,queue,177,https://hail.is,https://github.com/hail-is/hail/issues/13269,2,['queue'],['queue']
Performance,"### What happened?. Consider these two programs. The first is faster, taking advantage of the fact that the count of the number of first alternate allele observations depends only on `variant_data`. Could Hail do this automatically? Could we modify variant_qc to facilitate this optimization?; ```; vds = hl.vds.read_vds(""some_very_big.vds""); mt = vds.variant_data; mt = hl.split_multi_hts(mt); mt = hl.variant_qc(mt); mt = mt.annotate_rows(AC100 = mt.variant_qc.AC[1] > 99); mt = mt.filter_rows(mt.AC100); vds.variant_data = mt; mt = hl.vds.to_dense_mt(vds); mt.write(""filtered.mt"", overwrite=True); ```; ```; vds = hl.vds.read_vds(""some_very_big.vds""); mt = hl.vds.to_dense_mt(vds); mt = hl.split_multi_hts(mt); mt = hl.variant_qc(mt); mt = mt.annotate_rows(AC100 = mt.variant_qc.AC[1] > 99); mt = mt.filter_rows(mt.AC100); mt.write(""filtered.mt"", overwrite=True); ```. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13695:279,optimiz,optimization,279,https://hail.is,https://github.com/hail-is/hail/issues/13695,1,['optimiz'],['optimization']
Performance,"### What happened?. Creating a billing project is fairly annoying right now. I have to perform 2 + N_USERS form submission:. 1. Create the billing project on the billing project page (I have to scroll to the bottom of the page, which is now large).; 2. Go to the billing limit page and set the limit (I have to search for the BP name).; 3. For each user, go to the billing project page, search for the BP name, enter the first user, press enter. I want a form like:. ```; Billing project name: ______; Billing project limit: ______; Billing project users:; _____; _____; _____; ```. The users should just be a multi-line textbox. It should disable autocorrect etc. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13859:87,perform,perform,87,https://hail.is,https://github.com/hail-is/hail/issues/13859,1,['perform'],['perform']
Performance,"### What happened?. Currently, almost all of our tests are integration tests which require:; 1. Compiling Scala code.; 2. Building a JAR (takes ~30 seconds on my MBP); 3. Running pytest (can take as long as 20 seconds). All of this is a lot slower than iterating with a live running Scala process. We should have tests of various parts of the compiler operating at the IR level. For example, MatrixIR to TableIR lowering should have plenty of in Scala IR-level tests. Likewise for TableIR to CDAIR. The optimizer/simplifier should also have tests at each level which assert certain kinds of code is sufficiently cleaned up by the optimizer. ### Version. 0.2.122. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13638:503,optimiz,optimizer,503,https://hail.is,https://github.com/hail-is/hail/issues/13638,2,['optimiz'],['optimizer']
Performance,"### What happened?. Currently, in order to change the rate limit in `internal-gateway`, one has to manually edit `envoy.py` and redeploy CI. This is non-standard, time intensive and can be accidentally reverted if CI merges a new commit to `main`. CI already regularly updates the envoy configuration `internal-gateway` uses to account for services in new namespaces, so making the rate limit configurable should be a simple CRUD task that would greatly ease operation of batch under high load. One gotcha to keep in mind is that while we run CI as a control plane for our ""dynamic cluster topology"", it should still be possible to manually deploy `internal-gateway` in a standalone Batch cluster (see `internal-gateway/Makefile`), so `envoy.py` should still be runnable as a standalone script. ### Version. 0.2.128. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14399:489,load,load,489,https://hail.is,https://github.com/hail-is/hail/issues/14399,1,['load'],['load']
Performance,"### What happened?. Currently, the `ServiceBackend`'s implementation of collect distributed array submits a job group full of worker jobs (1 per partition) and waits for the job group to complete before reading the results of the worker jobs. For small analyses this is fine, but when a query has tens of thousands of partitions it can take time to schedule and complete all of the worker jobs and reading back those results on the driver can become a bottleneck. Below is one possible solution to this problem:. #### Expose log for job completions in a job group. The Query Driver should attempt to read worker job results while the stage is running, but to do this it needs the Batch API to provide an append-only log of completed jobs in a job group that the Query Driver can consume instead of issuing O(jobs) job status requests during each stage. It may be that this is already possible with the current database schema, but can at worst be achieved by creating an indexed column on jobs that contain the spot they completed in in the job group. . Completion of this feature would require:; - Carefully evaluating the Batch data model to determine if there are any database changes necessary to construct an append-only log of job completions in a job group from the state of the database; - If changes are needed, design and implement a batch front end API endpoint to query the log; - (Separately) Add support for streaming the log in the Scala BatchClient and use it to read partition results before the job group completes. ### Version. 0.2.132. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14607:452,bottleneck,bottleneck,452,https://hail.is,https://github.com/hail-is/hail/issues/14607,1,['bottleneck'],['bottleneck']
Performance,"### What happened?. Daniel M is seeing this bug running the script [here](https://github.com/broadinstitute/tgg_methods/blob/master/vrs/vrs_annotation_batch.py#L275) with the flag --annotate-original. The Zulip thread [here](https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/Batch.20backend.20.22multiple.20clients.20uploading.22.3F/near/353337596) has more context as well. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/batch/worker/worker.py"", line 2150, in run; await self.jvm.execute(local_jar_location, self.scratch, self.log_file, self.jar_url, self.argv); File ""/usr/local/lib/python3.7/dist-packages/batch/worker/worker.py"", line 2629, in execute; raise JVMUserError(exception); JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:224); at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:186); at is.hail.JVMEntryway.main(JVMEntryway.java:156); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at is.hail.JVMEntryway$1.run(JVMEntryway.java:107); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950:781,concurren,concurrent,781,https://hail.is,https://github.com/hail-is/hail/issues/12950,3,['concurren'],['concurrent']
Performance,### What happened?. Details here: https://discuss.hail.is/t/subset-matrix-table-to-a-medium-sized-list-of-variants/3362/5. ```; Java stack trace:; java.lang.ClassCastException: class org.apache.spark.sql.catalyst.expressions.GenericRow cannot be cast to class is.hail.variant.Locus (org.apache.spark.sql.catalyst.expressions.GenericRow is in unnamed module of loader 'app'; is.hail.variant.Locus is in unnamed module of loader org.apache.spark.util.MutableURLClassLoader @62435e70); at is.hail.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:124); at is.hail.expr.JSONAnnotationImpex$.$anonfun$exportAnnotation$5(AnnotationImpex.scala:129); at is.hail.expr.JSONAnnotationImpex$.$anonfun$exportAnnotation$5$adapted(AnnotationImpex.scala:128); at scala.collection.generic.GenTraversableFactory.tabulate(GenTraversableFactory.scala:150); at is.hail.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:128); at is.hail.types.virtual.Type.toJSON(Type.scala:184); at is.hail.expr.JSONAnnotationImpex$.$anonfun$exportAnnotation$4(AnnotationImpex.scala:125); at is.hail.utils.Interval.toJSON(Interval.scala:103); at is.hail.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:125); at is.hail.expr.JSONAnnotationImpex$.$anonfun$exportAnnotation$1(AnnotationImpex.scala:113); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238); at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); at scala.collection.TraversableLike.map(TraversableLike.scala:238); at scala.collection.TraversableLike.map$(TraversableLike.scala:231); at scala.collection.AbstractTraversable.map(Traversable.scala:108); at is.hail.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:113); at is.hail.expr.ir.Pretty.header(Pretty.scala:405); at is.hail.expr.ir.Pretty.pretty$1(Pretty,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046:360,load,loader,360,https://hail.is,https://github.com/hail-is/hail/issues/13046,2,['load'],['loader']
Performance,"### What happened?. Figure out why the k8s cache fails. Is this due to asyncio task cancellation? Is it a known rare transient error?. If this is a rare transient error, we should retry this a limited number of times. Example: https://batch.hail.is/batches/8071211/jobs/186. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13909:43,cache,cache,43,https://hail.is,https://github.com/hail-is/hail/issues/13909,1,['cache'],['cache']
Performance,"### What happened?. Hail Batch never forgets a batch. All batches, jobs, and attempts are forever persisted in the Batch database. This is rarely a performance problem, as the indexes ensure that old rows are rarely ever looked at, but the fact that the database storage is monotonically increasing is something that we have to reckon with, and it makes migrations very time intensive. There are certainly many improvements that can be made to waste less space in the database (like #14623), but ultimately we will need to make a decision about how long we should persist batches. We should quantify the utility of historic batches, what might be a good cutoff or alternative process for expiring batches, and whether we should provide some sort of export that users can use to own information about their batches. I imagine the most relevant information would be cost and logs. ### Version. 0.2.132. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14626:148,perform,performance,148,https://hail.is,https://github.com/hail-is/hail/issues/14626,1,['perform'],['performance']
Performance,"### What happened?. Hail's google/azure credential classes do not require the caller to specify scopes when requesting access tokens, and thus default to a [very wide set of scopes](https://github.com/hail-is/hail/blob/91f5a0bfc30927014b60b11a353a4d95db009427/hail/python/hailtop/aiocloud/aiogoogle/credentials.py#L140), making those access tokens excessively powerful. An access token does not need to have the `https://www.googleapis.com/auth/appengine.admin` scope to read a blob from GCS. This poses an unnecessary risk if such a token were leaked. These classes should instead require that scopes be specified when requesting an access token, and call sights should specify the minimum set of scopes necessary to perform their function. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13530:718,perform,perform,718,https://hail.is,https://github.com/hail-is/hail/issues/13530,1,['perform'],['perform']
Performance,"### What happened?. Hana Snow is the engineer for SEQR. Previously, SEQR used elastic search as its datastore. Unfortunately, elastic search was very expensive because, to get reasonable performance, SEQR indexed nearly every field. The ES index was huge and the VM resources necessary to run an ES instance on that index were expensive (like 1000s USD per month). I've been supporting Hana as much as I can, but she needs someone who can be more dedicated and responsive than me. She uses a k8s cluster. She has a SEQR frontend deployment. She also has a Hail deployment (statefulset maybe?). The Hail pod has an SSD mounted read-only. That SSD has all the SEQR data in Hail Table form. There are many tables with annotations (variant metadata, like ""probability this variant is damaging"" or ""likely causes this to happen to the protein""). There are also ""per-family"" tables which contain all the sequences within a single family. Many queries are directly against a particular family. Those tables are small and quick to read. There's also one giant table containing all the sequences from all the families. That table is large and expensive to read. A lot of our engineering work has been around making sure queries against that table are fast. Tim, at one point, had enough of her system locally that he could experiment with running queries on his laptop against his SSD. He hacked on the queries themselves and on Hail itself until the bandwidth was fast enough that the queries should complete fast enough on the full dataset. Fast enough varies but generally a couple tens of seconds is OK. The work here is to pair with Hana to diagnose performance issues and make changes until the queries are acceptably fast. The first thing I would do is update her to the latest Hail (with the array decoder improvement as well as the memory overhead stuff on which Daniel is working). Then, with Hana's help, test the timing of some queries. If the queries are still too slow, your options are:; 1. Chec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882:187,perform,performance,187,https://hail.is,https://github.com/hail-is/hail/issues/13882,1,['perform'],['performance']
Performance,"### What happened?. Hello,. It looks like Hail has a hard coded check to only run on Java 8 and 11, despite Spark supporting Java 17 for a couple years now, including on spark 3.3.x, which is the currently used release for `pip install hail`: https://spark.apache.org/releases/spark-release-3-3-0.html#build. **Would it be possible to add Java 17 support**, or possibly even remove the Java version check in general so that it can track what underlying Spark does without additional updates? . There are a bunch of benefits of moving to Java 17, including:; 1. https://kstefanj.github.io/2021/11/24/gc-progress-8-17.html - Significant garbage collector improvements that will likely improve throughput and reduce costs; 2. https://vmnotescom.wordpress.com/2021/09/14/java-17-whats-new-removed-and-preview-in-jdk-17/ - Better Apple Silicon support. I know that darwin-aarch64 has been backported to 8 and 11, but 17 is faster on that platform.; 3. https://spark.apache.org/releases/spark-release-3-5-0.html#removals-behavior-changes-and-deprecations - The next release of Spark will require Java 17 as a minimum version, and making the change now is easier than making more changes all at once in the future.; . > The following features will be removed in the next Spark major release; > ; > Support for Java 8 and Java 11, and the minimal supported Java version will be Java 17; > Support for Scala 2.12, and the minimal supported Scala version will be 2.13. Also, requiring specifically Java 8 or 11 has led to some friction for students and researchers who are first evaluating hail. In the past few weeks, I've talked to a lot of students and researchers who wanted to evaluate hail, followed the documentation to install Azul Java 8 but already had an existing Java install and did not update their PATH or JAVA_HOME. Most of their existing Java versions were 17, as 17 is the current default on most Linux distros and a common one to have been installed via Brew in the past few years on Mac. Alt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14433:691,throughput,throughput,691,https://hail.is,https://github.com/hail-is/hail/issues/14433,1,['throughput'],['throughput']
Performance,"### What happened?. Hi,; I am on a macOS Ventura and I have successfully installed hail (v 0.2.109) on a conda env. Everything seems to run properly, except that I don't get any plots. Bokeh was installed in the env, v1.4.0., pysark =3.13 and scala=2.11.8 are some relevant packages that may contribute to this issue. When starting Hail, this is the output I get:. 2023-02-20 11:07:38.798 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.1.3; SparkUI available at http://amaru-2.local:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.109-b71b065e4bb6; LOGGING: writing to /Users/alanmejiamaza/hail-20230220-1107-0.2.109-b71b065e4bb6.log. It seems to be that the issue comes from the spark version? which is the correct spark version for a conda env on a mac? I have followed the tutorials and seemed to work fine except for the plots. I don't have any output when invoking commands for plots. Can anyone tell me the specific versions needed to run all Hail properties?. Thanks. ### Version. 0.2.109. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12717:426,load,load,426,https://hail.is,https://github.com/hail-is/hail/issues/12717,1,['load'],['load']
Performance,"### What happened?. In older versions of hail (tested with 0.2.115), when starting a dataproc cluster with VEP, e.g.; ```{bash}; hailctl dataproc start hail-test --region australia-southeast1 --project my-project --vep GRCh38 --packages gnomad --num-workers 2; ```; the dataproc cluster command would be provided the following environment variable through the `--metadata` flag: `VEP_REPLICATE=aus-sydney`. This variable is used within the script `gs://hail-common/hailctl/dataproc/0.2.115/vep-GRCh38.sh` to determine which bucket to pull the VEP cache data from. In more recent versions (tested with 0.2.130), this `VEP_REPLICATE` variable has been changed to `VEP_REPLICATE=australia-southeast1`, however the Australian bucket containing the VEP cache data is still `aus-sydney`, meaning that the VEP data is not copied into the dataproc cluster, and when trying to run VEP I get the error `No cache found for homo_sapiens, version 95`. ### Version. 0.2.130. ### Relevant log output. ```shell; FatalError: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:547,cache,cache,547,https://hail.is,https://github.com/hail-is/hail/issues/14513,3,['cache'],['cache']
Performance,"### What happened?. JVMJobs exist to provide a warm JVM to Hail jobs; however, in practice, there are two issues:; 1. A JVM warmed for one JAR (i.e. version) of Hail has limited benefit for a different JAR. Only shared classes like those in `java.util` could have been JITed.; 2. As the number of non-JVM running jobs grows, the likelihood that a JVMJob lands on a worker with a warm JVM decreases. Suppose instead that, as a part of the deploy process, we executed a series of Hail pipelines using the LocalBackend and export the JIT cache. We then store *both* the JAR and the JIT cache in GCS. A user job loads both the JAR and the JIT cache and starts a fresh JVM that loads from that JIT cache. Every JVMJob now, by definition, lands on a hot JVM. References; - ""Compile Stashing"" https://docs.azul.com/prime/Compile-Stashing; - ""Tuning JIT Compilations"" https://docs.azul.com/prime/analyzing-tuning-warmup#tuning-jit-compilations; - ""ReadyNow Warm-Up Optimizer"" https://docs.azul.com/prime/analyzing-tuning-warmup#use-readynow-warm-up-optimizer. ### Version. 0.2.122. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13675:535,cache,cache,535,https://hail.is,https://github.com/hail-is/hail/issues/13675,8,"['Optimiz', 'cache', 'load', 'optimiz']","['Optimizer', 'cache', 'loads', 'optimizer']"
Performance,"### What happened?. Julia Sealock reported this https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/vep.20issue/near/352790173. We also saw it in test_dataproc. Cal also reported it.; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 56 in stage 4.0 failed 20 times, most recent failure: Lost task 56.19 in stage 4.0 (TID 48622) (jsealock-schema-sw-43bq.c.daly-neale-sczmeta.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 125; VEP Error output:; docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.; See 'docker run --help'. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.methods.VEP$.waitFor(VEP.scala:73); 	at is.hail.methods.VEP.$anonfun$execute$5(VEP.scala:231); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.hasNext(RichContextRDD.scala:69); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collectio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936:561,cache,cache,561,https://hail.is,https://github.com/hail-is/hail/issues/12936,1,['cache'],['cache']
Performance,### What happened?. Local java tests failing. Caused by https://github.com/hail-is/hail/pull/13551. ```; java.lang.ClassNotFoundException: org.apache.hadoop.mapreduce.lib.input.TextInputFormat; at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581); at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178); at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522); ```. ### Version. 0.2.124. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13706:220,load,loader,220,https://hail.is,https://github.com/hail-is/hail/issues/13706,5,['load'],"['loadClass', 'loader']"
Performance,"### What happened?. Struct decoding currently uses `Region.loadBit` which:; 1. Calculates the address of the byte has this bit (e.g. the 65th bit is in the second byte).; 2. Loads the byte out of memory.; 3. Masks the bit out of the byte.; 4. Compares to zero. We don't have concrete data, but we suspect that the JVM can't avoid loading the byte out of memory 8 times. If we can instead load it once per 8 missing fields, there may be a speed up for structs that are frequently decoded (e.g. an entry struct). ### See also. - https://github.com/hail-is/hail/issues/13792#issuecomment-1761652107 . ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13811:59,load,loadBit,59,https://hail.is,https://github.com/hail-is/hail/issues/13811,4,"['Load', 'load']","['Loads', 'load', 'loadBit', 'loading']"
Performance,"### What happened?. Suppose you're working with the [Wheat genome](https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/Other.20genome/near/397467764). The following is seemingly correct code but it doesn't work:; ```python3; import hail as hl. rgwheat = hl.ReferenceGenome('Wheat', ...). hl.init(default_reference=rgwheat); ```; The first problem is that the `@typecheck` on `hl.init`, `hl.init_spark`, etc. only allows a built-in reference genome. . Even if we relax that requirement, we encounter a deeper problem: creating the reference genome initializes Hail. In particular, [we call `Env.backend()`](https://github.com/hail-is/hail/blob/main/hail/python/hail/genetics/reference_genome.py#L117-L118) (which calls `Env.hc()`, which forces initialization) so that we can call `add_reference`. What does initialization mean? Historically, it meant connection to or starting a JVM/Spark process. In QoB/ServiceBackend, initialization just loads configurations, it doesn't really do anything irreversible. Regardless of what it does, we only allow initialization *once*. OK, so, there's two possible routes to fix this problem:; 1. Rewrite `ReferenceGenome.__init__` such that it does not initialize Hail. You have to decide how reference genomes are ultimately communicated to the backend. Do you hang a list of all created reference genomes off of the `ReferenceGenome` class? Do you require explicit registering a la `hl.register_reference`? The latter seems a bit silly. The former seems OK, but you could also ...; 2. Allow modification of the default reference after initialization. The default reference genome is just a field on the HailContext: `_default_ref` which is accessed through `hl.default_reference()`. Just modify `hl.default_reference` to *return* the reference with no arguments and *set* the reference with one argument. Now this works:. ```python3; import hail as hl; rgwheat = hl.ReferenceGenome('Wheat', ...); hl.default_reference(rgwheat); mt = hl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13856:968,load,loads,968,https://hail.is,https://github.com/hail-is/hail/issues/13856,1,['load'],['loads']
Performance,"### What happened?. TBD. ### Version. 0.2.172. ### Relevant log output. ```shell; 24/02/05 11:52:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.3.2; SparkUI available at http://192.168.1.140:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.127-d82c34a83360; LOGGING: writing to /private/tmp/varo/avro/hail-20240205-1152-0.2.127-d82c34a83360.log; 2024-02-05 11:53:03.679 Hail: INFO: import_gvs: Importing and writing site filters to temporary storage; Traceback (most recent call last):; File ""/Users/dking/projects/gatk/scripts/variantstore/wdl/extract/hail_gvs_import.py"", line 180, in <module>; create_vds(arguments, vds_path, references_path, temp_path, use_classic_vqsr,; File ""/Users/dking/projects/gatk/scripts/variantstore/wdl/extract/hail_gvs_import.py"", line 35, in create_vds; import_gvs.import_gvs(; File ""<decorator-gen-1896>"", line 2, in import_gvs; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/projects/gatk/scripts/variantstore/wdl/extract/import_gvs.py"", line 211, in import_gvs; site.write(site_path, overwrite=True); File ""<decorator-gen-1224>"", line 2, in write; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/hail/table.py"", line 2002, in write; Env.backend().execute(; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/hail/backend/backend.py"", line 190, in execute; raise e.maybe_user_error(ir) from None; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/hail/backend/backend.py"", l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14249:133,load,load,133,https://hail.is,https://github.com/hail-is/hail/issues/14249,1,['load'],['load']
Performance,"### What happened?. The basic problem is:; 1. The type for `ArrayMaximalIndependentSet` is `TArray(...)` where `...` is whatever the node type is. ; 2. We choose a PType based on the Type.; 3. We choose an SType based on the PType.; 4. `unwrapReturn` makes an incorrect assumption about which SType corresponds to a `TArray(String)`. In particular,; ```; Code.invokeScalaObject1[UnsafeIndexedSeq, IndexedSeq[Any]](Graph.getClass, ""maximalIndependentSet"", jEdges); ```; returns a Java array of whatever `svalueToJavaValue` returns. In that case, that's a `String[]` which we call `SJavaArrayString`. However, the SType chosen for `TArray(TString)` is `SIndexablePointer(SBinary)`. **I think the real fix here is to just pass region pointers into MaximalIndependentSet.** Just get an `elementIterator` from `PCanonicalArray` and use `loadElement`, etc. to populate the `Graph`. ```; In [1]: import hail as hl; ...: ht = hl.Table.parallelize([hl.Struct(i='A', j='B', kin=0.25), hl.Struct(i='A', j='C', kin=0.25), hl.Struct(i='D', j='E', kin=0.5)]); ...: hl.maximal_independent_set(ht.i, ht.j, False).collect(); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); Cell In[1], line 3; 1 import hail as hl; 2 ht = hl.Table.parallelize([hl.Struct(i='A', j='B', kin=0.25), hl.Struct(i='A', j='C', kin=0.25), hl.Struct(i='D', j='E', kin=0.5)]); ----> 3 hl.maximal_independent_set(ht.i, ht.j, False).collect(). File <decorator-gen-1148>:2, in collect(self, _localize, _timed). File ~/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py:584, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 581 @decorator; 582 def wrapper(__original_func, *args, **kwargs):; 583 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 584 return __original_func(*args_, **kwargs_). File ~/miniconda3/lib/python3.10/site-packages/hail/table.py:2162, in Table.collect(self, _localize, _timed);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13633:832,load,loadElement,832,https://hail.is,https://github.com/hail-is/hail/issues/13633,1,['load'],['loadElement']
Performance,### What happened?. The hailgenetics/hail and hailgenetics/hailtop images are commonly used by our users (the latter is used for the remove_tmpdir job). We should eagerly cache a few recent versions on the batch worker VMs to accelerate this common workload. ### Version. 0.2.124. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13913:171,cache,cache,171,https://hail.is,https://github.com/hail-is/hail/issues/13913,1,['cache'],['cache']
Performance,"### What happened?. The spark and local backends use `py4j` to execute methods on java backends. `py4j` uses a TCP socket and a text-based protocol to communicate between python and the jvm and handles marshaling of data between the two processes. Unfortunately it has poor memory performance with large byte arrays, as the text protocol requires base64 encoding byte arrays and it uses Java `String`s which, being UTF-16, more than double the size of the original data in memory. Hail should not use `py4j` for these operations and just open its own connection to the java backend. This gives us the control to not use more memory than is necessary to just ship bytes back and forth. This also provides an opportunity to deduplicate some code as the `ServiceBackend` already communicates writes its inputs over a socket instead of using `py4j` (there is no live JVM to communicate to in the `ServiceBackend` case, so it must serialize the requested operation to be run at a later time on a different machine). ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13756:281,perform,performance,281,https://hail.is,https://github.com/hail-is/hail/issues/13756,1,['perform'],['performance']
Performance,"### What happened?. This is a known issue in our underlying serialization library: https://github.com/uqfoundation/dill/issues/609. I've discovered that if you have dill 0.3.7 (latest) and try to use our image (which has dill 0.3.5.1) you get this error:. Traceback (most recent call last):; File ""<string>"", line 27, in <module>; File ""/usr/local/lib/python3.10/site-packages/dill/_dill.py"", line 373, in load; return Unpickler(file, ignore=ignore, **kwds).load(); File ""/usr/local/lib/python3.10/site-packages/dill/_dill.py"", line 646, in load; obj = StockUnpickler.load(self); File ""/usr/local/lib/python3.10/site-packages/dill/_dill.py"", line 805, in _create_code; return CodeType(args[0], 0, 0, *args[1:]); TypeError: code expected at most 16 arguments, got 19; Switching local dill to the remote version eliminates the error. Fix is to ensure the *de*-serializer is >0.3.5.1. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13535:406,load,load,406,https://hail.is,https://github.com/hail-is/hail/issues/13535,4,['load'],['load']
Performance,"### What happened?. This job is just issuing network requests, very little CPU. ---. Longer term musing:. They're also example of a relatively fast running job for which we'd probably want a ""fast queue"" where you get evicted after some short period of time, like 2 minutes. We could have these kinds of jobs fail over to ""slow queues"" if they run over time. This kind of queue is also valuable to use-cases like SEQR and other low latency UIs. Also, certain QoB jobs are known to be very fast (e.g. matrix type). ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13816:197,queue,queue,197,https://hail.is,https://github.com/hail-is/hail/issues/13816,4,"['latency', 'queue']","['latency', 'queue', 'queues']"
Performance,"### What happened?. Try writing to a bucket to which your service account has read-only access:; ```; hl.utils.range_table(5,n_partitions=5).write('gs://neale-bge/foo.ht'); ```. https://batch.hail.is/batches/8042383. The client gets an error like this:; ```; Java stack trace:; is.hail.relocated.com.google.cloud.storage.StorageException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/1-day/o/parallelizeAndComputeWithIndex%2FO3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=%2Fresult.0?alt=media; No such object: 1-day/parallelizeAndComputeWithIndex/O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=/result.0; 	at is.hail.relocated.com.google.cloud.storage.StorageException.translate(StorageException.java:165); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:298); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.load(HttpStorageRpc.java:729); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.lambda$readAllBytes$20(StorageImpl.java:610); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:65); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1515); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:610); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:599); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:280); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:278); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:25); 	at is.hail.backend.service.ServiceBac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:903,load,load,903,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['load'],['load']
Performance,"### What happened?. We have reported to their GitHub, but we don't have a simple enough repro for them to make progress. https://github.com/Azure/azure-sdk-for-java/issues/35125. Personal correspondence with some MSFT researchers suggested there could be an issue with threading:; > It sort of reminds me of an issue we saw with Cromwell where their old akka pool code caused a bunch of unexpected network behavior that broke their API in certain cases. I've asked if BlobServiceClient is thread-safe or not. We share an object of that class, but none of the things it produces (e.g. blobs). We know that the java.io libraries can improperly drop an HTTP response if it is followed by a TCP RST. In particular, we've seen this happen when a server is load shedding and sends an HTTP ""429 Too Many Requests"" rapidly followed by a TCP RST. This might explain the ""Connection reset"" errors that we sometimes see. We have fewer intuitions about the ""Stream is already closed"". That specific error was reported to Azure in the aforementioned GitHub issue. We treat both stream is closed and connection reset as ""limited retry"" errors. We might retry too quickly. Our initial delay is `100ms * x` where `x` is drawn uniformly from `[0, 1]`. Perhaps we should try an initial delay of at least 1s? . For example, [Azure gives as an example retrying after 2s, 4s, 10s, and 30s](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-performance-checklist#timeout-and-server-busy-errors). Google's [code examples](https://cloud.google.com/storage/docs/retry-strategy#client-libraries_1) suggest an initial delay of 1s with a multiplier of 2. AWS seems to use 500ms as the [default base backoff for ""throttled"" exceptions](https://github.com/aws/aws-sdk-java/blob/master/aws-java-sdk-core/src/main/java/com/amazonaws/retry/PredefinedBackoffStrategies.java#L39). ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13351:751,load,load,751,https://hail.is,https://github.com/hail-is/hail/issues/13351,3,"['load', 'perform', 'throttle']","['load', 'performance-checklist', 'throttled']"
Performance,"### What happened?. When I try select some rows (10) of a large matrixtable and convert it into a pandas dataframe the execution fails with `ClassTooLargeException`. The problem arises after I invoke `make_table()` and try to take some rows. I expected hail to be able to handle data with dimensions 10x3202, which is not too large. Data was downloaded from the 1000 Genomes ftp site: [link](https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000G_2504_high_coverage/working/20201028_3202_phased/). ```; # load data; vcf_path = "".../1000G_b38_20201028_3202_phased/CCDG_14151_B01_GRM_WGS_2020-08-05_chr*.filtered.shapeit2-duohmm-phased.vcf.gz""; mt = hl.import_vcf(vcf_path, force_bgz=True). # select a few random variants; n_selected_variants = 10; selected_variants = np.random.choice(mt.rsid.collect(), n_selected_variants); selected_variants = hl.array(list(selected_variants)). (; mt.filter_rows(selected_variants.contains(mt.rsid)); .select_rows('rsid'); .select_entries('GT'); ).count(); ```; ```; [Stage 18:=====================================================>(255 + 1) / 256]; (10, 3202); ```. Trying to convert to pandas dataframe `.make_table().to_pandas()`, or even just taking 1 row `.make_table().take(1)` results in the following error:; ```; (; mt.filter_rows(selected_variants.contains(mt.rsid)); .select_rows('rsid'); .select_entries('GT'); ).make_table().take(1); ```; ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); Cell In[10], [line 5](vscode-notebook-cell:?execution_count=10&line=5); [1](vscode-notebook-cell:?execution_count=10&line=1) (; [2](vscode-notebook-cell:?execution_count=10&line=2) mt.filter_rows(selected_variants.contains(mt.rsid)); [3](vscode-notebook-cell:?execution_count=10&line=3) .select_rows('rsid'); [4](vscode-notebook-cell:?execution_count=10&line=4) .select_entries('GT'); ----> [5](vscode-notebook-cell:?execution_count=10&line=5) ).make_table().take(1). File ...",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14362:516,load,load,516,https://hail.is,https://github.com/hail-is/hail/issues/14362,1,['load'],['load']
Performance,"### What happened?. When a job creates a log file in excess of 2GiB the batch worker can get into a bad state as it fails to upload the log. It does not stream the log file from disk, instead loading the whole log into memory as `bytes` and tries to upload that, but asyncio ssl has a limit of max-int sized non-streaming payloads. The batch worker should stream logs from disk when uploading them. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13329:192,load,loading,192,https://hail.is,https://github.com/hail-is/hail/issues/13329,1,['load'],['loading']
Performance,"### What happened?. When a job creates a log file in excess of about half a GB, loading the job page can cause the batch front-end pod to crash as it loads the log file into memory and interpolates it directly into the job page. The front-end should instead:. - Fully stream job logs in the log endpoint; - Show a truncated view of the log in the job page, with a pointer to download the full log if it's truncated. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13328:80,load,loading,80,https://hail.is,https://github.com/hail-is/hail/issues/13328,2,['load'],"['loading', 'loads']"
Performance,"### What happened?. When the QoB client on a user's laptop sends a request to create a QoB job, it sends a `jar_spec` parameter as part of the job spec that is either:; - `git_revision`: the git SHA that the hail was built with. The Batch front end takes this and resolves a URL for the published JAR that was created when that commit was merged to `main`.; - `jar_url`: A blob storage URL that points directly to the JAR to use. The Batch front end ensures that this URL is trusted. The `jar_url` setting is mainly for development and debugging purposes, allowing a dev or user to set a URL to a development JAR instead of using a merged commit. In normal configuration fashion, it is possible to set `jar_url` in `hailctl config`. This is an enormous footgun, as users may forget to unset this configuration and continue using the dev jar *even after they install a different hail wheel*. We must do two things:; 1. Remove the ability to set the jar_url through `hailctl` so as to avoid this footgun. Batch should also fully remove support for `jar_url`s so that any users who might be inadvertently using it are loudly alerted (though I suspect there are few if any such users now).; 2. Remove entirely the ability to specify a JAR other than that which was built along with the installed wheel. The proposed plan is to always send `git_revision` for QoB jobs. In order to enable development JARs, Batch should be augmented to search first for production JARs matching a certain revision, and then if that fails search a specified `dev/` subdirectory for the requested revision. These development JARs should not be cached on workers so as to enable debugging development without constant committing. ### Version. 0.2.130. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14539:1619,cache,cached,1619,https://hail.is,https://github.com/hail-is/hail/issues/14539,1,['cache'],['cached']
Performance,"### What happened?. `hl.maximal_independent_set` should return the same independent set regardless of the ordering of the input table. gnomAD team reports that the returned set can differ depending on whether or not the input table had been written or came directly from PC-Relate. I have yet to create a simple reproducible example. Permuting the entries in this array does not change the output. I always get 'a' and 'b'. I suspect this is because what really matters is the order in which we traverse the entries of the multi map which depends on the hash of the nodes. I think a durable fix might be to eliminate the MultiMap, insert all the nodes into the binary heap, then increment priority for each edge detected. This will perform more reflows of the heap, but eliminates the non-determinism of MultiMap iteration order. ```; import hail as hl; ht = hl.Table.parallelize([; hl.Struct(i=hl.Struct(s=x[0]), j=hl.Struct(s=x[1])); for x in [('c', 'a'), ('a', 'b'), ('b', 'c'), ]; ]); hl.maximal_independent_set(ht.i, ht.j, False).collect(); ```. ### Version. 0.2.122. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13635:732,perform,perform,732,https://hail.is,https://github.com/hail-is/hail/issues/13635,1,['perform'],['perform']
Performance,"### What happened?. `query_billing_projects_with_cost` runs a `select_and_fetchall` query against the database to load information about certain billing projects. It currently locks the affected rows in share mode, but I don't believe there's any reason to do this and it can lead to deadlock errors in `monitor_billing_limits`. It is also worth noting that in `monitor_billing_limits`, we might not want to reuse this method at all, as it only needs to load rows from the database that have exceeded their billing limit. In practice currently this doesn't much matter as the number of billing projects is fairly small, but it is still not ideal. ### Version. 0.2.128. ### Relevant log output. ```shell; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 900, in retry_long_running; return await f(*args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 944, in loop; await f(*args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/batch/driver/main.py"", line 1288, in monitor_billing_limits; records = await query_billing_projects_with_cost(db); File ""/usr/local/lib/python3.9/dist-packages/batch/utils.py"", line 165, in query_billing_projects_with_cost; async for record in db.select_and_fetchall(sql, tuple(args)):; File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 339, in select_and_fetchall; async for row in tx.execute_and_fetchall(sql, args, query_name):; File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 254, in execute_and_fetchall; await cursor.execute(sql, args); File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 239, in execute; await self._query(query); File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 457, in _query; await conn.query(q); File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 469, in query; await self._read_query_result(unbuffered=unbuffered); File ""/usr/loc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14423:114,load,load,114,https://hail.is,https://github.com/hail-is/hail/issues/14423,2,['load'],['load']
Performance,"### What happened?. gnomAD went with a custom CUDA-implementation of KING instead of PC-Relate because `hl.pc_relate` was very slow on their gnomAD v4 1M sample dataset. I'm fairly certain the bottleneck is writing out a 1M by 1M dense matrix of 64-bit floating point numbers (aka the relatedness matrix). This matrix is too large. Our users only care about the small subset of entires indicating close relatedness between the samples [1]. Instead of writing a dense BlockMatrix, we should write a Hail Table with the columns `sample1`, `sample2`, and `kinship`. I have some (very old) skeleton code for this [here](https://github.com/hail-is/hail/compare/main...danking:hail:sparse-pc-relate). If I recall correctly, this only calculates the kinship coefficient, not the full set of coefficients. I'm not sure it even works currently, but it demonstrates how we can generate a BlockMatrixIR directly rather than trying to construct it using the Python-level API. ---. #### Footnotes. [1] Consider Figure 2 from [the PC-Relate paper (Conomos, et al. 2016)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4716688/). Beyond 3rd degree relatedness (i.e. avuncular pair), SNP-based relatedness (as opposed to haplotype-based) isn't reliable (see Figures 6 and 7). 3rd degree pair have have kinship 0.0625, in expectation, so only keeping entries >=0.03 is very reasonable. gnomAD is even more aggressive [considering only 2nd degree or higher pairs](https://gnomad.broadinstitute.org/news/2021-09-using-the-gnomad-ancestry-principal-components-analysis-loadings-and-random-forest-classifier-on-your-dataset/), which presumably corresponds to keeping only entries >=0.0625. ---. #### Further reading. - https://en.wikipedia.org/wiki/Coefficient_of_relationship. ### Version. 0.2.124",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13798:193,bottleneck,bottleneck,193,https://hail.is,https://github.com/hail-is/hail/issues/13798,2,"['bottleneck', 'load']","['bottleneck', 'loadings-and-random-forest-classifier-on-your-dataset']"
Performance,### What happened?. https://ci.azure.hail.is/batches/3775837/jobs/45. ```; E java.util.concurrent.ExecutionException: java.lang.RuntimeException: Stream is already closed.; E 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); E 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); E 	at is.hail.backend.service.ServiceBackend.parallelizeAndComputeWithIndex(ServiceBackend.scala:150); E 	at is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:44); E 	at __C256669Compiled.__m256730split_CollectDistributedArray(Emit.scala); E 	at __C256669Compiled.__m256689split_Let(Emit.scala); E 	at __C256669Compiled.apply(Emit.scala); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$7(CompileAndEvaluate.scala:74); E 	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:74); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:19); E 	at is.hail.expr.ir.lowering.LowerDistributedSort$.distributedSort(LowerDistributedSort.scala:163); E 	at is.hail.backend.service.ServiceBackend.lowerDistributedSort(ServiceBackend.scala:354); E 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:100); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:23); E 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$4(RewriteBottomUp.scala:26); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:36); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:87,concurren,concurrent,87,https://hail.is,https://github.com/hail-is/hail/issues/12976,3,['concurren'],['concurrent']
Performance,"### What happened?. https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/concatenate.20data.20from.20multiple.20files.20into.20table. - [ ] skip_n_rows parameter to import_table which skips the first n rows.; - [ ] add a filename field. This is all in service of loading a particular kind of single cell data format. Matrix MTX format https://broadinstitute.github.io/wot/file_formats/#:~:text=The%20MTX%20format%20is%20a,row%20and%20column%20indices%2C%20respectively. ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14072:290,load,loading,290,https://hail.is,https://github.com/hail-is/hail/issues/14072,1,['load'],['loading']
Performance,"### What happened?. https://hail.zulipchat.com/#narrow/stream/128581-Cloud-support/topic/Inefficient.20computing.20in.20AoU.20workbench. At least the first one seems to be a genuine missed optimization in Hail; ```; import hail as hl; import os; bucket = os.getenv(""WORKSPACE_BUCKET""); vds_srwgs_path = os.getenv(""WGS_VDS_PATH""); vds = hl.vds.read_vds(vds_srwgs_path); vds = hl.vds.split_multi(vds, filter_changed_loci=False); vmt = vds.variant_data; vht = vmt.rows(); vht = vht.select('filters'); vht.write(f'{bucket}/aou_vat_with_filter_wlu.ht', overwrite=True); ```; The `vmt.rows()` should have avoided all entry-level work. This should really just explode the alleles array and write that to a file. That should be relatively quick. We should be able to reproduce this on any VDS we have, and see that the IR we actually execute still references the entry data. . ### Version. 0.2.107-2387bb00ceee. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13312:189,optimiz,optimization,189,https://hail.is,https://github.com/hail-is/hail/issues/13312,1,['optimiz'],['optimization']
Performance,"### What happened?. https://hail.zulipchat.com/#narrow/stream/223457-Hail-Batch-support/topic/QoB.20Error.3A.20GoogleJsonResponseException.3A.20404.20Not.20Found/near/398355473. > I was running hl.pca on the wheel you created for me -> 0.2.124-fcaafc533ec1. and there seems to be a transient error going on https://batch.hail.is/batches/8069235?q=state+%3D+failed, not sure whether this is the same as the previous ones. I just cancelled the job before error summary appears. and here is the code I am running:. ```python3; vat_ht = hl.read_table(get_aou_util_path(name=""vat"")); vat_ht = vat_ht.collect_by_key(); meta_ht = hl.read_table(get_sample_meta_path(annotation=True)); meta_ht = meta_ht.filter(~meta_ht.related); pops = args.pops.split("","") if (args.pops is not None) else POPS; for pop in pops:; mt = get_filtered_mt(analysis_type='variant', filter_variants=True, filter_samples=False,; adj_filter=True, pop=pop); variants_to_keep = vat_ht.filter(; (vat_ht.locus.in_autosome()) &; (hl.is_snp(vat_ht.alleles[0], vat_ht.alleles[1])) &; (vat_ht['values'][f'gvs_{pop}_af'][0] >= 0.0001) &; ((vat_ht.values[f""gvs_{pop}_an""][0] >= (N_SAMPLES[pop] * 2 * MIN_CALL_RATE[pop]))); ); print('Filtering Variants...'); mt = mt.filter_rows(hl.is_defined(variants_to_keep[mt.row_key])) # filter to high quality variants; print('Filtering Samples...'); mt = mt.filter_cols(hl.is_defined(meta_ht[mt.col_key])) # filter to unrelated samples -> later to project; print('Running PCA...'); eigenvalues, scores, loadings = hl.pca(; hl.int(hl.is_defined(mt.GT)),; compute_loadings=True,; k=50,; ); print('Writing tables...'); eigenvalues.write(; get_pca_ht_path(pop=pop, name='evals'),; overwrite=args.overwrite,; ); scores.write(; get_pca_ht_path(pop=pop, name='scores'),; overwrite=args.overwrite,; ); loadings.write(; get_pca_ht_path(pop=pop, name='loadings'),; overwrite=args.overwrite,; ); ```. ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979:1498,load,loadings,1498,https://hail.is,https://github.com/hail-is/hail/issues/13979,3,['load'],['loadings']
Performance,"### readAllBytes, Java concurrency, no semaphore:; 3.97912865 s.; 502.6226030666287 result/s.p; 0.007669412278238353 MiB/s.; https://batch.hail.is/batches/7228525/jobs/1; ### readAllBytes, Java concurrency, semaphore 100; 4.931811728 s.; 405.53048459760674 result/s.; 0.0061879041228882865 MiB/s.; https://batch.hail.is/batches/7228527/jobs/1; ### readAllBytes, Scala concurrency, semaphore 100; 112.306885221 s.; 17.808347155780833 result/s.; 2.7173381280183156E-4 MiB/s.; https://batch.hail.is/batches/7228528/jobs/1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12854#issuecomment-1500393140:23,concurren,concurrency,23,https://hail.is,https://github.com/hail-is/hail/pull/12854#issuecomment-1500393140,3,['concurren'],['concurrency']
Performance,#10957 introduced loading GCP-specific configuration from the global-config instead of environment variables. The auth-driver uses this but doesn't have a global-config mounted.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10962:18,load,loading,18,https://hail.is,https://github.com/hail-is/hail/pull/10962,1,['load'],['loading']
Performance,#13331 moved the initialization of the `WatchedBranch`s out of the top level in `ci.py` and into the end of `on_startup`. Notice that this new location is both after `app['task_manager'].ensure_future(update_loop(app))` is run *and* after an `await`-point. It's quite likely that the first time `update_loop` is run is *before* there are any watched branches. This loop then sleeps for five minutes before it runs again. I believe this is why sometimes on startup it can take multiple minutes before CI loads PR information.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13581:503,load,loads,503,https://hail.is,https://github.com/hail-is/hail/pull/13581,1,['load'],['loads']
Performance,"#14056 added a new optional field to the deploy config, `base_path` which is intended to phase out `default_namespace`. But for backwards compatibility reasons we cannot yet remove `default_namespace`. This should all work fine without breaking any workflows like switching back and forth between namespaces so long as `base_path` is not explicitly set in a developer's deploy config. But `hailctl dev config set <property> <value>` does not just set a single property, it loads the deploy config, sets the property, and then writes the whole deploy config back. If the deploy config does anything with default values, which it now does with `base_path`, this round trip does not work. Another simpler example is that currently in main, the following will make two changes to a deploy config not one:. ```; # deploy config of {'location': 'external', 'domain': 'hail.is', 'default_namespace': 'default'}; HAIL_DOMAIN=foo hailctl dev config set location gce. # deploy config will now read {'location': 'gce', 'domain': 'foo', 'default_namespace': 'default'}; ```. This PR should change `hailctl dev config set` so that the only changes that are made to the deploy config are the single property/value change described in the command.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14169:473,load,loads,473,https://hail.is,https://github.com/hail-is/hail/pull/14169,1,['load'],['loads']
Performance,"#1655 addresses the linear regression case with a fixed sample set, where there is the largest-fold performance gain to be had per variant. There are other situations we should return to as they become priorities for users.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/620#issuecomment-295316093:100,perform,performance,100,https://hail.is,https://github.com/hail-is/hail/issues/620#issuecomment-295316093,1,['perform'],['performance']
Performance,#6555 #6560 and #6570 cumulatively address endpoints on latency and request count for batch.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6491#issuecomment-513327039:56,latency,latency,56,https://hail.is,https://github.com/hail-is/hail/issues/6491#issuecomment-513327039,1,['latency'],['latency']
Performance,"#9634 Introduced a large performance regression in the `linear_regression_rows_nd` benchmark (making it about 4x slower). This PR fixes that by doing two things:. 1. Move all the global into one single `annotate_globals` expression, so that CSE can work properly. This required fixing a bug in some ndarray expressions that were not correctly tracking their source tables. To make sure I was only referencing the global versions of this computed things, rather accidentally recomputing, I wrapped the global setup in a function to scope the variables. This improvement was minor, didn't hit the real root of the problem. 2. Much more significantly, and not 100% clear why: `process_y_group` is now a function that returns a python dictionary, instead of a hail struct. I can guess that the allocation required by making a struct was wasteful, but it seems crazy that it was ""make the benchmark 4x slower"" amounts of wasteful. . While this is not user facing yet, would be good to get this in before an eventual 0.2.60 release if we want to avoid benchmarks regressing between versions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9666:25,perform,performance,25,https://hail.is,https://github.com/hail-is/hail/pull/9666,1,['perform'],['performance']
Performance,#concurrency,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6863#issuecomment-521764182:1,concurren,concurrency,1,https://hail.is,https://github.com/hail-is/hail/pull/6863#issuecomment-521764182,1,['concurren'],['concurrency']
Performance,#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.019ms self 0.016ms children 0.003ms %children 16.42%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:217959,Optimiz,Optimize,217959,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:209938,Optimiz,Optimize,209938,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.30%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:201986,Optimiz,Optimize,201986,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.024ms self 0.021ms children 0.003ms %children 13.55%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:197334,Optimiz,Optimize,197334,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); ...; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8338:11964,Optimiz,Optimize,11964,https://hail.is,https://github.com/hail-is/hail/issues/8338,17,['Optimiz'],['Optimize']
Performance,$.$anonfun$main$5(ServiceBackend.scala:460); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:460); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:458); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). 2023-05-04 01:04:37.742 GoogleStorageFS$: INFO: close: gs://cpg-acute-care-hail/batch-tmp/tmp/hail/pV2Mgy4FVKSGKMwZGafyTh/dRRY6iUfFz/out; 2023-05-04 01:04:38.077 GoogleStorageFS$: INFO: closed: gs://cpg-acute-care-hail/batch-tmp/tmp/hail/pV2Mgy4FVKSGKMwZGafyTh/dRRY6iUfFz/out; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:41518,concurren,concurrent,41518,https://hail.is,https://github.com/hail-is/hail/issues/12983,6,['concurren'],['concurrent']
Performance,"$.fatal(package.scala:28); 	at is.hail.expr.ir.TextTableReader$.splitLine(TextTableReader.scala:107); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:379); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:378); 	at is.hail.utils.WithContext.map(Context.scala:33); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:378); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:408); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:385); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:589); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:587); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Expected output:; ```; +-----+-----------+; | idx | value |; +-----+-----------+; | str | str |; +-----+-----------+; | ""1"" | ""\""foo\"""" |; | ""2"" | ""\""bar\"""" |; | ""3"" | ""\""baz\"""" |; +-----+-----------+; ```. Hail version: 0.2.12-9409c0635781. [test.txt](https://github.com/hail-is/hail/files/3049195/test.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5796:2176,concurren,concurrent,2176,https://hail.is,https://github.com/hail-is/hail/issues/5796,2,['concurren'],['concurrent']
Performance,$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anon,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:12967,Load,LoadMatrix,12967,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:5645,Load,LoadMatrix,5645,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,"$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 0 total 4.506ms self 3.993ms children 0.513ms %children 11.40%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.513ms self 0.513ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.120ms self 0.120ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardRelationalLets, iteration: 0 total 0.796ms self 0.796ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.111ms self 0.111ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/PruneDeadFields, iteration: 0 total 9.943ms self 9.943ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/FoldConstants, iteration: 1 total 0.414ms self 0.414ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:4669,Optimiz,Optimize,4669,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,"$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 1 total 0.579ms self 0.273ms children 0.306ms %children 52.84%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 1/is.hail.expr.ir.NormalizeNames.apply total 0.306ms self 0.306ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.143ms self 0.143ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardRelationalLets, iteration: 1 total 0.025ms self 0.025ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.151ms self 0.151ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/PruneDeadFields, iteration: 1 total 0.714ms self 0.714ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Verify total 0.022ms self 0.022ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#exe",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:7812,Optimiz,Optimize,7812,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:12459,concurren,concurrent,12459,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['concurren'],['concurrent']
Performance,$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGSc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:5126,concurren,concurrent,5126,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['concurren'],['concurrent']
Performance,$anonfun$parse_value_ir$2(SparkBackend.scala:691); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:62); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$1(SparkBackend.scala:345); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_value_ir$1(SparkBackend.scala:690); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); 	at is.hail.backend.spark.SparkBackend.parse_value_ir(SparkBackend.scala:689); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.base/java.lang.reflect.Method.invoke(Method.java:566); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182); 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.124-b115f6a6ec23; Error summary: ClassCastException: class is.hail.types.virtual.TStruct cannot be cast to class is.hail.types.virtual.TIterable (is.hail.types.virtual.TStruct and is.hail.types.virtual.TIterable are in unnamed module of loader 'app'); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13699:8750,load,loader,8750,https://hail.is,https://github.com/hail-is/hail/issues/13699,1,['load'],['loader']
Performance,$apply$23.apply(ContextRDD.scala:308); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-5d0f74cef4f2; Error summary: MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3790:13798,concurren,concurrent,13798,https://hail.is,https://github.com/hail-is/hail/issues/3790,2,['concurren'],['concurrent']
Performance,$foldConstants$1(FoldConstants.scala:47); E 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); E 	at is.hail.expr.ir.FoldConstants$.foldConstants(FoldConstants.scala:13); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$apply$1(FoldConstants.scala:10); E 	at is.hail.backend.ExecuteContext$.$anonfun$scopedNewRegion$1(ExecuteContext.scala:86); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.backend.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:83); E 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:9); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$4(Optimize.scala:22); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$1(Optimize.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.runOpt$1(Optimize.scala:15); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$2(Optimize.scala:22); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:18); E 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:40); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:7135,Optimiz,Optimize,7135,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Optimiz'],['Optimize']
Performance,$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:19156,Load,LoadVCF,19156,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,"': 'test_tiny_driver_has_tiny_memory'}, 'billing_project': 'test', 'closed': True, 'complete': True, ...}, 'job_status': {'attributes': {'name': 'driver'}, 'batch_id': 6627669, 'billing_project': 'test', 'cost': 0.0015413897092729028, ...}, 'log': {'main': ""2022-11-15 20:30:18.004 Tokens: INFO: tokens found for namespaces {default}\n2022-11-15 20:30:18.004 tls: INFO: ssl config file found at /batch/2bbb233e4e3c4a96bbffb515019daac9/secrets/ssl-config/ssl-config.json\n2022-11-15 20:30:18.006 GoogleStorageFS$: INFO: Initializing google storage client from service account key\n2022-11-15 20:30:18.114 root: INFO: RegionPool: initialized for thread 8: pool-1-thread-1\n2022-11-15 20:30:18.114 ServiceBackend$: INFO: executing: cEPZ5IV9gUtSnCiAiHXOPs None\n2022-11-15 20:30:18.127 root: INFO: optimize optimize: darrayLowerer, initial IR: before: IR size 17: \n(Let __rng_state\n (RNGStateLiteral (0 0 0 0))\n (MakeTuple (0)\n (TableAggregate\n (TableMapRows\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (InsertFields\n (SelectFields () (SelectFields (idx) (Ref row)))\n None\n (idx (GetField idx (Ref row)))))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row)))))))))\n2022-11-15 20:30:18.146 root: INFO: optimize optimize: darrayLowerer, initial IR: after: IR size 8:\n(MakeTuple (0)\n (TableAggregate\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row))))))))\n2022-11-15 20:30:18.146 root: INFO: optimize optimize: darrayLowerer, after LowerMatrixToTable: before: IR size 8: \n(MakeTuple (0)\n (TableAggregate\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row))))))))\n2022-11-15 20:30:18.148 root: INFO: optimize optimize: darrayLowerer, after LowerMatrixToTable: after: IR size 8:\n(MakeTuple (0)\n (TableAggregate\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:1412,optimiz,optimize,1412,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,4,['optimiz'],['optimize']
Performance,"'t be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (What Jupyter hub does.). > I thought it would take less time to get a subdirectory working than figure out how to add a new domain and a cert and deal with DNS. Fair. I added a wildcard *.staging.hail.is for staging, I'll do the same thing for Hail. Then you don't need to change the DNS to add a domain, and I'll write ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:1305,scalab,scalability-updates-in-kubernetes-,1305,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659,2,['scalab'],['scalability-updates-in-kubernetes-']
Performance,(227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached pandas-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB); Collecting parsimonious==0.10.0; Using cached parsimonious-0.10.0-py3-none-any.whl (48 kB); Collecting pillow==10.0.0; Using cached Pillow-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB); Collecting plotly==5.16.1; Using cached plotly-5.16.1-py2.py3-none-any.whl (15.6 MB); C,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:36917,cache,cached,36917,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,(4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB); Collecting dill==0.3.7; Using cached dill-0.3.7-py3-none-any.whl (115 kB); Collecting frozenlist==1.4.0; Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB); Collecting google-api-core==2.11.1; Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:35604,cache,cached,35604,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,(4.8 kB); Collecting aiohttp==3.8.5; Using cached aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB); Collecting aiosignal==1.3.1; Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB); Collecting async-timeout==4.0.3; Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:33030,cache,cached,33030,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"(64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor48.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:208) ~[scala-library-2.12.15.jar:?]; 	at is.hail.io.StreamBlockInputBuffer.readBlock(InputBuffers.scala:552) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.BlockingInputBuffer.ensure(InputBuffers.scala:384) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.BlockingInputBuffer.readInt(InputBuffers.scala:409) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at __C16collect_distributed_array_table_coerce_sortedness.__m20INPLACE_DECODE_r_int32_TO_r_int32(Unknown Source) ~[?:?]; 	at __C16collect_distributed_array_table_coerce_sortedness.__m19INPLACE_DECODE_r_struct_of_r_int32ANDr_int32ANDr_binaryA",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553:3199,concurren,concurrent,3199,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553,2,['concurren'],['concurrent']
Performance,(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.jdler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGScheduler,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:23771,concurren,concurrent,23771,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['concurren'],['concurrent']
Performance,(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at org.apache.hadoop.fs.Path.initialize(Path.java:205); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHadoopFileSystemBase.java:1279); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.net.URI.checkPath(URI.java:1823); 	at java.net.URI.<init>(URI.java:745); 	at org.apache.hadoop.fs.Path.initialize(Path.java:202); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHadoopFileSystemBase.java:1279); 	at java.util.concurrent.FutureTask.run(FutureTa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:5425,concurren,concurrent,5425,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrent']
Performance,"(ExecutionTimer.scala:59); at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$parseInputToCommandThunk$3(ServiceBackend.scala:650); at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:822); at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:447); at is.hail.backend.service.Main$.main(Main.scala:15); at is.hail.backend.service.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.124-87398e1b514e; Error summary: HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht; ```; </details>. The code is simple and clearly is running against a path that does not already exist:; ```; if not hl.hadoop_exists(get_aou_util_path('mt_sample_qc')):; print('Run sample qc MT.....'); mt = hl.read_matrix_table(ACAF_MT_PATH); mt = mt.filter_rows(mt.locus.in_autosome()); # mt = mt.filter_rows(mt.locus.contig == 'chr1'); ht = hl.sample_qc(mt, name='mt_sample_qc'); ht.write(get_aou_util_path('mt_sample_qc'), overwrite=args.overwrite); ```. Job log: https://batch.hail.is/batches/8058522/jobs/171029. <details>; <summary>The last TableIR logged</summary>. ```; 2023-10-13 02:14:44.213",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13809:6701,concurren,concurrent,6701,https://hail.is,https://github.com/hail-is/hail/issues/13809,1,['concurren'],['concurrent']
Performance,"(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:244); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:190); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:188); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:193); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-6e815ac; Error summary: HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; Unhandled exception in thread started by <bound method Thread.__bootstrap of <Thread(Thread-1, stopped daemon 140486823679744)>>; Traceback (most recent call last):; File ""/usr/lib/python2.7/threading.py"", line 783, in __bootstrap; self.__bootstrap_inner(); File ""/usr/lib/python2.7/threading.py"", line 823, in __bootstrap_inner; (self.name, _format_exc()))",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2407:14765,concurren,concurrent,14765,https://hail.is,https://github.com/hail-is/hail/issues/2407,2,['concurren'],['concurrent']
Performance,(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.Ca,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217222,concurren,concurrent,217222,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,(RegionPool.scala:166); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$9(BackendUtils.scala:89); E 	at is.hail.backend.local.LocalBackend.$anonfun$parallelizeAndComputeWithIndex$4(LocalBackend.scala:150); E 	at is.hail.utils.package$.using(package.scala:673); E 	at is.hail.backend.local.LocalBackend.$anonfun$parallelizeAndComputeWithIndex$3(LocalBackend.scala:150); E 	at is.hail.utils.package$.$anonfun$runAll$2(package.scala:1038); E 	at is.hail.CancellingExecutorService.$anonfun$newTaskFor$2(package.scala:1090); E 	at is.hail.CancellingExecutorService$CancellingTask.run(package.scala:1067); E 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515); E 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264); E 	at is.hail.relocated.com.google.common.util.concurrent.MoreExecutors$DirectExecutorService.execute(MoreExecutors.java:327); E 	at is.hail.CancellingExecutorService.execute(package.scala:1111); E 	at java.base/java.util.concurrent.ExecutorCompletionService.submit(ExecutorCompletionService.java:184); E 	at is.hail.utils.package$.$anonfun$runAll$1(package.scala:1038); E 	at scala.collection.Iterator.foreach(Iterator.scala:943); E 	at scala.collection.Iterator.foreach$(Iterator.scala:943); E 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); E 	at scala.collection.IterableLike.foreach(IterableLike.scala:74); E 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73); E 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56); E 	at is.hail.utils.package$.runAll(package.scala:1038); E 	at is.hail.utils.package$.$anonfun$runAllKeepFirstError$3(package.scala:1054); E 	at is.hail.backend.local.LocalBackend.parallelizeAndComputeWithIndex(LocalBackend.scala:146); E 	at is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:85); E 	at __C22901Compiled.__m23019split_CollectDistributedArray_region3_27(Emit.scala); E 	at __C22901Compiled.__m23019split_CollectDistributedArray(Emit,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14705:4474,concurren,concurrent,4474,https://hail.is,https://github.com/hail-is/hail/issues/14705,1,['concurren'],['concurrent']
Performance,"(for context, I found my way here from `AbstractTableSpec.rowAndGlobalPTypes`, which is called once per *optimization pass*.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12086#issuecomment-1215087991:105,optimiz,optimization,105,https://hail.is,https://github.com/hail-is/hail/pull/12086#issuecomment-1215087991,1,['optimiz'],['optimization']
Performance,"(with bad performance, probably. And this would probably insert a join in the DAG if someone wants to use it to filter)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2374#issuecomment-340212405:10,perform,performance,10,https://hail.is,https://github.com/hail-is/hail/pull/2374#issuecomment-340212405,1,['perform'],['performance']
Performance,) alexkotlar:~/projects/hail-clone/notebook-api:$ k logs notebook-worker-5xq2w -f; [I 21:29:01.483 NotebookApp] Writing notebook server cookie secret to /home/jovian/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:29:03.742 NotebookApp] Serving notebooks from local directory: /home/jovian; [I 21:29:03.743 NotebookApp] The Jupyter Notebook is running at:; [I 21:29:03.743 NotebookApp] http://localhost:8888/instance/notebook-worker-service-qzppk/?token=...; [I 21:29:03.743 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [W 21:29:03.750 NotebookApp] No web browser found: could not locate runnable browser. Dan’s; [I 21:44:38.439 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:44:38.808 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1; [I 21:44:38.898 NotebookApp] Jupyter-Spark enabled!; [I 21:44:38.942 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.6/site-packages/jupyterlab; [I 21:44:38.942 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab; [I 21:44:38.945 NotebookApp] Serving notebooks from local directory: /home/jovyan; [I 21:44:38.945 NotebookApp] The Jupyter Notebook is running at:; [I 21:44:38.946 NotebookApp] http://(notebook-worker-v7fr4 or 127.0.0.1):8888/instance/notebook-worker-service-sv5jl/?token=...; [I 21:44:38.946 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [I 21:44:55.324 NotebookApp] 302 GET /instance/notebook-worker-service-sv5jl/?access_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsImtpZCI6Ik16YzNRekpFUXpWRk5VSXdPRE0yTmpJMFF6VkZPVVk1TkRZME9UZzJOa00xUkRBek1ERTJOZyJ9.eyJpc3MiOiJodHRwczovL2hhaWwuYXV0aDAuY29tLyIsInN1YiI6Imdvb2dsZS1vYXV0aDJ8MTEwNzI2NTIxOTIxMjQ5NDQzNzYwIiwiYXVkIjpbImhhaWwiLCJodHRwczovL2hhaWwuYXV0aDAuY29tL3VzZXJpbmZvIl0sImlhdCI6MTU0OTIzMDI2MSwiZXhwIjoxNTQ5MjM3NDYxLCJhenAiOiJURDc4azI,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942:1363,load,loaded,1363,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942,2,['load'],['loaded']
Performance,); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:251); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:406); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1471); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1630); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1612); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:507); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469); 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365); 	at is.hail.io.fs.HadoopFSURL.<init>(HadoopFS.scala:76); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:88); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:85); 	at is.hail.io.fs.FS.exists(FS.scala:618); 	at is.hail.io.fs.FS.exists$(FS.scala:618); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:85); 	at __C5Compiled.apply(Emit.scala); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3(LocalBackend.scala:223); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3$adapted(LocalBackend.scala:223); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:144); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:144); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$2(LocalBack,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:7424,Cache,Cache,7424,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,2,['Cache'],['Cache']
Performance,"); 	at is.hail.services.memory_client.MemoryClient.write(MemoryClient.scala:45); 	at is.hail.io.fs.ServiceCacheableFS$$anon$1.close(ServiceCacheableFS.scala:46); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); 	at is.hail.utils.package$.using(package.scala:658); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$5(ServiceBackend.scala:127); 	at is.hail.backend.service.ServiceBackend$$Lambda$2194/482268176.apply$mcV$sp(Unknown Source); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:77); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$4(ServiceBackend.scala:127); 	at is.hail.backend.service.ServiceBackend$$Lambda$2191/716305671.apply$mcV$sp(Unknown Source); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.concurrent.Future$$$Lambda$2188/1126720330.apply(Unknown Source); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.Future$$Lambda$2189/609808342.apply(Unknown Source); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.Promise$$Lambda$2190/183883584.apply(Unknown Source); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). ""pool-2-thread-1"" #26 prio=5 os_prio=0 tid=0x00007f502866e000 nid=0x88c waiting on condition [0x00007f50275fd000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:3114,concurren,concurrent,3114,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['concurren'],['concurrent']
Performance,); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:19095,Load,LoadVCF,19095,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1806:2498,concurren,concurrent,2498,https://hail.is,https://github.com/hail-is/hail/issues/1806,1,['concurren'],['concurrent']
Performance,); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB); Collecting dill==0.3.7; Using cached dill-0.3.7-py3-none-any.whl (115 kB); Collecting frozenlist==1.4.0; Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB); Collecting google-api-core==2.11.1; ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:34070,cache,cached,34070,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; U,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:36443,cache,cached,36443,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB); Collecting typer==0.9.0; Using cached typer-0.9.0-py3-none-any.whl (45 kB); Collecting typing-extensions==4.7.1; Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB); Collecting tzdata==2023.3; Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB); Collecting urllib3==1.26.16; Using cached urllib3-1.26.16-py2.py3-none-,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:39685,cache,cached,39685,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$Yarn",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:214047,concurren,concurrent,214047,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,); at is.hail.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:113); at is.hail.expr.ir.Pretty.header(Pretty.scala:405); at is.hail.expr.ir.Pretty.pretty$1(Pretty.scala:463); at is.hail.expr.ir.Pretty.$anonfun$sexprStyle$4(Pretty.scala:453); at scala.collection.Iterator$$anon$10.next(Iterator.scala:459); at scala.collection.Iterator$ConcatIterator.next(Iterator.scala:230); at is.hail.utils.richUtils.RichIterator$$anon$3.next(RichIterator.scala:67); at is.hail.utils.prettyPrint.Doc$.advance$1(PrettyPrintWriter.scala:68); at is.hail.utils.prettyPrint.Doc$.render(PrettyPrintWriter.scala:139); at is.hail.utils.prettyPrint.Doc.render(PrettyPrintWriter.scala:163); at is.hail.utils.prettyPrint.Doc.render(PrettyPrintWriter.scala:167); at is.hail.expr.ir.Pretty.sexprStyle(Pretty.scala:466); at is.hail.expr.ir.Pretty.apply(Pretty.scala:429); at is.hail.expr.ir.Pretty$.apply(Pretty.scala:22); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:45); at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:30); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); at is.hail.expr.ir.lowering.Lowering,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046:2817,Optimiz,OptimizePass,2817,https://hail.is,https://github.com/hail-is/hail/issues/13046,1,['Optimiz'],['OptimizePass']
Performance,); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). java.net.SocketException: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:17285,concurren,concurrent,17285,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['concurren'],['concurrent']
Performance,); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.net.SocketException: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:7987,concurren,concurrent,7987,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['concurren'],['concurrent']
Performance,); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217064,concurren,concurrent,217064,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,")</li>; </ul>; <h3>Configuration</h3>; <ul>; <li>Do not format <code>__pypackages__</code> directories by default (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2836"">#2836</a>)</li>; <li>Add support for specifying stable version with <code>--required-version</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2832"">#2832</a>).</li>; <li>Avoid crashing when the user has no homedir (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2814"">#2814</a>)</li>; <li>Avoid crashing when md5 is not available (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2905"">#2905</a>)</li>; <li>Fix handling of directory junctions on Windows (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2904"">#2904</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Update pylint config documentation (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2931"">#2931</a>)</li>; </ul>; <h3>Integrations</h3>; <ul>; <li>Move test to disable plugin in Vim/Neovim, which speeds up loading (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2896"">#2896</a>)</li>; </ul>; <h3>Output</h3>; <ul>; <li>In verbose, mode, log when <em>Black</em> is using user-level config (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2861"">#2861</a>)</li>; </ul>; <h3>Packaging</h3>; <ul>; <li>Fix Black to work with Click 8.1.0 (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2966"">#2966</a>)</li>; <li>On Python 3.11 and newer, use the standard library's <code>tomllib</code> instead of <code>tomli</code>; (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2903"">#2903</a>)</li>; <li><code>black-primer</code>, the deprecated internal devtool, has been removed and copied to a; <a href=""https://github.com/cooperlees/black-primer"">separate repository</a> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2924"">#2924</a>)</li>; </ul>; <h3>Parser</h3>; <ul>; <li>Blac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11696:1970,load,loading,1970,https://hail.is,https://github.com/hail-is/hail/pull/11696,2,['load'],['loading']
Performance,"* Add a lightweight DSL for writing IR in Scala, which made the lowerings much easier to write, and read. It is implemented in `IRBuilder`, and can be used by importing `IRBuilder._` into scope. It's not complete, and I want to make it eagerly typecheck eventually, but we can build on it.; * Make `execute` protected on `MatrixIR` and `TableIR`, making `Interpret` the official place to execute IR.; * Add a compiler pass lowering some `MatrixIR` to `TableIR`. The `Interpret` gateway to `execute` always lowers, so we can safely remove the execute methods of IR nodes which are rewritten by the lowering.; * Fix `LoadBgen` to not create entries arrays when `dropCols` is true.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4707:615,Load,LoadBgen,615,https://hail.is,https://github.com/hail-is/hail/pull/4707,1,['Load'],['LoadBgen']
Performance,"* add --metric argument to choose between best, median, ...; * add --min-time option to set minimum time for benchmark inclusion.; This lets us write benchmarks for things we expect to be fast, but where there's enough variance we don't really think the exact number is very meaningful in the current benchmarking system. They will still catch performance regressions, though!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7105:344,perform,performance,344,https://hail.is,https://github.com/hail-is/hail/pull/7105,1,['perform'],['performance']
Performance,* all the caches!. * two caches and a fix. * space between methods. * cache comments,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2370:10,cache,caches,10,https://hail.is,https://github.com/hail-is/hail/pull/2370,3,['cache'],"['cache', 'caches']"
Performance,* don't use inefficient unapply; * Cache the returnType of AggSignatures so they persist through copies. ```; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_big_aggregate_compilation 85.6% 4.113 3.519; ----------------------; Geometric mean: 85.6%; Simple mean: 85.6%; Median: 85.6%; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7355:35,Cache,Cache,35,https://hail.is,https://github.com/hail-is/hail/pull/7355,1,['Cache'],['Cache']
Performance,"* wip: general VSM keys. cleanup. more cleanup. Complete. Addressed comments. Passes JVM tests with generalized row keys. Needs cleanup, work on pythons side. Minor cleanup. Cleanup around OrderedRDD. More minor cleanup. impex cleanup. VSMSubgen cleanup. Formatting. Passing Scala and Python tests. Fixed minor compile error in test. Minor fix. Addressed first round of comments. * Removed generic genotype. Support null genotypes in GenotypeStream. filterGenotypes just sets; filtered cells (genotypes) to null. This is a file-format breaking; change. Bumped the VDS file version, removed backwards compatability; tests. * Nuked generic from python. * More python cleanup, fixed tests. * Fixed tests. * Genotype can be null. All tests pass. * Fixed python tests. * Cleanup. * Fixed docs test. * Another doc test fix. * Fixed doc tests. * Added missing file. * Addressed comments. Added back SampleQC optimizations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2039:901,optimiz,optimizations,901,https://hail.is,https://github.com/hail-is/hail/pull/2039,1,['optimiz'],['optimizations']
Performance,* works?. * address PR comments. * optimize imports. * use existing interfaces. * fix imports. * fix syntax error. * remove bad import. * add LDMatrix import. * fix tests for ldmatrix. * fix tests. * clean paths before writing,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2124:35,optimiz,optimize,35,https://hail.is,https://github.com/hail-is/hail/pull/2124,1,['optimiz'],['optimize']
Performance,"*********** 2. row ***************************; id: 1; select_type: SIMPLE; table: attempts; partitions: NULL; type: ref; possible_keys: PRIMARY,attempts_instance_name; key: attempts_instance_name; key_len: 303; ref: batch.instances.name; rows: 91; filtered: 9.00; Extra: Using where; *************************** 3. row ***************************; id: 1; select_type: SIMPLE; table: jobs; partitions: NULL; type: eq_ref; possible_keys: PRIMARY,jobs_batch_id_state_always_run_cancelled,jobs_batch_id_state_always_run_inst_coll_cancelled,jobs_batch_id_update_id,jobs_batch_id_always_run_n_regions_regions_bits_rep_job_id,jobs_batch_id_ic_state_ar_n_regions_bits_rep_job_id,jobs_batch_id_job_group_id,jobs_batch_id_ic_state_ar_n_regions_bits_rep_job_group_id; key: PRIMARY; key_len: 12; ref: batch.attempts.batch_id,batch.attempts.job_id; rows: 1; filtered: 98.10; Extra: Using where; 3 rows in set, 1 warning (0.00 sec); ```. This is not great:; ```; rows: 1150201; filtered: 10.00; Extra: Using where; Using temporary; Using filesort; ```; what we want to see is a low number of rows, a high percent filtered, and something like `Extra: Using where; Using index;`. 3. We can then verify that this finding aligns with our current understanding of the database schema where there is no index on `instances.state`: ; https://github.com/hail-is/hail/blob/1f3a0503926b65f479dce6d5eb105236632f8d07/batch/sql/estimated-current.sql#L113-L138. ### Remaining questions; 1. Why do we need to join against the instances table to find orphaned attempts? Can we not?; 2. If we do need to join against the instances table, should we create an index on `instances.state`? How does `instances.removed` relate to this use case since it seems relevant and already has an index?. ### Deliverable; This query should perform indexed lookups of involved tables. The PR should compare the present and proposed EXPLAINs; and provide some manual timing comparisons. ### Version. 0.2.130. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14460:3962,perform,perform,3962,https://hail.is,https://github.com/hail-is/hail/issues/14460,1,['perform'],['perform']
Performance,"**Work in progress**; _worried the merging and rebasing was done poorly due to the message about merge conflicts at the bottom of this PR - would be useful if someone could let me know or walk me through resolving it_. **Additions:**. - adds method for Blanczos SVD, not yet following the exact interface of the current PCA call; - adds test for Blanczos SVD method; - adds multiple jupyter notebooks where this algorithm was implemented; - adds first version of benchmarking script; - update to requirements.txt regarding gcsfs version should probably be moved to separate PR. **Needs:**; - larger benchmarking; - better test; - hail method for Blanczos PCA to use exact interface and return eigenvalues, scores, and optional loadings as if it were the hail PCA method instead of the current non-centered SVD; - fix the norm(A - QQtA) computation - maybe make it blocked; - possibly block the error bound computation; - possibly replace the numpy library calls to SVD and QR decomposition with distributed hail versions, or at least the SVD call at a minimum since it is easier",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9222:727,load,loadings,727,https://hail.is,https://github.com/hail-is/hail/pull/9222,1,['load'],['loadings']
Performance,"**self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-0_0 (139802205742848) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~~~~~~~~ Stack of asyncio_0 (139802248206080) ~~~~~~~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++. ------------------------------ live log teardown -------------------------------; INFO hailtop.utils:utils.py:450 discarding exception; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 378, in rm_dir; await self.rmdir(path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 352, in rmdir; return await blocking_to_async(self._thread_pool, os.rmdir, path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 162, in blocking_to_async; return await asyncio.get_event_loop().run_in_executor(; File ""/usr/lib/python3.9/asyncio/futures.py"", line 284, in __await__; yield self # This tells Task to wait for completion.; File ""/usr/lib/python3.9/asyncio/tasks.py"", line 328, in __wakeup; future.result(); File ""/usr/lib/p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:2436,concurren,concurrent,2436,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['concurren'],['concurrent']
Performance,"**self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-1_0 (139802091452160) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-0_0 (139802205742848) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~~~~~~~~ Stack of asyncio_0 (139802248206080) ~~~~~~~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++. ------------------------------ live log teardown -------------------------------; INFO hailtop.utils:utils.py:450 discarding exception; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 378, in rm_dir; await self.rmdir(path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:1963,concurren,concurrent,1963,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['concurren'],['concurrent']
Performance,*sigh*. ```; E java.lang.RuntimeException: Stream is already closed.; E 	at com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:79); E 	at com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:89); E 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.$anonfun$call$1(ServiceBackend.scala:122); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); ```. Azure's `StorageOutputStream.close` method is not idempotent in the version that we use. It has been made idempotent in `12.18.0`. I would be surprised if spark let us upgrade to a version that recent,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12968#issuecomment-1532328901:1279,concurren,concurrent,1279,https://hail.is,https://github.com/hail-is/hail/pull/12968#issuecomment-1532328901,3,['concurren'],['concurrent']
Performance,"+ reference_data = reference_data.drop('END'); + else: # if END is missing, add it, _add_end is a no-op if END is already present; + reference_data = VariantDataset._add_end(reference_data); +; vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; ```. There was nothing in the IR that stood out when I examined it, but I will admit that I'm not the best at digging into it. ### Version. https://github.com/chrisvittal/hail/tree/vds/repro-example. ### Relevant log output. ```shell; E hail.utils.java.FatalError: RuntimeException: invalid memory access: 140a68008/00000001: not in 140a58008/00010000; E; E Java stack trace:; E java.lang.RuntimeException: invalid memory access: 140a68008/00000001: not in 140a58008/00010000; E 	at is.hail.annotations.Memory.checkAddress(Memory.java:226); E 	at is.hail.annotations.Memory.loadByte(Memory.java:130); E 	at is.hail.annotations.Region$.loadByte(Region.scala:28); E 	at is.hail.annotations.Region$.loadBit(Region.scala:86); E 	at __C23148collect_distributed_array_matrix_native_writer.__m23333split_ToArray(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply_region478_486(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply_region16_503(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply_region14_529(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply(Unknown Source); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$10(BackendUtils.scala:90); E 	at is.hail.utils.package$.using(package.scala:673); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$9(BackendUtils.scala:89); E 	at is.hail.backend.local.LocalBackend.$anonfun$parallelizeAnd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14705:2643,load,loadByte,2643,https://hail.is,https://github.com/hail-is/hail/issues/14705,1,['load'],['loadByte']
Performance,"++++ Timeout ++++++++++++++++++++++++++++++++++++. ------------------------------ live log teardown -------------------------------; INFO hailtop.utils:utils.py:450 discarding exception; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 378, in rm_dir; await self.rmdir(path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 352, in rmdir; return await blocking_to_async(self._thread_pool, os.rmdir, path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 162, in blocking_to_async; return await asyncio.get_event_loop().run_in_executor(; File ""/usr/lib/python3.9/asyncio/futures.py"", line 284, in __await__; yield self # This tells Task to wait for completion.; File ""/usr/lib/python3.9/asyncio/tasks.py"", line 328, in __wakeup; future.result(); File ""/usr/lib/python3.9/asyncio/futures.py"", line 201, in result; raise self._exception; File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 58, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 163, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); OSError: [Errno 39] Directory not empty: '/tmp/JnQ2m'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 409, in rmtree; await rm_dir(pool, contents_tasks_by_dir.get(path, []), path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 387, in rm_dir; excs = [exc; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 389, in <listcomp>; for exc in [t.exception()]; File ""/usr/lib/python3.9/asyncio/futures.py"", line 214, in exception; raise exc; asyncio.exceptions.CancelledError. [2023-08-02 05:33:14] test/hail/utils/test_hl_hadoop_and_hail_fs.py::test_hadoop_methods_3[local] ERROR; [2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:3547,concurren,concurrent,3547,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['concurren'],['concurrent']
Performance,", returns srcAddress, skipping construction. # <a name=""parray""></a> PArray. An abstract class for immutable ordered collections where all elements are of a single type. Does not contain the value constructor (e.g allocate). ## Core Methods. ```scala; def allocate(region: Region, length: Int): Long = ...; def allocate(region: Code[Region], length: Code[Int]): Code[Long] = ...; ```. - Allocate the memory needed for an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: Code[Long], length: Code[Int], setMissing: Boolean = false): Code[Unit] = ...; ```. - Initialize an allocated array by setting its elements to present or missing. ```scala; def isElementMissing(arrayAddress: Long, elementIndex: Int): Boolean= ...; def isElementMissing(arrayAddress: Long, elementIndex: Code[Int]): Code[Boolean] = ...; ```. - Does the element at the given index exist. ```scala; def loadLength(arrayAddress: Long): Int = ...; def loadLength(arrayAddress: Code[Long]): Code[Int] = ...; ```. - Gets the array length, will not exceed 2^31. ```scala; def loadElement(arrayAddress: Long, elementIndex: Int): Long = ...; def loadElement(arrayAddress: Code[Long], elementIndex: Code[Int]): Code[Long] = ...; ```. - Gets the address of the element at the given index.; - For pointer types loads the address at the offset into arrayAddress, otherwise returns that address. ## <a name=""parray""></a> PCanonicalArray. A growable array that is accessed by a pointer. ### Structure. Starting at `arrayAddress`:. [`4-byte length`, `n/8 byte missigness data`, `n * elementByteSize byte element data`]. # <a name=""parray""></a> PSet. An abstract class for immutable ordered collections where all elements are unique. ## Core Methods. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalSet. A PCanonicalArray-backed implementation of PSet. # ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:4805,load,loadLength,4805,https://hail.is,https://github.com/hail-is/hail/issues/7988,1,['load'],['loadLength']
Performance,",; 'pc1': hl.tfloat,; 'pc2': hl.tfloat,; 'pc3': hl.tfloat,; 'pc4': hl.tfloat,; 'pc5': hl.tfloat,; 'pc6': hl.tfloat,; 'pc7': hl.tfloat,; 'pc8': hl.tfloat,; 'pc9': hl.tfloat,; 'pc10': hl.tfloat}).key_by('vcfID')). mt=hl.read_matrix_table(mt_ld_fn); print(mt.count()); print(""running omit filter""); mt=mt.annotate_cols(pheno = table[mt.s]); mt = mt.filter_cols(mt.pheno.omit == 0); print(mt.count()); print(""running pc_relate""); pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); pairs = pc_rel.filter(pc_rel['kin'] >= 0.0883); print(""finding Max ind set""); related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j,keep=False); print(""writing related samples to remove""); related_samples_to_remove.export(""file:////restricted/projectnb/adgc/topmed.r2.analysis/pc_relate/related_samples_""+pop+"".tsv""); result = mt.filter_cols(; hl.is_defined(related_samples_to_remove[mt.col_key]), keep=False); print(""final unrelated count""); print(result.count); eigenvalues, scores, loadings = hl.hwe_normalized_pca(result.GT, k=10); scores.export('file:////restricted/projectnb/adgc/topmed.r2.analysis/pc_relate/unrelated_pcs_'+pop+'.tsv'). ```. ```; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to logs/adgc_pc_relate.autosome_all.log; 2020-08-20 10:14:00 Hail: INFO: Reading table to impute column types; [Stage 0:===========================================================(1 + 0) / 1]2020-08-20 10:14:07 Hail: INFO: Loading 76 fields. Counts by type:; 66 fields: user-specified int32; 6 fields: user-specified str; 3 fields: user-specified float64; 1 field: imputed int32; (63110, 64048); running omit filter; [Stage 1:> (0 + 1) / 1](63110, 52877); running pc_relate; [Stage 3:==================================================>(12794 + 2) / 12796]2020-08-20 10:14:38 Hail: INFO: hwe_normalized_pca: runni",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:3887,load,loadings,3887,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['load'],['loadings']
Performance,- Added convenience method to export each block of a BlockMatrix into its own file using `export_rectangles`; - Added static method to load rectangle files into a NumPy NDArray,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5516:135,load,load,135,https://hail.is,https://github.com/hail-is/hail/pull/5516,1,['load'],['load']
Performance,"- All subprocess calls are async. The UI is much more responsive now.; - Make refresh (rather than heal) non-reentrant. There was a race condition where we could update the Github state of a PR we were actively deploying. This seemed error prone.; - We need to lock the repos until the build is fully deployed. I now protect pr.heal.; - Set `batch_changed = True` whenever heal needs to be rerun becomes some of its inputs (build_state, source or target sha, collection of PRs) changed. Next up: deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5921:132,race condition,race condition,132,https://hail.is,https://github.com/hail-is/hail/pull/5921,1,['race condition'],['race condition']
Performance,"- Create a cache that stores an instance's token which can be looked up by the instance's name; - Use this cache in the active_instances_only decorator to avoid making DB request on every invocation; - Add monitoring of caches' hits, misses, evictions, and load latencies",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11346:11,cache,cache,11,https://hail.is,https://github.com/hail-is/hail/pull/11346,4,"['cache', 'load']","['cache', 'caches', 'load']"
Performance,"- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:1867,Optimiz,Optimize,1867,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance,"- Get shape for an `NDArrayExpression`. While this should (and in some cases does) **not** actually perform computations like element wise maps or matmul's, it doesn't in all cases because not everything is worked into being deforested. I'm going to take a closer look at deforesting/shape calculation and tie up loose ends, but there are some subtleties and I think this is a good chunk.; - Changed IR nodes that are concerned with shape (`MakeNDArray`, `NDArrayReshape`) take shapes as hail tuples (instead of Scala sequences of IR, or IR with another parameter for length). The only thing Scala sequences were really useful for were statically knowing the number of dimensions which we can get with tuples.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6077:100,perform,perform,100,https://hail.is,https://github.com/hail-is/hail/pull/6077,1,['perform'],['perform']
Performance,"- I left in pvc_size for backwards compatibility. We can rip it out at some point.; - I'm not happy with how the batch_worker_image seems slower now with the gsutil addition. Not sure if there's a better solution here. I tested a lot of this by hand on dev deploy. For example, making sure the flocks were right, the project quotas were right, the cache was working, the garbage collector, and the backwards compatibility with pvc_size was working. Let me know if you think there are other things I should test. If this is too much, I can think about splitting the storage and the cache into two PRs. I just figured since I almost had it all done together to push on that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9074:348,cache,cache,348,https://hail.is,https://github.com/hail-is/hail/pull/9074,2,['cache'],['cache']
Performance,"- Most of the code is actually the same, but I was intentionally not deforesting until now to get some benchmarks.; - Basically all you need to do is compose the `outputElement`s of your children (the body of the loops) and compute what the ultimate bounds (shape) of the nested loops should be.; - For Reindex, we statically reorder the loop variables used to index into the NDArray instead of permuting the shape/strides at runtime. Not a huge performance improvement but in the broadcasting case (adding dimensions) it's at least fewer multiplications at runtime in the loop body to compute the index.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6026:446,perform,performance,446,https://hail.is,https://github.com/hail-is/hail/pull/6026,1,['perform'],['performance']
Performance,"- On *deploys*, makes sure that whatever is in our third-party images is in our private registry before starting builds like hail-ubuntu that might depend on those images. This means that we can update our ubuntu base image without the australians needing to deploy any images by hand. However, this does not run in PRs because I 1) didn't want to add that kind of latency for PRs and 2) we don't do any kind of namespacing for our images so if we did include this for a PR that ultimately wasn't merged we would have to manually remove the image anyway so why not manually add it if you're going to PR it… I think point 2 is a little weak but I recall this being what we agreed on a couple months back when we discussed this. I'm wondering if we should just eat the minute or so latency at the beginning of PRs to be safe but it also feels like a shame for something that changes so infrequently. . - Again on deploys, upload the hailgenetics/* images to the private registry if they don't already exist there. This way any deployments that aren't hail team's GCP deployment can get these images automatically when they deploy a new SHA instead of uploading them manually. It won't backfill skipped versions, but we decided that was ok. This seems less relevant for testing on PRs as it will get triggered on releases and we can easily dev deploy to rectify the image if this breaks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12818:365,latency,latency,365,https://hail.is,https://github.com/hail-is/hail/pull/12818,2,['latency'],['latency']
Performance,- Refactored `exportRectangles` on BlockMatrix from a static function with an input and output file to an instance function with an output file that writes the instance.; - Added `BlockMatrixRectanglesWriter` so exporting rectangles happens through the IR.; - Updated tests and deleted parts of tests that dealt with invalid inputs that aren't applicable when exporting from an already loaded BlockMatrix.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5478:386,load,loaded,386,https://hail.is,https://github.com/hail-is/hail/pull/5478,1,['load'],['loaded']
Performance,"- Removed `export_samples`, `export_variants`, `export_genotypes`. To perform same functionality, need to convert to KeyTable and then use `select` and `export`. - Changed Python interface for `select`, `drop`, and `key_by` to take varargs rather than a String or List of String. - Select is not backwards compatible with 0.1. To select a column name with periods in it, must use backticks now. - Not certain whether left hand side of named expression should be treated as an identifier or an annotationIdentifier. Right now, it's treated as an identifier. If it's an identifier, than `A.B = 5` will have a signature of `(""A.B"", TInt)`. If it's an annotationIdentifier path, than the signature would be `(""A"", TStruct((""B"", TInt)))`. Thoughts?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2279:70,perform,perform,70,https://hail.is,https://github.com/hail-is/hail/pull/2279,1,['perform'],['perform']
Performance,"- Signup page with web socket and spinner while waiting for account to create; - Upon account creation, a billing project named `{username}-trial` is created with $10 limit and a user `{username}`; - When deleting an account, the billing project is reopened if it's closed, then remove the user, and finally close the billing project. This behavior might be debatable. We may not need to remove the user from the billing project. I think it's better to be safe in case the billing project is reopened. Auth-driver is implicitly dependent on the batch front end, but this dependency isn't stated in build.yaml. An event queue is a future solution. I can get rid of my personal email being whitelisted, but I think it will be useful if we need to debug later and for a possible demo on Monday (although I'll probably just use some screenshots). If you want to test it in your namespace, make sure to comment out all the create and delete steps that are not related to billing projects.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9658:619,queue,queue,619,https://hail.is,https://github.com/hail-is/hail/pull/9658,1,['queue'],['queue']
Performance,"- Time execution stages in the backend (optimization, compilation, runtime); - Return a dictionary back to the front end of nanoseconds and formatted times for each stage.; - Add `hl.eval_timed` which propagates returns the evaluated IR as well as the timings dictionary. If called with `timed=False` (default), `Env.backend.execute()` drops the timings it received from the backend. Could change it to log them instead or always.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5857:40,optimiz,optimization,40,https://hail.is,https://github.com/hail-is/hail/pull/5857,1,['optimiz'],['optimization']
Performance,"- [ ] (@tpoterba) caf1e1e673 add fails_service_backend; - [ ] (@tpoterba, @cseed) a979dfba58 [hail] introduce and use mktemp and mktempd; - [ ] (@tpoterba) dcf026b01c [hail] make is.hail.expr.ir.functions threadsafe; - [ ] (@tpoterba) 807f38c20e [hail] fix use of row requiredness in lowerDistributedSort; - [ ] (@catoverdrive) 12df8eb456 [query-service] handle void-typed IRs in query-service; - [ ] (@catoverdrive) 03357ee83d [query-service] make user cache thread-safe; - [ ] (@tpoterba) 6c6734bc71 [query-service] bugfix: preserve globals through a shuffle; - [ ] (@catoverdrive) a3d2572ce7 [shuffler] log ShuffleCodecSpec anytime it is created; - [ ] (@daniel-goldstein) 8949dfec3c [scala-lsm] bugfix: least key may equal greatest key; - [ ] (@daniel-goldstein) 6067bd8e51 [services] discovered new transient error; - [ ] (@daniel-goldstein) c8356d30bb [shuffler] more assertions in ShuffleClient; - [ ] (@daniel-goldstein) 9991da90f0 [shuffler] bugfix: shuffler needs a HailContext to decode loci; - [ ] (@daniel-goldstein) bc0140ab6f [query-service] move hail.jar earlier in Dockerfile; - [ ] (@daniel-goldstein) f96c28174d [query-service] permit pod scaling and remove cpu limit; - [ ] (@catoverdrive) 6ae26339fe [query-service] simplify socket handling; - [ ] (@jigold) f3db30e23f [batch] teach JVMJob where to find the hail configuration files; - [ ] (@daniel-goldstein) b5c6d85554 [query-service] switch to services team approved logging; - [ ] (@tpoterba) 35a306c066 [query-service] query workers need a hail context; - [ ] (@daniel-goldstein, @catoverdrive) 051c89b8e7 [query-service] use a UNIX Domain Socket for Py-Scala communication; - [ ] (@daniel-goldstein, @catoverdrive) ad9ea73d7a [query-service] run tests against query service",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10072:454,cache,cache,454,https://hail.is,https://github.com/hail-is/hail/pull/10072,1,['cache'],['cache']
Performance,- [ ] attributes PR (per-batch attributes); - [ ] per-job attributes; - [ ] use queue and concurrent worker pool for all k8s communication; - [ ] batch avoids scheduling more than 150k pods,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6493:80,queue,queue,80,https://hail.is,https://github.com/hail-is/hail/issues/6493,2,"['concurren', 'queue']","['concurrent', 'queue']"
Performance,"- [x] Remove TSample (it's just string); - [x] Remove count, or rework it to return a tuple; - [ ] Change concordance to use TDict; - [x] Filter_samples_list should take a list; - [x] TextTableConfig goes away; - [x] Annotate_samples_table and annotate_variants_table go away; - [x] Import_annotations_table goes away; - [x] KeyTable to VariantDataset conversion; - [ ] Precompiled binaries (!!!!!!!); - [ ] Fix log output to jupyter notebooks; - [ ] add all aggregator functions to sets and arrays (or fix aggregable scope issue); - [x] add python file-like objects so people can write to cloud file systems / HDFS; - [ ] ~~improved performance on python object conversion (or lazy evaluation at the least)~~ back compatible; - [x] annotate_samples_fam goes away; - [x] annotate VDS with interval keytable; - [x] read/write keytables to parquet; - [ ] rename logreg/ linreg / lmmreg to be more descriptive; - [x] no methods take a file; - [x] first-class object for Pedigree in python; - [x] make annotation-of-counts behavior consistent across the regression methods; - [x] make linreg / logreg / lmmreg consistent on whether they output count annotations. If anybody has other tasks, edit this post to add them here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1505:634,perform,performance,634,https://hail.is,https://github.com/hail-is/hail/issues/1505,1,['perform'],['performance']
Performance,"- [x] reduce number of database calls when creating a pod; - [x] put the source jobs of a batch on a queue which has 16-concurrent workers sending create_pod requests to k8s; - [x] modify the server API to /create, /create_jobs *, /close (prevents holding 12M jobs in memory on batch server); - [x] record pod status JSON in database for debugging purposes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6490:101,queue,queue,101,https://hail.is,https://github.com/hail-is/hail/issues/6490,2,"['concurren', 'queue']","['concurrent', 'queue']"
Performance,"- [x] set resource requests on input and output pods (https://github.com/hail-is/hail/pull/6507); - [x] request metrics (req/s, latency) for Hail services (at least batch) in Prometheus/Grafana; - [x] for both REST and web endpoints; - [ ] metrics for DB requests in batch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6491:128,latency,latency,128,https://hail.is,https://github.com/hail-is/hail/issues/6491,1,['latency'],['latency']
Performance,"- `TContainer.loadLength` is now on the object so creating byte code to; read the length does not require knowledge of the element type. - in `elementsOffset`, use the non-Code pattern of delegating to; `roundUpAlignment`. - add staged `TContainer.elementOffset`, `TContainer.loadElement`,; `TStruct.fieldOffset`, `UnsafeUtils.roundUpAlignment`. - fix bug: loadElement reads addresses using `loadAddress` instead of; `loadInt`. - add several `loadX` methods to `RichCodeMemoryBuffer`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2347:14,load,loadLength,14,https://hail.is,https://github.com/hail-is/hail/pull/2347,6,['load'],"['loadAddress', 'loadElement', 'loadInt', 'loadLength', 'loadX']"
Performance,"- automatically download catch.hpp from GitHub if it's missing; - remove builtin rules and suffix based rules to improve performance and ease debugging; - change Makefile variable definitions to match our standard `FOO OP VAL` (note: two spaces); - rely on `clang -MM` (see: [Clang command line reference](https://clang.llvm.org/docs/ClangCommandLineReference.html) and below explanation) to generate precise dependencies for object files (including source and header files); - explicitly specify which files depend on `NUMBER_OF_GENOTYPES_PER_ROW` to be set (namely the dependency file and the object file associated with `ibs.cpp`); - break `TEST_OBJECTS` into two steps so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5331:121,perform,performance,121,https://hail.is,https://github.com/hail-is/hail/pull/5331,1,['perform'],['performance']
Performance,"- create KeyTable from DataFrame; - to make above useful, we need a way to ""contract"" native types (opposite of expand_types), that, convert Struct with the appropriate fields to Variant, etc. One alternative is to use SparkAnnotationImpex and have the user specify the Hail type. (Also means we need a way to build Hail types in python.); - annotate variants or samples with keytable; - load fam file as keytable; - remove annotevariants table, vcf, vds and annotatesamples fam, table, list, vds. They can all be implemented with annotate with keytable.; - same with filter list",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1158:388,load,load,388,https://hail.is,https://github.com/hail-is/hail/issues/1158,1,['load'],['load']
Performance,- emit streams for `If` and `ReadPartition` IR. ; - delete `Emit.emitOldArrayIterator()` because all streamable IR's can be emitted with the new interface. before this is merged we probably want to make sure there are no significant performance regressions or anything of that sort,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7736:233,perform,performance,233,https://hail.is,https://github.com/hail-is/hail/pull/7736,1,['perform'],['performance']
Performance,- long running jobs are causing too much latency for interactive stuff (e.g. CI); - overscheduling core calculation wasn't behaving how we wanted,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7981:41,latency,latency,41,https://hail.is,https://github.com/hail-is/hail/pull/7981,1,['latency'],['latency']
Performance,"- put_on_ready should never be in an ensure_future, but I had to keep it there when used in Pod.create() because I didn't want to block the create/delete pool waiting for the pods to be on the ready queue; otherwise, it should be fixed everywhere else. - I added `Binds: None` in the Docker config as the default because if we specify a HostConfig, then I believe Docker uses a default of {} which might be allocating a volume unnecessarily. I can't find where I read that before. I can double check the performance of the change if you want. I'm pretty sure it helps.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7347:199,queue,queue,199,https://hail.is,https://github.com/hail-is/hail/pull/7347,2,"['perform', 'queue']","['performance', 'queue']"
Performance,"- set up tests that evaluate performance tradeoffs; - build interface for controlling compression levels in import / write; - also test parquet LZ4 / snappy on write with Mitja pipeline, with size",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/293:29,perform,performance,29,https://hail.is,https://github.com/hail-is/hail/issues/293,1,['perform'],['performance']
Performance,"- starting batch; (nginx timed out, but we got 30k submitted); Should multiplex client sending requests to the server. - close batch should start running all pods, don't create pods before batch has been closed. - wait should not list jobs (separate end point?). - add log statement if we delete a pod. - figure out discrepancy between kubernetes and batch on how many pods have been completed / created. - why were logs accessed twice? Both batch or is one of those fluentd?. - race condition still between accessing the log and deleting the pod?. - See whether our containers are being garbage collected: ; https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/; https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6566#issuecomment-508267674:479,race condition,race condition,479,https://hail.is,https://github.com/hail-is/hail/issues/6566#issuecomment-508267674,1,['race condition'],['race condition']
Performance,"- store globals, cols out of line, don't load when loading metadata; - removed MatrixLocalValue (also Table), just store in MatrixValue; - don't require vds, kt extensions; - /path/to/ds/cols is now a valid table fine format; - important bug fix: read_table was completely broken",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2854:41,load,load,41,https://hail.is,https://github.com/hail-is/hail/pull/2854,2,['load'],"['load', 'loading']"
Performance,"-- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 3.975ms, total 407.299ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- LoweringTransformation : 77.664ms, total 485.244ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 1.780ms, total 487.281ms, tagged coverage 0.0; ...; ...; ...; 2019-11-06 18:44:11 root: INFO: Timer: aggregate:; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'Verify' (8): 8.109ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'ConvertToSafeValue' (2): 12.828ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'InitializeCompiledFunction' (2): 14.536ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'ForwardRelationalLets' (7): 20.388ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'FoldConst",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:2893,Optimiz,Optimize,2893,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance,"--------- Captured log setup ------------------------------; INFO hailtop.aiocloud.aiogoogle.credentials:credentials.py:89 using credentials file /test-gsa-key/key.json: GoogleServiceAccountCredentials for test-665@hail-vdc.iam.gserviceaccount.com; ------------------------------ Captured log call -------------------------------; INFO hailtop.aiocloud.aiogoogle.credentials:credentials.py:89 using credentials file /test-gsa-key/key.json: GoogleServiceAccountCredentials for test-665@hail-vdc.iam.gserviceaccount.com; WARNING hailtop.aiocloud.aiogoogle.client.storage_client:storage_client.py:225 resumable upload chunk PUT request finished before writing data; WARNING hailtop.aiocloud.aiogoogle.client.storage_client:storage_client.py:117 dropping preempted task exception; Traceback (most recent call last):; File ""/usr/lib/python3.9/asyncio/tasks.py"", line 258, in __step; result = coro.throw(exc); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/utils.py"", line 30, in feed; await self._queue.put(next); File ""/usr/lib/python3.9/asyncio/queues.py"", line 128, in put; await putter; File ""/usr/lib/python3.9/asyncio/futures.py"", line 284, in __await__; yield self # This tells Task to wait for completion.; File ""/usr/lib/python3.9/asyncio/tasks.py"", line 328, in __wakeup; future.result(); File ""/usr/lib/python3.9/asyncio/futures.py"", line 196, in result; raise exc; asyncio.exceptions.CancelledError. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiocloud/aiogoogle/client/storage_client.py"", line 111, in __aexit__; value = await self._task; File ""/usr/lib/python3.9/asyncio/futures.py"", line 284, in __await__; yield self # This tells Task to wait for completion.; File ""/usr/lib/python3.9/asyncio/tasks.py"", line 328, in __wakeup; future.result(); File ""/usr/lib/python3.9/asyncio/futures.py"", line 196, in result; raise exc; asyncio.exceptions.CancelledError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13742:4490,queue,queues,4490,https://hail.is,https://github.com/hail-is/hail/issues/13742,1,['queue'],['queues']
Performance,"-------------------------------------------------. I'm working through the GWAS tutorial and getting some strange errors with two different functions. On the 4th line calling hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True) I'm getting the following error:. ```; File ""gwas_tutorial.py"", line 12, in <module>; hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt',overwrite=True); File ""</Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/decorator.py:decorator-gen-946>"", line 2, in write; File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/matrixtable.py"", line 2494, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/backend/backend.py"", line 106, in execute; result = json.loads(Env.hail().backend.spark.SparkBackend.executeJSON(self._to_java_ir(ir))); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/utils/java.py"", line 240, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: ScalaSigParserError: Unexpected failure. Java stack trace:; org.json4s.scalap.ScalaSigParserError: Unexpected failure; 	at org.json4s.scalap.Rules$$anonfun$expect$1.apply(Rules.scala:73); 	at org.json4s.scalap.scalasig.ClassFileParser$.parse(ClassFileParser.scala:95); 	at org.json4s.reflect.ScalaSigReader$.parseClassFileFromByteCode(ScalaSigReader.scala:178); 	at org.json4s.reflect.ScalaSigReader$.findScalaSig(ScalaSigReader.scala:172); 	at org.json4s.reflect.ScalaSigReader$.findClass(ScalaSigReader.scala:53); 	at org.json4s.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6299:1237,load,loads,1237,https://hail.is,https://github.com/hail-is/hail/issues/6299,1,['load'],['loads']
Performance,-----------------------------------; Latency ; ------------------------------------------------------------------------------; Operation Size Trials Mean (ms) Std Dev (ms) Median (ms) 90th % (ms); ========= ========= ====== ========= ============ =========== ===========; Delete 0 B 5 43.1 6.4 40.9 50.9 ; Delete 1 KiB 5 44.2 12.7 42.5 58.1 ; Delete 100 KiB 5 44.7 10.4 42.8 56.3 ; Delete 1 MiB 5 41.5 3.7 40.2 45.7 ; Download 0 B 5 74.6 7.9 73.2 84.0 ; Download 1 KiB 5 84.3 15.9 80.6 103.4 ; Download 100 KiB 5 81.9 16.0 82.7 99.6 ; Download 1 MiB 5 90.6 6.5 94.5 96.8 ; Metadata 0 B 5 23.6 2.7 23.6 26.3 ; Metadata 1 KiB 5 25.5 2.1 26.9 27.4 ; Metadata 100 KiB 5 26.2 3.6 27.3 29.9 ; Metadata 1 MiB 5 24.0 3.7 23.3 28.4 ; Upload 0 B 5 98.1 16.6 95.5 117.9 ; Upload 1 KiB 5 116.7 21.8 115.5 142.1 ; Upload 100 KiB 5 116.5 17.8 115.1 135.1 ; Upload 1 MiB 5 168.2 18.5 179.6 185.6 . ------------------------------------------------------------------------------; Write Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Write throughput: 977.7 Mibit/s.; Parallelism strategy: both. ------------------------------------------------------------------------------; Read Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Read throughput: 1.11 Gibit/s.; Parallelism strategy: both. ------------------------------------------------------------------------------; System Information ; ------------------------------------------------------------------------------; IP Address: ; 172.21.46.11; Temporary Directory: ; /tmp; Bucket URI: ; gs://hail-jigold/; gsutil Version: ; 5.24; boto Version: ; 2.49.0; Measurement time: ; 2023-06-05 03:25:16 PM ; Running on GCE: ; True; GCE Instance:; 	; Bucket location: ; US-CENTRAL1; Bucket storage class: ; REGIONAL; Google Server: ; ; Google Server IP Ad,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597:1835,Throughput,Throughput,1835,https://hail.is,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597,1,['Throughput'],['Throughput']
Performance,"--conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". ```; Here is a sample of the yarn log....; ...; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/data04/hadoop/yarn/local/usercache/farrell/filecache/40/__spark_libs__5184408978318087972.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.4.0.0-169/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; 19/01/22 13:11:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; ERROR: dlopen(""/data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so""): /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so); java.lang.UnsatisfiedLinkError: /da",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:2698,load,load,2698,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258,1,['load'],['load']
Performance,"-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJWT, pygments, prompt-toolkit, pickleshare, pexpect, parsimonious",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:7104,cache,cached,7104,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,-SNAPSHOT]; 	at is.hail.utils.package$.using(package.scala:657) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$3(Worker.scala:135) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$2(Worker.scala:134) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]; 	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]; 	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]; 	... 3 more; Caused by: javax.crypto.AEADBadTagException: Tag mismatch!; 	at com.sun.crypto.provider.GaloisCounterMode.decryptFinal(GaloisCounterMode.java:620) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.finalNoPadding(CipherCore.java:1116) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.fillOutputBuffer(CipherCore.java:1053) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.doFinal(CipherCore.java:941) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.AESCipher.engineDoFinal(AESCipher.java:491) ~[sunjce_provider.jar:1.8.0_392];,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:7183,concurren,concurrent,7183,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,1,['concurren'],['concurrent']
Performance,"-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:6538,cache,cached,6538,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"-calls-starting-in-intervals false --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --help false --version false --showHidden false --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --disable-tool-default-read-filters false"",Version=4.0.1.2,Date=""March 22, 2018 1:12:03 AM EDT"">; ##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">; ##INFO=<ID=DB,Number=0,Type=Flag,Description=""dbSNP Membership"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ##VEP=""v91"" time=""2018-03-22 04:15:25"" cache=""/media/SE5/.vep/homo_sapiens/91_GRCh38"" db=""homo_sapiens_core_91_38@ensembldb.ensembl.org"" ensembl-variation=91.c78d8b4 ensembl-funcgen=91.4681d69 ensembl-io=91.923d668 ensembl=91.18ee742 1000genomes=""phase3"" COSMIC=""82"" ClinVar=""201710"" ESP=""V2-SSA137"" HGMD-PUBLIC=""20172"" assembly=""GRCh38.p10"" dbSNP=""150"" gencode=""GENCODE 27"" genebuild=""2014-07"" gnomAD=""170228"" polyphen=""2.2.2"" regbuild=""16"" sift=""sift5.2.2""; ##INFO=<ID=CSQ,Number=.,Type=String,Description=""Consequence annotations from Ensembl VEP. Format: Allele|Consequence|IMPACT|SYMBOL|Gene|Feature_type|Feature|BIOTYPE|EXON|INTRON|HGVSc|HGVSp|cDNA_position|CDS_position|Protein_position|Amino_acids|Codons|Existing_variation|DISTANCE|STRAND|FLAGS|SYMBOL_SOURCE|HGNC_ID|SIFT|PolyPhen|AF|AFR_AF|AMR_AF|EAS_AF|EUR_AF|SAS_AF|AA_AF|EA_AF|gnomAD_AF|gnomAD_AFR_AF|gnomAD_AMR_AF|gnomAD_ASJ_AF|gnomAD_EAS_AF|gnomAD_FIN_AF|gnomAD_NFE_AF|gn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:24705,cache,cache,24705,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['cache'],['cache']
Performance,"-conf spark.executor.extraClassPath=./hail-all-spark.jar \; > --py-files $HAIL_HOME/python/hail.zip \; > --conf spark.sql.files.openCostInBytes=1099511627776 \; > --conf spark.sql.files.maxPartitionBytes=1099511627776 \; > --conf spark.hadoop.parquet.block.size=1099511627776; ```. 4) Attempted to run the Hail tutorial, received an error when calling `common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))')`. ### What went wrong (all error messages here, including the full java stack trace):; ```; Python 2.7.14 |Anaconda, Inc.| (default, Oct 16 2017, 17:29:19); [GCC 7.2.0] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/02/22 20:29:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/02/22 20:29:10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(spark.sparkContext); Running on Apache Spark version 2.0.2; SparkUI available at http://192.168.1.122:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-20613ed; >>> table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); 2018-02-22 20:29:45 Hail: INFO: Reading table to impute column types; 2018-02-22 20:29:45 Hail: INFO: Finished type im",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:1528,load,load,1528,https://hail.is,https://github.com/hail-is/hail/issues/2966,1,['load'],['load']
Performance,"-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:4587,cache,cached,4587,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,-fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT build/Upcalls.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region_test.cpp -MG -M -MF build/Region_test.d -MT build/Region_test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region.cpp -MG -M -MF build/Region.d -MT build/Region.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -f,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5659:1950,cache,cache-tests,1950,https://hail.is,https://github.com/hail-is/hail/issues/5659,1,['cache'],['cache-tests']
Performance,"-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5446,cache,cachetools,5446,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cachetools']
Performance,-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached pandas-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB); Collecting parsimonious==0.10.0; Using cached parsimonious-0.10.0-py3-none-any.whl (48 kB); Collecting pillow==10.0.0; Using cached Pillow-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB); Colle,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:36836,cache,cached,36836,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,". I had to set --rows-per-partition to 40m to fix a `The requested number of tablets is over the permitted maximum (100)` error. I was able to write a small table. When I tried to write a larger file (~900 exomes) and I got:. ```; hail: writekudu: caught exception: org.kududb.client.NonRecoverableException: Too many attempts: KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6, DeadlineTracker(timeout=10000, elapsed=7721), Deferred@1490962783(state=PENDING, result=null, callback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505), errback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505))); ```. In the Kudu logs, I'm seeing tons of:. ```; W0411 15:20:09.832504 129721 catalog_manager.cc:1880] TS a72be89d736f49a799e1b544197675be: Create Tablet RPC failed for tablet 6652d540f73a4ba5a0b9758a3aeeb1e4: Remote error: Service unavailable: CreateTablet request on kudu.tserver.TabletServerAdminService from 69.173.65.227:42904 dropped due to backpressure. The service queue is full; it has 50 items.; ```. Suggestions on how to proceed? Should I increase the service queue size?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-208516279:1934,queue,queue,1934,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208516279,2,['queue'],['queue']
Performance,". The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). The major new build step is `create_certs` which creates a certificate, key, and; list of trusted ""principals"" for each ""principal"". ""Principal"" is a computer; security term referring to an authenticatable identity. In our system, the; services are each unique principals and every client (e.g. the test_batch CI; step) is also a principal. A principal's certificate is a unforgeable proof of; their identity. A principal's ""key"", in our system, is actually a public-private; (i.e. asymmetric) key pair which the client and server use to establish a; symmetric key for each new connection. A list of trusted principals is a list of; certificates. Every incoming connection must provide a certificate in the; trusted list or the server will drop the connection. Every service depends on the `create_certs` step because their deployment's load; secrets created by `create_certs`. The blog service is implemented by Ghost. Ghost only supports HTTP. As a result; we cannot make all network traffic in our cluster TLS-secured. However, we can; use an nginx sidecar on the blog pod which terminates TLS connections and sends; plaintext traffic on the loopback interface to Ghost. Thus, our goal is: no; plaintext traffic on non-loopback interfaces. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. We require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, app",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:1102,load,load,1102,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['load'],['load']
Performance,". The original motivation for considering aiohttp: ; https://magic.io/blog/uvloop-blazing-fast-python-networking. In short, one of the creators of asyncio discusses uvloop performance relative to other libraries. They key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:1562,perform,performance,1562,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['perform'],['performance']
Performance,".. using builtin-java classes where applicable; ERROR: dlopen(""/data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so""): /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so); java.lang.UnsatisfiedLinkError: /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); at is.hail.annotations.Region.<init>(Region.scala:27); at is.hail.annotations.Region$.apply(Region.scala:10); at is.hail.rvd.RVDContext$.default(RVDContext.scala:8); at is.hail.rvd.package$RVDContextIsPointed$.point(package.scala:8); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:4147,load,load,4147,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258,3,['load'],"['load', 'loadLibrary']"
Performance,"... in the sense that they always aggregate in partition order. This is achieved by a `AssociativeCombiner` which greedily combOp's aggregator state from adjacent partitions. I think this is the best you can do. Later we should have a `CommutativeCombiner` and choose between the two based on the whether the user-level aggregators are (duh) associative (like collect and prev_nonnull) or commutative (like collectAsSet or count). This is slightly conservative as I converted, with slavish consistency, all aggregators and reducers, included one only used by concordance, which I think is commutative. I'm OK with that mostly because this is safer and concordance really needs to get rewritten in Python (I think @tpoterba has a draft but it needed some performance work). FYI @chrisvittal this should fix any prev_nonnull aggregator/scan issues.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5420:754,perform,performance,754,https://hail.is,https://github.com/hail-is/hail/pull/5420,1,['perform'],['performance']
Performance,".0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, com2, executor 1): java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:178); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:222); 	at org.apache.spark.util.Utils$.t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:1942,concurren,concurrent,1942,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['concurren'],['concurrent']
Performance,.0.0; Using cached Pillow-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB); Collecting plotly==5.16.1; Using cached plotly-5.16.1-py2.py3-none-any.whl (15.6 MB); Collecting portalocker==2.7.0; Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB); Collecting protobuf==3.20.2; Using cached protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:38732,cache,cached,38732,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,".0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (66",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:4079,cache,cached,4079,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,".1.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/java-native-access/jna/blob/master/CHANGES.md"">jna's changelog</a>.</em></p>; <blockquote>; <h1>Release 5.12.1</h1>; <h2>Bug Fixes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/1447"">#1447</a>: Null-check cleanable in <code>c.s.j.Memory#close</code> - <a href=""https://github.com/dbwiddis""><code>@​dbwiddis</code></a>.</li>; </ul>; <h1>Release 5.12.0</h1>; <h2>Features</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1433"">#1433</a>: Add <code>CFEqual</code>, <code>CFDictionaryRef.ByReference</code>, <code>CFStringRef.ByReference</code> to <code>c.s.j.p.mac.CoreFoundation</code> - <a href=""https://github.com/shalupov""><code>@​shalupov</code></a></li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/978"">#978</a>: Remove use of finalizers in JNA and improve concurrency for <code>Memory</code>, <code>CallbackReference</code> and <code>NativeLibrary</code> - <a href=""https://github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1440"">#1440</a>: Support for LoongArch64 - <a href=""https://github.com/Panxuefeng-loongson""><code>@​Panxuefeng-loongson</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1444"">#1444</a>: Update embedded libffi to 1f14b3fa92d4442a60233e9596ddec428a985e3c and rebuild native libraries - <a href=""https://github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; </ul>; <h2>Bug Fixes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1438"">#1438</a>: Handle arrays in structures with differing size - <a href=""https://github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/jav",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12438:1057,concurren,concurrency,1057,https://hail.is,https://github.com/hail-is/hail/pull/12438,1,['concurren'],['concurrency']
Performance,.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB); Collecting dill==0.3.7; Using cached dill-0.3.7-py3-none-any.whl (115 kB); Collecting frozenlist==1.4.0; Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB); Collecting google-api-core==2.11.1; Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:34527,cache,cached,34527,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,".358ms children 36.889ms %children 99.04%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Verify total 0.268ms self 0.268ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform total 36.599ms self 2.385ms children 34.214ms %children 93.48%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/FoldConstants, iteration: 0 total 4.078ms self 4.078ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ExtractIntervalFilters, iteration: 0 total 0.898ms self 0.898ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 0 total 2.553ms self 0.011ms children 2.543ms %children 99.59%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 2.543ms self 2.543ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/Simplify, iteration: 0 total 8.675ms self 8.675ms children 0.000ms %children 0.00%; timing is.hail.backend.Backend",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:2648,Optimiz,Optimize,2648,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,".4.1</a> (2022-01-21)</h3>; <h3>Bug Fixes</h3>; <ul>; <li>urllib3 import (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/953"">#953</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/c8b5cae3da5eb9d40067d38dac51a4a8c1e0763e"">c8b5cae</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/google-auth-library-python/compare/v2.3.3...v2.4.0"">2.4.0</a> (2022-01-20)</h2>; <h3>Features</h3>; <ul>; <li>add 'py.typed' declaration (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/919"">#919</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/c99350455d0f7fd3aab950ac47b43000c73dd312"">c993504</a>)</li>; <li>add api key support (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/826"">#826</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/3b15092b3461278400e4683060f64a96d50587c4"">3b15092</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li><strong>deps:</strong> allow cachetools 5.0 for python 3.7+ (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/937"">#937</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/1eae37db7f6fceb32d6ef0041962ce1755d2116c"">1eae37d</a>)</li>; <li>fix the message format for metadata server exception (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/916"">#916</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/e756f08dc78616040ab8fbd7db20903137ccf0c7"">e756f08</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>fix intersphinx link for 'requests-oauthlib' (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/921"">#921</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/967be4f4e2a43ba7e240d7acb01b6b992d40e6ec"">967be4f</a>)</li>; <li>note Valu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11546:7698,cache,cachetools,7698,https://hail.is,https://github.com/hail-is/hail/pull/11546,1,['cache'],['cachetools']
Performance,.446957 ms.; 2023-09-27 16:44:22.623 GoogleStorageFS$: INFO: createNoCompression: gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g=/result.7028; 2023-09-27 16:44:22.668 GoogleStorageFS$: INFO: close: gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g=/result.7028; 2023-09-27 16:44:33.788 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/1-day/o?name=parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g%3D/result.7028&uploadType=resumable&upload_id=ADPycdv7y3A6GjTh6Kv7vrUu2ap2Kv0peLVWsVTAghIs7RCZk9X3fI1BDkeHag1cd9g3etP2sS4f13bN6iJPU_sbnRnyRE91VPtjUpuYLPqmOq13; 	|> content-range: bytes */*; 	| ; 	|< HTTP/1.1 308 Resume Incomplete; 	|< content-length: 0; 	|< conte,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:6954,concurren,concurrent,6954,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['concurren'],['concurrent']
Performance,".46.11; Temporary Directory: ; /tmp; Bucket URI: ; gs://hail-jigold/; gsutil Version: ; 5.24; boto Version: ; 2.49.0; Measurement time: ; 2023-06-05 03:25:16 PM ; Running on GCE: ; True; GCE Instance:; 	; Bucket location: ; US-CENTRAL1; Bucket storage class: ; REGIONAL; Google Server: ; ; Google Server IP Addresses: ; 142.250.128.128; 142.251.6.128; 108.177.112.128; 74.125.124.128; 172.217.212.128; 172.217.214.128; 172.253.119.128; 108.177.111.128; 142.250.1.128; 108.177.121.128; 142.250.103.128; 108.177.120.128; 142.250.159.128; 142.251.120.128; 142.251.161.128; 74.125.126.128; Google Server Hostnames: ; ib-in-f128.1e100.net; ic-in-f128.1e100.net; jo-in-f128.1e100.net; jp-in-f128.1e100.net; jq-in-f128.1e100.net; jr-in-f128.1e100.net; jt-in-f128.1e100.net; jv-in-f128.1e100.net; jw-in-f128.1e100.net; jx-in-f128.1e100.net; jy-in-f128.1e100.net; jz-in-f128.1e100.net; ie-in-f128.1e100.net; if-in-f128.1e100.net; ig-in-f128.1e100.net; ik-in-f128.1e100.net; Google DNS thinks your IP is: ; ; CPU Count: ; 16; CPU Load Average: ; [32.39, 33.2, 19.0]; Total Memory: ; 57.5 GiB; Free Memory: ; 38.41 GiB; TCP segment counts not available because ""netstat"" was not found during test runs; Disk Counter Deltas:; disk reads writes rbytes wbytes rtime wtime ; loop0 0 0 0 0 0 0 ; loop1 0 0 0 0 0 0 ; loop3 0 0 0 0 0 0 ; loop4 0 0 0 0 0 0 ; loop5 0 0 0 0 0 0 ; nvme0n1 4385 4694 581857280 1743810560 6453 527129 ; sda1 0 544 0 3731456 0 429 ; sda14 0 0 0 0 0 0 ; sda15 0 0 0 0 0 0 ; TCP /proc values:; tcp_timestamps = 1; tcp_sack = 1; tcp_window_scaling = 1; Boto HTTPS Enabled: ; True; Requests routed through proxy: ; False; Latency of the DNS lookup for Google Storage server (ms): ; 1.5; Latencies connecting to Google Storage server IPs (ms):; 74.125.126.128 = 1.1. ------------------------------------------------------------------------------; In-Process HTTP Statistics ; ------------------------------------------------------------------------------; Total HTTP requests made: 149; HTTP 5xx ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597:3577,Load,Load,3577,https://hail.is,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597,1,['Load'],['Load']
Performance,".5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 2.0 failed 4 times, most recent failure: Lost task 20.3 in stage 2.0 (TID 485, scc-q08.scc.bu.edu, executor 2): is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:775); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:768); at is.hail.utils.package$.using(package.scala:575); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$ano",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:3592,Load,LoadVCF,3592,https://hail.is,https://github.com/hail-is/hail/issues/3379,1,['Load'],['LoadVCF']
Performance,.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached pandas-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB); Collecting parsimonious==0.10.0; Using cached parsimonious-0.10.0-py3-none-any.whl (48 kB); Collecting pillow==10.0.0; Using cached Pillow-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB); Collecting plotly==5.16.1; Using cached plotly-5.16.1-py2.py3-none-any.whl (15.6 MB); Collecting portalocker==2.7.0; Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB); Collecting protobuf==3.20.2; Using cached protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB);,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:37130,cache,cached,37130,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,".7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(spark.sparkContext); Running on Apache Spark version 2.0.2; SparkUI available at http://192.168.1.122:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-20613ed; >>> table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); 2018-02-22 20:29:45 Hail: INFO: Reading table to impute column types; 2018-02-22 20:29:45 Hail: INFO: Finished type imputation; Loading column `Sample' as type String (imputed); Loading column `Population' as type String (imputed); Loading column `SuperPopulation' as type String (imputed); Loading column `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed); >>> common_vds = hc.read('/mnt/d/metistream/hail/data/1kg.vds'); >>> common_vds = common_vds.annotate_samples_table(table, root='sa'); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; >>> common_vds = common_vds.sample_qc(); >>> common_vds = common_vds.filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); >>> common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))'); // class version 52.0 (52); // access flags 0x1; public class is/hail/codegen/generated/C0 implements java/io/Serializable is/hail/asm4s/AsmFunction2 {. // access flags 0x1; public apply([Ljava/lang/Object;Lscala/collection/mutable/ArrayBuffer;)Ljava/lang/Object;; L0; ALOAD 2; LDC 2; INVOKEVIRTUAL scala/collection/mutable/ArrayBuffer.apply (I)Ljava/lang/Object;; CHECKCAST org/apache/spark/sql/Row; ASTORE 3; ALOAD 3; IFNULL L1; ALOAD 3; LDC 5; INVOKEINTERFACE org/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:3017,load,load,3017,https://hail.is,https://github.com/hail-is/hail/issues/2966,1,['load'],['load']
Performance,".85950, 1.85650, 1.83358, 1.82607, 1.81018, 1.77451, 1.75714, 1.74538, 1.73411; 2017-08-28 21:47:47 Hail: INFO: lmmreg: Evals 250 to 230: 0.00000, 0.25129, 0.25898, 0.26631, 0.26798, 0.27447, 0.27757, 0.28447, 0.29418, 0.30385, 0.31053, 0.31348, 0.31638, 0.32141, 0.32747, 0.33643, 0.34130, 0.34292, 0.34890, 0.34984; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: beta = Map(intercept -> 0.0370042272400176, sa.cov -> -0.012886009824596447); 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: sigmaG2 = 0.13829390418697945; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: sigmaE2 = 0.8304138510277874; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: delta = 6.004703214575758; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: h2 = 0.1427612233333665; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: seH2 = 0.13770872661270844; 2017-08-28 21:47:48 Hail: INFO: Reading table with no type imputation; Loading column `Sample' as type `String' (type not specified); Loading column `Cov1' as type `Float64' (user-specified); Loading column `Cov2' as type `Float64' (user-specified). 2017-08-28 21:47:48 Hail: INFO: Reading table with no type imputation; Loading column `Sample' as type `String' (type not specified); Loading column `Pheno' as type `Float64' (user-specified). 2017-08-28 21:47:48 Hail: INFO: No multiallelics detected.; 2017-08-28 21:47:48 Hail: INFO: Coerced sorted dataset; 2017-08-28 21:47:48 Hail: WARN: called redundant `filtermulti' on an already split or multiallelic-filtered VDS; 2017-08-28 21:47:48 Hail: INFO: rrm: Computing Realized Relationship Matrix...; 2017-08-28 21:47:49 Hail: INFO: rrm: RRM computed using 3 variants.; 2017-08-28 21:47:49 Hail: WARN: 1 of 8 samples have a missing phenotype or covariate.; 2017-08-28 21:47:49 Hail: INFO: lmmreg: running lmmreg on 7 samples with 3 sample covariates including intercept...; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Computing eigendecomposition of kinship matrix...;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835:5214,Load,Loading,5214,https://hail.is,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835,3,['Load'],['Loading']
Performance,".9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5769,cache,cached,5769,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached pandas-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB); Collecting parsimonious==0.10.0; Using cached parsimonious-0.10.0-py3-none-any.whl (48 kB); Collecting pillow==10.0.0; Using cached Pillow-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB); Collecting plotly==5.16.1; Using cached plotly-5.16.1-py2.py3-none-any.whl (15.6 MB); Collecting portalocker==2.7.0; Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB); Collecting protobuf==3.20.2; Using cached protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Usin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:38341,cache,cached,38341,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:241); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:234); at is.hail.sparkextras.OrderedRDD$$anonfun$apply$8$$anon$2.next(OrderedRDD.scala:20,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2407:12267,concurren,concurrent,12267,https://hail.is,https://github.com/hail-is/hail/issues/2407,1,['concurren'],['concurrent']
Performance,".BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardRelationalLets, iteration: 0 total 0.796ms self 0.796ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.111ms self 0.111ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/PruneDeadFields, iteration: 0 total 9.943ms self 9.943ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/FoldConstants, iteration: 1 total 0.414ms self 0.414ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ExtractIntervalFilters, iteration: 1 total 0.021ms self 0.021ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 1 total 0.384ms self 0.005ms children 0.379ms %children 98.72%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 1/is.hail.expr.ir.NormalizeNames.apply total 0.379ms self 0.379ms children 0.000ms %children 0.00%; timing is.hail.backen",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:5514,Optimiz,Optimize,5514,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.238ms self 0.238ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.070ms self 0.070ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:198884,Optimiz,Optimize,198884,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,".ContextRDD.collect(ContextRDD.scala:143); at is.hail.io.RichContextRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:1179); at is.hail.rvd.OrderedRVD.writeRowsSplit(OrderedRVD.scala:454); at is.hail.expr.MatrixValue.write(Relational.scala:122); at is.hail.variant.MatrixTable$$anonfun$write$2.apply(MatrixTable.scala:2301); at is.hail.variant.MatrixTable$$anonfun$write$2.apply(MatrixTable.scala:2301); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:511); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:39); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:15); at is.hail.variant.MatrixTable.write(MatrixTable.scala:2301); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). Hail version: devel-4f13f27cd28d; Error summary: SparkException: Job 2 cancelled because SparkContext was shut down; [farrell@scc-hadoop ad.v1]$ Exception in thread ""Executor task launch worker for task 766"" java.lang.NullPointerException; at org.apache.spark.scheduler.Task.metrics$lzycompute(Task.scala:66); at org.apache.spark.scheduler.Task.metrics(Task.scala:65); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:473); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755:7859,concurren,concurrent,7859,https://hail.is,https://github.com/hail-is/hail/issues/4755,2,['concurren'],['concurrent']
Performance,".FilterOutputStream.close(FilterOutputStream.java:159); 	at is.hail.utils.package$.using(package.scala:658); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$5(ServiceBackend.scala:127); 	at is.hail.backend.service.ServiceBackend$$Lambda$2194/482268176.apply$mcV$sp(Unknown Source); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:77); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$4(ServiceBackend.scala:127); 	at is.hail.backend.service.ServiceBackend$$Lambda$2191/716305671.apply$mcV$sp(Unknown Source); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.concurrent.Future$$$Lambda$2188/1126720330.apply(Unknown Source); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.Future$$Lambda$2189/609808342.apply(Unknown Source); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.Promise$$Lambda$2190/183883584.apply(Unknown Source); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). ""pool-2-thread-1"" #26 prio=5 os_prio=0 tid=0x00007f502866e000 nid=0x88c waiting on condition [0x00007f50275fd000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method); 	at is.hail.services.package$.sleepAndBackoff(package.scala:32); 	at is.hail.services.package$.retryTransientErrors(package.scala:86); 	at is.hail.services.Requester.reques",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:3287,concurren,concurrent,3287,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['concurren'],['concurrent']
Performance,.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'; 	at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:247); 	at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:545); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Unsupported/Unknown Elasticsearch version 6.0.0; 	at org.elasticsearch.hadoop.util.EsMajorVersion.parse(EsMajorVersion.java:79); 	at org.elasticsearch.hadoop.rest.RestClient.remoteEsVersion(RestClient.java:613); 	at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:240); 	at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:545); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.T,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4138:6637,concurren,concurrent,6637,https://hail.is,https://github.com/hail-is/hail/issues/4138,1,['concurren'],['concurrent']
Performance,.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215642,concurren,concurrent,215642,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.344ms self 0.344ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 29.693ms self 9.419ms children 20.274ms %children 68.28%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 2.942ms self 2.942ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.506ms self 0.506ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.500ms self 0.500ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 7.523ms self 7.523ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:3366,Optimiz,Optimize,3366,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3790:6215,concurren,concurrent,6215,https://hail.is,https://github.com/hail-is/hail/issues/3790,1,['concurren'],['concurrent']
Performance,.ScalaValueWriter.doWriteScala(ScalaValueWriter.scala:63); 	at org.elasticsearch.spark.serialization.ScalaValueWriter.write(ScalaValueWriter.scala:46); 	at org.elasticsearch.hadoop.serialization.builder.ContentBuilder.value(ContentBuilder.java:53); 	at org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.doWriteObject(TemplatedBulk.java:71); 	at org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.write(TemplatedBulk.java:58); 	at org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:168); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:67); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:107); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:107); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4250:5329,concurren,concurrent,5329,https://hail.is,https://github.com/hail-is/hail/issues/4250,1,['concurren'],['concurrent']
Performance,".ScalaValueWriter.doWriteScala(ScalaValueWriter.scala:63); 	at org.elasticsearch.spark.serialization.ScalaValueWriter.write(ScalaValueWriter.scala:46); 	at org.elasticsearch.hadoop.serialization.builder.ContentBuilder.value(ContentBuilder.java:53); 	at org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.doWriteObject(TemplatedBulk.java:71); 	at org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.write(TemplatedBulk.java:58); 	at org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:168); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:67); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:107); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:107); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-408f188; Error summary: EsHadoopIllegalArgumentException: Spark SQL types are not handled through basic RDD saveToEs() calls; typically this is a mistake(as the SQL schema will be ignored). Use 'org.elasticsearch.spark.sql' package instead; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [ffc9fb0b99f64080b674ab7a07962df9] entered state [ERROR] while waiting for [DONE].; ```. Ideally it would get exported as nested objects: https://www.elastic.co/guide/en/elasticsearch/reference/current/nested.html#_using_literal_nested_literal_fields_for_arrays_of_objects. with elasticsearch mapping:; ```; u'vep': {'type': 'nested', 'properties': {u'category': {'type': 'keyword'}, u'major_consequence': {'type': 'keyword'}, u'gene_id': {'type': 'keyword'}, u'major_consequence_rank': {'type': 'in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4250:10883,concurren,concurrent,10883,https://hail.is,https://github.com/hail-is/hail/issues/4250,1,['concurren'],['concurrent']
Performance,".ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$5(ServiceBackend.scala:127); 	at is.hail.backend.service.ServiceBackend$$Lambda$2194/482268176.apply$mcV$sp(Unknown Source); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:77); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$4(ServiceBackend.scala:127); 	at is.hail.backend.service.ServiceBackend$$Lambda$2191/716305671.apply$mcV$sp(Unknown Source); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.concurrent.Future$$$Lambda$2188/1126720330.apply(Unknown Source); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.Future$$Lambda$2189/609808342.apply(Unknown Source); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.Promise$$Lambda$2190/183883584.apply(Unknown Source); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). ""pool-2-thread-1"" #26 prio=5 os_prio=0 tid=0x00007f502866e000 nid=0x88c waiting on condition [0x00007f50275fd000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method); 	at is.hail.services.package$.sleepAndBackoff(package.scala:32); 	at is.hail.services.package$.retryTransientErrors(package.scala:86); 	at is.hail.services.Requester.requestWithHandler(Requester.scala:69); 	at is.hail.services.Requester.request(Requester.scala:94); 	at is.hail.services.memory_client.MemoryC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:3423,concurren,concurrent,3423,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['concurren'],['concurrent']
Performance,".SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/LowerMatrixToTable/Verify total 0.012ms self 0.012ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/LowerMatrixToTable/Transform total 3.992ms self 3.992ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/LowerMatrixToTable/Verify total 0.019ms self 0.019ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.TypeCheck.apply total 0.132ms self 0.132ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable total 2.591ms self 0.010ms children 2.581ms %children 99.63%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Verify total 0.011ms self 0.011ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform total 2.548ms self 0.302ms children 2.246ms %children 88.15%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/FoldConstants, iteration: 0 total 0.307ms self 0.307ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:10262,Optimiz,Optimize,10262,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,".SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/is.hail.expr.ir.TypeCheck.apply total 0.127ms self 0.127ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardRelationalLets, iteration: 0 total 0.030ms self 0.030ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/is.hail.expr.ir.TypeCheck.apply total 0.095ms self 0.095ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/PruneDeadFields, iteration: 0 total 0.572ms self 0.572ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Verify total 0.022ms self 0.022ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.TypeCheck.apply total 0.144ms self 0.144ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/LiftRelationalValuesToRelationalLets total 1.135ms self 0.009ms children 1.126ms %children 99.21%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/LiftRelationalValuesToRelat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:14044,Optimiz,Optimize,14044,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply total 1.106s self 33.060ms children 1.072s %children 97.01%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 31.509ms self 1.445ms children 30.064ms %children 95.41%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.344ms self 0.344ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 29.693ms self 9.419ms children 20.274ms %children 68.28%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 2.942ms self 2.942ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.506ms self 0.506ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.ex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:2709,Optimiz,Optimize,2709,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:13168,Load,LoadMatrix,13168,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,".apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGSched",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:5937,Load,LoadVCF,5937,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:206); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.Spillable$class.maybeSpill(Spillable.scala:93); at org.apache.spark.util.collection.ExternalAppendOnlyMap.maybeSpill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:158); at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); at java.util.concurrent.ThreadPoolExecut,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/801#issuecomment-247861703:3845,Cache,CacheManager,3845,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703,1,['Cache'],['CacheManager']
Performance,".backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.analyses.SemanticHash.apply total 29.792ms self 6.254ms children 23.538ms %children 79.01%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.analyses.SemanticHash.apply/is.hail.expr.ir.NormalizeNames.apply total 23.538ms self 23.538ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.backend.spark.SparkBackend#_jvmLowerAndExecute total 0.053ms self 0.053ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply total 1.211s self 27.866ms children 1.183s %children 97.70%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR total 37.247ms self 0.358ms children 36.889ms %children 99.04%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Verify total 0.268ms self 0.268ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform total 36.599ms self 2.385ms children 34.214ms %children 93.48%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/FoldConstants, iteration: 0 total 4.078ms self 4.078ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Opti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:1631,Optimiz,Optimize,1631,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,".com/googleapis/google-auth-library-python/commit/a8eb4c8693055a3420cfe9c3420aae2bc8cd465a"">a8eb4c8</a>)</li>; </ul>; <h2>v2.4.1</h2>; <h3>Bug Fixes</h3>; <ul>; <li>urllib3 import (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/953"">#953</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/c8b5cae3da5eb9d40067d38dac51a4a8c1e0763e"">c8b5cae</a>)</li>; </ul>; <h2>v2.4.0</h2>; <h3>Features</h3>; <ul>; <li>add 'py.typed' declaration (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/919"">#919</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/c99350455d0f7fd3aab950ac47b43000c73dd312"">c993504</a>)</li>; <li>add api key support (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/826"">#826</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/3b15092b3461278400e4683060f64a96d50587c4"">3b15092</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li><strong>deps:</strong> allow cachetools 5.0 for python 3.7+ (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/937"">#937</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/1eae37db7f6fceb32d6ef0041962ce1755d2116c"">1eae37d</a>)</li>; <li>fix the message format for metadata server exception (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/916"">#916</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/e756f08dc78616040ab8fbd7db20903137ccf0c7"">e756f08</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>fix intersphinx link for 'requests-oauthlib' (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/921"">#921</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/967be4f4e2a43ba7e240d7acb01b6b992d40e6ec"">967be4f</a>)</li>; <li>note Valu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11546:2403,cache,cachetools,2403,https://hail.is,https://github.com/hail-is/hail/pull/11546,1,['cache'],['cachetools']
Performance,.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:750). java.lang.ClassFormatError: Too many arguments in method signature in class file __C2866stream; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:635); 	at is.hail.asm4s.HailClassLoader.liftedTree1$1(HailClassLoader.scala:10); 	at is.hail.asm4s.HailClassLoader.loadOrDefineClass(HailClassLoader.scala:6); 	at is.hail.asm4s.ClassesBytes.$anonfun$load$1(ClassBuilder.scala:64); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); 	at is.hail.asm4s.ClassesBytes.load(ClassBuilder.scala:62); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:715); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:708); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1(Compile.scala:311); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1$adapted(Compile.scala:310); 	at is.hail.expr.ir.lowering.TableStageToRVD$.$anonfun$apply$9(RVDToTableStage.scala:106); 	at is.hail.sparkextras.ContextRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1234); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1233); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(Contex,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:12169,load,load,12169,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['load'],['load']
Performance,.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567); at java.nio.CharBuffer.toString(CharBuffer.java:1241); at org.apache.hadoop.io.Text.decode(Text.java:412); at org.apache.hadoop.io.Text.decode(Text.java:389); at org.apache.hadoop.io.Text.toString(Text.java:280); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:788); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:64); at is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:37); at is.hail.utils.package$.using(package.scala:587); at is.hail.rvd.RVDPartitionInfo$.apply(RVDPartitionInfo.scala:37); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1059); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1057); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:9124,Load,LoadVCF,9124,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['Load'],['LoadVCF']
Performance,.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). 	Java stack trace:; 	java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 			at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 			at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 			at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 			at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); 			at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); 			at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); 			at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.appl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:6018,concurren,concurrent,6018,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['concurren'],['concurrent']
Performance,.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 al,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:17938,concurren,concurrent,17938,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['concurren'],['concurrent']
Performance,.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:10683,concurren,concurrent,10683,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['concurren'],['concurrent']
Performance,.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-279ddd2; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3235:12793,concurren,concurrent,12793,https://hail.is,https://github.com/hail-is/hail/issues/3235,2,['concurren'],['concurrent']
Performance,".concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:6187,Load,LoadVCF,6187,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Itera,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:11164,Load,LoadVCF,11164,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,".dependabot.com/plotly/plotly.py/pull/3765"">#3765</a> with thanks to <a href=""https://github.com/jvdd""><code>@​jvdd</code></a></li>; </ul>; <h3>Updated</h3>; <ul>; <li>Allow non-string extras in <code>flaglist</code> attributes, to support upcoming changes to <code>ax.automargin</code> in plotly.js <a href=""https://github-redirect.dependabot.com/plotly/plotly.js/pull/6193"">plotly.js#6193</a>, <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3749"">#3749</a></li>; </ul>; <h2>[5.8.2] - 2022-06-10</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed a syntax error that caused rendering issues in Databricks notebooks and likely elsewhere. <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3763"">#3763</a> with thanks to <a href=""https://github.com/fwetdb""><code>@​fwetdb</code></a></li>; </ul>; <h2>[5.8.1] - 2022-06-08</h2>; <p>(no changes, due to a mixup with the build process!)</p>; <h2>[5.8.0] - 2022-05-09</h2>; <h3>Fixed</h3>; <ul>; <li>Improve support for type checking and IDE auto-completion by bypassing lazy-loading when type checking. <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3425"">#3425</a> with thanks to <a href=""https://github.com/JP-Ellis""><code>@​JP-Ellis</code></a></li>; <li>line dash-style validators are now correctly used everywhere so that values like <code>10px 2px</code> are accepted <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3722"">#3722</a></li>; <li>Resolved various deprecation warning messages and compatibility issues with upstream dependencies and Python 3.11, plus removed dependency on <code>six</code>, with thanks to <a href=""https://github.com/maresb""><code>@​maresb</code></a>, <a href=""https://github.com/hugovk""><code>@​hugovk</code></a>, <a href=""https://github.com/tirkarthi""><code>@​tirkarthi</code></a>, <a href=""https://github.com/martinRenou""><code>@​martinRenou</code></a>, and <a href=""https://github.com/BjoernLudwigPTB""><code>@​BjoernLudwigPTB</code></a></li>; <l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12113:3675,load,loading,3675,https://hail.is,https://github.com/hail-is/hail/pull/12113,1,['load'],['loading']
Performance,.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.450ms self 0.450ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.396ms self 0.396ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.043ms self 0.043ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:55688,Optimiz,Optimize,55688,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.607ms self 0.607ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.447ms self 0.447ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.051ms self 0.051ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:47160,Optimiz,Optimize,47160,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,.expr.ir.lowering.LowerAndExecuteShufflesPass/is.hail.expr.ir.lowering.LowerAndExecuteShuffles.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass/is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass#after total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.600ms self 0.007ms children 0.594ms %children 98.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.575ms self 0.049ms children 0.527ms %children 91.53%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.239ms self 0.239ms childr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:179634,Optimiz,OptimizePass,179634,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.032ms children 0.070ms %children 68.44%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:215963,Optimiz,OptimizePass,215963,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.070ms self 0.070ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.108ms self 0.034ms children 0.074ms %children 68.32%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.067ms self 0.067ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:199990,Optimiz,OptimizePass,199990,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.064ms self 0.064ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.101ms self 0.033ms children 0.068ms %children 67.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:207942,Optimiz,OptimizePass,207942,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.072ms self 0.072ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.035ms children 0.067ms %children 66.15%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:195338,Optimiz,OptimizePass,195338,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,".hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ExtractIntervalFilters, iteration: 0 total 0.898ms self 0.898ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 0 total 2.553ms self 0.011ms children 2.543ms %children 99.59%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 2.543ms self 2.543ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/Simplify, iteration: 0 total 8.675ms self 8.675ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 0 total 4.506ms self 3.993ms children 0.513ms %children 11.40%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.513ms self 0.513ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.120ms self 0.120ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:3528,Optimiz,Optimize,3528,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,".hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ExtractIntervalFilters, iteration: 1 total 0.021ms self 0.021ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 1 total 0.384ms self 0.005ms children 0.379ms %children 98.72%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 1/is.hail.expr.ir.NormalizeNames.apply total 0.379ms self 0.379ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/Simplify, iteration: 1 total 0.103ms self 0.103ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 1 total 0.579ms self 0.273ms children 0.306ms %children 52.84%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 1/is.hail.expr.ir.NormalizeNames.apply total 0.306ms self 0.306ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/is.hail.expr.ir.TypeCheck.apply total 0.143ms self 0.143ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:6671,Optimiz,Optimize,6671,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,".hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.analyses.SemanticHash.apply/is.hail.expr.ir.NormalizeNames.apply total 23.538ms self 23.538ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.backend.spark.SparkBackend#_jvmLowerAndExecute total 0.053ms self 0.053ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply total 1.211s self 27.866ms children 1.183s %children 97.70%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR total 37.247ms self 0.358ms children 36.889ms %children 99.04%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Verify total 0.268ms self 0.268ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform total 36.599ms self 2.385ms children 34.214ms %children 93.48%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/FoldConstants, iteration: 0 total 4.078ms self 4.078ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ExtractIntervalFilters, iteration: 0 total 0.898ms self 0.898ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.ha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:1873,Optimiz,Optimize,1873,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,.hail.driver.FilterVariants$$anonfun$2.apply(FilterVariants.scala:45); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:415); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Opti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/120:4068,concurren,concurrent,4068,https://hail.is,https://github.com/hail-is/hail/issues/120,1,['concurren'],['concurrent']
Performance,.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 8388608; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 	at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 	at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:317); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:299); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:317); 	at java.io.DataOutputStream.flush(DataOutputStream.java:123); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:158); 	at is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:2237,concurren,concurrent,2237,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['concurren'],['concurrent']
Performance,".hail.utils.package$.using(package.scala:658); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$5(ServiceBackend.scala:127); 	at is.hail.backend.service.ServiceBackend$$Lambda$2194/482268176.apply$mcV$sp(Unknown Source); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:77); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$4(ServiceBackend.scala:127); 	at is.hail.backend.service.ServiceBackend$$Lambda$2191/716305671.apply$mcV$sp(Unknown Source); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.concurrent.Future$$$Lambda$2188/1126720330.apply(Unknown Source); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.Future$$Lambda$2189/609808342.apply(Unknown Source); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.Promise$$Lambda$2190/183883584.apply(Unknown Source); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). ""pool-2-thread-1"" #26 prio=5 os_prio=0 tid=0x00007f502866e000 nid=0x88c waiting on condition [0x00007f50275fd000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method); 	at is.hail.services.package$.sleepAndBackoff(package.scala:32); 	at is.hail.services.package$.retryTransientErrors(package.scala:86); 	at is.hail.services.Requester.requestWithHandler(Requester.scala:69); 	at is.hail.services.Request",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:3349,concurren,concurrent,3349,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['concurren'],['concurrent']
Performance,.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(L,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:18446,concurren,concurrent,18446,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['concurren'],['concurrent']
Performance,.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.sche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:8892,concurren,concurrent,8892,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['concurren'],['concurrent']
Performance,.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12002,concurren,concurrent,12002,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['concurren'],['concurrent']
Performance,.ir.FoldConstants$.$anonfun$apply$1(FoldConstants.scala:10); E 	at is.hail.backend.ExecuteContext$.$anonfun$scopedNewRegion$1(ExecuteContext.scala:86); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.backend.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:83); E 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:9); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$4(Optimize.scala:22); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$1(Optimize.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.runOpt$1(Optimize.scala:15); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$2(Optimize.scala:22); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:18); E 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:40); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:36); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:7611,Optimiz,OptimizePass,7611,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Optimiz'],['OptimizePass']
Performance,.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.561ms self 0.561ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.144ms self 0.144ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.561ms self 0.732ms children 0.829ms %children 53.12%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.loweri,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:107807,Optimiz,Optimize,107807,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.614ms self 0.614ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.292ms self 0.292ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.500ms self 0.649ms children 0.852ms %children 56.76%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.loweri,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:93369,Optimiz,Optimize,93369,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.759ms self 0.759ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.137ms self 0.137ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.862ms self 0.753ms children 1.109ms %children 59.55%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.loweri,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:72190,Optimiz,Optimize,72190,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.785ms self 0.785ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.143ms self 0.143ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.010ms self 0.828ms children 1.182ms %children 58.80%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.loweri,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:57792,Optimiz,Optimize,57792,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.948ms self 0.948ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.222ms self 0.222ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.451ms self 0.949ms children 1.502ms %children 61.30%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.loweri,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:49264,Optimiz,Optimize,49264,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 3.651ms self 3.651ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 12.144ms self 12.144ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 9.513ms self 3.957ms children 5.556ms %children 58.40%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:40734,Optimiz,Optimize,40734,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.409ms self 0.648ms children 0.762ms %children 54.04%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.463ms self 0.463ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.058ms self 0.058ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:138776,Optimiz,Optimize,138776,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.228ms self 1.353ms children 0.875ms %children 39.29%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.340ms self 0.340ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:154563,Optimiz,Optimize,154563,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.252ms self 0.765ms children 1.487ms %children 66.02%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.505ms self 0.505ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.044ms self 0.044ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:123058,Optimiz,Optimize,123058,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Exec,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:6981,Load,LoadVCF,6981,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['Load'],['LoadVCF']
Performance,".json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-2e237ca; Error summary: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1683:4554,load,loading,4554,https://hail.is,https://github.com/hail-is/hail/issues/1683,1,['load'],['loading']
Performance,.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.concurrent.TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.handleTimeout(FluxTimeout.java:294); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.doTimeout(FluxTimeout.java:279); E 	at reactor.core.publisher.FluxTimeout$TimeoutTimeoutSubscriber.onNext(FluxTimeout.java:418); E 	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onNext(FluxOnErrorResume.java:79); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.propagateDelay(MonoDelay.java:270); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.run(MonoDelay.java:285); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:68); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:28); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180); E 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748). ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:5375,concurren,concurrent,5375,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,5,['concurren'],['concurrent']
Performance,.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.621ms self 0.621ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.245ms self 0.245ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.034ms self 0.034ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.E,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:159083,Optimiz,Optimize,159083,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.650ms self 0.650ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.254ms self 0.254ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.E,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:143296,Optimiz,Optimize,143296,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 4.283ms self 4.283ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.308ms self 0.308ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.E,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:127578,Optimiz,Optimize,127578,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,.manylinux2014_x86_64.whl (1.1 MB); Collecting aiosignal==1.3.1; Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB); Collecting async-timeout==4.0.3; Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:33123,cache,cached,33123,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,".p.win32.Crypt32</code> - <a href=""https://github.com/shalupov""><code>@​shalupov</code></a></li>; </ul>; <h2>Bug Fixes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1411"">#1411</a>: Do not throw <code>Win32Exception</code> on success for empty section in <code>Kernel32Util#getPrivateProfileSection</code> - <a href=""https://github.com/mkarg""><code>@​mkarg</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1414"">#1414</a>: Fix definition of <code>c.s.j.p.unix.X11.XK_Shift_R</code> - <a href=""https://github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/1323"">#1323</a>. Fix crashes in direct callbacks on mac OS aarch64 - <a href=""https://github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1422"">#1422</a>: Load jawt library relative to <code>sun.boot.library.path</code> system on unix OSes - <a href=""https://github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1427"">#1427</a>: Rebuild all binaries with fix from <a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/1422"">#1422</a> and <a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/1323"">#1323</a> - <a href=""https://github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; </ul>; <h1>Release 5.10.0</h1>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/java-native-access/jna/commit/3705b849892aa3c37e5608e640eff19047811a5c""><code>3705b84</code></a> Release 5.12.1</li>; <li><a href=""https://github.com/java-native-access/jna/commit/2f919e56bad203494fe9589206d6d23f27ef4f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12438:4827,Load,Load,4827,https://hail.is,https://github.com/hail-is/hail/pull/12438,1,['Load'],['Load']
Performance,".py"", line 188, in execute; result, timings = self._rpc(ActionTag.EXECUTE, payload); File ""/usr/local/lib/python3.10/dist-packages/hail/backend/py4j_backend.py"", line 220, in _rpc; raise fatal_error_from_java_error_triplet(; hail.utils.java.FatalError: HailException: VCF spec does not support phased haploid calls. Java stack trace:; is.hail.utils.HailException: VCF spec does not support phased haploid calls.; at __C83collect_distributed_array_matrix_vcf_writer.apply_region154_245(Unknown Source); at __C83collect_distributed_array_matrix_vcf_writer.apply_region133_246(Unknown Source); at __C83collect_distributed_array_matrix_vcf_writer.apply_region1_250(Unknown Source); at __C83collect_distributed_array_matrix_vcf_writer.apply(Unknown Source); at __C83collect_distributed_array_matrix_vcf_writer.apply(Unknown Source); at is.hail.backend.BackendUtils.$anonfun$collectDArray$19(BackendUtils.scala:142); at is.hail.utils.package$.using(package.scala:665); at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:170); at is.hail.backend.BackendUtils.$anonfun$collectDArray$18(BackendUtils.scala:141); at is.hail.backend.spark.SparkBackend$$anon$5.compute(SparkBackend.scala:474); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:136); at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.127-bb535cd096c5; Error summary: HailException: VCF spec does not support phased haploid calls.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14330:3389,concurren,concurrent,3389,https://hail.is,https://github.com/hail-is/hail/issues/14330,2,['concurren'],['concurrent']
Performance,".py"", line 2629, in execute; raise JVMUserError(exception); JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:224); at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:186); at is.hail.JVMEntryway.main(JVMEntryway.java:156); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at is.hail.JVMEntryway$1.run(JVMEntryway.java:107); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); ... 7 more; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/rwalters-hail-tmp/o?name=merged_round2_sumstats.fix_lowconf.mt/entries/rows/parts/part-15801-2fde3786-67cb-42ed-8aac-f900cfcc4c00&uploadType=resumable&upload_id=ADPycduMEzX6d_uX4CiP6_XItJKmP8UnUnYBfyPoselMbyLUkxs1wDL",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950:1692,concurren,concurrent,1692,https://hail.is,https://github.com/hail-is/hail/issues/12950,1,['concurren'],['concurrent']
Performance,".rdd.RDD$$anonfun$8.apply(RDD.scala:332); at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330); at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935); at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283). Konrad Karczewski @konradjk 16:24; this should work, so i think it's a bug. but in the short run, you could hdfs dfs -cp file:///tmp/clinvar.vcf.gz / and then just load /clinvar.vcf.gz; copy to hdfs; (you shouldn't have to, but ¯\_(ツ)_/¯). bw2 @bw2 16:27; that worked. thanks!. ### What went wrong (all error messages here, including the full java stack trace):. Traceback (most recent call last):; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/load_clinvar_to_es_pipeline.py"", line 31, in <module>; vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hado",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:4354,load,load,4354,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['load'],['load']
Performance,.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.load(HttpStorageRpc.java:726); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.lambda$readAllBytes$24(StorageImpl.java:574); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:60); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1476); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:574); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:563); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:288); 	at is.hail.services.package$.retryTransientErrors(package.scala:163); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:286); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:26); 	at is.hail.backend.service.ServiceBackend$$anon$4.call(ServiceBackend.scala:239); 	at is.hail.backend.service.ServiceBackend$$anon$4.call(ServiceBackend.scala:235); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.120-f00f916faf78; Error summary: GoogleJsonResponseException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/wes-bipolar-tmp-4day/o/bge-wave-1-VQSR%2FparallelizeAndComputeWithIndex%2FgCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=%2Fresult.2706?alt=media; No such object: wes-bipolar-tmp-4day/bge-wave-1-VQSR/parallelizeAndComputeWithIndex/gCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=/result.2706; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13409:9712,concurren,concurrent,9712,https://hail.is,https://github.com/hail-is/hail/issues/13409,3,['concurren'],['concurrent']
Performance,.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.schedule,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:2602,Load,LoadVCF,2602,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['Load'],['LoadVCF']
Performance,.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-bfea6715901c; Error summary: HailException: OrderedRVD error! Unexpected key in partition 7; Range bounds for partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Key should be in partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Invalid key: [0.9986274705095608]; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4096:11103,concurren,concurrent,11103,https://hail.is,https://github.com/hail-is/hail/issues/4096,2,['concurren'],['concurrent']
Performance,.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143); 	... 34 more; Caused by: org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: java.lang.ArrayIndexOutOfBoundsException; ```. And the actual informative trace nested in the `hail.log`:; ```; Caused by: java.lang.ArrayIndexOutOfBoundsException: 1; at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.BinaryFun.apply(Fun.scala:108); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:143); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$lookupMethod$1$$anonfun$36.apply(FunctionRegistry.scala:228); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLik,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1202:7523,concurren,concurrent,7523,https://hail.is,https://github.com/hail-is/hail/issues/1202,1,['concurren'],['concurrent']
Performance,.scala:358); 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345); 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1431); 	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2276); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182); 	at scala.Option.foreach(Option.scala:407); 	at org.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936:4866,concurren,concurrent,4866,https://hail.is,https://github.com/hail-is/hail/issues/12936,1,['concurren'],['concurrent']
Performance,".scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); 	at is.hail.asm4s.ClassesBytes.load(ClassBuilder.scala:62); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:715); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:708); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1(Compile.scala:311); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1$adapted(Compile.scala:310); 	at is.hail.expr.ir.lowering.TableStageToRVD$.$anonfun$apply$9(RVDToTableStage.scala:106); 	at is.hail.sparkextras.ContextRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1234); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1233); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.105-3f053140ad00; Error summary: ClassFormatError: Too many arguments in method signature in class file __C2866stream; ```. This used to work fine in earlier Hail versions, e.g. 0.2.85.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:13570,concurren,concurrent,13570,https://hail.is,https://github.com/hail-is/hail/issues/12532,2,['concurren'],['concurrent']
Performance,.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3901:6695,concurren,concurrent,6695,https://hail.is,https://github.com/hail-is/hail/issues/3901,2,['concurren'],['concurrent']
Performance,.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3465:8321,concurren,concurrent,8321,https://hail.is,https://github.com/hail-is/hail/issues/3465,5,['concurren'],['concurrent']
Performance,.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:3802,concurren,concurrent,3802,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,2,['concurren'],['concurrent']
Performance,.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:1658,concurren,concurrent,1658,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['concurren'],['concurrent']
Performance,.service.ServiceBackendSocketAPI2$.$anonfun$main$6$adapted(ServiceBackend.scala:460); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5(ServiceBackend.scala:460); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:460); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:458); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.113-0b5bc2eb0c95; Error summary: SocketException: Connection reset; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12982:25481,concurren,concurrent,25481,https://hail.is,https://github.com/hail-is/hail/issues/12982,6,['concurren'],['concurrent']
Performance,.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBr,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:12604,load,loadClass,12604,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['load'],['loadClass']
Performance,.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAG,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:4553,Load,LoadVCF,4553,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Ite,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:11366,Load,LoadVCF,11366,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anon,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:4423,Load,LoadVCF,4423,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,".whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5128,cache,cached,5128,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,".write('data/subsetted_chr22.mt', overwrite=True)`. However this error keeps happening:; 2020-04-13 18:03:29 Hail: INFO: Number of BGEN files parsed: 1; 2020-04-13 18:03:29 Hail: INFO: Number of samples in BGEN files: 36; 2020-04-13 18:03:29 Hail: INFO: Number of variants across all BGEN files: 1255683; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/decorator.py:decorator-gen-1058>"", line 2, in write; File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/matrixtable.py"", line 2508, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Hail only supports 8-bit probabilities, found 16. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8545:1361,load,loads,1361,https://hail.is,https://github.com/hail-is/hail/issues/8545,1,['load'],['loads']
Performance,.writeRowsPartition(RowStore.scala:510); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:207); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:198); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:2002,concurren,concurrent,2002,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['concurren'],['concurrent']
Performance,".zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 7.0 failed 20 times, most recent failure: Lost task 4.19 in stage 7.0 (TID 601, mycluster-w-0.c.ukbb-all-phenos.internal, executor 2): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartitionInfo.scala:30); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:536); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:534); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.apply(ContextRDD.scala:299); at is.hail.sparkextras.Co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:2473,load,loadAddress,2473,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['load'],['loadAddress']
Performance,"/3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258; 1259 for temp_arg in temp_args:. /Library/Python/3.7/site-packages/hail/backend/spark_backend.py in deco(*args, **kwargs); 49 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 50 'Hail version: %s\n'; ---> 51 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None; 52 except pyspark.sql.utils.CapturedException as e:; 53 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz. Java stack trace:; java.io.IOException: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.concurrentGlobInternal(GoogleHadoopFileSystemBase.java:1284); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1261); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1229); 	at is.hail.io.fs.HadoopFS.listStatus(HadoopFS.scala:104); 	at is.hail.utils.Py4jUtils$class.ls(Py4jUtils.scala:55); 	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCom",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:1760,concurren,concurrentGlobInternal,1760,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrentGlobInternal']
Performance,"/code></a>]</li>; <li>Support reading BC4U and DX10 BC1 images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/6486"">#6486</a> [<a href=""https://github.com/REDxEYE""><code>@​REDxEYE</code></a>]</li>; <li>Optimize ImageStat.Stat.extrema <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7593"">#7593</a> [<a href=""https://github.com/florath""><code>@​florath</code></a>]</li>; <li>Handle pathlib.Path in FreeTypeFont <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7578"">#7578</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use list comprehensions to create transformed lists <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7597"">#7597</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></a>]</li>; <li>Added support for reading DX10 BC4 DDS images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7603"">#7603</a> [<a href=""https://github.com/sambvfx""><code>@​sambvfx</code></a>]</li>; <li>Optimized ImageStat.Stat.count <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7599"">#7599</a> [<a href=""https://github.com/florath""><code>@​florath</code></a>]</li>; <li>Moved error from truetype() to FreeTypeFont <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7587"">#7587</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Correct PDF palette size when saving <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7555"">#7555</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fixed closing file pointer with olefile 0.47 <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7594"">#7594</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>ruff: Minor optimizations of list comprehensions, x in set, etc. <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7524"">#7524</a> [<a href=""https://github.com/cclauss""><cod",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14191:8004,Optimiz,Optimized,8004,https://hail.is,https://github.com/hail-is/hail/pull/14191,3,['Optimiz'],['Optimized']
Performance,"/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:972); 	at org.apache.hadoop.util.Shell.run(Shell.java:869); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; When I dig into the container logs, the stdout is empty on most, stderr is full of warnings, but no errors:; ```; 18/03/02 15:28:07 WARN com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel: Channel for 'gs://gnomad/coverage/hail-0.2/coverage/exomes/parts/part_partition1049.vds/entries/rows/parts/part-0095' is not open.; ```; But then one machine I logged into had:; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa70eb59074, pid=4361, tid=0x00007fa707702700; #; # JRE version: OpenJDK Runtime Environment (8.0_131-b11) (build 1.8.0_131-8u131-b11-1~bpo8+1-b11); # Java VM: OpenJDK 64-Bit Server VM (25.131-b11 mixed mode ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3053:3505,concurren,concurrent,3505,https://hail.is,https://github.com/hail-is/hail/issues/3053,1,['concurren'],['concurrent']
Performance,"/dist-packages/batch/worker/worker.py"", line 2150, in run; await self.jvm.execute(local_jar_location, self.scratch, self.log_file, self.jar_url, self.argv); File ""/usr/local/lib/python3.7/dist-packages/batch/worker/worker.py"", line 2629, in execute; raise JVMUserError(exception); JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:224); at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:186); at is.hail.JVMEntryway.main(JVMEntryway.java:156); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at is.hail.JVMEntryway$1.run(JVMEntryway.java:107); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); ... 7 more; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950:1471,concurren,concurrent,1471,https://hail.is,https://github.com/hail-is/hail/issues/12950,1,['concurren'],['concurrent']
Performance,"/github-redirect.dependabot.com/sveltejs/svelte/pull/7407"">#7407</a>)</li>; </ul>; <h2>3.46.5</h2>; <ul>; <li>Add TypeScript interfaces for typing actions (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/6538"">#6538</a>)</li>; <li>Do not generate <code>unused-export-let</code> warning inside <code>&lt;script context=&quot;module&quot;&gt;</code> blocks (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7055"">#7055</a>)</li>; <li>Do not collapse whitespace-only CSS vars (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7152"">#7152</a>)</li>; <li>Add <code>aria-description</code> to the list of allowed ARIA attributes (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7301"">#7301</a>)</li>; <li>Fix attribute escaping during SSR (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7327"">#7327</a>)</li>; <li>Prevent <code>.innerHTML</code> optimization from being used when <code>style:</code> directive is present (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7386"">#7386</a>)</li>; </ul>; <h2>3.46.4</h2>; <ul>; <li>Avoid <code>maximum call stack size exceeded</code> errors on large components (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/4694"">#4694</a>)</li>; <li>Preserve leading space with <code>preserveWhitespace: true</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/4731"">#4731</a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/sveltejs/svelte/commit/52153dbce0237f0c36e4ff36377398d7f95276ef""><code>52153db</code></a> -&gt; v3.49.0</li>; <li><a href=""https://github.com/sveltejs/svelte/commit/3798808e7484b7eeee6acb2860c45bb2e59d84bd""><code>3798808</code></a> update changelog</li>; <li><a href=""https://github.com/sveltejs/svelte/commit/0fa0a38d5168a1767843fdb0a43c00a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12032:5627,optimiz,optimization,5627,https://hail.is,https://github.com/hail-is/hail/pull/12032,3,['optimiz'],['optimization']
Performance,"/hail-vdc/foo` repository. It is *not* sufficient for an image to be present in the repository and untagged or with a different tag from `latest`. In particular, every push to the `cache` tag prevents us from using other images even though they are in the registry! For example, I pushed two images to `cache`:. ```; (base) # gcloud container images list-tags gcr.io/hail-vdc/dktest; DIGEST TAGS TIMESTAMP; fb551d9bdb94 2022-06-10T14:16:39; afb4c5ad2d7b cache,latest 2022-06-10T14:15:55; ```. If I rebuild [1] the most recently pushed image with; ```; --import-cache type=registry,ref=gcr.io/hail-vdc/dktest:cache; ```; it succeeds in getting the cache. If I rebuild the other image with the same import-cache, it does not see that the (untagged) image is already there! . ---. This all suggests that all our attempts at image caching are failing terribly. Options:; 1. Only deploy builds push to a `:cache` tag, everyone uses that tag.; 2. List all the tags in the repository and include them all as --cache-from's (this doesn't actually work: https://github.com/moby/moby/issues/34715#issuecomment-425933774); 3. Push a tag for each git SHA and then include as --cache-from's the last ten git SHAs on this branch, the most recent common commit with main (i.e. `git merge-base origin/main this-branch`), maybe the current main, and maybe the PR number?; 4. Write our own OCI image builder so we can write our own OCI image cache that actually works the way it ought to (everything in the registry is considered fair game for the cache). It seems like 3 is actually a decent solution that should enable lots of caching.; 1. The last ten SHAs on the branch should speed up repeated builds when you're fixing little bugs.; 2. The most recent common commit with main should avoid rebuilds unless the packages changed.; 3. I suspect the current main is actually not helpful (either 2 will work or 3 wouldn't help).; 4. Pushing to something like `cache-11907` would allow force pushes to still access the ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800:1671,cache,cache-from,1671,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800,2,['cache'],['cache-from']
Performance,"/index; 2023-09-22 19:11:12.656 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=656384, peakBytesReadable=641.00 KiB, chunks requested=4, cache hits=2; 2023-09-22 19:11:12.656 : INFO: RegionPool: FREE: 641.0K allocated (257.0K blocks / 384.0K chunks), regions.size = 5, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:12.656 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""mes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:9842,concurren,concurrent,9842,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.344ms self 0.344ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 29.693ms self 9.419ms children 20.274ms %children 68.28%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 2.942ms self 2.942ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.506ms self 0.506ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.500ms self 0.500ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 7.523ms self 7.523ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:3337,Optimiz,OptimizePass,3337,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"/lib/py4j-0.10.4-src.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/py4j-0.10.4-src.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_conf__963896229742184890.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_conf__.zip; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:31 Client: INFO: Submitting application application_1542127286896_0174 to ResourceManager; 2019-01-22 13:11:32 YarnClientImpl: INFO: Submitted application application_1542127286896_0174; 2019-01-22 13:11:32 SchedulerExtensionServices: INFO: Starting Yarn extension services with app application_1542127286896_0174 and attemptId None; 2019-01-22 13:11:33 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:33 Client: INFO:; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: N/A; ApplicationMaster RPC port: -1; queue: default; start time: 1548180691687; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0174/; user: farrell; 2019-01-22 13:11:34 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:35 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:36 Client: INFO: Application report for applica",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:17551,queue,queue,17551,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['queue'],['queue']
Performance,"/microsoft/debugpy/issues/869"">#869</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/973"">#973</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/987"">#987</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/995"">#995</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/1008"">#1008</a></p>; <p>Improvements: <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/951"">#951</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/1001"">#1001</a></p>; <h2>debugpy v1.6.2</h2>; <p>Fixes unintentional breaking change in public API in debugpy 1.6.1 (<a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/975"">#975</a>).</p>; <p>Other fixes: <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/969"">#969</a></p>; <h2>debugpy v1.6.1</h2>; <p>debugpy API now has type annotations.</p>; <p>Optimizations based on frame evaluation API are re-enabled by default.</p>; <p>Other improvements: <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/743"">#743</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/774"">#774</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/893"">#893</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/945"">#945</a></p>; <p>Bug fixes: <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/705"">#705</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/731"">#731</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/861"">#861</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/865"">#865</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/882"">#882</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/889"">#889</a>, <a href=""https://github-redirect.d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12103:1297,Optimiz,Optimizations,1297,https://hail.is,https://github.com/hail-is/hail/pull/12103,2,['Optimiz'],['Optimizations']
Performance,"/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in written_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/utils/ui.py"", line 139, in iter; for x in it:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 560, in resp_read; decode_content=False):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream; data = self.read(amt=amt, decode_content=decode_content); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 401, in read; raise IncompleteRead(self._fp_bytes_read, self.length_remaining); File ""/usr/lib/python3.6/contextlib.py"", line 99, in __exit__; self.gen.throw(type, value, traceback); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 307, in _error_catcher; raise ReadTimeoutError(self._pool, None, 'Read timed out.'); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.; The command '/bin/sh -c python3 -m pip install --no-cache-dir -r requirements.txt -r dev-requirements.txt' returned a non-zero code: 2; [0m; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8390:3131,cache,cache-dir,3131,https://hail.is,https://github.com/hail-is/hail/issues/8390,1,['cache'],['cache-dir']
Performance,"/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API vers",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:2267,Cache,CacheDir,2267,https://hail.is,https://github.com/hail-is/hail/issues/14513,8,['Cache'],['CacheDir']
Performance,"/worker.py"", line 634, in _localize_rootfs; await self._pull_image(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 587, in _pull_image; await pull(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 566, in pull; raise ImageCannotBePulled from e; ImageCannotBePulled. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1887, in run_container; await container.run(on_completion); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 920, in run; await on_completion(*args, **kwargs); File ""/usr/lib/python3.9/contextlib.py"", line 137, in __exit__; self.gen.throw(typ, value, traceback); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 1154, in step; yield; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1873, in on_completion; await self.worker.fs.read(container.log_path),; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/fs/fs.py"", line 281, in read; async with await self.open(url) as f:; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/router_fs.py"", line 76, in open; return await fs.open(url); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 252, in open; f = await blocking_to_async(self._thread_pool, open, self._get_path(url), 'rb'); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 181, in blocking_to_async; return await asyncio.get_event_loop().run_in_executor(; File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 58, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 182, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); FileNotFoundError: [Errno 2] No such file or directory: '/batch/00a8b257731544b494247db2813c7a83/main/container.log'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13907:4182,concurren,concurrent,4182,https://hail.is,https://github.com/hail-is/hail/issues/13907,1,['concurren'],['concurrent']
Performance,"0, partition=38854, attempt=0, peakBytes=65536, peakBytesReadable=64.00 KiB, chunks requested=0, cache hits=0; 2023-09-13 16:37:38.903 : INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor48.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:208) ~[scala-library-2.12.15.jar:?]; 	at is.hail.io.StreamBlockInputBuffer.readBlock(InputBuffers.scala:552) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.BlockingInputBuffer.ensure(InputBuffers.scala:384) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.BlockingInputBuffer.readInt(InputBuffers.scala:409) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at __C16collect_distributed_array_table_coerce_sortedness.__m20I",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553:3022,concurren,concurrent,3022,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553,2,['concurren'],['concurrent']
Performance,"0,0.00,0.25,0.52;AS_ReadPosRankSum=-0.200,.,0.500,-0.220;AS_SOR=2.300,.,1.600,3.000;BaseQRankSum=0.200;DP=600000;ExcessHet=0.0477;FS=0.900;MQ=55.02;MQRankSum=-0.553;QD=1.00;ReadPosRankSum=-0.162;SOR=0.792;VarDP=650	GT:AD:DP:GQ:PGT:PID:PL:PS:SB	0/0:.:21:30	0/0:.:300:20	0/0:.:30:72	0/0:.:31:98	0|1:29,3,0,0,0:33:78:0|1:113_GG_G:78,0,1100,140,1400,1200,172,1600,1200,1000,175,1100,1100,1300,1000:113:19,19,2,1	0/0:.:20:19	0/0:.:19:20	0/0:.:25:50		0|1:90,2,0,0,0:30:40:0|1:113_GG_G:40,0,600,70,650,600,90,640,900,300,60,800,400,900,900:113:2,14,2,0	0/0:.:20:10	0/0:.:9:20	0/0:.:30:40	0/0:.:37:38		0/4:5,0,0,0,1:5:33:.:.:30,40,400,50,220,220,38,270,270,270,0,200,200,200,202:.:5,0,0,1	. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:22); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:22); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1921); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:7576,Load,LoadVCF,7576,https://hail.is,https://github.com/hail-is/hail/issues/14102,2,['Load'],['LoadVCF']
Performance,0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB); Collecting typer==0.9.0; Using cached typer-0.9.0-py3-none-any.whl (45 kB); Collecting typing,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:39443,cache,cached,39443,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform total 36.599ms self 2.385ms children 34.214ms %children 93.48%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/FoldConstants, iteration: 0 total 4.078ms self 4.078ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ExtractIntervalFilters, iteration: 0 total 0.898ms self 0.898ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 0 total 2.553ms self 0.011ms children 2.543ms %children 99.59%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/NormalizeNames, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 2.543ms self 2.543ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/Simplify, iteration: 0 total 8.675ms self 8.675ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, initial IR/Transform/ForwardLets, iteration: 0 total 4.506ms self 3.993ms children 0.513ms %children 11.40%; timing is.hail.backend.BackendHttpHa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:2934,Optimiz,Optimize,2934,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,0.10.0; Using cached parsimonious-0.10.0-py3-none-any.whl (48 kB); Collecting pillow==10.0.0; Using cached Pillow-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB); Collecting plotly==5.16.1; Using cached plotly-5.16.1-py2.py3-none-any.whl (15.6 MB); Collecting portalocker==2.7.0; Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB); Collecting protobuf==3.20.2; Using cached protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB);,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:38643,cache,cached,38643,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"0.128.0.126:36044: java.nio.channels.ClosedChannelException; java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(1, bw2-sw-dp3j.c.seqr-project.internal, 43693, None); 2019-07-14 20:55:04 BlockManagerMaster: INFO: Removed 1 successfully in removeExecutor; 2019-07-14 20:55:04 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 1 at RPC address 10.128.0.126:36052, but got no response. Marking as slave lost.; java.io.IOException: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:357); at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:334); at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507); at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481); at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420); at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122); at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:2044,concurren,concurrent,2044,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['concurren'],['concurrent']
Performance,"0/site-packages/hail/backend/backend.py:192) value = None. File .../python3.10/site-packages/hail/backend/backend.py:188, in Backend.execute(self, ir, timed); [186](.../python3.10/site-packages/hail/backend/backend.py:186) payload = ExecutePayload(self._render_ir(ir), '{""name"":""StreamBufferSpec""}', timed); [187](.../python3.10/site-packages/hail/backend/backend.py:187) try:; --> [188](.../python3.10/site-packages/hail/backend/backend.py:188) result, timings = self._rpc(ActionTag.EXECUTE, payload); [189](.../python3.10/site-packages/hail/backend/backend.py:189) except FatalError as e:; [190](.../python3.10/site-packages/hail/backend/backend.py:190) raise e.maybe_user_error(ir) from None. File .../python3.10/site-packages/hail/backend/py4j_backend.py:220, in Py4JBackend._rpc(self, action, payload); [218](.../python3.10/site-packages/hail/backend/py4j_backend.py:218) if resp.status_code >= 400:; [219](.../python3.10/site-packages/hail/backend/py4j_backend.py:219) error_json = orjson.loads(resp.content); --> [220](.../python3.10/site-packages/hail/backend/py4j_backend.py:220) raise fatal_error_from_java_error_triplet(; [221](.../python3.10/site-packages/hail/backend/py4j_backend.py:221) error_json['short'], error_json['expanded'], error_json['error_id']; [222](.../python3.10/site-packages/hail/backend/py4j_backend.py:222) ); [223](.../python3.10/site-packages/hail/backend/py4j_backend.py:223) return resp.content, resp.headers.get('X-Hail-Timings', ''). FatalError: ClassTooLargeException: Class too large: __C67907collect_distributed_array_table_collect. Java stack trace:; is.hail.relocated.org.objectweb.asm.ClassTooLargeException: Class too large: __C67907collect_distributed_array_table_collect; 	at is.hail.relocated.org.objectweb.asm.ClassWriter.toByteArray(ClassWriter.java:599); 	at is.hail.lir.Emit$.apply(Emit.scala:234); 	at is.hail.lir.Classx.$anonfun$asBytes$4(X.scala:109); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterato",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14362:6697,load,loads,6697,https://hail.is,https://github.com/hail-is/hail/issues/14362,1,['load'],['loads']
Performance,00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.032ms self 0.028ms children 0.003ms %children 10.95%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelational,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:174370,Optimiz,OptimizePass,174370,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.056ms self 0.056ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.050ms self 0.041ms children 0.010ms %children 19.41%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelational,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:31053,Optimiz,OptimizePass,31053,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/dK3o5ZfXmYSkP5TA/specs BlockBlob Hot 1264 application/octet-stream 2023-06-09T12:43:37+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/dK3o5ZfXmYSkP5TA/specs.idx BlockBlob Hot 16 application/octet-stream 2023-06-09T12:43:37+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/eOrFpVrN98GBIizi/specs BlockBlob Hot 1264 application/octet-stream 2023-06-09T12:43:34+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/eOrFpVrN98GBIizi/specs.idx BlockBlob Hot 16 application/octet-stream 2023-06-09T12:43:34+00:00; ```. I looked at the status:. ```; az storage blob download --account-name haildevtest --container test --name batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/2/31Owgv/status.json | jq '.' | less; ```. which contained an error (I un-escaped the string here):. ```; JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethod,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13160:3849,concurren,concurrent,3849,https://hail.is,https://github.com/hail-is/hail/pull/13160,1,['concurren'],['concurrent']
Performance,00ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.660ms self 0.648ms children 0.012ms %children 1.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 2.298ms self 2.298ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.050ms self 0.050ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.356ms self 0.356ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.O,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:6624,Optimiz,Optimize,6624,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,01ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.043ms self 0.043ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNon,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:33299,Optimiz,OptimizePass,33299,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,02 DAGScheduler: INFO: Shuffle files lost for executor: 13 (epoch 13); 2019-01-22 13:12:02 YarnScheduler: ERROR: Lost executor 15 on scc-q10.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:159695,concurren,concurrent,159695,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"04 tls: INFO: ssl config file found at /batch/2bbb233e4e3c4a96bbffb515019daac9/secrets/ssl-config/ssl-config.json\n2022-11-15 20:30:18.006 GoogleStorageFS$: INFO: Initializing google storage client from service account key\n2022-11-15 20:30:18.114 root: INFO: RegionPool: initialized for thread 8: pool-1-thread-1\n2022-11-15 20:30:18.114 ServiceBackend$: INFO: executing: cEPZ5IV9gUtSnCiAiHXOPs None\n2022-11-15 20:30:18.127 root: INFO: optimize optimize: darrayLowerer, initial IR: before: IR size 17: \n(Let __rng_state\n (RNGStateLiteral (0 0 0 0))\n (MakeTuple (0)\n (TableAggregate\n (TableMapRows\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (InsertFields\n (SelectFields () (SelectFields (idx) (Ref row)))\n None\n (idx (GetField idx (Ref row)))))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row)))))))))\n2022-11-15 20:30:18.146 root: INFO: optimize optimize: darrayLowerer, initial IR: after: IR size 8:\n(MakeTuple (0)\n (TableAggregate\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row))))))))\n2022-11-15 20:30:18.146 root: INFO: optimize optimize: darrayLowerer, after LowerMatrixToTable: before: IR size 8: \n(MakeTuple (0)\n (TableAggregate\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row))))))))\n2022-11-15 20:30:18.148 root: INFO: optimize optimize: darrayLowerer, after LowerMatrixToTable: after: IR size 8:\n(MakeTuple (0)\n (TableAggregate\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row))))))))\n2022-11-15 20:30:18.160 root: INFO: Aggregate: useTreeAggregate=false\n2022-11-15 20:30:18.160 root: INFO: Aggregate: commutative=false\n2022-11-15 20:30:18.163 root: INFO: optimize optimize: compileLowerer, initial IR: before: IR size 70: \n(MakeTuple (0)\n (Let __iruid_455\n (MakeStruct)\n (Let __iruid_458\n (Let global",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:1857,optimiz,optimize,1857,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,4,['optimiz'],['optimize']
Performance,05 DAGScheduler: INFO: Shuffle files lost for executor: 12 (epoch 15); 2019-01-22 13:12:05 YarnScheduler: ERROR: Lost executor 21 on scc-q12.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:181743,concurren,concurrent,181743,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,06 DAGScheduler: INFO: Shuffle files lost for executor: 14 (epoch 16); 2019-01-22 13:12:06 YarnScheduler: ERROR: Lost executor 12 on scc-q03.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:191099,concurren,concurrent,191099,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,09883' is not within the range [1-135006516] for reference genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:2225,Load,LoadPlink,2225,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,"0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting Mar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:4945,cache,cached,4945,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,0ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.06%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.004ms self 0.004ms chi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:184165,Optimiz,Optimize,184165,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,0ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.19%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.023ms self 0.023ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms chi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:191190,Optimiz,Optimize,191190,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,0ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.038ms self 0.029ms children 0.009ms %children 22.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.189ms self 0.189ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.043ms self 0.043ms chi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:17900,Optimiz,Optimize,17900,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,0ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.044ms self 0.035ms children 0.009ms %children 20.05%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.193ms self 0.193ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.042ms self 0.042ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.028ms self 0.028ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.051ms self 0.051ms chi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:10916,Optimiz,Optimize,10916,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"1-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:7103,load,load,7103,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['load'],['load']
Performance,"1. Add options to store the scores as sample annotations and the loadings as variant annotations (or, eventually, store by default and write out optionally); 2. Once LD-pruning is implemented, should it be performed first automatically? Probably not, but perhaps an option and the doc should mention the issue.; 3. PLINK has an option to use X-chromosome variants. What is it doing exactly? There are several decisions around encoding hemizygous sites for males. More importantly, does anyone use it? Should we support it?; 4. What about PCA of things other than genotypes, such as missingness? Analysts have mentioned applications to QC and flagged the latter specifically, which is implemented in GCTA.; 5. Extension to multiallelics? Probably not so important as few variants have more than two common alleles and each individual variant generally contributes little. If we did it, a good approach is probably a one-hot encoding although the variance normalization needs some care. For microsatellites/STRs a quantitative rather than categorical encoding may be better.; 6. Support for outlier detection a la SmartPCA and/or EIGENSTRAT?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/205:65,load,loadings,65,https://hail.is,https://github.com/hail-is/hail/issues/205,2,"['load', 'perform']","['loadings', 'performed']"
Performance,"1. I don't think this should be a user option. Too fine grained. 2. init has an explosion of options. I think we should have a centralized location for all the options and parameters. The feature flag stuff is a step in that direction, but needs to be more pervasive. Keep this all private/experimental for now. 3. I guess just 3 is going to get nearly all the benefit, but we'll need to track performance to know of sure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5862#issuecomment-483869816:394,perform,performance,394,https://hail.is,https://github.com/hail-is/hail/pull/5862#issuecomment-483869816,1,['perform'],['performance']
Performance,"1. I really don't understand what this code is trying to do. Can you give me a short explanation in English?. 2. I think you just need single-use Let forwarding to optimize this. It looks like:. Push down the TableCount. (TableCount (Paralellelize ...)) should turn into (ArrayLen (GetField rows ...)). Push the ArrayLen/GetField into the Let. (Is this what your ""MaximizeLets"" is doing? I think that would traditionally be called let lifting.). Now you have (Let __cols_and_globals (ArrayLen (GetField __cols (Ref __cols_and_globals))). Then you forward the single-use Let, and the rest of the code simplifies into (ArrayLen (TableCollect (TableRead ""cols""))), and that should have the static number of rows and be able to be simplified. 3. So how should single-use Let forwarding work? You need two things: to determine there is only one use, and that the single use isn't in a more expensive context, e.g. you don't want to forward a single-use Let into a loop: (Let expensive X (ArrayMap a x <use expensive once>)). Since our control flow is structured, there is a static notion of ""loop nesting depth"", e.g. in the above code, the Let has nesting depth 0, and the use in the ArrayMap body has nesting depth 1. You can only forward into the same nesting depth. 4. Final question is, do you want the let forwarding pass to also delete unused Lets? If you delete a let, that might delete references that make other lets single (or zero) use. In this case, it might be nice to build a ""use-def chain"" data structure, that keeps, for each let, its list of uses (and vice versa). Then, when you delete a Let, you can dynamically delete the references for the right hand side, possibly creating additional optimization opportunities.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5041#issuecomment-452863737:164,optimiz,optimize,164,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-452863737,4,['optimiz'],"['optimization', 'optimize']"
Performance,"1. I taught `get_or_add_user` to serialize loading of a single user's secrets; from kubernetes. The old code would issue multiple requests in parallel for; a user not yet in `users`. 2. The whole situation with the worker pool and the files in progress set did; not seem right to me. If a write comes in, we create a future corresponding; to the write and store the future in a dictionary. If a read comes in, we; first check redis, then we check if there is already a write or read of cloud; storage in progress. If there is no write nor read in progress, we initiate; a read. Before a read or write completes, it removes itself from the in-progress; files (future reads will now see the file in redis). Writes are permitted to; overwrite other writes that are already in progress.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11202:43,load,loading,43,https://hail.is,https://github.com/hail-is/hail/pull/11202,1,['load'],['loading']
Performance,"1. Lazily load the tokens on the end user machine because the first time someone logs in to the default namespace, they will have no tokens. 2. Teach `get_tokens_file` to default to the default end-user location. 3. Add a bunch of types which would have caught this error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11054:10,load,load,10,https://hail.is,https://github.com/hail-is/hail/pull/11054,1,['load'],['load']
Performance,"1. Move the classes into is.hail.rvd. They aren't used anywhere else, are they?. 2. @maccum is working over LD prune, the GeneralRDD stuff shouldn't be necessary anymore. I think she's just working on optimization, so I feel like her current code is an improvement and should go in before the performance improvements are ready. 3. I agree, but, yeah, SKAT can't go until we get expr ndarray. 4. MatrixTable.same just uses zip, you can rewrite it in terms of zip based on my above comments. You might also need a version of zip that returns a RDD[T]. Compare RVD.map and RVD.mapPartitions. 5. I think the right thing here is an OrderedRVD.orderedIntervalJoin. Compare @patrick-schultz recent OrderedRVD.orderedJoin (pending): https://github.com/hail-is/hail/pull/3159/files#diff-b173fb9bd584d50afcfa6724954ef3b5R127",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3186#issuecomment-374449723:201,optimiz,optimization,201,https://hail.is,https://github.com/hail-is/hail/pull/3186#issuecomment-374449723,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"1. NativeModule manages the lifecycle of Scala-generated C++ functions. Generate the C++; source code as a Scala String, then construct ; NativeModule(compileOptions, sourceString, forceBuild); From that object you can do getKey(): String and getBinary(): Array[Byte] to get the compiled ; code in a serializable form which can be passed to other nodes in a cluster, and then used to; construct a local NativeModule(key, binary). 2. NativeCode.java now does a two-stage bootstrap on Linux to make sure that libhail.so is; loaded with the RTLD_GLOBAL flag so that dynamic-generated C++ can use functions; defined in libhail.so. On Mac, this works ok without the bootstrap. [We needed this for the; RowStore with generated-C++ decoders]. 3. Added a NativeStatus* parameter to all NativeLongFunc's, to encourage consistent handling; of errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973:522,load,loaded,522,https://hail.is,https://github.com/hail-is/hail/pull/3973,1,['load'],['loaded']
Performance,"1. Replace kaniko with buildkit.; 2. For any repository, the `cache` tag is a dump of layers that have been built before.; 3. Allow ci to start docker jobs that are ""unconfined"" by app armor and seccomp.; 4. Add `docker-build.sh` which enforces use of buildkit, knows about the `cache` tag, and ensures the cache is updated on each build.; 5. Remove the `gcloud` binary from Dockerfile.base. Replace uses with either the gcloud-sdk image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10502:62,cache,cache,62,https://hail.is,https://github.com/hail-is/hail/pull/10502,4,['cache'],['cache']
Performance,"1. Separating the `apk update` from the `apk add` means that if the apk package repository metadata changes (say, the URL of some repository changes) and we change our `apk add` line (say we add a new package), then the `apk add` will fail (e.g. because it has an out of date URL for the repository that should contain the new package). The `apk add --no-cache ...` invocation is essentially the same as `apk update && apk add ... && rm -rf /path/to/repo/cache`. When using docker, it is good practice remove unnecessary files so that they do not get included in the ""image diff"" for that line of the Dockerfile. `apk add --no-cache ...` succinctly performs exactly what we want. 2. Keeping each package on a separate line and sorting those lines makes diffs easy-to-read with one line per new package. 3. Because `COPY` moves all the *contents* of a source folder into the destination path (creating it if it does not exist) which must be a folder, it seems more clear to say `/batch/batch/`, indicating that we are moving data into a folder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4791:355,cache,cache,355,https://hail.is,https://github.com/hail-is/hail/pull/4791,4,"['cache', 'perform']","['cache', 'performs']"
Performance,"1. Until we scale up the memory service's throughput, avoid use on the client; and the worker if there are more than 50 partitions. 2. On the driver, open no more than 100 concurrent connections to Google Cloud; Storage. 3. Set a timeout of five seconds to connect or read from Google Cloud Storage.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11688:42,throughput,throughput,42,https://hail.is,https://github.com/hail-is/hail/pull/11688,2,"['concurren', 'throughput']","['concurrent', 'throughput']"
Performance,1. Use encoded bytes to transfer result from Scala to Python; 2. Use encoded bytes for RelationalLet literals; 3. Optimize after lifting relational operations to eliminate trivial lets; 4. Avoid the memory service for large jobs (eventually we need to scale memory),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11670:114,Optimiz,Optimize,114,https://hail.is,https://github.com/hail-is/hail/pull/11670,1,['Optimiz'],['Optimize']
Performance,"1. We currently do not do a per-variant variance normalization. There should be an option to include normalization, or even a partial normalization dividing by standard deviation raised to a power between zero and one.; 2. Include an option to compute and output the ""other"" singular vectors, corresponding to the loadings of the PCs on variants rather than samples.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/100:314,load,loadings,314,https://hail.is,https://github.com/hail-is/hail/issues/100,1,['load'],['loadings']
Performance,"1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 670.88ms 898.81ms 7.89s 86.58%; Req/Sec 318.17 108.06 1.47k 74.96%; 226300 requests in 1.00m, 41.34MB read; Socket errors: connect 0, read 2053, write 0, timeout 19; Requests/sec: 3765.55; Transfer/sec: 704.34KB. Runs were interleaved two reduce chance that the programs would benefit from caching across runs. First run for each had somewhat more background tasks open. Starlette will be run later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:5761,Latency,Latency,5761,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['Latency'],['Latency']
Performance,"1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:2169,cache,cached,2169,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached pandas-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB); Collecting parsimonious==0.10.0; Using cached parsimonious-0.10.0-py3-none-any.whl (48 kB); Collecting pillow==10.0.0; Using cached Pillow-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB); Collecting plotly==5.16.1; Using cached plotly-5.16.1-py2.py3-none-any.whl (15.6 MB); Collecting portalocker==2.7.0; Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB); Collecting protobuf==3.20.2; Using cached protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:37661,cache,cached,37661,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Prom,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216969,concurren,concurrent,216969,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(spark.sparkContext); Running on Apache Spark version 2.0.2; SparkUI available at http://192.168.1.122:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-20613ed; >>> table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); 2018-02-22 20:29:45 Hail: INFO: Reading table to impute column types; 2018-02-22 20:29:45 Hail: INFO: Finished type imputation; Loading column `Sample' as type String (imputed); Loading column `Population' as type String (imputed); Loading column `SuperPopulation' as type String (imputed); Loading column `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed); >>> common_vds = hc.read('/mnt/d/metistream/hail/data/1kg.vds'); >>> common_vds = common_vds.annotate_samples_table(table, root='sa'); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; >>> common_vds = common_vds.sample_qc(); >>> common_vds = common_vds.filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); >>> common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))'); // class version 52.0 (52); // access flags 0x1; public class is/hail/codegen/generated/C0 implements java/i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:2534,Load,Loading,2534,https://hail.is,https://github.com/hail-is/hail/issues/2966,6,['Load'],['Loading']
Performance,10); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:3425,concurren,concurrent,3425,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['concurren'],['concurrent']
Performance,10); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:10251,concurren,concurrent,10251,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['concurren'],['concurrent']
Performance,109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Itera,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:13344,Load,LoadMatrix,13344,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,11ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.123ms self 0.054ms children 0.068ms %children 55.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.19%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:189695,Optimiz,Optimize,189695,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,128 MiB partitions are a much more reasonable default than 1MB. This will result; in better file read/write performance.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7957:108,perform,performance,108,https://hail.is,https://github.com/hail-is/hail/pull/7957,1,['perform'],['performance']
Performance,13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.442ms self 0.442ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.526ms self 0.526ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.660ms self 0.648ms children 0.012ms %children 1.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluat,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:5508,Optimiz,Optimize,5508,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,140ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.154ms self 0.069ms children 0.085ms %children 55.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.073ms self 0.073ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:172077,Optimiz,OptimizePass,172077,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,14ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.114ms self 0.048ms children 0.067ms %children 58.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.06%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:182670,Optimiz,Optimize,182670,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"15; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 20 times, most recent failure: Lost task 8.19 in stage 1.0 (TID 2899) (hail-test-w-1.australia-southeast1-a.c.pb-dev-312200.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open databa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:3721,cache,cache,3721,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['cache'],['cache']
Performance,18%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.153ms self 0.153ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.038ms self 0.038ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.038ms self 0.029ms children 0.009ms %children 22.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvalua,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:16783,Optimiz,Optimize,16783,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,187ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.031ms self 0.031ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.346ms self 0.121ms children 0.225ms %children 65.07%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.154ms self 0.154ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:28760,Optimiz,OptimizePass,28760,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"1894 force_bgz,; -> 1895 force; 1896 ); 1897 return MatrixTable(jmt). ~/bin/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4775:2566,Load,LoadVCF,2566,https://hail.is,https://github.com/hail-is/hail/issues/4775,1,['Load'],['LoadVCF']
Performance,"1</a> [component: bokehjs] Save tool in gridplot initiates multiple downloads</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/8684"">#8684</a> Allow at least partial alignment of fixed sized frames</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9113"">#9113</a> [component: bokehjs] Empty group widgets don't size properly once populated</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9133"">#9133</a> [BUG] Tabs ignore explicitly set dimensions</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9208"">#9208</a> [component: bokehjs] [BUG] sizing_mode='stretch_width' makes plot too wide if scrollbar is showing</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9320"">#9320</a> [BUG] Bokeh rendering performance</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9448"">#9448</a> [component: bokehjs] [BUG] Google Fonts not loading on Glyph on standalone HTML until interacting with Glyph</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9744"">#9744</a> [component: bokehjs] [BUG] bokeh server layout overlap on toggle visibility</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9763"">#9763</a> [BUG] <code>gridplot</code> <code>merge_tools</code> removes distinct tools it thinks are repeated, e.g., <code>xpan</code> and <code>ypan</code></li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9764"">#9764</a> [component: bokehjs] [BUG] MultiChoice placeholder text not displayed</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9992"">#9992</a> [component: bokehjs] [BUG] Select widget hiding tabs, when selecting a plot</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/10125"">#10125</a> [component: bokehjs] [BUG] widgets overlap each other</li>; <li><a href=""https://github-redirect.dependabot.com/b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12454:3561,load,loading,3561,https://hail.is,https://github.com/hail-is/hail/pull/12454,1,['load'],['loading']
Performance,"1_70_65,GAAGTCTCATCCCATC-1_45;AACGTCAGTGTTTCAG-1_74:1:19:255	0/1:2,4:6:60:101,0,60:.:.:.:.; chr1	12198	rs62635282	G	C	69.6	.	BaseQRankSum=0;ClippingRankSum=0;DB;ExcessHet=3.0103;FS=4.771;MQ=42;MQRankSum=-0.967;QD=23.2;ReadPosRankSum=0.967;SOR=2.225;CSQ=C|non_coding_transcript_exon_variant|MODIFIER|DDX11L1|ENSG00000223972|Transcript|ENST00000450305|transcribed_unprocessed_pseudogene|2/6||||68|||||||1||HGNC|HGNC:37102|||||||||||||||||||||||||||,C|non_coding_transcript_exon_variant|MODIFIER|DDX11L1|ENSG00000223972|Transcript|ENST00000456328|processed_transcript|1/3||||330|||||||1||HGNC|HGNC:37102|||||||||||||||||||||||||||,C|downstream_gene_variant|MODIFIER|WASH7P|ENSG00000227232|Transcript|ENST00000488147|unprocessed_pseudogene|||||||||||2206|-1||HGNC|HGNC:38034|||||||||||||||||||||||||||;DP=3;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1	GT:AD:DP:GQ:PL	./.:.:.:.:.	0/1:1,2:3:37:77,0,37; ```. Getting this error message:; ```; INFO: [pid 11941] Worker Worker(salt=943636132, workers=1, host=seqr-loading-cluster-m, username=root, pid=11941) running SeqrVCFToMTTask(source_paths=gs://seqr-bw/merged_phased_3P5CH.split.vcf.gz, dest_path=gs://seqr-bw/merged_phased_3P5CH.mt, genome_version=38, vep_runner=VEP, reference_ht_path=gs://seqr-reference-data/GRCh38/all_reference_data/combined_reference_data_grch38.ht, clinvar_ht_path=gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht, hgmd_ht_path=None, sample_type=WGS, validate=False, dataset_type=VARIANTS, remap_path=, subset_path=, vep_config_json_path=); Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.5; SparkUI available at http://seqr-loading-cluster-m.c.seqr-project.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-914bd8a10ca2; LOGGING: writing to /tmp/c7e0443c47b54e91b295e2bff7b554b9/hail-20200405-1408-0.2.34-914bd8a10ca2.log; {'_Task__hash': -3818947167740532127,; 'clinvar_ht_path': 'gs://seqr-reference-data/GRCh38/clinva",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:36112,load,loading-cluster-m,36112,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['load'],['loading-cluster-m']
Performance,"1f8/00010000\n\tat is.h...ava:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\n'; error_id = -1. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); > raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; E hail.utils.java.FatalError: RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000; E ; E Java stack trace:; E java.lang.RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000; E 	at is.hail.annotations.Memory.checkAddress(Memory.java:226); E 	at is.hail.annotations.Memory.loadInt(Memory.java:140); E 	at is.hail.annotations.Region$.loadInt(Region.scala:20); E 	at __C92844etypeDecode.__m92863ord_compareNonnull(Unknown Source); E 	at __C92844etypeDecode.__m92862ord_compareNonnull(Unknown Source); E 	at __C92844etypeDecode.__m92861ord_ltNonnull(Unknown Source); E 	at __C92844etypeDecode.__m92860ord_lt(Unknown Source); E 	at __C92844etypeDecode.__m92857arraySorter_merge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:3811,load,loadInt,3811,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['load'],['loadInt']
Performance,"2 19:11:12.656 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""domain"": ""global"",; ""reason"": ""forbidden""; }; ]; }; }. 	at is.hail.relocated.com.google.cloud.storage.StorageException.translate(St",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:10188,concurren,concurrent,10188,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,2 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB); Collecting typer==0.9.0; Using cached typer-0.9.0-py3-none-any.whl (45 kB); Collecting typing-extensions==4.7.1; Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB); Collecting tzdata==2023.3; Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB); Collecting urllib3==1.26.16; Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB); Collecting uvloop==0.17.0; Using cached uvloop-0.17.0-cp39-cp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:39767,cache,cached,39767,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,2 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB); Collecting typer==0.9.0; Using cached typer-0.9.0-py3-none-any.whl (45 kB); Collecting typing-extensions==4.7.1; Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB); Collecting tzdata==2023.3; Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB); Collecting urllib3==1.26.16; Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB); Collecting uvloop==0.17.0; Using cached uvloop-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB); Collecting wrapt==1.15.0; Using cached wrapt-1.15.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB); Collecting xyzservices==2023.7.0; Using cached xyzservices-2023.7.0-py3-none-any.whl (56 kB); Collecting yarl==1.9.2; Using cached yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB); Building wheels for collected packages: avro; Building wheel for avro (pyproject.toml): started; Building wheel for avro (pyproject.toml): finished with status 'done'; Created wheel for avro: filen,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:40390,cache,cached,40390,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"2); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.21-1d317a44e5fd; Error summary: NoSuchElementException: key not found: GRCh37; ```. ### Error No. 2; ```python; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj); 343 method = get_real_method(obj, self.print_method); 344 if method is not None:; --> 345 return method(); 346 return None; 347 else:. /usr/local/lib/python3.6/site-packages/hail/matrixtable.py in _repr_html_(self); 2524 ; 2525 def _repr_html_(self):; -> 2526 s = self.table_show._repr_html_(); 2527 if self.displayed_n_cols != self.actual_n_cols:; 2528 s += '<p style=""background: #fdd; padding: 0.4em;"">'. /usr/local/lib/python3.6/site-packages/hail/table.py in ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:25945,concurren,concurrent,25945,https://hail.is,https://github.com/hail-is/hail/issues/7044,1,['concurren'],['concurrent']
Performance,2); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); 	at is.hail.expr.ir.IRParser$.type_field(Parser.scala:324); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$.repsepUntil(Parser.scala:289); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.apply(Parser.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:19604,concurren,concurrent,19604,https://hail.is,https://github.com/hail-is/hail/issues/7044,2,['concurren'],['concurrent']
Performance,2); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); 	at is.hail.expr.ir.IRParser$.type_field(Parser.scala:324); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$.repsepUntil(Parser.scala:289); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.appl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:7881,concurren,concurrent,7881,https://hail.is,https://github.com/hail-is/hail/issues/7044,2,['concurren'],['concurrent']
Performance,"2-11-15 20:30:18.160 root: INFO: Aggregate: commutative=false\n2022-11-15 20:30:18.163 root: INFO: optimize optimize: compileLowerer, initial IR: before: IR size 70: \n(MakeTuple (0)\n (Let __iruid_455\n (MakeStruct)\n (Let __iruid_458\n (Let global\n (Ref __iruid_455)\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (InitOp 0 (Collect (CollectStateSig +PInt32)) ()))\n (MakeTuple (0)\n (AggStateValue 0 (CollectStateSig +PInt32)))))\n (Let __iruid_460\n (CollectDistributedArray\n table_aggregate_singlestage __iruid_456\n __iruid_459\n (ToStream False\n (Literal Array[Struct{start:Int32,end:Int32}]\n <literal value>))\n (MakeStruct\n (__iruid_455 (Ref __iruid_455))\n (__iruid_458 (Ref __iruid_458)))\n (Let __iruid_458\n (GetField __iruid_458 (Ref __iruid_459))\n (Let __iruid_455\n (GetField __iruid_455 (Ref __iruid_459))\n (Let global\n (Ref __iruid_455)\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (Begin\n (InitFromSerializedValue 0 \n (CollectStateSig +PInt32)\n (GetTupleElement 0 (Ref __iruid_458))))\n (StreamFor row\n (StreamMap __iruid_457\n (StreamRange -1 True\n (GetField start (Ref __iruid_456))\n (GetField end (Ref __iruid_456))\n (I32 1))\n (MakeStruct (idx (Ref __iruid_457))))\n (Begin\n (SeqOp 0 (Collect (CollectStateSig +PInt32))\n ((GetField idx (Ref row)))))))\n (MakeTuple (0)\n (AggStateValue 0 (CollectStateSig +PInt32)))))))\n (NA String))\n (Let global\n (Ref __iruid_455)\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (Begin\n (InitFromSerializedValue 0 \n (CollectStateSig +PInt32)\n (GetTupleElement 0 (Ref __iruid_458))))\n (StreamFor __iruid_461\n (ToStream True (Ref __iruid_460))\n (Begin\n (CombOpValue 0 (Collect (CollectStateSig +PInt32))\n (GetTupleElement 0 (Ref __iruid_461))))))\n (Let __iruid_454\n (MakeTuple (0)\n (ResultOp 0 (Collect (CollectStateSig +PInt32))))\n (MakeStruct\n (idx\n (GetTupleElement 0 (Ref __iruid_454)))))))))))\n2022-11-15 20:30:18.168 root: INFO: Prune: MakeStruct: eliminating field '__iruid_455'\n2022-11-15 20:3",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:2825,optimiz,optimize,2825,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,4,['optimiz'],['optimize']
Performance,"2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 20050, localhost): org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'; 	at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:247); 	at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:545); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Unsupported/Unknown Elasticsearch version 6.0.0; 	at org.elasticsearch.hadoop.util.EsMajorVersion.parse(EsMajorVersion.java:79); 	at org.elasticsearch.hadoop.rest.RestClient.remoteEsVersion(RestClient.java:613); 	at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:240); 	... 10 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.for",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4138:2494,concurren,concurrent,2494,https://hail.is,https://github.com/hail-is/hail/issues/4138,1,['concurren'],['concurrent']
Performance,"2.013327 ms; 2018-10-09 14:46:41 Executor: INFO: Finished task 0.0 in stage 0.0 (TID 0). 972 bytes result sent to driver; 2018-10-09 14:46:41 TaskSetManager: INFO: Finished task 0.0 in stage 0.0 (TID 0) in 406 ms on localhost (executor driver) (1/1); 2018-10-09 14:46:41 TaskSchedulerImpl: INFO: Removed TaskSet 0.0, whose tasks have all completed, from pool ; 2018-10-09 14:46:41 DAGScheduler: INFO: ResultStage 0 (collect at utils.scala:44) finished in 0.420 s; 2018-10-09 14:46:41 DAGScheduler: INFO: Job 0 finished: collect at utils.scala:44, took 0.669318 s; 2018-10-09 14:46:41 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:41 SparkSqlParser: INFO: Parsing command: table7e606a8b83f4; 2018-10-09 14:46:41 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:41 SparkSqlParser: INFO: Parsing command: CACHE TABLE `table7e606a8b83f4`; 2018-10-09 14:46:41 SparkSqlParser: INFO: Parsing command: `table7e606a8b83f4`; 2018-10-09 14:46:41 CodeGenerator: INFO: Code generated in 17.141549 ms; 2018-10-09 14:46:41 CodeGenerator: INFO: Code generated in 10.417049 ms; 2018-10-09 14:46:41 SparkContext: INFO: Starting job: sql at NativeMethodAccessorImpl.java:0; 2018-10-09 14:46:41 DAGScheduler: INFO: Registering RDD 12 (sql at NativeMethodAccessorImpl.java:0); 2018-10-09 14:46:41 DAGScheduler: INFO: Got job 1 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions; 2018-10-09 14:46:41 DAGScheduler: INFO: Final stage: ResultStage 2 (sql at NativeMethodAccessorImpl.java:0); 2018-10-09 14:46:41 DAGScheduler: INFO: Parents of final stage: List(ShuffleMapStage 1); 2018-10-09 14:46:41 DAGScheduler: INFO: Missing parents: List(ShuffleMapStage 1); 2018-10-09 14:46:41 DAGScheduler: INFO: Submitting ShuffleMapStage 1 (MapPartitionsRDD[12] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents; 2018-10-09 14:46:41 MemoryStore: INFO: Bl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:35800,CACHE,CACHE,35800,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['CACHE'],['CACHE']
Performance,2.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3516:3318,concurren,concurrent,3318,https://hail.is,https://github.com/hail-is/hail/issues/3516,1,['concurren'],['concurrent']
Performance,"2.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Under ""Failed Stages"", these were the details for what I was running:; ```; org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:456); is.hail.sparkextras.ContextRDD.head(ContextRDD.scala:433); is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:285); is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:21); is.hail.rvd.RVD$class.takeAsBytes(RVD.scala:243); is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:21); is.hail.rvd.RVD$class.take(RVD.scala:247); is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:21); is.hail.table.Table.take(Table.scala:990); is.hail.table.Table.showString(Table.scala:1031); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); java.la",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508:6128,concurren,concurrent,6128,https://hail.is,https://github.com/hail-is/hail/issues/3508,1,['concurren'],['concurrent']
Performance,2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216169,concurren,concurrent,216169,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,2/TopMed_8k.853.vcf.bgz \ splitmulti \; write -o TOPMed.6998.chr22.vds`. `[Stage 0:====================================================> (52 + 4) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: importvcf: caught exception: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer; at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:143); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:142); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.rdd.OrderedRDD$.calculateKeyRanges(OrderedRDD.scala:142); at org.apache.spark.rdd.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/669:1215,Load,LoadVCF,1215,https://hail.is,https://github.com/hail-is/hail/issues/669,1,['Load'],['LoadVCF']
Performance,"2079a85bce (41 minutes):. I could try to make the tests even more fine-grained and split up even more long-running tests. Seems like some of the bottlenecks I'm hitting now are:; 1. Introducing an image with the wheel already installed isn't worthwhile, it adds 2.5 min latency.; 2. The large number of splits often requires default Hail to scale up adding a 2min delay (It would be great to get that down). I'm gonna revert the change that added images and maybe try to reduce service backend parallelism a bit. 36 minutes is an improvement. We should probably focus on making Hail faster rather than trying to squeeze lower latency out of parallelism. <img width=""2032"" alt=""Screen Shot 2023-05-22 at 12 30 47"" src=""https://github.com/hail-is/hail/assets/106194/aaa3fbb7-176d-4487-b65e-586c235e2089"">; <img width=""541"" alt=""Screen Shot 2023-05-22 at 12 31 23"" src=""https://github.com/hail-is/hail/assets/106194/016f1089-d08d-4555-ae86-c01353f39c78"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13076#issuecomment-1557548015:145,bottleneck,bottlenecks,145,https://hail.is,https://github.com/hail-is/hail/pull/13076#issuecomment-1557548015,3,"['bottleneck', 'latency']","['bottlenecks', 'latency']"
Performance,22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:36199,cache,cached,36199,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"23-09-22 19:11:12.656 : INFO: RegionPool: FREE: 641.0K allocated (257.0K blocks / 384.0K chunks), regions.size = 5, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:12.656 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:10011,concurren,concurrent,10011,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,23ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 4.611ms self 3.590ms children 1.020ms %children 22.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.442ms self 0.442ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.526ms self 0.526ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.660ms self 0.648ms children 0.012ms %children 1.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:5130,Optimiz,Optimize,5130,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.It,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:18968,Load,LoadVCF,18968,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,"25 25780 (InsnX ISHL; 31026 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2346null Z; 31027 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31028 25782 (LdcX 0 I))); 31029 25783 (InsnX ISHL; 31030 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2348null Z; 31031 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31032 25785 (LdcX 1 I))); 31033 25786 (InsnX ISHL; 31034 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2350null Z; 31035 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31036 25788 (LdcX 2 I))); 31037 25789 (InsnX ISHL; 31038 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2352null Z; 31039 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31040 25791 (LdcX 3 I))))); 31041 (ReturnX). # Elsewhere, this split method is called, then the resulting field is loaded and written to the output buffer. 11325 (MethodStmtX INVOKEVIRTUAL __C1527collect_distributed_array.__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616_region0_0 (L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)V; 11326 (LoadX arg:0 L__C1527collect_distributed_array;); 11327 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 11328 25772 (MethodStmtX INVOKEINTERFACE is/hail/io/OutputBuffer.writeByte (B)Vinterface; 11329 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2354null Lis/hail/io/OutputBuffer;; 11330 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 11331 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:5422,Load,LoadX,5422,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['Load'],['LoadX']
Performance,"25 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:6169,cache,cached,6169,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,25ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.315ms self 0.116ms children 0.199ms %children 63.18%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.153ms self 0.153ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.038ms self 0.038ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.038ms self 0.029ms children 0.009ms %children 22.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:16405,Optimiz,Optimize,16405,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,28ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.328ms self 0.115ms children 0.213ms %children 64.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.164ms self 0.164ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.044ms self 0.035ms children 0.009ms %children 20.05%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:9421,Optimiz,Optimize,9421,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"2de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip; Copying file://build/distributions/hail-python.zip [Content-Type=application/zip]...; / [0 files][ 0.0 B/ 1.4 MiB] ; / [1 files][ 1.4 MiB/ 1.4 MiB] ; Operation completed over 1 objects/1.4 MiB. . real	0m2.852s; user	0m1.179s; sys	0m0.429s; + cluster start ci-test-6boype3d --master-machine-type n1-standard-1 --master-boot-disk-size 40 --worker-machine-type n1-standard-1 --worker-boot-disk-size 40 --version 0.2 --spark 2.2.0 --max-idle 10m --bucket=hail-ci-0-1-dataproc-staging-bucket --jar gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar --zip gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip --vep; Waiting on operation [projects/broad-ctsa/regions/global/operations/2b6b5772-e45f-3873-be2f-0e04327d29d7].; Waiting for cluster creation operation...; WARNING: For PD-Standard, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; .....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:1563,perform,performance,1563,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518,2,['perform'],['performance']
Performance,"2ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable total 2.591ms self 0.010ms children 2.581ms %children 99.63%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Verify total 0.011ms self 0.011ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform total 2.548ms self 0.302ms children 2.246ms %children 88.15%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/FoldConstants, iteration: 0 total 0.307ms self 0.307ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ExtractIntervalFilters, iteration: 0 total 0.016ms self 0.016ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/NormalizeNames, iteration: 0 total 0.363ms self 0.004ms children 0.358ms %children 98.82%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/NormalizeNames, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.358ms self 0.358ms childr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:11040,Optimiz,Optimize,11040,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,3); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:78); at org.apache.spark.network.client.TransportResponseHandler.failOutstandingRequests(TransportResponseHandler.java:117); at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:108); at io.netty.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:219058,concurren,concurrent,219058,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"3-4x increase in job-scheduling throughput. ## Benchmarking; Below are before-and-after profiles of the same benchmark (30,000 1s jobs) under the proposed higher rate limit, showing CPU time:; <img width=""1889"" alt=""Screen Shot 2022-03-21 at 5 10 13 PM"" src=""https://user-images.githubusercontent.com/24440116/159364769-6fd60840-5745-40ab-802e-68b8d4f32078.png"">; <img width=""1885"" alt=""Screen Shot 2022-03-21 at 5 12 33 PM"" src=""https://user-images.githubusercontent.com/24440116/159364787-ca7ec307-877d-479c-9c19-8746b5e82eab.png"">. Looking at Wall time, the before profile is nearly identical because at the current rate limit the driver uses 100% of its CPU shares under this benchmark. On this branch, CPU utilization drops to 40-60%, giving the following wall time profile:; <img width=""1879"" alt=""Screen Shot 2022-03-21 at 5 30 39 PM"" src=""https://user-images.githubusercontent.com/24440116/159367182-0830d6ff-3b6f-4fa7-8004-0fc43283ec4a.png"">. So we can be confident that driver CPU is no longer a bottleneck even in the increased rate limit. ## So what's the bottleneck now?; Since the higher rate limit still leaves the driver plenty of CPU room (I've seen it peak at 60% of a vCPU), why not crank it higher? Well, we're increasing concurrency so latent deadlocks start to be a bigger issue again. We start to see tens of deadlocks per second in the proposed rate limit and hundreds at higher rate limits. As a result, we're spending more cycles repeating queries instead of actually scheduling faster. Next steps should focus on eliminating deadlocks before we can continue to max out CPU use. ## Miscellaneous; We've lumped in a few other monitoring changes that were helpful in this process and tried to leave the commits tidy. The one potentially rude change is enforcing a minimum wait time of half a second for `run_if_changed` loops. This dramatically reduced the number of scheduling loop invocations we were executing, greatly reducing the number of `compute_fair_share` queries, wh",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11638:1380,bottleneck,bottleneck,1380,https://hail.is,https://github.com/hail-is/hail/pull/11638,1,['bottleneck'],['bottleneck']
Performance,"3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Col",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3383,cache,cached,3383,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,3.3 MB); Collecting plotly==5.16.1; Using cached plotly-5.16.1-py2.py3-none-any.whl (15.6 MB); Collecting portalocker==2.7.0; Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB); Collecting protobuf==3.20.2; Using cached protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:38818,cache,cached,38818,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.019ms self 0.016ms children 0.003ms %children 16.42%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:218732,Optimiz,OptimizePass,218732,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:210711,Optimiz,OptimizePass,210711,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.30%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:202759,Optimiz,OptimizePass,202759,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.024ms self 0.021ms children 0.003ms %children 13.55%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.238ms self 0.238ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEval,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:198107,Optimiz,OptimizePass,198107,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"3/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; WARNING: Hail primarily tested with Spark 3.3.0, use other versions at your own risk. > Task :shadedazure:compileJava NO-SOURCE; > Task :shadedazure:processResources NO-SOURCE; > Task :shadedazure:classes UP-TO-DATE; > Task :shadedazure:shadowJar; > Task :compileJava NO-SOURCE; > Task :compileScala; > Task :processResources; > Task :classes; > Task :shadowJar. BUILD SUCCESSFUL in 4m 20s; 4 actionable tasks: 4 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; WARNING: The wheel package is not available.; WARNING: The wheel package is not available.; installing to build/bdist.linux-x86_64/wheel; creating build/bdist.linux-x86_64/wheel/hail-0.2.124.dist-info/WHEEL; creating 'dist/hail-0.2.124-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it; adding 'hail/__init__.py'; adding 'hail/builtin_references.py'; adding 'hail/conftest.py'; adding 'hail/context.py'; adding 'hail/hail_logging.py'; adding 'hail/hail_pip_version'; adding 'hail/hail_revision'; adding 'hail/hail_version'; adding 'hail/matrixtable.py'; adding 'hail/table.py'; adding 'hail/backend/__init__.py'; adding 'hail/backend/backend.py'; adding 'hail/backend/hail-all-spark.jar'; adding 'hail/backend/local_backend.py'; adding 'hail/backend/py4j_backend.py'; adding 'hail/backend/service_backend.py'; adding 'hail/backend/spark_backend.py'; adding 'hail/experimental/__init__.py'; a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:16823,cache,cache,16823,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cache']
Performance,30ms self 0.027ms children 0.003ms %children 11.06%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass total 0.035ms self 0.018ms children 0.018ms %childre,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:184483,Optimiz,OptimizePass,184483,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,30ms self 0.027ms children 0.003ms %children 11.19%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.023ms self 0.023ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply total 20.227ms self 0.484ms children 19.744ms %children 97.61%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:191508,Optimiz,OptimizePass,191508,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.06%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvalua,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:183048,Optimiz,Optimize,183048,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"31053, 0.31348, 0.31638, 0.32141, 0.32747, 0.33643, 0.34130, 0.34292, 0.34890, 0.34984; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: beta = Map(intercept -> 0.0370042272400176, sa.cov -> -0.012886009824596447); 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: sigmaG2 = 0.13829390418697945; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: sigmaE2 = 0.8304138510277874; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: delta = 6.004703214575758; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: h2 = 0.1427612233333665; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: seH2 = 0.13770872661270844; 2017-08-28 21:47:48 Hail: INFO: Reading table with no type imputation; Loading column `Sample' as type `String' (type not specified); Loading column `Cov1' as type `Float64' (user-specified); Loading column `Cov2' as type `Float64' (user-specified). 2017-08-28 21:47:48 Hail: INFO: Reading table with no type imputation; Loading column `Sample' as type `String' (type not specified); Loading column `Pheno' as type `Float64' (user-specified). 2017-08-28 21:47:48 Hail: INFO: No multiallelics detected.; 2017-08-28 21:47:48 Hail: INFO: Coerced sorted dataset; 2017-08-28 21:47:48 Hail: WARN: called redundant `filtermulti' on an already split or multiallelic-filtered VDS; 2017-08-28 21:47:48 Hail: INFO: rrm: Computing Realized Relationship Matrix...; 2017-08-28 21:47:49 Hail: INFO: rrm: RRM computed using 3 variants.; 2017-08-28 21:47:49 Hail: WARN: 1 of 8 samples have a missing phenotype or covariate.; 2017-08-28 21:47:49 Hail: INFO: lmmreg: running lmmreg on 7 samples with 3 sample covariates including intercept...; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Computing eigendecomposition of kinship matrix...; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Estimating delta using REML... ; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Evals 1 to 7: 3.09757, 2.66667, 2.23576, 0.00000, 0.00000, -0.00000, -0.00000; 2017-08-28 21:47:49 Hail: INFO: lmmreg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835:5464,Load,Loading,5464,https://hail.is,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835,2,['Load'],['Loading']
Performance,34+00:00; ```. I looked at the status:. ```; az storage blob download --account-name haildevtest --container test --name batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/2/31Owgv/status.json | jq '.' | less; ```. which contained an error (I un-escaped the string here):. ```; JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	... 7 more; Caused by: is.hail.backend.service.EndOfInputException; 	at is.hail.backend.service.ServiceBackendSocketAPI2.read(ServiceBackend.scala:497); 	at is.hail.backend.service.ServiceBackendSocketAPI2.readInt(ServiceBackend.scala:510); 	at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:561); 	at is.h,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13160:4430,concurren,concurrent,4430,https://hail.is,https://github.com/hail-is/hail/pull/13160,1,['concurren'],['concurrent']
Performance,"35); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.schedule",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:6161,Load,LoadVCF,6161,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,35006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:6082,Load,LoadPlink,6082,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,37 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB); Collecting typer==0.9.0; Using cached typer-0.9.0-py3-none-any.whl (45 kB); Collecting typing-extensions==4.7.1; Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB); Collecting tzdata==2023.3; Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB); Collecting urllib3==1.26.16; Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB); Collecting uvloop==0.17.0; Using cached uvloop-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB); Collecting wrapt==1.15.0; Using cached wrapt-1.15.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB); Collecting xyzservices==2023.7.0; Using cached xyzservices-2023.7.0-py3-none-any.whl (56 kB); Collecting yarl==1.9.2; Using cached yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB); Building wheels for collected packages: avro; Building wheel for avro (pyproject.toml): started; Building wheel for avro (pyproject.toml): finished with status 'done'; Created wheel for avro: filename=avro-1.11.2-py2.py3-none-any.whl size=119738 sha256=d7f238f86de270b449b018590930a06270766887328bdb51066eccff2cd696a6; Stored in directory: /home/hadoop/.cache/pip/wheels/e3/a2/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:40568,cache,cached,40568,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$2(Worker.scala:134) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]; 	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]; 	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]; 	... 3 more; Caused by: javax.crypto.AEADBadTagException: Tag mismatch!; 	at com.sun.crypto.provider.GaloisCounterMode.decryptFinal(GaloisCounterMode.java:620) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.finalNoPadding(CipherCore.java:1116) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.fillOutputBuffer(CipherCore.java:1053) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.doFinal(CipherCore.java:941) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.AESCipher.engineDoFinal(AESCipher.java:491) ~[sunjce_provider.jar:1.8.0_392]; 	at javax.crypto.CipherSpi.bufferCrypt(CipherSpi.java:779) ~[?:1.8.0_392]; 	at javax.crypto.CipherSpi.engineDoFinal(CipherSpi.java:730) ~[?:1.8.0_392]; 	at javax.crypto.Cipher.doFinal(Cipher.java:2463) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLCipher$T12GcmReadCipherGenerator$GcmReadCipher.decryp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:7479,concurren,concurrent,7479,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,1,['concurren'],['concurrent']
Performance,38ms self 0.029ms children 0.009ms %children 22.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.189ms self 0.189ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.043ms self 0.043ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LiftRelationalValuesToRelationalLets total 1.058ms self 0.966ms children 0.092ms %chil,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:18218,Optimiz,OptimizePass,18218,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,4 concurrent builds for 10 engineers with a couple branches each just isn't tenable.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7728:2,concurren,concurrent,2,https://hail.is,https://github.com/hail-is/hail/pull/7728,1,['concurren'],['concurrent']
Performance,"4.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Coll",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:6077,cache,cached,6077,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,44ms self 0.035ms children 0.009ms %children 20.05%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.193ms self 0.193ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.042ms self 0.042ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.028ms self 0.028ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.051ms self 0.051ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerMatrixToTablePass total 4.041ms self 3.971ms children 0.070ms %children 1.74%; is,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:11234,Optimiz,OptimizePass,11234,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"482268176.apply$mcV$sp(Unknown Source); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:77); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$4(ServiceBackend.scala:127); 	at is.hail.backend.service.ServiceBackend$$Lambda$2191/716305671.apply$mcV$sp(Unknown Source); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.concurrent.Future$$$Lambda$2188/1126720330.apply(Unknown Source); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.Future$$Lambda$2189/609808342.apply(Unknown Source); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.Promise$$Lambda$2190/183883584.apply(Unknown Source); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). ""pool-2-thread-1"" #26 prio=5 os_prio=0 tid=0x00007f502866e000 nid=0x88c waiting on condition [0x00007f50275fd000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method); 	at is.hail.services.package$.sleepAndBackoff(package.scala:32); 	at is.hail.services.package$.retryTransientErrors(package.scala:86); 	at is.hail.services.Requester.requestWithHandler(Requester.scala:69); 	at is.hail.services.Requester.request(Requester.scala:94); 	at is.hail.services.memory_client.MemoryClient.write(MemoryClient.scala:45); 	at is.hail.io.fs.ServiceCacheableFS$$anon$1.close(ServiceCacheableFS.scala:46); 	at java.io.FilterOutput",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:3564,concurren,concurrent,3564,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['concurren'],['concurrent']
Performance,49); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iter,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:11236,Load,LoadVCF,11236,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,490ms self 0.008ms children 1.482ms %children 99.45%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.013ms self 0.013ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.460ms self 0.066ms children 1.394ms %children 95.49%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.654ms self 0.654ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.072ms self 0.072ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:194232,Optimiz,Optimize,194232,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:10554,Load,LoadMatrix,10554,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,"4ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;); 31019 25774 (InsnX I2B; 31020 25775 (InsnX IOR; 31021 25776 (InsnX IOR; 31022 25777 (InsnX IOR; 31023 25778 (InsnX IOR; 31024 25779 (LdcX 0 I); 31025 25780 (InsnX ISHL; 31026 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2346null Z; 31027 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31028 25782 (LdcX 0 I))); 31029 25783 (InsnX ISHL; 31030 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2348null Z; 31031 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31032 25785 (LdcX 1 I))); 31033 25786 (InsnX ISHL; 31034 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2350null Z; 31035 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31036 25788 (LdcX 2 I))); 31037 25789 (InsnX ISHL; 31038 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2352null Z; 31039 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31040 25791 (LdcX 3 I))))); 31041 (ReturnX). # Elsewhere, this split method is called, then the resulting field is loaded and written to the output buffer. 11325 (MethodStmtX INVOKEVIRTUAL __C1527collect_distributed_array.__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616_region0_0 (L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)V; 11326 (LoadX arg:0 L__C1527collect_distributed_array;); 11327 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 11328 25772 (MethodStmtX INVOKEINTERFACE is/hail/io/OutputBuffer.writeByte (B)Vinterface; 11329 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2354null Lis/hail/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:5156,Load,LoadX,5156,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['Load'],['LoadX']
Performance,"5.8 GB; 17/01/17 09:24:46 INFO SparkEnv: Registering OutputCommitCoordinator; 17/01/17 09:24:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/01/17 09:24:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://129.94.72.55:4040; 17/01/17 09:24:46 INFO Executor: Starting executor ID driver on host localhost; 17/01/17 09:24:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37833.; 17/01/17 09:24:46 INFO NettyBlockTransferService: Server created on 129.94.72.55:37833; 17/01/17 09:24:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 129.94.72.55, 37833); 17/01/17 09:24:46 INFO BlockManagerMasterEndpoint: Registering block manager 129.94.72.55:37833 with 15.8 GB RAM, BlockManagerId(driver, 129.94.72.55, 37833); 17/01/17 09:24:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 129.94.72.55, 37833); hail: info: running: read test.in.vds; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; hail: info: running: annotatevariants expr -c 'va = {}'; hail: info: running: write -o test.out.vds; [Stage 1:==> (1 + 24) / 25]hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:149); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); at org.apache.spark.sql.execution.datasources.InsertInto",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1260:2591,load,load,2591,https://hail.is,https://github.com/hail-is/hail/issues/1260,1,['load'],['load']
Performance,50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:60); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1476); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:574); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:563); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:288); 	at is.hail.services.package$.retryTransientErrors(package.scala:163); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:286); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:26); 	at is.hail.backend.service.ServiceBackend$$anon$4.call(ServiceBackend.scala:239); 	at is.hail.backend.service.ServiceBackend$$anon$4.call(ServiceBackend.scala:235); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/wes-bipolar-tmp-4day/o/bge-wave-1-VQSR%2FparallelizeAndComputeWithIndex%2FgCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=%2Fresult.2706?alt=media; No such object: wes-bipolar-tmp-4day/bge-wave-1-VQSR/parallelizeAndComputeWithIndex/gCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=/result.2706; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37); 	at com.google.api.client.googleapis.services.AbstractGoogleCl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13409:6778,concurren,concurrent,6778,https://hail.is,https://github.com/hail-is/hail/issues/13409,1,['concurren'],['concurrent']
Performance,"50</a> from tylerjereddy/treddy_scipy_192_more_backports</li>; <li><a href=""https://github.com/scipy/scipy/commit/6b098c25223e224ff44101f86bbc86efecffe1d9""><code>6b098c2</code></a> TST: optimize.milp: remove problematic timeout/iteration test</li>; <li><a href=""https://github.com/scipy/scipy/commit/24dce9760b87934f1be046ec817c758b0f3952dc""><code>24dce97</code></a> DOC: stats.pearsonr: typo in coeffic<em>i</em>ent (<a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17153"">#17153</a>)</li>; <li><a href=""https://github.com/scipy/scipy/commit/a6ba7cad3b54c35d2ccb55c595691689004742c1""><code>a6ba7ca</code></a> MAINT: misc 1.9.2 updates</li>; <li><a href=""https://github.com/scipy/scipy/commit/ed9760e60a28b8f13e5644494033e2dab9aafbcd""><code>ed9760e</code></a> MAINT: stats.pearson3: fix ppf for negative skew (<a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17055"">#17055</a>)</li>; <li><a href=""https://github.com/scipy/scipy/commit/6fb67007dd7105755057f3379fb7ef423eae524e""><code>6fb6700</code></a> FIX: optimize.milp: return feasible solution if available on timeout/node lim...</li>; <li><a href=""https://github.com/scipy/scipy/commit/bcfce27fc061cbde6ac6531799362e0420ea4796""><code>bcfce27</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17132"">#17132</a> from tylerjereddy/treddy_192_backports</li>; <li><a href=""https://github.com/scipy/scipy/commit/2bc973a2c28c4b6b5bea0e288631834fe34b526e""><code>2bc973a</code></a> BLD: set version to 1.9.2.dev0 (and trigger wheel build CI)</li>; <li>Additional commits viewable in <a href=""https://github.com/scipy/scipy/compare/v1.2.1...v1.9.2"">compare view</a></li>; </ul>; </details>; <br />. Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12352:2788,optimiz,optimize,2788,https://hail.is,https://github.com/hail-is/hail/pull/12352,1,['optimiz'],['optimize']
Performance,518ms self 0.004ms children 0.514ms %children 99.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.496ms self 0.040ms children 0.456ms %children 91.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.228ms self 0.228ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.064ms self 0.064ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:206836,Optimiz,Optimize,206836,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,525ms self 0.004ms children 0.521ms %children 99.28%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.503ms self 0.061ms children 0.443ms %children 87.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.219ms self 0.219ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:214857,Optimiz,Optimize,214857,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,58 DAGScheduler: INFO: Shuffle files lost for executor: 11 (epoch 11); 2019-01-22 13:11:59 YarnScheduler: ERROR: Lost executor 11 on scc-q17.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:137786,concurren,concurrent,137786,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.19%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvalua,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:190073,Optimiz,Optimize,190073,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,6); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); ... 18 more; Caused by: java.lang.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:3820,concurren,concurrent,3820,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['concurren'],['concurrent']
Performance,6); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); at org.apache.spark.broadcast.TorrentBroadcast.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:9978,concurren,concurrent,9978,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['concurren'],['concurrent']
Performance,6); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:12116,concurren,concurrent,12116,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['concurren'],['concurrent']
Performance,"6); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": {; ""zoneUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-ce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:2335,concurren,concurrent,2335,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,2,['concurren'],['concurrent']
Performance,"614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.sca",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:6881,load,loads,6881,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['load'],['loads']
Performance,64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB); Collecting typer==0.9.0; Using cached typer-0.9.0-py3-none-any.whl (45 kB); Collecting typing-extensions==4.7.1; Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB); Collecting tzdata==2023.3; Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB); Collecting urllib3==1.26.16; Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB); Collecting uvloop==0.17.0; Using cached uvloop-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB); Collecting wrapt==1.15.0; Using cached wrapt-1.15.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB); Collecting xyzservices==2023.7.0; Using cached xyzservices-2023.7.0-py3-none-any.whl (56 kB); Collecting yarl==1.9.2; Using cached yarl-1.9.2-cp39-cp39-manylinux_2_17_x8,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:40151,cache,cached,40151,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"6456</a></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/psf/requests/blob/main/HISTORY.md"">requests's changelog</a>.</em></p>; <blockquote>; <h2>2.32.0 (2024-05-20)</h2>; <p><strong>Security</strong></p>; <ul>; <li>Fixed an issue where setting <code>verify=False</code> on the first request from a; Session will cause subsequent requests to the <em>same origin</em> to also ignore; cert verification, regardless of the value of <code>verify</code>.; (<a href=""https://github.com/psf/requests/security/advisories/GHSA-9wx4-h78v-vm56"">https://github.com/psf/requests/security/advisories/GHSA-9wx4-h78v-vm56</a>)</li>; </ul>; <p><strong>Improvements</strong></p>; <ul>; <li><code>verify=True</code> now reuses a global SSLContext which should improve; request time variance between first and subsequent requests. It should; also minimize certificate load time on Windows systems when using a Python; version built with OpenSSL 3.x. (<a href=""https://redirect.github.com/psf/requests/issues/6667"">#6667</a>)</li>; <li>Requests now supports optional use of character detection; (<code>chardet</code> or <code>charset_normalizer</code>) when repackaged or vendored.; This enables <code>pip</code> and other projects to minimize their vendoring; surface area. The <code>Response.text()</code> and <code>apparent_encoding</code> APIs; will default to <code>utf-8</code> if neither library is present. (<a href=""https://redirect.github.com/psf/requests/issues/6702"">#6702</a>)</li>; </ul>; <p><strong>Bugfixes</strong></p>; <ul>; <li>Fixed bug in length detection where emoji length was incorrectly; calculated in the request content-length. (<a href=""https://redirect.github.com/psf/requests/issues/6589"">#6589</a>)</li>; <li>Fixed deserialization bug in JSONDecodeError. (<a href=""https://redirect.github.com/psf/requests/issues/6629"">#6629</a>)</li>; <li>Fixed bug where an ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14555:4915,load,load,4915,https://hail.is,https://github.com/hail-is/hail/pull/14555,1,['load'],['load']
Performance,660ms self 0.648ms children 0.012ms %children 1.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 2.298ms self 2.298ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.050ms self 0.050ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.356ms self 0.356ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:6942,Optimiz,OptimizePass,6942,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1059); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:21); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:17); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.HailContext.importVCFs(HailContext.scala:498); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:10,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1806:4640,Load,LoadVCF,4640,https://hail.is,https://github.com/hail-is/hail/issues/1806,1,['Load'],['LoadVCF']
Performance,667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1059); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:29); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:25); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.HailContext.importVCFs(HailContext.scala:557); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:10506,Load,LoadVCF,10506,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['Load'],['LoadVCF']
Performance,68); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.keytable.KeyTable$$anonfun$8.apply(KeyTable.scala:68); at is.hail.keytable.KeyTable$$anonfun$8.apply(KeyTable.scala:65); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:247); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1275:9092,concurren,concurrent,9092,https://hail.is,https://github.com/hail-is/hail/issues/1275,2,['concurren'],['concurrent']
Performance,"6_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3206,cache,cached,3206,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Main.main(Main.scala) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 		at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 		at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 		at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; 	Caused by: com.google.api.client.http.HttpResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""domain"": ""global"",; ""reason"": ""forbidde",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:25428,concurren,concurrent,25428,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,7 16:44:33.788 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/1-day/o?name=parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g%3D/result.7028&uploadType=resumable&upload_id=ADPycdv7y3A6GjTh6Kv7vrUu2ap2Kv0peLVWsVTAghIs7RCZk9X3fI1BDkeHag1cd9g3etP2sS4f13bN6iJPU_sbnRnyRE91VPtjUpuYLPqmOq13; 	|> content-range: bytes */*; 	| ; 	|< HTTP/1.1 308 Resume Incomplete; 	|< content-length: 0; 	|< content-type: text/plain; charset=utf-8; 	|< x-guploader-uploadid: ADPycdv7y3A6GjTh6Kv7vrUu2ap2Kv0peLVWsVTAghIs7RCZk9X3fI1BDkeHag1cd9g3etP2sS4f13bN6iJPU_sbnRnyRE91VPtjUpuYLPqmOq13; 	| ; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionFailureScenario.toStorageException(JsonResumableSessionFailureScenario.java:185) ~[gs:__hail-test-,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:7300,concurren,concurrent,7300,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['concurren'],['concurrent']
Performance,7 bytes); 2019-01-22 13:12:03 YarnScheduler: ERROR: Lost executor 13 on scc-q16.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:170305,concurren,concurrent,170305,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,7); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:430); E 	at is.hail.backend.service.Main$.main(Main.scala:33); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.concurrent.TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.handleTimeout(FluxTimeout.java:294); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.doTimeout(FluxTimeout.java:279); E 	at reactor.core.publisher.FluxTimeout$TimeoutTimeoutSubscriber.onNext(FluxTimeout.java:418); E 	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onNext(FluxOnErrorResume.java:79); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.propagateDelay(MonoDelay.java:270); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.run(MonoDelay.java:285); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:68); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:28); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(Schedul,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:4510,concurren,concurrent,4510,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['concurren'],['concurrent']
Performance,"7,.,2.144,8.001;AS_MQ=55.75,.,38.98,40.20;AS_MQRankSum=0.200,.,-1.050,-0.500;AS_QD=0.50,0.00,0.25,0.52;AS_ReadPosRankSum=-0.200,.,0.500,-0.220;AS_SOR=2.300,.,1.600,3.000;BaseQRankSum=0.200;DP=600000;ExcessHet=0.0477;FS=0.900;MQ=55.02;MQRankSum=-0.553;QD=1.00;ReadPosRankSum=-0.162;SOR=0.792;VarDP=650	GT:AD:DP:GQ:PGT:PID:PL:PS:SB	0/0:.:21:30	0/0:.:300:20	0/0:.:30:72	0/0:.:31:98	0|1:29,3,0,0,0:33:78:0|1:113_GG_G:78,0,1100,140,1400,1200,172,1600,1200,1000,175,1100,1100,1300,1000:113:19,19,2,1	0/0:.:20:19	0/0:.:19:20	0/0:.:25:50		0|1:90,2,0,0,0:30:40:0|1:113_GG_G:40,0,600,70,650,600,90,640,900,300,60,800,400,900,900:113:2,14,2,0	0/0:.:20:10	0/0:.:9:20	0/0:.:30:40	0/0:.:37:38		0/4:5,0,0,0,1:5:33:.:.:30,40,400,50,220,220,38,270,270,270,0,200,200,200,202:.:5,0,0,1	. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:22); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:22); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1921); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:7486,Load,LoadVCF,7486,https://hail.is,https://github.com/hail-is/hail/issues/14102,2,['Load'],['LoadVCF']
Performance,7.9 73.2 84.0 ; Download 1 KiB 5 84.3 15.9 80.6 103.4 ; Download 100 KiB 5 81.9 16.0 82.7 99.6 ; Download 1 MiB 5 90.6 6.5 94.5 96.8 ; Metadata 0 B 5 23.6 2.7 23.6 26.3 ; Metadata 1 KiB 5 25.5 2.1 26.9 27.4 ; Metadata 100 KiB 5 26.2 3.6 27.3 29.9 ; Metadata 1 MiB 5 24.0 3.7 23.3 28.4 ; Upload 0 B 5 98.1 16.6 95.5 117.9 ; Upload 1 KiB 5 116.7 21.8 115.5 142.1 ; Upload 100 KiB 5 116.5 17.8 115.1 135.1 ; Upload 1 MiB 5 168.2 18.5 179.6 185.6 . ------------------------------------------------------------------------------; Write Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Write throughput: 977.7 Mibit/s.; Parallelism strategy: both. ------------------------------------------------------------------------------; Read Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Read throughput: 1.11 Gibit/s.; Parallelism strategy: both. ------------------------------------------------------------------------------; System Information ; ------------------------------------------------------------------------------; IP Address: ; 172.21.46.11; Temporary Directory: ; /tmp; Bucket URI: ; gs://hail-jigold/; gsutil Version: ; 5.24; boto Version: ; 2.49.0; Measurement time: ; 2023-06-05 03:25:16 PM ; Running on GCE: ; True; GCE Instance:; 	; Bucket location: ; US-CENTRAL1; Bucket storage class: ; REGIONAL; Google Server: ; ; Google Server IP Addresses: ; 142.250.128.128; 142.251.6.128; 108.177.112.128; 74.125.124.128; 172.217.212.128; 172.217.214.128; 172.253.119.128; 108.177.111.128; 142.250.1.128; 108.177.121.128; 142.250.103.128; 108.177.120.128; 142.250.159.128; 142.251.120.128; 142.251.161.128; 74.125.126.128; Google Server Hostnames: ; ib-in-f128.1e100.net; ic-in-f128.1e100.net; jo-in-f128.1e100.net; jp-in-f128.1e100.net; jq-in-f128.1e100.net; jr-in-f128.1e100.net; jt-,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597:2301,throughput,throughput,2301,https://hail.is,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597,1,['throughput'],['throughput']
Performance,"7/python/pyspark/java_gateway.py in launch_gateway(conf); 75 def preexec_func():; 76 signal.signal(signal.SIGINT, signal.SIG_IGN); ---> 77 proc = Popen(command, stdin=PIPE, preexec_fn=preexec_func, env=env); 78 else:; 79 # preexec_fn not supported on Windows. /scratch/PI/dpwall/computeEnvironments/miniconda2/lib/python2.7/subprocess.pyc in __init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags); 388 p2cread, p2cwrite,; 389 c2pread, c2pwrite,; --> 390 errread, errwrite); 391 except Exception:; 392 # Preserve original exception in case os.close raises. /scratch/PI/dpwall/computeEnvironments/miniconda2/lib/python2.7/subprocess.pyc in _execute_child(self, args, executable, preexec_fn, close_fds, cwd, env, universal_newlines, startupinfo, creationflags, shell, to_close, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite); 1022 raise; 1023 child_exception = pickle.loads(data); -> 1024 raise child_exception; 1025; 1026. OSError: [Errno 2] No such file or directory. and the second error we would get would be. ------------------------------------------------------------------------- Py4JJavaError Traceback (most recent call last) <ipython-input-6-93fa734a63bb> in <module>() ----> 1 hc_nate = HailContext() /scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.pyc in __init__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir) 60 self._jhc = scala_object(self._hail, 'HailContext').apply( 61 jsc, appName, joption(master), local, log, quiet, append, ---> 62 parquet_compression, min_block_size, branching_factor, tmp_dir) 63 64 self._jsc = self._jhc.sc() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args) 1131 answer = self.gateway_client.send_command(command) 1132 return_value = get_return_value( -> 1133 answer, self.gateway_clie",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1525:3058,load,loads,3058,https://hail.is,https://github.com/hail-is/hail/issues/1525,1,['load'],['loads']
Performance,"701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:972); 	at org.apache.hadoop.util.Shell.run(Shell.java:869); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; When I dig into the container logs, the stdout is empty on most, stderr is full of warnings, but no errors:; ```; 18/03/02 15:28:07 WARN com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel: Channel for 'gs://gnomad/coverage/hail-0.2/coverage/exomes/parts/part_partition1049.vds/entries/rows/parts/part-0095' is not open.; ```; But then one machine I logged into had:; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa70eb59074, pid=4361, tid=0x00007fa707702700; #; # JRE version: OpenJDK Runtime Environment (8.0_131-b11) (build 1.8.0_131-8u131-b11-1~bpo8+1-b11); # Java VM: OpenJDK 64-Bit Server VM (25.131-b11 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libconscrypt_openjdk_jni.so+0x43074]; [error occurred during error reporting (printing pro",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3053:3652,concurren,concurrent,3652,https://hail.is,https://github.com/hail-is/hail/issues/3053,1,['concurren'],['concurrent']
Performance,714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:26); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:22); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.HailContext.importVCFs(HailContext.scala:529); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBro,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:5025,Load,LoadVCF,5025,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['Load'],['LoadVCF']
Performance,"76.0K blocks / 32.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.709 : INFO: RegionPool: REPORT_THRESHOLD: 32.8M allocated (576.0K blocks / 32.3M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:44:22.426 GoogleStorageFS$: INFO: createNoCompression: gs://1-day/tmp/hail/TSOfOrgZUmbVixnRiKWQFB/aggregate_intermediates/-Pt3gNtQW5WoBdCTDPQiwHda9c265f2-fbd8-4f1b-bcde-fbf29180c347; 2023-09-27 16:44:22.495 GoogleStorageFS$: INFO: close: gs://1-day/tmp/hail/TSOfOrgZUmbVixnRiKWQFB/aggregate_intermediates/-Pt3gNtQW5WoBdCTDPQiwHda9c265f2-fbd8-4f1b-bcde-fbf29180c347; 2023-09-27 16:44:22.620 GoogleStorageFS$: INFO: closed: gs://1-day/tmp/hail/TSOfOrgZUmbVixnRiKWQFB/aggregate_intermediates/-Pt3gNtQW5WoBdCTDPQiwHda9c265f2-fbd8-4f1b-bcde-fbf29180c347; 2023-09-27 16:44:22.621 : INFO: TaskReport: stage=0, partition=7028, attempt=0, peakBytes=62266032, peakBytesReadable=59.38 MiB, chunks requested=72126, cache hits=72121; 2023-09-27 16:44:22.622 : INFO: RegionPool: FREE: 59.4M allocated (25.2M blocks / 34.2M chunks), regions.size = 11, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:44:22.623 WorkerTimer$: INFO: executeFunction took 71843.446957 ms.; 2023-09-27 16:44:22.623 GoogleStorageFS$: INFO: createNoCompression: gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g=/result.7028; 2023-09-27 16:44:22.668 GoogleStorageFS$: INFO: close: gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g=/result.7028; 2023-09-27 16:44:33.788 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:5703,cache,cache,5703,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['cache'],['cache']
Performance,"79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Job aborted due to stage failure: Task 754 in stage 1.0 failed 1 times, most recent failure: Lost task 754.0 in stage 1.0 (TID 1625, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:241); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2407:4961,concurren,concurrent,4961,https://hail.is,https://github.com/hail-is/hail/issues/2407,1,['concurren'],['concurrent']
Performance,"790a4d87-9035-41ae-afc6-326f710d9a89; 2023-09-24 01:58:51.513 : INFO: TaskReport: stage=0, partition=9571, attempt=0, peakBytes=4507648, peakBytesReadable=4.30 MiB, chunks requested=51, cache hits=0; 2023-09-24 01:58:51.513 : INFO: RegionPool: FREE: 4.3M allocated (2.2M blocks / 2.1M chunks), regions.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:51.515 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor42.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/aou_tmp/o?name=tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89&uploadType=resumable&upload_id=ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PLCuFsExsT0aEh2b4FrHtAiUktumQbvgl1U0icw; 	|> content-range: bytes */*; 	| ; 	|< HTTP/1.1 308 Resume Incomplete; 	|< content-length: 0; 	|< content-type: text/plain; charset=utf-8; 	|< x-guplo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721:4391,concurren,concurrent,4391,https://hail.is,https://github.com/hail-is/hail/issues/13721,1,['concurren'],['concurrent']
Performance,"7e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va)))))); (InsertFields; (Ref row); (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (Let g; (ArrayRef; (Ref global))); (SelectFields (locus alleles); (Ref row)))); 2019-01-08 18:19:48 root: INFO: optimize: after:; (TableCount; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (CastTableToMatrix `the entries! [877f12a8827e18f61222c6c8c5fb04a8]` __cols (s); (TableMapRows; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va)))))); (InsertFields; (Ref row); (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (ArrayRef; (GetField `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (Ref row)); (Ref i))))))); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va))))))); ```; ### What went wrong (all error messages here, including the full java stack trace):; This wasn't optimized to a read of the metadata.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5100:2542,optimiz,optimized,2542,https://hail.is,https://github.com/hail-is/hail/issues/5100,1,['optimiz'],['optimized']
Performance,"8469</a> Modifying a child element in a tab causes the whole tab to rerender</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/8531"">#8531</a> [component: bokehjs] Save tool in gridplot initiates multiple downloads</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/8684"">#8684</a> Allow at least partial alignment of fixed sized frames</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9113"">#9113</a> [component: bokehjs] Empty group widgets don't size properly once populated</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9133"">#9133</a> [BUG] Tabs ignore explicitly set dimensions</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9208"">#9208</a> [component: bokehjs] [BUG] sizing_mode='stretch_width' makes plot too wide if scrollbar is showing</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9320"">#9320</a> [BUG] Bokeh rendering performance</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9448"">#9448</a> [component: bokehjs] [BUG] Google Fonts not loading on Glyph on standalone HTML until interacting with Glyph</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9744"">#9744</a> [component: bokehjs] [BUG] bokeh server layout overlap on toggle visibility</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9763"">#9763</a> [BUG] <code>gridplot</code> <code>merge_tools</code> removes distinct tools it thinks are repeated, e.g., <code>xpan</code> and <code>ypan</code></li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9764"">#9764</a> [component: bokehjs] [BUG] MultiChoice placeholder text not displayed</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9992"">#9992</a> [component: bokehjs] [BUG] Select widget hiding tabs, when selecting a plot</li>; <li><a href=""https://github-redirect.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12454:3412,perform,performance,3412,https://hail.is,https://github.com/hail-is/hail/pull/12454,1,['perform'],['performance']
Performance,853.vcf.bgz \ splitmulti \; write -o TOPMed.6998.chr22.vds`. `[Stage 0:====================================================> (52 + 4) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: importvcf: caught exception: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer; at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:143); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:142); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.rdd.OrderedRDD$.calculateKeyRanges(OrderedRDD.scala:142); at org.apache.spark.rdd.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/669:1230,Load,LoadVCF,1230,https://hail.is,https://github.com/hail-is/hail/issues/669,1,['Load'],['LoadVCF']
Performance,86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached pandas-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB); Collecting parsimonious==0.10.0; Usi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:36655,cache,cached,36655,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:13306,Load,LoadMatrix,13306,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,"872, in execute; raise JVMUserError(exception); JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	... 7 more; Caused by: java.lang.IllegalArgumentException: bound must be positive; 	at java.util.Random.nextInt(Random.java:388); 	at scala.util.Random.nextInt(Random.scala:70); 	at is.hail.services.package$.delayMsForTry(package.scala:47); 	at is.hail.services.package$.retryTransientErrors(package.scala:186); 	at is.hail.io.fs.GoogleStorageFS$$anon$1.retryingRead(GoogleStorageFS.scala:220); 	at is.hail.io.fs.GoogleStorageFS$$anon$1.readHandlingRequesterPays(GoogleStorageFS.scala:226); 	at is.hail.io.fs.GoogleStorageFS$$anon$1.fill(GoogleStorageFS.scala:257); 	at i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888:1233,concurren,concurrent,1233,https://hail.is,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888,1,['concurren'],['concurrent']
Performance,"88 Any; 189 """"""; --> 190 return eval_timed(expression)[0]; 191 ; 192 . <decorator-gen-714> in eval_timed(expression). ~/projects/hail/hail/python/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615 ; 616 return wrapper. ~/projects/hail/hail/python/hail/expr/expressions/expression_utils.py in eval_timed(expression); 154 if ir_type != expression.dtype:; 155 raise ExpressionException(f'Expression type and IR type differed: \n{ir_type}\n vs \n{expression_type}'); --> 156 return Env.backend().execute(expression._ir, True); 157 else:; 158 uid = Env.get_uid(). ~/projects/hail/hail/python/hail/backend/spark_backend.py in execute(self, ir, timed); 294 jir = self._to_java_value_ir(ir); 295 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); --> 296 result = json.loads(self._jhc.backend().executeJSON(jir)); 297 value = ir.typ._from_json(result['value']); 298 timings = result['timings']. ~/miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/projects/hail/hail/python/hail/backend/spark_backend.py in deco(*args, **kwargs); 39 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 40 'Hail version: %s\n'; ---> 41 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 42 except pyspark.sql.utils.CapturedException as e:; 43 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: Index 5 is out of bounds for axis 0 with size 2. Java stack trace:; is.hail.utils.HailException: Index 5 is out of bounds for axis 0 with size 2; 	at __C889Compiled.apply(Unknown Source); 	at is.hail.expr.ir.Com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9226:1772,load,loads,1772,https://hail.is,https://github.com/hail-is/hail/issues/9226,1,['load'],['loads']
Performance,89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.164ms self 0.164ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.044ms self 0.035ms children 0.009ms %children 20.05%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvalua,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:9799,Optimiz,Optimize,9799,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,9); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:247); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1275:3523,concurren,concurrent,3523,https://hail.is,https://github.com/hail-is/hail/issues/1275,1,['concurren'],['concurrent']
Performance,9-01-22 13:11:53 DAGScheduler: INFO: Shuffle files lost for executor: 3 (epoch 9); 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:46724,concurren,concurrent,46724,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,92ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLetsPass#after total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#exec,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:176616,Optimiz,OptimizePass,176616,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,95); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:5745,Load,LoadMatrix,5745,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,98654_0001_01_000002/tmp/libhail8271834084559267793.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); java.lang.UnsatisfiedLinkError: /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:27); at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); at is.hail.annotations.Region.<init>(Region.scala:33); at is.hail.annotations.Region$.apply(Region.scala:15); at is.hail.rvd.RVDContext$.default(RVDContext.scala:8); at is.hail.rvd.package$RVDContextIsPointed$.point(package.scala:8); at is.hail.rvd.package$RVDContextIsPointed$.point(package.scala:6); at is.hail.utils.package$.point(package.scala:593); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD.is$hail$sparkextras$ContextRDD$$sparkManagedContext(ContextRDD.scala:129); at is.hail.sparkextras.ContextRDD$$,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:1514,load,load,1514,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186,1,['load'],['load']
Performance,99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:108); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780:3386,concurren,concurrent,3386,https://hail.is,https://github.com/hail-is/hail/issues/4780,1,['concurren'],['concurrent']
Performance,"9:09 SparkContext: INFO: Created broadcast 0 from broadcast at SparkBackend.scala:311; 2022-05-14 12:09:11 root: INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 30: Thread-4; 2022-05-14 12:09:11 root: ERROR: HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:1884,Load,LoadPlink,1884,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,": 'E'}` for machine types in Azure. This corresponds to 2Gi/core, 4Gi/core, and 8Gi/core.; - Spot price is set to -1 for now until we figure out a better billing strategy; - We look for existing network security groups to tell if a VM has been fully cleaned up already in the garbage collection loop. # To-Do:. ## Services. - Use global config and make an `AzureConfig` (@daniel-goldstein not sure if you're already doing this) instead of optional environment variables; - Azure user disks are not implemented; There's a maximum number of disks that can be mounted per machine type with a maximum of 32 along with figuring out the API calls. We'll need a semaphore of some sort.; - No activity logs loop. Not necessary for initial development and preemption billing is not working how intended anyways (will add to the list to fix!). We also don't track vm creation success rates per zone like we do with GCP. It might be good to look for VM deletion events to remove instances that are no longer present and then do a deep delete as then we'll have some redundancy and faster response times.; - Figure out how to do a deep-delete as much as possible for VMs when using the create VM REST API. This is essential for cleaning up resources for idled out workers when the driver is down for a long period of time.; - User billing based on resources used based on the `AzureInstanceConfig`; - Spot billing strategy; - Check network IP settings in the worker; - Add garbage collection CLI commands to build.yaml to clean up VMs, disks, nics, public ip addresses, and network security groups based on a tag; - Fix batch tests to be cloud agnostic. ## Infrastructure. - Create a shared SSH public key on the VMs for development purposes; - Consider having every PR / namespace deploy resources in a separate resource group rather than having one resource group for all Batch resources! We'd need something to name the resource groups something like `hail-dev_jigold` and `jigold_jigold` for example to handle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10970:2100,response time,response times,2100,https://hail.is,https://github.com/hail-is/hail/pull/10970,1,['response time'],['response times']
Performance,": ; 2.49.0; Measurement time: ; 2023-06-05 03:25:16 PM ; Running on GCE: ; True; GCE Instance:; 	; Bucket location: ; US-CENTRAL1; Bucket storage class: ; REGIONAL; Google Server: ; ; Google Server IP Addresses: ; 142.250.128.128; 142.251.6.128; 108.177.112.128; 74.125.124.128; 172.217.212.128; 172.217.214.128; 172.253.119.128; 108.177.111.128; 142.250.1.128; 108.177.121.128; 142.250.103.128; 108.177.120.128; 142.250.159.128; 142.251.120.128; 142.251.161.128; 74.125.126.128; Google Server Hostnames: ; ib-in-f128.1e100.net; ic-in-f128.1e100.net; jo-in-f128.1e100.net; jp-in-f128.1e100.net; jq-in-f128.1e100.net; jr-in-f128.1e100.net; jt-in-f128.1e100.net; jv-in-f128.1e100.net; jw-in-f128.1e100.net; jx-in-f128.1e100.net; jy-in-f128.1e100.net; jz-in-f128.1e100.net; ie-in-f128.1e100.net; if-in-f128.1e100.net; ig-in-f128.1e100.net; ik-in-f128.1e100.net; Google DNS thinks your IP is: ; ; CPU Count: ; 16; CPU Load Average: ; [32.39, 33.2, 19.0]; Total Memory: ; 57.5 GiB; Free Memory: ; 38.41 GiB; TCP segment counts not available because ""netstat"" was not found during test runs; Disk Counter Deltas:; disk reads writes rbytes wbytes rtime wtime ; loop0 0 0 0 0 0 0 ; loop1 0 0 0 0 0 0 ; loop3 0 0 0 0 0 0 ; loop4 0 0 0 0 0 0 ; loop5 0 0 0 0 0 0 ; nvme0n1 4385 4694 581857280 1743810560 6453 527129 ; sda1 0 544 0 3731456 0 429 ; sda14 0 0 0 0 0 0 ; sda15 0 0 0 0 0 0 ; TCP /proc values:; tcp_timestamps = 1; tcp_sack = 1; tcp_window_scaling = 1; Boto HTTPS Enabled: ; True; Requests routed through proxy: ; False; Latency of the DNS lookup for Google Storage server (ms): ; 1.5; Latencies connecting to Google Storage server IPs (ms):; 74.125.126.128 = 1.1. ------------------------------------------------------------------------------; In-Process HTTP Statistics ; ------------------------------------------------------------------------------; Total HTTP requests made: 149; HTTP 5xx errors: 0; HTTP connections broken: 0; Availability: 100%. Output file written to '/tmp/output.json'.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597:4184,Latency,Latency,4184,https://hail.is,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597,1,['Latency'],['Latency']
Performance,": Lost task 0.0 in stage 0.0 (TID 0, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 6 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 6 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 6; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 7 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 7 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 7; 2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:82230,concurren,concurrent,82230,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,": Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCod",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:2689,concurren,concurrent,2689,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705,1,['concurren'],['concurrent']
Performance,": Lost task 1.0 in stage 0.0 (TID 1, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 21.0 in stage 0.0 (TID 21, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:59051,concurren,concurrent,59051,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,": Lost task 2.0 in stage 0.0 (TID 2, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 22.0 in stage 0.0 (TID 22, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:94794,concurren,concurrent,94794,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,": Lost task 3.0 in stage 0.0 (TID 3, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 33.0 in stage 0.0 (TID 33, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:88512,concurren,concurrent,88512,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,": Lost task 4.0 in stage 0.0 (TID 4, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 34.0 in stage 0.0 (TID 34, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:64743,concurren,concurrent,64743,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,": Lost task 5.0 in stage 0.0 (TID 5, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 25.0 in stage 0.0 (TID 25, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:129870,concurren,concurrent,129870,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,": Lost task 6.0 in stage 0.0 (TID 6, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 1 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 1 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 1; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 2 on scc-q12.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_0",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:74791,concurren,concurrent,74791,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,": Lost task 7.0 in stage 0.0 (TID 7, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 37.0 in stage 0.0 (TID 37, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:121104,concurren,concurrent,121104,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,": Lost task 8.0 in stage 0.0 (TID 8, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 38.0 in stage 0.0 (TID 38, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:110886,concurren,concurrent,110886,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,": Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4810 bytes); 2018-10-09 15:04:36 Executor: INFO: Running task 0.0 in stage 0.0 (TID 0); 2018-10-09 15:04:36 Executor: INFO: Fetching spark://10.32.119.167:61887/jars/sparklyr-2.2-2.11.jar with timestamp 1539122673186; 2018-10-09 15:04:36 TransportClientFactory: INFO: Successfully created connection to /10.32.119.167:61887 after 11 ms (0 ms spent in bootstraps); 2018-10-09 15:04:36 Utils: INFO: Fetching spark://10.32.119.167:61887/jars/sparklyr-2.2-2.11.jar to /private/var/folders/w4/9k0my8pd6113d61pq05fvqlr0000gn/T/spark-4d23a45e-e197-4f14-ac11-3973337df8a3/userFiles-33b96853-73f9-423a-ac6a-bcdb9106012a/fetchFileTemp414690014855588879.tmp; 2018-10-09 15:04:36 Executor: INFO: Adding file:/private/var/folders/w4/9k0my8pd6113d61pq05fvqlr0000gn/T/spark-4d23a45e-e197-4f14-ac11-3973337df8a3/userFiles-33b96853-73f9-423a-ac6a-bcdb9106012a/sparklyr-2.2-2.11.jar to class loader; 2018-10-09 15:04:36 CodeGenerator: INFO: Code generated in 140.241861 ms; 2018-10-09 15:04:36 Executor: INFO: Finished task 0.0 in stage 0.0 (TID 0). 1015 bytes result sent to driver; 2018-10-09 15:04:36 TaskSetManager: INFO: Finished task 0.0 in stage 0.0 (TID 0) in 395 ms on localhost (executor driver) (1/1); 2018-10-09 15:04:36 TaskSchedulerImpl: INFO: Removed TaskSet 0.0, whose tasks have all completed, from pool ; 2018-10-09 15:04:36 DAGScheduler: INFO: ResultStage 0 (collect at utils.scala:44) finished in 0.412 s; 2018-10-09 15:04:36 DAGScheduler: INFO: Job 0 finished: collect at utils.scala:44, took 0.679005 s; 2018-10-09 15:04:36 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:36 SparkSqlParser: INFO: Parsing command: table8508c46074; 2018-10-09 15:04:36 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:36 SparkSqlParser: INFO: Parsing command: CACHE TABLE `table",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:17272,load,loader,17272,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['load'],['loader']
Performance,": Task 76 in stage 1.0 failed 1 times, most recent failure: Lost task 76.0 in stage 1.0 (TID 77, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567); at java.nio.CharBuffer.toString(CharBuffer.java:1241); at org.apache.hadoop.io.Text.decode(Text.java:412); at org.apache.hadoop.io.Text.decode(Text.java:389); at org.apache.hadoop.io.Text.toString(Text.java:280); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:788); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:64); at is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:37); at is.hail.utils.package$.using(package.scala:587); at is.hail.rvd.RVDPartitionInfo$.apply(RVDPartitionInfo.scala:37); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1059); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1057); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:3511,Load,LoadVCF,3511,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['Load'],['LoadVCF']
Performance,: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:46869,concurren,concurrent,46869,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:50850,concurren,concurrent,50850,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 BlockManagerMasterEndpoint: INFO: Trying to remove executor 9 from BlockManagerMaster.; 2019-01-22 13:11:55 BlockManagerMaster: INFO: Removal of executor 9 requested; 2019-01-22 13:11:55 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 9; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 3 on scc-q09.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.had,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:125480,concurren,concurrent,125480,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 1 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.l,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:52177,concurren,concurrent,52177,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,": WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 11.0 in stage 0.0 (TID 11, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:54842,concurren,concurrent,54842,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:48196,concurren,concurrent,48196,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.Con,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:49523,concurren,concurrent,49523,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,": WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 39.0 in stage 0.0 (TID 39, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:106491,concurren,concurrent,106491,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,": WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 17.0 in stage 0.0 (TID 17, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:118347,concurren,concurrent,118347,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,": WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 29.0 in stage 0.0 (TID 29, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:100804,concurren,concurrent,100804,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,": WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 27.1 in stage 0.0 (TID 44, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:139258,concurren,concurrent,139258,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,": WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 35.3 in stage 0.0 (TID 62, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:192571,concurren,concurrent,192571,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,": WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 7.2 in stage 0.0 (TID 53, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecuto",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:171716,concurren,concurrent,171716,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,": WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 15.2 in stage 0.0 (TID 50, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:161167,concurren,concurrent,161167,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,": WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 25.1 in stage 0.0 (TID 41, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:148037,concurren,concurrent,148037,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,": WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 9.1 in stage 0.0 (TID 65, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecuto",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:183215,concurren,concurrent,183215,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,": six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJWT, pygments, prompt-toolkit, pickleshare, pexpect, parsimonious, pandas, nest-asyncio, jedi, hurry.filesize, humanize, google-cloud-storage, gcsfs, dill, Deprecated, bokeh, backcall, asyncinit, aiohttp-session, ipython, hail; Successfully installed Deprecated-1.2.12 Jinja2-2.11.3 MarkupSafe-1.1.1 PyJWT-2.0.1 PyYAML-5.4.1 aiohttp-3.7.4 aiohttp-session-2.7.0 async-timeout-3.0.1 asyncinit-0.2.4 attrs-20.3.0 backcall-0.2.0 bokeh-1.4.0 cachetools-4.2.1 certifi-2020.12.5 chardet-3.0.4 decorator-4.4.2 dill-0.3.3 fsspec-0.8.7 gcsfs-0.7.2 google-api-core-1.26.1 google-auth-1.27.1 google-auth-oauthlib-0.4.3 google-cloud-core-1.6.0 google-cloud-storage-1.25.0 google-resumable-media-0.5.1 googleapis-common-protos-1.53.0 hail-0.2.64 humanize-1.0.0 hurry.filesize-0.9 idna-2.8 ipython-7.21.0 ipython-genutils-0.2.0 jedi-0.18.0 multidict-5.1.0 nest-asyncio-1.5.1 numpy-1.20.1 oauthlib-3.1.0 packaging-20.9 pandas-1.1.4 parsimonious-0.8.1 parso-0.8.1 pexpect-4.8.0 pickleshare-0.7.5 pillow-8.1.2 prompt-toolkit-3.0.17 protobuf-3.15.6 ptyprocess-0.7.0 py4j-0.10.7 pyasn1-0.4.8 pyasn1-modules-0.2.8 pygments-2.8.1 pyparsing-2.4.7 pyspark-2.4.1 python-dateutil-2.8.1 python-json-logger-0.1.11 pytz-2021.1 requests-2.22.0 requests-oauthlib-1.3.0 rsa-4.7.2 scipy-1.6.1 six-1.15.0 tabulate-0.8.3 tornado-6.1 tqdm-4.42.1 traitlets-5.0.5 typing-extensions-3.7.4.3 urllib3-1.25.11 wcwidth-0.2.5 wrapt-1.12.1 yarl-1.6.3; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:8465,cache,cachetools-,8465,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cachetools-']
Performance,"://github-redirect.dependabot.com/samtools/htsjdk/issues/1616"">#1616</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/d15a5bacbb5ed54f1a474aede9a2c3cb9d8832fb""><code>d15a5ba</code></a> Added ULTIMA and ELEMENT as valid value for RG-PL according to SAM spec. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1619"">#1619</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/489c4192dd9682a9d1e53f4c8f6f7bb826e33589""><code>489c419</code></a> Support CRAM reference regions. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1605"">#1605</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/f461401e38fe95362a6d3c5afd8b592964b4bd29""><code>f461401</code></a> Silence AsciiLineReader warning when creating a FASTA sequence index (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1559"">#1559</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/1449dec45b4e95293db14595ec0d11a3839bac23""><code>1449dec</code></a> Support loading of CSI from URLs/streams. <a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1507"">#1507</a> (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1595"">#1595</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/22aec6782b33f8d169a5d1cf63e952126a3f09e0""><code>22aec67</code></a> Fix decoding of CRAM Scores read feature during normalization. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1592"">#1592</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/70e42597ee8e2db6241f7b147f1356a1f8a846bc""><code>70e4259</code></a> Remove unnecessary println in test (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1602"">#1602</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/6507249a4422d021b984e710e8f031816f6d8da2""><code>6507249</code></a> Make the CRAM MD5 failure message more user friendly. (<a href=""https://github-redirect.dependab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12229:7755,load,loading,7755,https://hail.is,https://github.com/hail-is/hail/pull/12229,1,['load'],['loading']
Performance,"://github-redirect.dependabot.com/samtools/htsjdk/issues/1616"">#1616</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/d15a5bacbb5ed54f1a474aede9a2c3cb9d8832fb""><code>d15a5ba</code></a> Added ULTIMA and ELEMENT as valid value for RG-PL according to SAM spec. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1619"">#1619</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/489c4192dd9682a9d1e53f4c8f6f7bb826e33589""><code>489c419</code></a> Support CRAM reference regions. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1605"">#1605</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/f461401e38fe95362a6d3c5afd8b592964b4bd29""><code>f461401</code></a> Silence AsciiLineReader warning when creating a FASTA sequence index (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1559"">#1559</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/1449dec45b4e95293db14595ec0d11a3839bac23""><code>1449dec</code></a> Support loading of CSI from URLs/streams. <a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1507"">#1507</a> (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1595"">#1595</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/22aec6782b33f8d169a5d1cf63e952126a3f09e0""><code>22aec67</code></a> Fix decoding of CRAM Scores read feature during normalization. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1592"">#1592</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/samtools/htsjdk/compare/2.24.1...3.0.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.github.samtools:htsjdk&package-manager=gradle&previous-version=2.24.1&new-version=3.0.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#abo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12310:8103,load,loading,8103,https://hail.is,https://github.com/hail-is/hail/pull/12310,1,['load'],['loading']
Performance,:104); 	at is.hail.utils.Py4jUtils$class.ls(Py4jUtils.scala:55); 	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at java.util.concurrent.AbstractExecutorService.doInvokeAny(AbstractExecutorService.java:193); 	at java.util.concurrent.AbstractExecutorService.invokeAny(AbstractExecutorService.java:215); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.concurrentGlobInternal(GoogleHadoopFileSystemBase.java:1282); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1261); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1229); 	at is.hail.io.fs.HadoopFS.listStatus(HadoopFS.scala:104); 	at is.hail.utils.Py4jUtils$class.ls(Py4jUtils.scala:55); 	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorI,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:3093,concurren,concurrent,3093,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrent']
Performance,":1153, in Table.export(self, output, types_file, header, parallel, delimiter); 1150 hl.current_backend().validate_file(output); 1152 parallel = ir.ExportType.default(parallel); -> 1153 Env.backend().execute(; 1154 ir.TableWrite(self._tir, ir.TableTextWriter(output, types_file, header, parallel, delimiter))). File ~/.local/lib/python3.10/site-packages/hail/backend/backend.py:178, in Backend.execute(self, ir, timed); 176 payload = ExecutePayload(self._render_ir(ir), '{""name"":""StreamBufferSpec""}', timed); 177 try:; --> 178 result, timings = self._rpc(ActionTag.EXECUTE, payload); 179 except FatalError as e:; 180 raise e.maybe_user_error(ir) from None. File ~/.local/lib/python3.10/site-packages/hail/backend/py4j_backend.py:210, in Py4JBackend._rpc(self, action, payload); 208 path = action_routes[action]; 209 port = self._backend_server_port; --> 210 resp = self._requests_session.post(f'http://localhost:{port}{path}', data=data); 211 if resp.status_code >= 400:; 212 error_json = orjson.loads(resp.content). File /opt/conda/lib/python3.10/site-packages/requests/sessions.py:635, in Session.post(self, url, data, json, **kwargs); 624 def post(self, url, data=None, json=None, **kwargs):; 625 r""""""Sends a POST request. Returns :class:`Response` object.; 626 ; 627 :param url: URL for the new :class:`Request` object.; (...); 632 :rtype: requests.Response; 633 """"""; --> 635 return self.request(""POST"", url, data=data, json=json, **kwargs). File /opt/conda/lib/python3.10/site-packages/requests/sessions.py:587, in Session.request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json); 582 send_kwargs = {; 583 ""timeout"": timeout,; 584 ""allow_redirects"": allow_redirects,; 585 }; 586 send_kwargs.update(settings); --> 587 resp = self.send(prep, **send_kwargs); 589 return resp. File /opt/conda/lib/python3.10/site-packages/requests/sessions.py:701, in Session.send(self, request, **kwargs); 698 start = preferred_cloc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960:11727,load,loads,11727,https://hail.is,https://github.com/hail-is/hail/issues/13960,1,['load'],['loads']
Performance,":11:48 root: INFO: is/hail/codegen/generated/C3.method3 instruction count: 252; 2019-01-22 13:11:48 CodecPool: INFO: Got brand-new decompressor [.gz]; 2019-01-22 13:11:50 root: INFO: Index reader cache queries: 1168; 2019-01-22 13:11:50 root: INFO: Index reader cache hit rate: 0.747431506849315; 2019-01-22 13:11:51 root: INFO: optimize: before:; (TableCount; (MatrixRowsTable; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],rsid:String,varid:String},entry:Struct{GT:Call,GP:+Array[+Float64],dosage:+Float64}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}""))); 2019-01-22 13:11:51 root: INFO: optimize: after:; (TableCount; (MatrixRowsTable; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}""))); 2019-01-22 13:11:51 root: INFO: optimize: before:; (TableCount; (TableMapRows; (TableMapGlobals; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}"")); (SelectFields (); (Ref global))); (SelectFiel",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:28032,optimiz,optimize,28032,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['optimiz'],['optimize']
Performance,":323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$app",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:13020,Load,LoadVCF,13020,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,":458); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.115-71fc978b5c22; Error summary: SocketException: Connection reset. -------------------. Some more content from the failing worker job:. ...; 2023-05-04 01:04:35.959 : INFO: executing D-Array [shuffle_initial_write] with 1 tasks; 2023-05-04 01:04:35.960 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:35.965 GoogleStorageFS$: INFO: createNoCompression: gs://cpg-acute-care-hail/batch-tmp/tmp/hail/pV2Mgy4FVKSGKMwZGafyTh/hail_shuffle_temp_initial-ktRgTs8RfA9fHie5JKHmUy0e020450-e61c-4fa9-9419-2278528f3c86; 2023-05-04 01:04:37.559 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=132096, peakBytesReadabl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:21656,concurren,concurrent,21656,https://hail.is,https://github.com/hail-is/hail/issues/12983,1,['concurren'],['concurrent']
Performance,:458); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.net.SocketException: Connection reset; 	at java.net.SocketInputStream.read(SocketInputStream.java:210); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:464); 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:237); 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190); 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109); 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1400); 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1368); 	at sun.security.ssl.SSLSo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12982:13942,concurren,concurrent,13942,https://hail.is,https://github.com/hail-is/hail/issues/12982,3,['concurren'],['concurrent']
Performance,:58); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$foldConstants$1(FoldConstants.scala:47); E 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); E 	at is.hail.expr.ir.FoldConstants$.foldConstants(FoldConstants.scala:13); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$apply$1(FoldConstants.scala:10); E 	at is.hail.backend.ExecuteContext$.$anonfun$scopedNewRegion$1(ExecuteContext.scala:86); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.backend.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:83); E 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:9); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$4(Optimize.scala:22); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$1(Optimize.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.runOpt$1(Optimize.scala:15); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$2(Optimize.scala:22); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:18); E 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:40); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(Lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:7093,Optimiz,Optimize,7093,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Optimiz'],['Optimize']
Performance,:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Opt,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:4488,Load,LoadVCF,4488,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,; 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized$cl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:6183,Optimiz,Optimize,6183,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,; 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:60); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1476); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:574); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:563); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:288); 	at is.hail.services.package$.retryTransientErrors(package.scala:163); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:286); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:26); 	at is.hail.backend.service.ServiceBackend$$anon$4.call(ServiceBackend.scala:239); 	at is.hail.backend.service.ServiceBackend$$anon$4.call(ServiceBackend.scala:235); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/wes-bipolar-tmp-4day/o/bge-wave-1-VQSR%2FparallelizeAndComputeWithIndex%2FgCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=%2Fresult.2706?alt=media; No such object: wes-bipolar-tmp-4day/bge-wave-1-VQSR/parallelizeAndComputeWithIndex/gCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=/result.2706; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonC,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13409:6693,concurren,concurrent,6693,https://hail.is,https://github.com/hail-is/hail/issues/13409,1,['concurren'],['concurrent']
Performance,; 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332); 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-74bf1eb; Error summary: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:14317,concurren,concurrent,14317,https://hail.is,https://github.com/hail-is/hail/issues/3760,2,['concurren'],['concurrent']
Performance,; 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1806:2413,concurren,concurrent,2413,https://hail.is,https://github.com/hail-is/hail/issues/1806,1,['concurren'],['concurrent']
Performance,; 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:834). is.hail.utils.HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.ut,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:5783,Load,LoadPlink,5783,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,; 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(Lowerin,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5353,Optimiz,Optimize,5353,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,; 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.16-e95038bbed35; Error summary: MatchError: locus<GRCh,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:5778,optimiz,optimizeIR,5778,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['optimiz'],['optimizeIR']
Performance,; 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3074:5162,concurren,concurrent,5162,https://hail.is,https://github.com/hail-is/hail/issues/3074,2,['concurren'],['concurrent']
Performance,; 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3790:6130,concurren,concurrent,6130,https://hail.is,https://github.com/hail-is/hail/issues/3790,1,['concurren'],['concurrent']
Performance,"; -> 1895 force; 1896 ); 1897 return MatrixTable(jmt). ~/bin/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no files; ```. Ba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4775:2587,Load,LoadVCF,2587,https://hail.is,https://github.com/hail-is/hail/issues/4775,1,['Load'],['LoadVCF']
Performance,"; 2023-09-13 16:37:36.614 Worker$: INFO: is.hail.backend.service.Worker be9d88a80695b04a2a9eb5826361e0897d94c042; 2023-09-13 16:37:36.614 Worker$: INFO: running job 38854/47960 at root gs://gnomad-tmp-4day/parallelizeAndComputeWithIndex/s_yyHm37RY7YTSWH29gP5SM0RwKxgs9EXbg9_YMf7ho= with scratch directory '/batch/1c00c7157d4d41bcbf508f12d75329b1'; 2023-09-13 16:37:36.617 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2023-09-13 16:37:36.821 services: WARN: A limited retry error has occured. We will automatically retry 4 more times. Do not be alarmed. (next delay: 1938). The most recent error was javax.net.ssl.SSLException: Connection reset.; 2023-09-13 16:37:38.893 WorkerTimer$: INFO: readInputs took 2278.496020 ms.; 2023-09-13 16:37:38.893 : INFO: RegionPool: initialized for thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 : INFO: TaskReport: stage=0, partition=38854, attempt=0, peakBytes=65536, peakBytesReadable=64.00 KiB, chunks requested=0, cache hits=0; 2023-09-13 16:37:38.903 : INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor48.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecuto",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553:2124,cache,cache,2124,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553,2,['cache'],['cache']
Performance,"; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pytest-dev/pytest-asyncio/blob/master/CHANGELOG.rst"">pytest-asyncio's changelog</a>.</em></p>; <blockquote>; <h1>0.20.1 (22-10-21)</h1>; <ul>; <li>Fixes an issue that warned about using an old version of pytest, even though the most recent version was installed. <code>[#430](https://github.com/pytest-dev/pytest-asyncio/issues/430) &lt;https://github.com/pytest-dev/pytest-asyncio/issues/430&gt;</code>_</li>; </ul>; <h1>0.20.0 (22-10-21)</h1>; <ul>; <li>BREAKING: Removed <em>legacy</em> mode. If you're upgrading from v0.19 and you haven't configured <code>asyncio_mode = legacy</code>, you can upgrade without taking any additional action. If you're upgrading from an earlier version or you have explicitly enabled <em>legacy</em> mode, you need to switch to <em>auto</em> or <em>strict</em> mode before upgrading to this version.</li>; <li>Deprecate use of pytest v6.</li>; <li>Fixed an issue which prevented fixture setup from being cached. <code>[#404](https://github.com/pytest-dev/pytest-asyncio/issues/404) &lt;https://github.com/pytest-dev/pytest-asyncio/pull/404&gt;</code>_</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/c8d017407d39dd81d6864fa9a58ba1240d54be9f""><code>c8d0174</code></a> fix: Do not warn about outdated pytest version when pytest&gt;=7 is installed. (...</li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/6450ddbe974f5359d56317ba8bdda8b2ab48655a""><code>6450ddb</code></a> Prepare release of v0.20.0. (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-asyncio/issues/428"">#428</a>)</li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/150f29c107fbd76641de47e040d43840769ef92c""><code>150f29c</code></a> Build(deps): Bump hypothesis in /dependencies/default (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12390:3610,cache,cached,3610,https://hail.is,https://github.com/hail-is/hail/pull/12390,1,['cache'],['cached']
Performance,; Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:4527,Load,LoadVCF,4527,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,; Delete 1 KiB 5 44.2 12.7 42.5 58.1 ; Delete 100 KiB 5 44.7 10.4 42.8 56.3 ; Delete 1 MiB 5 41.5 3.7 40.2 45.7 ; Download 0 B 5 74.6 7.9 73.2 84.0 ; Download 1 KiB 5 84.3 15.9 80.6 103.4 ; Download 100 KiB 5 81.9 16.0 82.7 99.6 ; Download 1 MiB 5 90.6 6.5 94.5 96.8 ; Metadata 0 B 5 23.6 2.7 23.6 26.3 ; Metadata 1 KiB 5 25.5 2.1 26.9 27.4 ; Metadata 100 KiB 5 26.2 3.6 27.3 29.9 ; Metadata 1 MiB 5 24.0 3.7 23.3 28.4 ; Upload 0 B 5 98.1 16.6 95.5 117.9 ; Upload 1 KiB 5 116.7 21.8 115.5 142.1 ; Upload 100 KiB 5 116.5 17.8 115.1 135.1 ; Upload 1 MiB 5 168.2 18.5 179.6 185.6 . ------------------------------------------------------------------------------; Write Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Write throughput: 977.7 Mibit/s.; Parallelism strategy: both. ------------------------------------------------------------------------------; Read Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Read throughput: 1.11 Gibit/s.; Parallelism strategy: both. ------------------------------------------------------------------------------; System Information ; ------------------------------------------------------------------------------; IP Address: ; 172.21.46.11; Temporary Directory: ; /tmp; Bucket URI: ; gs://hail-jigold/; gsutil Version: ; 5.24; boto Version: ; 2.49.0; Measurement time: ; 2023-06-05 03:25:16 PM ; Running on GCE: ; True; GCE Instance:; 	; Bucket location: ; US-CENTRAL1; Bucket storage class: ; REGIONAL; Google Server: ; ; Google Server IP Addresses: ; 142.250.128.128; 142.251.6.128; 108.177.112.128; 74.125.124.128; 172.217.212.128; 172.217.214.128; 172.253.119.128; 108.177.111.128; 142.250.1.128; 108.177.121.128; 142.250.103.128; 108.177.120.128; 142.250.159.128; 142.251.120.128; 142.251.161.128; 74.125.126.128; Google Server Hostnames: ; i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597:2139,Throughput,Throughput,2139,https://hail.is,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597,1,['Throughput'],['Throughput']
Performance,; E 	at __C23148collect_distributed_array_matrix_native_writer.apply(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply(Unknown Source); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$10(BackendUtils.scala:90); E 	at is.hail.utils.package$.using(package.scala:673); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$9(BackendUtils.scala:89); E 	at is.hail.backend.local.LocalBackend.$anonfun$parallelizeAndComputeWithIndex$4(LocalBackend.scala:150); E 	at is.hail.utils.package$.using(package.scala:673); E 	at is.hail.backend.local.LocalBackend.$anonfun$parallelizeAndComputeWithIndex$3(LocalBackend.scala:150); E 	at is.hail.utils.package$.$anonfun$runAll$2(package.scala:1038); E 	at is.hail.CancellingExecutorService.$anonfun$newTaskFor$2(package.scala:1090); E 	at is.hail.CancellingExecutorService$CancellingTask.run(package.scala:1067); E 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515); E 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264); E 	at is.hail.relocated.com.google.common.util.concurrent.MoreExecutors$DirectExecutorService.execute(MoreExecutors.java:327); E 	at is.hail.CancellingExecutorService.execute(package.scala:1111); E 	at java.base/java.util.concurrent.ExecutorCompletionService.submit(ExecutorCompletionService.java:184); E 	at is.hail.utils.package$.$anonfun$runAll$1(package.scala:1038); E 	at scala.collection.Iterator.foreach(Iterator.scala:943); E 	at scala.collection.Iterator.foreach$(Iterator.scala:943); E 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); E 	at scala.collection.IterableLike.foreach(IterableLike.scala:74); E 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73); E 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56); E 	at is.hail.utils.package$.runAll(package.scala:1038); E 	at is.hail.utils.package$.$anonfun$runAl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14705:4115,concurren,concurrent,4115,https://hail.is,https://github.com/hail-is/hail/issues/14705,1,['concurren'],['concurrent']
Performance,; E 	at is.hail.io.fs.FS.open(FS.scala:572); E 	at is.hail.io.fs.FS.open$(FS.scala:571); E 	at is.hail.io.fs.HadoopFS.open(HadoopFS.scala:85); E 	at is.hail.io.fs.FS.open(FS.scala:569); E 	at is.hail.io.fs.FS.open$(FS.scala:569); E 	at is.hail.io.fs.HadoopFS.open(HadoopFS.scala:85); E 	at is.hail.io.index.IndexReader$.readTypes(IndexReader.scala:65); E 	at is.hail.io.bgen.LoadBgen$.$anonfun$getBgenFileMetadata$2(LoadBgen.scala:208); E 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); E 	at scala.collection.TraversableLike.map(TraversableLike.scala:286); E 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279); E 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198); E 	at is.hail.io.bgen.LoadBgen$.getBgenFileMetadata(LoadBgen.scala:207); E 	at is.hail.io.bgen.MatrixBGENReader$.apply(LoadBgen.scala:387); E 	at is.hail.io.bgen.MatrixBGENReader$.fromJValue(LoadBgen.scala:365); E 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:116); E 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1815); E 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1738); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:2164); E 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:2136); E 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:2164); E 	at is.hail.backend.Backend.$anonfun$matrixTableType$2(Backend.scala:186); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); E 	at is.hail.utils.package$.using(package.scala:664); E 	at i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001:3186,Load,LoadBgen,3186,https://hail.is,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001,1,['Load'],['LoadBgen']
Performance,; E 	at org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream.init(GzipCompressorInputStream.java:185); E 	at org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream.<init>(GzipCompressorInputStream.java:168); E 	at is.hail.io.fs.GZipCompressionCodec$.makeInputStream(FS.scala:125); E 	at is.hail.io.fs.FS.open(FS.scala:563); E 	at is.hail.io.fs.FS.open$(FS.scala:560); E 	at is.hail.io.fs.HadoopFS.open(HadoopFS.scala:85); E 	at is.hail.io.fs.FS.open(FS.scala:578); E 	at is.hail.io.fs.FS.open$(FS.scala:577); E 	at is.hail.io.fs.HadoopFS.open(HadoopFS.scala:85); E 	at is.hail.io.fs.FS.open(FS.scala:572); E 	at is.hail.io.fs.FS.open$(FS.scala:571); E 	at is.hail.io.fs.HadoopFS.open(HadoopFS.scala:85); E 	at is.hail.io.fs.FS.open(FS.scala:569); E 	at is.hail.io.fs.FS.open$(FS.scala:569); E 	at is.hail.io.fs.HadoopFS.open(HadoopFS.scala:85); E 	at is.hail.io.index.IndexReader$.readTypes(IndexReader.scala:65); E 	at is.hail.io.bgen.LoadBgen$.$anonfun$getBgenFileMetadata$2(LoadBgen.scala:208); E 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); E 	at scala.collection.TraversableLike.map(TraversableLike.scala:286); E 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279); E 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198); E 	at is.hail.io.bgen.LoadBgen$.getBgenFileMetadata(LoadBgen.scala:207); E 	at is.hail.io.bgen.MatrixBGENReader$.apply(LoadBgen.scala:387); E 	at is.hail.io.bgen.MatrixBGENReader$.fromJValue(LoadBgen.scala:365); E 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:116); E 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1815); E 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1738); E 	at is.hail.uti,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001:2607,Load,LoadBgen,2607,https://hail.is,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001,1,['Load'],['LoadBgen']
Performance,; at is.hail.variant.VariantDatasetFunctions.filterGenotypes(VariantDataset.scala:449); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: is.hail.asm4s.AsmFunction2; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.lang.ClassLoader.defineClass(ClassLoader.java:642); at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:254); at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:250); at is.hail.asm4s.package$.loadClass(package.scala:261); at is.hail.asm4s.FunctionBuilder$$anon$2.apply(FunctionBuilder.scala:218); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:80); at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:53); at is.hail.expr.Parser$$anonfun$evalTypedExpr$1.apply(Parser.scala:71); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:324); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:321); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:13657,load,loadClass,13657,https://hail.is,https://github.com/hail-is/hail/issues/2966,1,['load'],['loadClass']
Performance,"; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(Ro",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:13354,Load,LoadVCF,13354,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,; at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); ... 18 more; Caused by: java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.U,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:3904,concurren,concurrent,3904,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['concurren'],['concurrent']
Performance,; at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.Torren,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:10062,concurren,concurrent,10062,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['concurren'],['concurrent']
Performance,; at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.Ja,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:12200,concurren,concurrent,12200,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['concurren'],['concurrent']
Performance,; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/eOrFpVrN98GBIizi/specs.idx BlockBlob Hot 16 application/octet-stream 2023-06-09T12:43:34+00:00; ```. I looked at the status:. ```; az storage blob download --account-name haildevtest --container test --name batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/2/31Owgv/status.json | jq '.' | less; ```. which contained an error (I un-escaped the string here):. ```; JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	... 7 more; Caused by: is.hail.backend.service.EndOfInputException; 	at is.hail.backend.service.ServiceBackendSocketAPI2.read(ServiceBackend.scala:497); 	at is.hail.backend.service.ServiceBackendSocketAPI2.re,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13160:4291,concurren,concurrent,4291,https://hail.is,https://github.com/hail-is/hail/pull/13160,1,['concurren'],['concurrent']
Performance,; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux FS.cpp -MG -M -MF build/FS.d -MT build/FS.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Encoder.cpp -MG -M -MF build/Encoder.d -MT build/Encoder.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Decoder.cpp -MG -M -MF build/Decoder.d -MT build/Decoder.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux ApproximateQuantiles_test.cpp -MG -M -MF build/ApproximateQuantiles_test.d -MT build/ApproximateQuantiles_te; st.o; make[1]: Leaving directory `/mnt/tmp/hail/hail/src/main/c'; make[1]: Entering directory `/mnt/tmp/hail/hail/src/main/c'; g++ -o build/NativeBoot.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -MD -MF build/NativeBoot.d -MT build/NativeBoot.o -c NativeBoot.cpp; g++ -fvisibility=default -rdynamic -shared -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux build/NativeBoot.o -o lib/linux-x86-,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:9545,cache,cache-tests,9545,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cache-tests']
Performance,; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.032ms self 0.028ms children 0.003ms %children 10.95%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:173791,Optimiz,OptimizePass,173791,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.015ms self 0.015ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.056ms self 0.056ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.050ms self 0.041ms children 0.010ms %children 19.41%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:30474,Optimiz,OptimizePass,30474,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"</p>; <ul>; <li>Support for Jinja2 version 3</li>; </ul>; <p>__ <a href=""https://pypi.org/project/nbsphinx/0.8.6/"">https://pypi.org/project/nbsphinx/0.8.6/</a>; __ <a href=""https://github.com/spatialaudio/nbsphinx/compare/0.8.5...0.8.6"">https://github.com/spatialaudio/nbsphinx/compare/0.8.5...0.8.6</a></p>; <p>Version 0.8.5 -- 2021-05-12 -- PyPI__ -- diff__</p>; <ul>; <li>Freeze Jinja2 version to 2.11 (for now, until a bugfix is found)</li>; <li>Add <code>theme_comparison.py</code> tool for creating multiple versions; (with different HTML themes) of the docs at once</li>; </ul>; <p>__ <a href=""https://pypi.org/project/nbsphinx/0.8.5/"">https://pypi.org/project/nbsphinx/0.8.5/</a>; __ <a href=""https://github.com/spatialaudio/nbsphinx/compare/0.8.4...0.8.5"">https://github.com/spatialaudio/nbsphinx/compare/0.8.4...0.8.5</a></p>; <p>Version 0.8.4 -- 2021-04-29 -- PyPI__ -- diff__</p>; <ul>; <li>Support for <code>mathjax3_config</code> (for Sphinx &gt;= 4)</li>; <li>Force loading MathJax on HTML pages generated from notebooks; (can be disabled with <code>nbsphinx_assume_equations = False</code>)</li>; </ul>; <p>__ <a href=""https://pypi.org/project/nbsphinx/0.8.4/"">https://pypi.org/project/nbsphinx/0.8.4/</a>; __ <a href=""https://github.com/spatialaudio/nbsphinx/compare/0.8.3...0.8.4"">https://github.com/spatialaudio/nbsphinx/compare/0.8.3...0.8.4</a></p>; <p>Version 0.8.3 -- 2021-04-09 -- PyPI__ -- diff__</p>; <ul>; <li>Increase <code>line_length_limit</code> (for <code>docutils</code> 0.17+)</li>; </ul>; <p>__ <a href=""https://pypi.org/project/nbsphinx/0.8.3/"">https://pypi.org/project/nbsphinx/0.8.3/</a>; __ <a href=""https://github.com/spatialaudio/nbsphinx/compare/0.8.2...0.8.3"">https://github.com/spatialaudio/nbsphinx/compare/0.8.2...0.8.3</a></p>; <p>Version 0.8.2 -- 2021-02-28 -- PyPI__ -- diff__</p>; <ul>; <li>Add support for <code>data-footcite</code> HTML attribute</li>; <li>Disable automatic highlighting in notebooks,; setting <code>highlight_language</code> is no",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11477:3687,load,loading,3687,https://hail.is,https://github.com/hail-is/hail/pull/11477,1,['load'],['loading']
Performance,"<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collectin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5301,cache,cached,5301,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"<a href=""https://pypi.org/project/nbsphinx/0.8.7/"">https://pypi.org/project/nbsphinx/0.8.7/</a></p>; <ul>; <li>Fix assertion error in LaTeX build with Sphinx 4.1.0+</li>; </ul>; <h2>nbsphinx 0.8.6</h2>; <p><a href=""https://pypi.org/project/nbsphinx/0.8.6/"">https://pypi.org/project/nbsphinx/0.8.6/</a></p>; <ul>; <li>Support for Jinja2 version 3</li>; </ul>; <h2>nbsphinx 0.8.5</h2>; <p><a href=""https://pypi.org/project/nbsphinx/0.8.5/"">https://pypi.org/project/nbsphinx/0.8.5/</a></p>; <ul>; <li>Freeze Jinja2 version to 2.11 (for now, until a bugfix is found)</li>; <li>Add <code>theme_comparison.py</code> tool for creating multiple versions (with different HTML themes) of the docs at once</li>; </ul>; <h2>nbsphinx 0.8.4</h2>; <p><a href=""https://pypi.org/project/nbsphinx/0.8.4/"">https://pypi.org/project/nbsphinx/0.8.4/</a></p>; <ul>; <li>Support for <code>mathjax3_config</code> (for Sphinx &gt;= 4)</li>; <li>Force loading MathJax on HTML pages generated from notebooks (can be disabled with <code>nbsphinx_assume_equations = False</code>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/spatialaudio/nbsphinx/blob/master/NEWS.rst"">nbsphinx's changelog</a>.</em></p>; <blockquote>; <p>Version 0.8.8 -- 2021-12-31 -- PyPI__ -- diff__</p>; <ul>; <li>Support for the <code>sphinx_codeautolink</code> extension</li>; <li>Basic support for the <code>text</code> builder</li>; </ul>; <p>__ <a href=""https://pypi.org/project/nbsphinx/0.8.8/"">https://pypi.org/project/nbsphinx/0.8.8/</a>; __ <a href=""https://github.com/spatialaudio/nbsphinx/compare/0.8.7...0.8.8"">https://github.com/spatialaudio/nbsphinx/compare/0.8.7...0.8.8</a></p>; <p>Version 0.8.7 -- 2021-08-10 -- PyPI__ -- diff__</p>; <ul>; <li>Fix assertion error in LaTeX build with Sphinx 4.1.0+</li>; </ul>; <p>__ <a href=""https://pypi.org/project/nbsphinx/0.8.7/"">https://pypi.org/project/nbsphinx/0.8.7/</a>; __ <a href=""https://github.com/spatialaudio/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11477:1477,load,loading,1477,https://hail.is,https://github.com/hail-is/hail/pull/11477,1,['load'],['loading']
Performance,"<code>detect_all</code> return child prober confidences (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/210"">#210</a>)</li>; <li>Updated examples in docs (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/223"">#223</a>, <a href=""https://github.com/domdfcoding""><code>@​domdfcoding</code></a>)</li>; <li>Documentation fixes (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/212"">#212</a>, <a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/224"">#224</a>, <a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/225"">#225</a>, <a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/226"">#226</a>, <a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/220"">#220</a>, <a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/221"">#221</a>, <a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/244"">#244</a> from too many to mention)</li>; <li>Minor performance improvements (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/252"">#252</a>, <a href=""https://github.com/deedy5""><code>@​deedy5</code></a>)</li>; <li>Add support for Python 3.10 when testing (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/232"">#232</a>, <a href=""https://github.com/jdufresne""><code>@​jdufresne</code></a>)</li>; <li>Lots of little development cycle improvements, mostly thanks to <a href=""https://github.com/jdufresne""><code>@​jdufresne</code></a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/chardet/chardet/commit/ff5dcb25a59990e43683b8e9057f6f746bfb2658""><code>ff5dcb2</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/254"">#254</a> from chardet/master</li>; <li><a href=""https://github.com/chardet/chardet/commit/322229573173307e1380eb151ea446b8c6fe2c3b""><code>3222295</code></a> Linte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12107:2456,perform,performance,2456,https://hail.is,https://github.com/hail-is/hail/pull/12107,1,['perform'],['performance']
Performance,"<code>wheel</code>), and specify their; own version of OpenSSL. For those users, the <code>CFLAGS</code>, <code>LDFLAGS</code>,; <code>INCLUDE</code>, <code>LIB</code>, and <code>CRYPTOGRAPHY_SUPPRESS_LINK_FLAGS</code> environment; variables will no longer be respected. Instead, users will need to; configure their builds <code>as documented here</code>_.</li>; <li>Added support for; :ref:<code>disabling the legacy provider in OpenSSL 3.0.x&lt;legacy-provider&gt;</code>.</li>; <li>Added support for disabling RSA key validation checks when loading RSA; keys via; :func:<code>~cryptography.hazmat.primitives.serialization.load_pem_private_key</code>,; :func:<code>~cryptography.hazmat.primitives.serialization.load_der_private_key</code>,; and; :meth:<code>~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateNumbers.private_key</code>.; This speeds up key loading but is :term:<code>unsafe</code> if you are loading potentially; attacker supplied keys.</li>; <li>Significantly improved performance for; :class:<code>~cryptography.hazmat.primitives.ciphers.aead.ChaCha20Poly1305</code></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pyca/cryptography/commit/d6951dca25de45abd52da51b608055371fbcde4e""><code>d6951dc</code></a> changelog + security fix backport (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8231"">#8231</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/138da90c8450446b19619e3faa77b9da54c34be3""><code>138da90</code></a> workaround scapy bug in downstream tests (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8218"">#8218</a>) (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8228"">#8228</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/69527bc79095c9646d7e839121f0783477892ecc""><code>69527bc</code></a> bookworm is py311 now (<a href=""https:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12668:3117,perform,performance,3117,https://hail.is,https://github.com/hail-is/hail/pull/12668,4,['perform'],['performance']
Performance,"<li>The project’s sdist now includes all needed files to run the test suite (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/issues/49"">#49</a>, <a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/50"">#50</a>). Thanks to <a href=""https://github.com/jayvdb""><code>@​jayvdb</code></a>.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/20d279d3cba11d64529ee88c5a2092c5c09919b6""><code>20d279d</code></a> Release v0.13.1</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/5bd2922633791277f3073135f779fee3e6684bb4""><code>5bd2922</code></a> Update <code>patch_click</code> to fix compatibility issue with click 8.1.0. Fix <a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/issues/132"">#132</a> (#...</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/e16056828523d9af3e13b67243d62830ff03d89d""><code>e160568</code></a> chore(deps): update actions/cache action to v3</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/bce95021c33e9206104512c412751ee435a6606b""><code>bce9502</code></a> chore(deps): update dependency prettier to v2.6.1</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/c7ebbaedf5bc6a8c562a46941681d7bc8598497b""><code>c7ebbae</code></a> chore(deps): update dependency prettier to v2.6.0</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/efd710b42cf0982582bb8a4f345ccfa967866b97""><code>efd710b</code></a> chore(deps): update dependency coverage to v6.3.2</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/438cf6131c1784de8bc9b34970beace1ec7c52af""><code>438cf61</code></a> Update tested Python versions on GitHub (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/issues/122"">#122</a>)</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/b64ec22effafffc6a1371e544c560e6bfc24b56e""><code>b64",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11713:7472,cache,cache,7472,https://hail.is,https://github.com/hail-is/hail/pull/11713,1,['cache'],['cache']
Performance,"=""analysis_type=CombineVariants input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=[(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.recalibrated.vcf), (RodBinding name=variant2 source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.indels.filtered.vcf)] out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub genotypemergeoption=UNSORTED filteredrecordsmergetype=KEEP_IF_ANY_UNFILTERED multipleallelesmergetype=BY_TYPE rod_priority_list=null printComplexMerges=false filteredAreUncalled=false minimalVCF=false",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:2797,perform,performanceLog,2797,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['perform'],['performanceLog']
Performance,=/opt/local/include/gcc49/c++/ --with-gmp=/opt/local --with-mpfr=/opt/local --with-mpc=/opt/local --with-isl=/opt/local --disable-isl-version-check --with-cloog=/opt/local --disable-cloog-version-check --enable-stage1-checking --disable-multilib --enable-lto --enable-libstdcxx-time --with-as=/opt/local/bin/as --with-ld=/opt/local/bin/ld --with-ar=/opt/local/bin/ar --with-bugurl=https://trac.macports.org/newticket --with-pkgversion='MacPorts gcc49 4.9.3_0' --with-build-config=bootstrap-debug; Thread model: posix; gcc version 4.9.3 (MacPorts gcc49 4.9.3_0) . **sysctl -a | grep machdep.cpu**; machdep.cpu.tsc_ccc.denominator: 0; machdep.cpu.tsc_ccc.numerator: 0; machdep.cpu.thread_count: 8; machdep.cpu.core_count: 4; machdep.cpu.address_bits.virtual: 48; machdep.cpu.address_bits.physical: 36; machdep.cpu.tlb.shared: 512; machdep.cpu.tlb.data.large: 32; machdep.cpu.tlb.data.small: 64; machdep.cpu.tlb.inst.large: 8; machdep.cpu.tlb.inst.small: 64; machdep.cpu.cache.size: 256; machdep.cpu.cache.L2_associativity: 8; machdep.cpu.cache.linesize: 64; machdep.cpu.arch_perf.fixed_width: 48; machdep.cpu.arch_perf.fixed_number: 3; machdep.cpu.arch_perf.events: 0; machdep.cpu.arch_perf.events_number: 7; machdep.cpu.arch_perf.width: 48; machdep.cpu.arch_perf.number: 4; machdep.cpu.arch_perf.version: 3; machdep.cpu.xsave.extended_state1: 1 0 0 0; machdep.cpu.xsave.extended_state: 7 832 832 0; machdep.cpu.thermal.energy_policy: 0; machdep.cpu.thermal.hardware_feedback: 0; machdep.cpu.thermal.package_thermal_intr: 1; machdep.cpu.thermal.fine_grain_clock_mod: 1; machdep.cpu.thermal.core_power_limits: 1; machdep.cpu.thermal.ACNT_MCNT: 1; machdep.cpu.thermal.thresholds: 2; machdep.cpu.thermal.invariant_APIC_timer: 1; machdep.cpu.thermal.dynamic_acceleration: 1; machdep.cpu.thermal.sensor: 1; machdep.cpu.mwait.sub_Cstates: 135456; machdep.cpu.mwait.extensions: 3; machdep.cpu.mwait.linesize_max: 64; machdep.cpu.mwait.linesize_min: 64; machdep.cpu.processor_flag: 4; machdep.cpu.microcode_ver,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1274#issuecomment-274242543:1657,cache,cache,1657,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-274242543,1,['cache'],['cache']
Performance,"=0x00007f50c4f23000 nid=0x82c waiting on condition [0x00007f5084eeb000]; java.lang.Thread.State: WAITING (parking); 	at sun.misc.Unsafe.park(Native Method); 	- parking to wait for <0x00000000e8ddaea0> (a scala.concurrent.impl.Promise$CompletionLatch); 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); 	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242); 	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258); 	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263); 	at scala.concurrent.Await$.$anonfun$result$1(package.scala:220); 	at scala.concurrent.Await$$$Lambda$2201/1092639564.apply(Unknown Source); 	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:57); 	at scala.concurrent.Await$.result(package.scala:146); 	at is.hail.backend.service.ServiceBackend.parallelizeAndComputeWithIndex(ServiceBackend.scala:145); ...; ```. This is the line that waits to upload the compiled code for the workers to Google Cloud Storage. The other threads appear to be waiting on the memory service:; ```; ""pool-2-thread-2"" #27 prio=5 os_prio=0 tid=0x00007f5028ad9000 nid=0x88d waiting on condition [0x00007f50274fc000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method); 	at is.hail.services.package$.sleepAndBackoff(package.scala:32); 	at is.hail.services.package$.retryTransientErrors(package.scala:86); 	at is.hail.services.Requester.requestWithHandler(Requester.scala:69); 	at is.hail.services.Requester.request(Requester.scala:94); 	at is.hail.services.memory_client.MemoryClient.w",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:1165,concurren,concurrent,1165,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['concurren'],['concurrent']
Performance,==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB); Collecting dill==0.3.7; Using cached dill-0.3.7-py3-none-any.whl (115 kB); Collecting frozenlist==1.4.0; Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB); Collecting google-api-core==2.11.1; Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:35284,cache,cached,35284,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,==============================================>(1049 + 1) / 1050]hail: info: Ordering unsorted dataset with network shuffle[A^[[A; [Stage 1:====================================================>(1043 + 7) / 1050]hail: importvcf: caught exception: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer; at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:143); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:142); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.rdd.OrderedRDD$.calculateKeyRanges(OrderedRDD.scala:142); at org.apache.spark.rdd.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/673:1219,Load,LoadVCF,1219,https://hail.is,https://github.com/hail-is/hail/issues/673,1,['Load'],['LoadVCF']
Performance,==================================>(1049 + 1) / 1050]hail: info: Ordering unsorted dataset with network shuffle[A^[[A; [Stage 1:====================================================>(1043 + 7) / 1050]hail: importvcf: caught exception: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer; at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:143); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:142); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.rdd.OrderedRDD$.calculateKeyRanges(OrderedRDD.scala:142); at org.apache.spark.rdd.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/673:1234,Load,LoadVCF,1234,https://hail.is,https://github.com/hail-is/hail/issues/673,1,['Load'],['LoadVCF']
Performance,"==========> (1 + 1) / 2]hail: info: running: vep --force --config /home/users/cseed/vep.properties; [Stage 1:======================================> (12 + 6) / 18]hail: vep: caught exception: Job aborted due to stage failure: Task 17 in stage 1.0 failed 4 times, most recent failure: Lost task 17.3 in stage 1.0 (TID 22, nid00019.urika.com): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:836); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:125); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:113); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:127); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:134); at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:512); at org.apache.spark.storage.BlockManager.getLocal(BlockManager.scala:429); at org.apache.spark.storage.BlockManager.get(BlockManager.scala:618); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44); at org.apache.spark.rdd.RDD.iterator(RDD.scala:262); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/430:1214,Cache,CacheManager,1214,https://hail.is,https://github.com/hail-is/hail/issues/430,4,"['Cache', 'concurren']","['CacheManager', 'concurrent']"
Performance,"=registry,ref=gcr.io/hail-vdc/foo; ```; Will use as a cache source the `latest` tag in the `gcr.io/hail-vdc/foo` repository. It is *not* sufficient for an image to be present in the repository and untagged or with a different tag from `latest`. In particular, every push to the `cache` tag prevents us from using other images even though they are in the registry! For example, I pushed two images to `cache`:. ```; (base) # gcloud container images list-tags gcr.io/hail-vdc/dktest; DIGEST TAGS TIMESTAMP; fb551d9bdb94 2022-06-10T14:16:39; afb4c5ad2d7b cache,latest 2022-06-10T14:15:55; ```. If I rebuild [1] the most recently pushed image with; ```; --import-cache type=registry,ref=gcr.io/hail-vdc/dktest:cache; ```; it succeeds in getting the cache. If I rebuild the other image with the same import-cache, it does not see that the (untagged) image is already there! . ---. This all suggests that all our attempts at image caching are failing terribly. Options:; 1. Only deploy builds push to a `:cache` tag, everyone uses that tag.; 2. List all the tags in the repository and include them all as --cache-from's (this doesn't actually work: https://github.com/moby/moby/issues/34715#issuecomment-425933774); 3. Push a tag for each git SHA and then include as --cache-from's the last ten git SHAs on this branch, the most recent common commit with main (i.e. `git merge-base origin/main this-branch`), maybe the current main, and maybe the PR number?; 4. Write our own OCI image builder so we can write our own OCI image cache that actually works the way it ought to (everything in the registry is considered fair game for the cache). It seems like 3 is actually a decent solution that should enable lots of caching.; 1. The last ten SHAs on the branch should speed up repeated builds when you're fixing little bugs.; 2. The most recent common commit with main should avoid rebuilds unless the packages changed.; 3. I suspect the current main is actually not helpful (either 2 will work or 3 wouldn't",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800:1569,cache,cache,1569,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800,2,['cache'],['cache']
Performance,"> 1. aiohttp is an option, but appears to be generally considered slow on a per-response basis. Can you point me to the benchmarks? The only head-to-head one I found was this:. https://github.com/samuelcolvin/aiohttp-vs-sanic-vs-japronto. where sanic is faster for smaller examples but aiohttp performs MUCH better (3x latency, 10x throughput) when interacting with a db.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461195082:294,perform,performs,294,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461195082,3,"['latency', 'perform', 'throughput']","['latency', 'performs', 'throughput']"
Performance,"> 1. publish-public-images.sh should publish to ${DOCKER_PREFIX}/hailgenetics/$name:$hail_pip_version. Yeah. My bad on that, I broke this by changing the other code to insert `hailgenetics` without changing `publish-public-images.sh`. > 2. ... This seems great to me. ---. I don't want to users to write `gcr.io/hail-vdc/hailgenetics/python-dill` because I think of GCR as an implementation detail of Hail Batch. Instead, I want them to believe they're pulling from `docker.io/hailgenetics/python-dill`, but with a cache in between docker hub and us.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10382#issuecomment-827842202:515,cache,cache,515,https://hail.is,https://github.com/hail-is/hail/pull/10382#issuecomment-827842202,1,['cache'],['cache']
Performance,"> 2. It felt outside of the scope of my PR to change that to some queue solution. OK, great, thanks for clarifying!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461202032:66,queue,queue,66,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461202032,1,['queue'],['queue']
Performance,"> > 1. It felt outside of the scope of my PR to change that to some queue solution.; > ; > OK, great, thanks for clarifying!. Sure, this is something I am focusing on improving moving forward. Thanks for your feedback.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461210710:68,queue,queue,68,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461210710,2,['queue'],['queue']
Performance,"> > 1. aiohttp is an option, but appears to be generally considered slow on a per-response basis; > ; > Can you point me to the benchmarks? The only head-to-head one I found was this:; > ; > https://github.com/samuelcolvin/aiohttp-vs-sanic-vs-japronto; > ; > where sanic is faster for smaller examples but aiohttp performs MUCH better (3x latency, 10x throughput) when interacting with a db. Sanic was chosen because it's a near drop-in for Flask, and is the most popular afaik library built around asyncio. It has 2x as many stars as aiohttp, slightly more forks. The original motivation for considering aiohttp: ; https://magic.io/blog/uvloop-blazing-fast-python-networking. In short, one of the creators of asyncio discusses uvloop performance relative to other libraries. They key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/29",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:314,perform,performs,314,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,12,"['bottleneck', 'latency', 'perform', 'throughput']","['bottleneck', 'latency', 'performance', 'performs', 'throughput']"
Performance,"> > I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first.; > ; > Can you elaborate more? I'm not sure which code paths you are referring to. Mainly that the commit procedure branches on whether the start id is 1 and that we sometimes grab the update id from the batch token and sometimes from the update table. Not very different code paths but slightly different, which could lead to some confusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1220825403:606,perform,performance,606,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1220825403,2,['perform'],['performance']
Performance,"> @akotlar Ok, so I finally managed to remove the internal requests. The asyncio learning curve was higher than I expected.; > ; > The final product is a lot tighter than I expected. I eliminated all the run_forever and run_once stuff, all the threading, and I was able to move the log back into server.py.; > ; > It involves three new things:; > ; > * [kill the whole loop if anything goes wrong](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R899), which works wonderfully with k8s' automatic pod restarting; > * [use a concurrent thread pool](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R904) for any legacy blocking operations; > * [a blocking-to-async convertor](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R823) and a [blocking iterator to async iterator](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R828) both of which stick blocking operations on a separate thread pool.; > ; > Legacy blocking operations might end up queueing behind one another in the ""blocking pool"", but the rest of the application continues without interruption on the main event loop.; > ; > I took the chance to reorder the k8s refresh and the k8s watch functions to be closer together, but that made the diff worse :/.; > ; > Probably demands another review on Monday.; > ; > cc: @cseed, possibly some asyncio engineering best practices in this. Awesome work. I think this will make batch much more performant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-482761679:561,concurren,concurrent,561,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-482761679,6,"['concurren', 'perform', 'queue']","['concurrent', 'performant', 'queueing']"
Performance,"> @cseed, do you recall why you chose to delete and then use apply in ci/create_database.py?. I think no reason. I wasn't aware of the race condition and didn't have an alternative for applying changes at hand. This looks good to me. I ran a few examples but I couldn't actually tell how `--save-config` changes the output.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10217#issuecomment-808525352:135,race condition,race condition,135,https://hail.is,https://github.com/hail-is/hail/pull/10217#issuecomment-808525352,1,['race condition'],['race condition']
Performance,"> @patrick-schultz have you thought about how to wrap MakeStream (split up to avoid JVM bytecode limits) in the old or new infrastructure?. In general, I don't know how to wrap pieces of a stream in methods, since streams need to be able to jump between labels defined by different stream nodes. Maybe to handle large `MakeStream`s, and decomposing other streams into multiple methods, we could compile some streams into iterators. That shouldn't be hard to do, and would let us experiment with the performance effects. (I've been nervous about stream code never getting jit compiled, and never considered this option. I think I like it.). > Also, ToArray(MakeStream(...)) seems seems like it will be less efficient since it switches of the index while MakeArray inlines the array construction. Yeah, that seems unavoidable. If MakeArray(...) generates better code than ToArray(MakeStream(...)), is it worth keeping MakeArray, with a ToArray(MakeStream(...)) -> MakeArray(...) simplify rule?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8148#issuecomment-590861391:499,perform,performance,499,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590861391,2,['perform'],['performance']
Performance,"> A user reported this error concurrent.futures._base.TimeoutError with no stack trace while copying files in a batch job. . This. No stack trace. If you look at this output, the previous stack trace is part of the WARNING message. ```; INFO:deploy_config:deploy config file not found: None; INFO:hailtop.aiocloud.aiogoogle.credentials:using credentials file /gsa-key/key.json: GoogleServiceAccountCredentials for XXXXX@PROJECT.iam.gserviceaccount.com; WARNING:hailtop.utils:Encountered 2 errors (current delay: 0.2). My stack trace is File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiotools/copy.py"", line 128, in <module>; asyncio.run(main()); File ""/usr/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 735, in retry_transient_errors; st = ''.join(traceback.format_stack()); . Most recent error was; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 729, in retry_transient_errors; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 134, in request_and_raise_for_status; resp = await self.client_session._request(method, url, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 634, in _request; break; File ""/usr/local/lib/python3.7/dist-packages/aiohttp/helpers.py"", line 721, in __exit__; raise asyncio.TimeoutError from None; concurrent.futures._base.TimeoutError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11817#issuecomment-1117642330:29,concurren,concurrent,29,https://hail.is,https://github.com/hail-is/hail/pull/11817#issuecomment-1117642330,2,['concurren'],['concurrent']
Performance,"> Ah sorry, I forgot to update the original commit message as that's not actually correct. The optimization is that we don't need to iterate through all blobs until we find the exact blob matching our path name. The list operation returns all blobs that start with the prefix of that path. If we see a blob with a different name that is a child of our path f'{path}/foo, then we know it's a directory and don't need to iterate anymore (although it could be a file as well, but in Scala we don't currently throw errors on paths that are both files and directories, so we just choose the first we see). If we see a blob that matches the path exactly, then we know it's a file and stop iterating. The only reason we need to iterate through more than one blob is if there's blobs that are like '{path}zzzzz/foo or '{path}szzzzz. We need to ignore these as they don't provide any information on whether {path} is a file or directory. This is where isChildOf is needed because we need to make sure the blob is actually a child of the path such as '{path}/file and not {path}zzzzz/file. Ah thanks, this makes more sense to me now. > The other option is to do 2 queries. One to check if it's a file and the next to check if there are any child blobs. This does sound simpler conceptually. I'm not sure off the top of my head what is better performance-wise: the two network requests for a directory or loading a whole page of results in a single request when we only need max 2 results. What do you prefer?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13390#issuecomment-1677489464:95,optimiz,optimization,95,https://hail.is,https://github.com/hail-is/hail/pull/13390#issuecomment-1677489464,6,"['load', 'optimiz', 'perform']","['loading', 'optimization', 'performance-wise']"
Performance,"> Ah, I thought I said I was happy to fix the optimized version rather than revert. I do think it can be simplified, though, per my comments.; > ; > Cotton also had the suggestion of writing this function unstaged using two utility functions:; > ; > ```scala; > def findFirstNonZeroByte(addr: Long, n: Long): Long; > def allPresent(addr: Long, n: Long): Long // uses findFirstNonZeroByte; > ```. How do we convert from address: Code[Long] to Long without going through emit? I suppose you mean creating an unstated version, and then calling it through invokeScalaObject?. I'm happy to write this. edit: In this version, if we don't want to assume alignment, what is an easier way other than checking the alignment of the addr? I think we're assuming int alignment in much of our codebase, but this is also not guaranteed (since we only aim to guarantee power of 2, as discussed last week).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7646#issuecomment-561475978:46,optimiz,optimized,46,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561475978,2,['optimiz'],['optimized']
Performance,"> ApproxCDF should probably be marked non-commutative. Isn't it also non-associative? We're assuming everything is associative. > isn't TakeBy also non-commutative?. I don't think we guarantee a stable sort, so I'm inclined to leave it as an unstable sort for now in the name of performance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6837#issuecomment-519629507:279,perform,performance,279,https://hail.is,https://github.com/hail-is/hail/pull/6837#issuecomment-519629507,1,['perform'],['performance']
Performance,"> Banning versions completely is a little tricky because the user can specify a JAR url directly instead of a version. JARs don't currently have a simple way to report pip version to the worker, though we could cook something up. We could also just delete the old JARs. I feel like we should make a (cached) request to ensure that the JAR exists in the front-end upon job submission and return a 400 if it doesn't exist instead of waiting for the worker to error. It would:. - Allow us to remove support by deleting old jars; - Fail fast (I know I have accidentally messed up deploying a dev jar and had to wait until a worker came online to find out); - Avoid alerts from workers that can't find dev jars due to mistakes like I mention above",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12941#issuecomment-1529754664:300,cache,cached,300,https://hail.is,https://github.com/hail-is/hail/pull/12941#issuecomment-1529754664,2,['cache'],['cached']
Performance,"> For best performance, include precisely those fields required for your analysis. We'll be able to do this automatically very soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3612#issuecomment-389888267:11,perform,performance,11,https://hail.is,https://github.com/hail-is/hail/issues/3612#issuecomment-389888267,1,['perform'],['performance']
Performance,"> Hmm, flake is failing on this; > ; > ```; > + flake8 batch; > batch/server/__init__.py:1:1: F401 '.server.serve' imported but unused; > batch/server/__init__.py:1:1: F401 '.server.run_once' imported but unused; > ```; > But that's an incorrect bug. I wonder if the CI is loading the wrong python version. I'll check. Strange. .serve is unmodified from master; run_once is only used in a test, could that be an issue?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5065#issuecomment-451274130:273,load,loading,273,https://hail.is,https://github.com/hail-is/hail/pull/5065#issuecomment-451274130,1,['load'],['loading']
Performance,"> I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. The point isn't to save the few untars, the point is to make the general facilities performant for everyone. Basically, I think directory outputs are untenable in the current design unless they are very small, which is why you're doing all this for your use case, and the tar is still probably as good (if not better) in that case. Fix it for everyone's use case so nobody has to go through this in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7626#issuecomment-560459474:334,perform,performant,334,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560459474,1,['perform'],['performant']
Performance,"> I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first. Can you elaborate more? I'm not sure which code paths you are referring to.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1219867494:604,perform,performance,604,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219867494,2,['perform'],['performance']
Performance,"> I think the invariant is basically that if you get a Code[T], you must not use it more than once, since you're inlining arbitrary code. I've tried to enforce this model in my implementations. Yes that's definitely true, should not use more than once. In the example you linked, body is only being used once, calling the load on a specific srcField.index right? The ""loop"" here is statically unrolled (since iterating over a statically-known collection), each of these `body` invocations should only be called once I think",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8099#issuecomment-586416114:322,load,load,322,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586416114,1,['load'],['load']
Performance,> I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job. This is a good idea. My current thinking for the autoscaler is to utilize multiple input parameters such as the ones you list to determine the optimal number of instances to create.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8149#issuecomment-592059899:90,perform,performance,90,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-592059899,4,"['latency', 'load', 'perform']","['latency', 'load', 'performance']"
Performance,"> I think we can also get rid of TIterable by refining the type casting in ToSet/ToDict/ToArray/ToStream nodes to reflect the new semantics. Yes, I will consider that in a later pass. > For python---the idea is that we're not really introducing any user-visible changes, right?. Correct. And yes, the optimizer should strip out any glue. > I'm a little nervous about the ToArray(ToStream(dict/set) idiom we're using here. OK, this is fixed. I added a CastToArray which takes a TContainer and does a no-op cast to array type. I use it wherever I can and I added a simplify rule to introduce it when possible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8171#issuecomment-592219318:301,optimiz,optimizer,301,https://hail.is,https://github.com/hail-is/hail/pull/8171#issuecomment-592219318,2,['optimiz'],['optimizer']
Performance,"> I thought in these cases the code would be evaluated (inner -> outer). Sorry, don't know what you mean here. Can you explain?. > ... added to the function stack. What actually happens?. There's no function stack - just because our codegen routine calls a new method doesn't mean the generated code has a method. We'd have to explicitly put a method boundary in. In the linked example, you're right that there is no generated loop, just a bunch of flat code. You're passing `loadElement(...)` as the `srcAddress` to a field's copyFromType method, which may use it multiple times, inlining load element (and doing a bunch of unnecessary pointer math / dereferences) each time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8099#issuecomment-586417359:476,load,loadElement,476,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586417359,2,['load'],"['load', 'loadElement']"
Performance,"> I thought we were going to load the genome reference upon initializing the HailContext. That is correct. I just want to double-check the VDS reference matches the global one. We won't support multiple datasets with different references in a single Hail session with this change. That's an intentional assumption to make the first version of reference support simpler. . Right, so you should also add a reference option the HailContext constructor (last argument, named with default to GRCh37 not to break backward compatibility) to set the reference.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1789#issuecomment-302433569:29,load,load,29,https://hail.is,https://github.com/hail-is/hail/pull/1789#issuecomment-302433569,2,['load'],['load']
Performance,"> I understand what is going on now. The issue is that the temp directory is getting removed after the first batch.run(), but we're assuming those input files are still there.; > ; > I think we should just clear the files and definitions cache after submission. Ah I see, ya let's not go that far. In that case we don't need any of the changes in this PR that deal with the python input files right? Other than to clear those dictionaries after submit?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12530#issuecomment-1428760971:238,cache,cache,238,https://hail.is,https://github.com/hail-is/hail/pull/12530#issuecomment-1428760971,2,['cache'],['cache']
Performance,"> I'm confused. What do you mean by this? Are you planning on using Dan's version of notebook (the rainbow gradient notebook.hail.is), or mine? If Dan's, he and I spoke about it, and he specifically stated that his version of Notebook is no longer needed, which is why I deleted the existing notebook. My apologies, @danking was wrong. I'm currently planning to use Dan's version because yours isn't ready. If yours is ready, I will use it. What does ready mean? From our recent email:. - it needs to get in master,; - and and integrated with our CI/CD (we can't be fixing issues and doing manual deployments leading up to or during a tutorial),; - it need to be beaten on by the team to look for issues (including scale issues), and; - it needs to be scale tested (@danking has a script for the old one that fires up N notebooks and reports any failures and summarizes the latency to notebook available),; - @tpoterba and I need to be comfortable enough with it we have confidence we can fix issues that arise during the tutorial. I probably also need time to review the workshop auth flow since I think that changed. If we're requiring login, we'll need to support more social login providers and/or email/password.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-464229358:874,latency,latency,874,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464229358,1,['latency'],['latency']
Performance,"> I'm not sure I understand. `hail/python/hail/docs/_templates/layout.html` references `/navbar.css` which should be present on the deployed site. I might misunderstand `conf.py`, but, AFAICT, this makes unused copies of navbar.css and the PNG: https://hail.is/docs/0.2/navbar.css and https://hail.is/docs/0.2/hail-logo-cropped.png. The page at https://hail.is/docs/0.2/ loads `hail.is/navbar.css`. Sorry, ignore, I misread the comment. Agreed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8956#issuecomment-644228546:371,load,loads,371,https://hail.is,https://github.com/hail-is/hail/pull/8956#issuecomment-644228546,1,['load'],['loads']
Performance,"> I've been thinking about how to support VCF formatted floats properly and in a performant manner as well. Sorry for the fast review, I just don't want this merged without sharing my thoughts.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13134#issuecomment-1572349409:81,perform,performant,81,https://hail.is,https://github.com/hail-is/hail/pull/13134#issuecomment-1572349409,1,['perform'],['performant']
Performance,"> If the query plans are identical I’d expect identical performance. Maybe the DB version was incremented and the optimizer is better?. Thanks for your reply! Maybe/I hope so? IMO the `EXPLAIN` output can be a little hard to decipher and I suspect doesn't tell the full story. For academic purposes, I was also interested in seeing what difference `GROUP BY` has vs `DISTINCT` but 0 records makes this challening!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14366#issuecomment-2253231541:56,perform,performance,56,https://hail.is,https://github.com/hail-is/hail/pull/14366#issuecomment-2253231541,2,"['optimiz', 'perform']","['optimizer', 'performance']"
Performance,"> In the course of this work I also fixed a problem with the staged code generated by the copyFromType methods -- the addresses to copy from were never bound to variables, so in nested types, we ended up duplicating a lot of code (an array of Tuple10s of Tuple10s of Tuple10s would duplicate the top `loadElement` 1000x!). Could you show me where I forgot to bind address returns? This was an oversight; I understand the cost of not binding address-generating code (Need to call Code.store and load that, instead of re-running the generating function)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8099#issuecomment-586410150:301,load,loadElement,301,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586410150,2,['load'],"['load', 'loadElement']"
Performance,"> Introducing an image with the wheel already installed isn't worthwhile, it adds 2.5 min latency. Agreed. I think our best path for speed is keeping these images totally cacheable so basically dependencies (nothing that will have to change on every commit, e.g. the wheel). Installing the wheels in the image is just adding more latency and work of localization. > The large number of splits often requires default Hail to scale up adding a 2min delay (It would be great to get that down).; I'm gonna revert the change that added images and maybe try to reduce service backend parallelism a bit. 36 minutes is an improvement. We should probably focus on making Hail faster rather than trying to squeeze lower latency out of parallelism. Totally agree.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13076#issuecomment-1561101182:90,latency,latency,90,https://hail.is,https://github.com/hail-is/hail/pull/13076#issuecomment-1561101182,4,"['cache', 'latency']","['cacheable', 'latency']"
Performance,"> Is there a Hive CLI equivalent of `LIKE PARQUET <file>`? I can't figure out how to get Hive to infer the schema from the Parquet file rather than specifying it explicitly. I don't think there is a Hive equivalent unfortunately. > It would be awesome to be able to query the genotypes, too. It seems like we could write a SerDe (now I'm thinking ImpEx isn't so bad :)) to unpack the genotypes. Does that sound like the right approach?. In Hive a UDTF (user-defined table function; ImpEx naming is growing on me :) would allow you to return a genotype per row. Impala doesn't support UDTFs, but it should be possible to write a UDF that returns an array of genotypes. The UDF to do this could be written in Java so that Hive or Impala can use it; it's also possible to write an optimized version in C++ for Impala. On standard Parquet storage: thanks for the numbers. I don't have a good idea of how to close the gap. I ran some experiments with a standard Parquet representation (in Quince) but have now come to the conclusion that custom encoding of the genotypes is a pragmatic way of achieving the best performance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/480#issuecomment-234498769:778,optimiz,optimized,778,https://hail.is,https://github.com/hail-is/hail/pull/480#issuecomment-234498769,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"> Isn't it also non-associative? We're assuming everything is associative. I was thinking there was still some benefit to using the associative combiner, but I take it back. ApproxCDF is *approximately* commutative, so if we aren't achieving determinism anyways, you're right the commutative combiner is fine. > I don't think we guarantee a stable sort, so I'm inclined to leave it as an unstable sort for now in the name of performance. Even if we don't guarantee stable sort, we might hope to guarantee determinism (no promise what answer you get, but you'll get the same answer each time you run the same pipeline). I think the associative combiner *might* be enough to ensure determinism. Though if you want to leave it nondeterministic for now, and wait for the better fix of guaranteeing stable sort by appending indices to the sort keys, that's fine with me.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6837#issuecomment-519633134:425,perform,performance,425,https://hail.is,https://github.com/hail-is/hail/pull/6837#issuecomment-519633134,1,['perform'],['performance']
Performance,"> Just seems more complicated than using argparse to get repeated arguments. Actually looks like we can use nargs=2 to get: `--header 'key1' 'value1' --header 'key2' 'value2'`. Why introduce a JSON unparser and parser?. Up to you. JSON parser/unparser is stdlib, and it looks like the command line tool is little used (meaning, executed typically as part of a larger script). So I'm optimizing for the typical use case (it's easier to specify the YAML property using the dict method, and easier to write/maintain the parser, since no need to go from array to dict). Neat about args 2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548133056:383,optimiz,optimizing,383,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548133056,1,['optimiz'],['optimizing']
Performance,"> Just starting to explore this PR. I compared this in-progress CI run to 7126798 (from #12737). The time to service backend starting is ~7 minutes. In the other PR, its ~8 minutes. I suppose that's because this PR isn't hitting any caches, right?; > ; > Hmm, it also seems like the critical path to the service backend test is through `build_hail_jar_and_wheel_only`. I wonder if we double the cores, would the time halve? On my laptop a fresh build is like 3m.; > . Ya I really only focus here on docker image steps, not on the overall critical path. But I like the idea of adding more cores I'll try that. The docker images are hitting cache though since every PR caches from its own previous runs in addition to main. The reason some images still take ~1 minute is because they introduce a new layer (normally the hail_version has changed since the SHA has changed) and they spend most of the time localizing the layer that has the dependencies installed (why I want lazy pulling). A retry of this PR should build the images very quickly, though I haven't tried that recently.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12578#issuecomment-1458611223:233,cache,caches,233,https://hail.is,https://github.com/hail-is/hail/pull/12578#issuecomment-1458611223,3,['cache'],"['cache', 'caches']"
Performance,"> Just to be clear, the other things you tried besides the nginx config shouldn't be in the PR?. I'm not sure. When I was debugging the issue, I tried a bunch of things, and in the end I'm not sure which ones actually worked. I didn't like timing out the scheduler because that can lead to double-schedule. I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256:395,perform,performance,395,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256,8,"['latency', 'load', 'perform']","['latency', 'load', 'performance']"
Performance,"> Maybe it's because OutputStream.write has a default implementation in Java?. Yeah, that default implementation should be WAY slower than an implementation that uses System.arrayCopy though. That's why I'm surprised you don't see performance regressions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10766#issuecomment-896337273:231,perform,performance,231,https://hail.is,https://github.com/hail-is/hail/pull/10766#issuecomment-896337273,1,['perform'],['performance']
Performance,"> Maybe lazy fields should not subclass Value, and to access a lazy field requires an explicit load(cb: CodeBuilder): Value[T]. I think this is probably the right model.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10907#issuecomment-961973892:95,load,load,95,https://hail.is,https://github.com/hail-is/hail/pull/10907#issuecomment-961973892,1,['load'],['load']
Performance,"> OrderedRDD; [Stage 1:> (7 + 28) / 4969]Traceback (most recent call last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:2197,load,loadLength,2197,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['load'],['loadLength']
Performance,"> Ready to look at. Remaining question: should we encode the fact that in EmitStream context, If and Let should have TStream children?. This is an optimization, not a requirement. We've also removed `If` stream emitters for now, because they were broken",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586754725:147,optimiz,optimization,147,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586754725,1,['optimiz'],['optimization']
Performance,"> Slight question about the shuffle ID/client/server model, mostly just to make sure I understand: each HailContext has one ShuffleClient,. There should be a ShuffleClient per-shuffle. I think in our current model, this means one active ShuffleClient at any given time (because there's only one active shuffle at any given time). However, I intend Hail service pipelines to be able to use multiple concurrent shuffles, if useful. In particular, a ShuffleClient has as type and an encoding, so its only useful for one type of dataset. Though you could theoretically re-use the object on a different dataset of the same type by calling `start` again. > which communicates with a ShuffleServer (which only connects to one ShuffleClient). I intended the ShuffleServer to serve an arbitrary number of non-adversarial clients (perhaps two different users in nascent hail service). I think it's pretty secure, but I don't think `UUID.randomUUID().toString()` is cryptographically random, so an adversary could probably guess it and thus get access to shuffle data. Moreover, the ShuffleServer must support concurrent connections from all the workers of a Hail pipeline. `ShuffleServer.serve` starts a fresh server thread for every connection. During the read or write phases of the shuffler, the idea is 1:1 mappings from workers to connections to server `Handler` threads. > Every time the hail context wants some data to be shuffled, the server creates a new shuffle ID to associate with the shuffle (starting the shuffle) and then the client sends over the data and can then access it, in ranges, using get. As soon as it starts a new shuffle, it can no longer access the data from the previous shuffle (at least through current interfaces---the shuffle server never deletes shuffled data, so we could theoretically define a put and get that take the uuid and then keep accessing older shuffles as long as we know the uuid). Does that sound about right?. The hail leader node could keep multiple `ShuffleC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052:398,concurren,concurrent,398,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052,1,['concurren'],['concurrent']
Performance,"> So in this case, does the PType method loadLength do anything with a region?. Nope! That's why we should remove it. That discussion on Zulip resulted in me being convinced that we never need a region on loadLength.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7826#issuecomment-575732251:41,load,loadLength,41,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575732251,2,['load'],['loadLength']
Performance,"> So, looks like a bug. Is this causing correctness/perf problems?. We aren't seeing correctness problems. Performance, I dunno. You'd hope Spark would take advantage of non-missingness, but this makes me think it isn't. I doubt the only barrier is tracking the information at the type level.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1421#issuecomment-282325862:107,Perform,Performance,107,https://hail.is,https://github.com/hail-is/hail/pull/1421#issuecomment-282325862,1,['Perform'],['Performance']
Performance,"> Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`).; > ; > ; > ; > I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this. Right I realize this, why I mentioned unit test (to make unit tests work without calling hail, we would need to wrap each function we want to test in a cli interface, and then containerise those functions...this seems not so friendly to contributors). A 2nd question: how would a contributor submit write hybrid hail query/batch code that called batch from within a hail query script? So hail transforms data, say generates pc's, writes table, and then issues some batch commands that use a pre-built image, as opposed to requiring them to containerise their hail cod3. This will be useful: containerisation is the stated biggest pain point of batch, from my interviews. And one of the biggest hail issues is shuttling data between hail and other processes, which has led people to ask for tighter integration, or more statistical tools (so that they don't need to go out to those other tools, like plink). Showing Batch working from within hail-query scripts would be useful because it will show a much easier integration, and those examples or contributed modules should live somewhere (and that seems not to be hailtop, because this requires batch to be called from code that is calling hail directly, rather than through an image).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432:235,perform,performs,235,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432,2,['perform'],['performs']
Performance,"> Sorry, don't know what you mean here. Can you explain?. I just mean that if I call. ```; In [4]: def foo(integer): ; ...: if(integer == 5): ; ...: print(""GOT 5"") ; ...: print(f""Added 1, got {integer + 1}"") ; ...: ; ...: def bar(): ; ...: foo(someComplicatedGeneratingFunctionThatReturnsAnInt()) ; ...: ; ...: def someComplicatedGeneratingFunctionThatReturnsAnInt(): ; ...: print(""Called complex function"") ; ...: return 5 ; ...: . In [5]: bar() ; Called complex function; GOT 5; Added 1, got 6; ```. Will evaluate someComplicatedGeneratingFunctionThatReturnsAnInt, and use the return value as the argument to foo. It won't store the function as an argument and lazily eval n times with no memoization. > You're passing loadElement(...) as the srcAddress to a field's copyFromType method, which may use it multiple times, inlining load element (and doing a bunch of unnecessary pointer math / dereferences) each time. Thanks for catching that, understood.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8099#issuecomment-586421823:721,load,loadElement,721,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586421823,2,['load'],"['load', 'loadElement']"
Performance,"> Struct decoding (the fourth generated code one, with 399 own time and 229 samples) is pretty branchy: it checks a bit for each field. I'm not sure how to speed this up. Consider a struct of 8 optional fields. There are 2^8 possible missingness pattern. Each pattern corresponds to a different sequence of field-decoders. I suppose we could generate 256 different patterns and jump to them? That seems excessive. We could maybe generate 16 patterns but that only saves 3/4 of the branches. Maybe that's enough for a substantial speedup?. With the array decoding, I suspect a lot of the speedup wasn't from avoiding branches, but from avoiding a bunch of extra operations handling missing bits one at a time, especially computing the address of the containing byte and loading it from memory every time. We should be able to do something similar for structs, though it will be more complicated. I think that's worth trying independently of trying to reduce branches.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13792#issuecomment-1761652107:769,load,loading,769,https://hail.is,https://github.com/hail-is/hail/issues/13792#issuecomment-1761652107,1,['load'],['loading']
Performance,"> Sure, so the proposal is that loadLength takes some region-containing object, and this object has a loadAddress method on it?. No, I think the interfaces you've created in your remove-region-from-load PR are correct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7826#issuecomment-575733539:32,load,loadLength,32,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575733539,3,['load'],"['load', 'loadAddress', 'loadLength']"
Performance,"> That should be a simple fix, though perhaps at this point not worth it as this is not a fruitful optimization for this query. Agreed, although depending on the time line for a good optimization for this query I may circle back on this, as there is currently a bug in this deduplication so if the actual optimizaion won;t be available for a while it might be worth fixing in the meantime",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1839244525:99,optimiz,optimization,99,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1839244525,6,['optimiz'],"['optimizaion', 'optimization']"
Performance,"> The interface is fully async, so we'll need to build some wrappers if you want a synchronous interface. The async interface will get you concurrency within operations (copy, rmtree), the sync interface only gets you currency within operations. +1 for this. Synchronous wrappers would make replacing existing uses of `hl.hadoop_*` much easier.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10043#issuecomment-778393144:139,concurren,concurrency,139,https://hail.is,https://github.com/hail-is/hail/pull/10043#issuecomment-778393144,1,['concurren'],['concurrency']
Performance,"> The thinking behind this is that there's a huge amount of re-use of code for an individual from one Hail analysis to the next. So I'm not sure I buy this. Hail is a exploratory data analysis platform, it isn't a SQL engine running nightly billing reports. In particular, as we do whole stage optimization and code generation. It isn't clear to me how you do code-reuse when we're specializing each operation into the global context (happy to hear the plan). For example, tables that have many fields, you will need to load different ones for different queries and in general it is infeasible (exponential) to generate them all. Also, to get sharing you need to break the code up and now you're running the compiler multiple times which also seems bad. Given our focus on large-scale analysis, introducing optimization boundaries for code reuse seems like a bad trade off to me. A significant amount of analysis happens in the cloud where $HOME is ephemeral so you won't get savings between sessions. Finally, there are pipelines that are more standardized but it is my impression they are run on extremely large datasets (hours, overnight) in which case compilation speed isn't important. Finally, there's the complexity around locking that isn't easy and have real technical risk. A compiler cache potentially becomes more appealing in the context of an always-on service. There's no locking issue, and you can start to do things like speculative compilation (e.g. immediately start compiling the decoder (the full decoder? Hmm.) when a user opens the dataset.). I would say getting in a 3x decoder improvement is way more important than this. I would have punted it down the road and instrumented to estimate cache hit rates before building this out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410358596:294,optimiz,optimization,294,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410358596,10,"['cache', 'load', 'optimiz']","['cache', 'load', 'optimization']"
Performance,"> These files aren't actually referenced in the docs. The docs refer directly to /navbar.css. This is used in layout.html. Doc shares the site navbar. We could modify layout.html to load this using jquery, append to head.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8956#issuecomment-644207590:182,load,load,182,https://hail.is,https://github.com/hail-is/hail/pull/8956#issuecomment-644207590,1,['load'],['load']
Performance,"> This seems fine, but it seems like you really want parallelism to match bandwidth. When you're on a gigabit ethernet connection (like me at my desk) you're leaving a lot on the table. No. More concurrency isn't going to increase the bandwidth from a single process if the network stack is full. Increasing the value of parallelism here has two benefits that I can see: (1) amortizing request overhead, which is already very small for 1MB requests. 2-way parallelism would probably be sufficient for that. (2) Increasing the parallelism on the server if the batch or database processes are single-process limited. For the latter, we can increase the parallelism but should correspondingly decrease the message size. My bandwidth numbers were the minimum required for things to work without back-off. There's no relationship between that and the maximum bandwidth this code can support.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7971#issuecomment-578807275:195,concurren,concurrency,195,https://hail.is,https://github.com/hail-is/hail/pull/7971#issuecomment-578807275,1,['concurren'],['concurrency']
Performance,"> ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. Without a concrete example, I'm having some trouble understanding what this problem is. What IR nodes can return a container type that aren't included in the streamify match? Isn't it just ToSet/ToDict/GroupByKey? Calling `boundary` on a ToSet is no problem at all, I think. > ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). Can you write such an IR, and what it should return after streamify?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586582159:556,perform,perform,556,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586582159,2,['perform'],['perform']
Performance,"> Two comments, and a meta-comment:; > ; > * I had looked over async http libraries and had preferred aiohttp over sanic because (1) ""aiohttp"" is a blessed aio library, (2) performance seemed comparable, (4) aiohttp seemed like a simpler solution which was attractive because the microservices are looking more and more like services and less like web servers (even more so moving all the rendering to the front end with the web app, the legacy version of scorecard using jinja is not the representative case). Did you look at aiohttp?; > * From the code:; > > Global variables that are modified ...; > ; > ; > I don't want to have to think about shared state and locking. I want a shared-nothing architecture in the microservices where the only globals are true constants and threads communication by sending immutable data through queues.; > * Finally, a meta-comment. I started reviewing this when it was just ujson, I did a bit of research about json packages to understand your choices and when I came back, the PR had expanded with all the async stuff. I would have approved the ujson stuff. The async stuff could have been a separate PR. Nobody wants to review a moving target, so the scope of a change should be roughly frozen when you assign a PR and additional changes should be minimized and restricted to that scope. You're welcome to have an open PR with no reviewer if you're still fleshing out the scope, of course. Thanks!. In response:. 1) aiohttp is an option, but appears to be generally considered slow on a per-response basis (published benchmarks, haven't had a chance to try it), even potentially slower than flask. It seems wrong to choose something slower if there are are reasonable alternatives.; 2) The globals were a feature of the initial implementation (the GitHub cache). It felt outside of the scope of my PR to change that to some queue solution. Meta comment. Ok. I didn't think it had been looked at, and expanded what it did pretty quickly, as I realized that ujso",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461191051:173,perform,performance,173,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461191051,4,"['perform', 'queue']","['performance', 'queues']"
Performance,"> We can't use aggregate/collect agg.collect doesn't respect row order. I don't want to commit to row order there, because we can't optimize as effectively if we do. I'm a bit dubious about this argument. I feel like we don't often sacrifice the right semantics for performance. What optimizations specifically are you concerned about here?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5015#issuecomment-448839950:132,optimiz,optimize,132,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448839950,3,"['optimiz', 'perform']","['optimizations', 'optimize', 'performance']"
Performance,"> We have a 35K cohort. The VCF format of chr1 is 2.4T. Heh. So, yes, ""project"" VCFs grow super-linearly in the number of samples. I (and others) are currently pushing very hard for the VCF spec to support two sparse representations: ""local alleles"" (samtools/hts-specs#434) and ""reference blocks"" (samtools/hts-specs#435). When using these two sparse representations, you should be able to store 35,000 whole genomes in ~10TiB of GZIP-compressed VCF. What is your calling pipeline? Do you generate GVCFs? If yes, I strongly recommend you use the [VDS Combiner](https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html#hail.vds.combiner.VariantDatasetCombiner) to produce a [VDS](https://hail.is/docs/0.2/vds/index.html). You can read more details in [this recent preprint we wrote](https://www.biorxiv.org/content/10.1101/2024.01.09.574205v1.full.pdf), but a VDS of 35,000 whole genomes should be a few terabytes. I'd guess 4 TiB, but it depends on your reference block granularity. I strongly recommend using size 10 GQ buckets. ---. > I don't know the Kryo JAR. I tested on both docker images hailgenetics/hail:0.2.126-py3.11 and hailgenetics/hail:0.2.127-py3.11. Those should use Kryo 4.0.2. OK. My conclusion is that Kryo still has a bug preventing the serialization of very large objects. This becomes a limitation in Hail: we cannot support PLINK files with tens of millions of variants. Our community is largely transitioning to GVCFs and VDS, so I doubt we'll improve our PLINK1 importer to support such large PLINK1 files. That said, PRs are always welcome if loading such large PLINK1 files is a hard requirement for you all.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168#issuecomment-1922048526:1588,load,loading,1588,https://hail.is,https://github.com/hail-is/hail/issues/14168#issuecomment-1922048526,1,['load'],['loading']
Performance,"> What's the pressing reason for this to join the mainline?. Ben has wanted this for a while, and there's interest from the group. > I'd prefer to stick to the usual process on services team where thorough code review of code and test-code is done for stuff under our purview (as things in hailtop/batch do). Sure. Regarding testing, in my experience regenie (the C++ program) crashes when given improper inputs, rather than writing anything out, so there is a reduced testing need (we should. verify the expected number of non-empty outputs are created I think, and some basic tests that we parse inputs). My plan for getting regenie in is roughly 4 part, of which this is 1. ; 1) Get the most basic / canonical version of regenie up, on the local backend. This version should take all of the arguments demonstrated in the [regenie tutorial / example](https://rgcgithub.github.io/regenie/options/), pass them to step 1 and step 2, and write an output. ; * Here I'm mostly interested in making sure I'm using Batch Resource classes correctly.; 2) Get the same working for the ServiceBackend.; 3) Expand the service offering to run the paper's example (may require some tweaks to the input handling code), and test that we can run this at scale; 4) Introduce a per-phenotype parallelism mode, which loses a bit of per-core efficiency in favor of greater scale out, benchmark that.; 5) Handle all other combinations of inputs.; 6) Optimize performance (local RAID-0 SSDs up to 9TB, larger instances). So, if at all possible, I would like to stay focused on the first task, within the bounds of what you need to accomplish as leader of the services team; I'd really appreciate your help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-667434082:1429,Optimiz,Optimize,1429,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-667434082,2,"['Optimiz', 'perform']","['Optimize', 'performance']"
Performance,"> Why would performance be different than just `nan`?. We can put `nan`s into BLAS, but not `NA`. I'd be fine with sparsify generating `nan`s as an option, though if you do really any operation that's not element-wise on that matrix, you'll get nans everywhere.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6097#issuecomment-492708806:12,perform,performance,12,https://hail.is,https://github.com/hail-is/hail/issues/6097#issuecomment-492708806,1,['perform'],['performance']
Performance,"> Your tool should also examine the first word of the MAKEFLAGS variable and look for the character n. If this character is present then make was invoked with the ‘-n’ option and your tool should stop without performing any operations. Added. > Your tool should be sure to write back the tokens it read, even under error conditions. This includes not only errors in your tool but also outside influences such as interrupts (SIGINT), etc. You may want to install signal handlers to manage this write-back. I mean, I doubt anyone is sending signals other than SIGKILL to our build system, but I added some signal handlers that just `sys.exit(0)` which triggers the finally (I checked). > We also get a lot of ‘warning: jobserver unavailable: using -j1. Add +' to parent make rule.’warnings when runningmake jvm-test`. This is because our C++ backend uses make to drive compilation (wtf‽). I strip MAKEFLAGS before calling gradle now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6923#issuecomment-524446104:209,perform,performing,209,https://hail.is,https://github.com/hail-is/hail/pull/6923#issuecomment-524446104,1,['perform'],['performing']
Performance,"> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13 - 2024-02-03</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h2>3.9.11 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/ijl/orjson/commit/a348f59f0b55d92a1364523560f52f5b3cf9c12a""><code>a348f59</code></a> 3.9.15</li>; <li><a href=""https://github.com/ijl/orjson/commit/b0e4d2c06ce06c6e63981bf0276e4b7c74e5845e""><code>b0e4d2c</code></a> yyjson 0eca326, recursion limit</li>; <li><a href=""https://github.com/ijl/orjson/commit/5067eadc84cf516e4eb33bcb09ad756bb59dc42e""><code>5067ead</code></a> impl_esca",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14357:2419,optimiz,optimization,2419,https://hail.is,https://github.com/hail-is/hail/pull/14357,3,['optimiz'],['optimization']
Performance,"> buildSkip does not need a ptype. This method is used to skip encoded data, which is never getting decoded into a physical representation. So in EArray:. ```scala; def _buildSkip(mb: EmitMethodBuilder, r: Code[Region], in: Code[InputBuffer]): Code[Unit] = {; val len = mb.newLocal[Int](""len""); val i = mb.newLocal[Int](""i""); val skip = elementType.buildSkip(mb). if (elementType.required) {; Code(; len := in.readInt(),; i := 0,; Code.whileLoop(i < len,; Code(; skip(r, in),; i := i + const(1)))); } else {; val mbytes = mb.newLocal[Long](""mbytes""); val nMissing = mb.newLocal[Int](""nMissing""); Code(; len := in.readInt(),; nMissing := PCanonicalArray.nMissingBytes(len),; mbytes := r.allocate(const(1), nMissing.toL),; in.readBytes(r, mbytes, nMissing),; i := 0,; Code.whileLoop(i < len,; Region.loadBit(mbytes, i.toL).mux(; Code._empty,; skip(r, in)),; i := i + const(1))); }; }; ```. Do you want to just code (len + 7) >>> 3 in EArray (say `val nMissingBytes = (len+7) >>> 3` in the constructor). This would be a fast way to delete `PCanonicalArray.nMissingBytes`. The actual missing ness encoding scheme doesn't seem tied to PArrays (notable the allocation is using a different alignment altogether).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7701#issuecomment-564225592:798,load,loadBit,798,https://hail.is,https://github.com/hail-is/hail/issues/7701#issuecomment-564225592,1,['load'],['loadBit']
Performance,"> collectJSON. See #4971 . We can't use aggregate/collect agg.collect doesn't respect row order. I don't want to commit to row order there, because we can't optimize as effectively if we do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5015#issuecomment-448768620:157,optimiz,optimize,157,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448768620,1,['optimiz'],['optimize']
Performance,> maybe we should make an issue to support this optimization for non-ascending orders?. good idea.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6757#issuecomment-516102881:48,optimiz,optimization,48,https://hail.is,https://github.com/hail-is/hail/pull/6757#issuecomment-516102881,1,['optimiz'],['optimization']
Performance,"> the TableIR doesn't define partitionCounts. statically known partition counts are used to optimize `.count()` when we know the partition sizes. Here that doesn't apply, so I don't think you need to define that method (it inherits `def partitionCounts = None`). > perhaps ""LiftLiterals"" was changed to ""LiftNonCompilable"". Yes, it was. No need to write a rule for this. Separately, I think we should delete the checklist. It'll never be correct, since it's not checked against the codebase. To add an IR node, one needs to understand the compiler, and we can't adequately document that in a bullet list right now (over time, things should get simpler). . If a node is missing from somewhere it needs to appear, then tests should catch that case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6689#issuecomment-513935766:92,optimiz,optimize,92,https://hail.is,https://github.com/hail-is/hail/pull/6689#issuecomment-513935766,2,['optimiz'],['optimize']
Performance,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-233041581:505,load,load,505,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581,4,"['concurren', 'load']","['concurrency', 'load']"
Performance,"> we don't wait for all JVMs to be intitialized before accepting JVM jobs and the queue is FIFO so we reuse the same JVMs that are warm already?. No. We have no way to accept only JVM jobs or only Batch jobs, so we either accept all jobs or no jobs. In main, we accept jobs before the JVMs have initialized. We wait for all JVMs to initialize before giving JVMs to any JVM Job. So, concretely, in main and in this PR we *accept* jobs before JVMs are ready; however, in this PR we don't wait for all JVMs to initialize before *running* jobs. There are two improvements in this PR:; 1. If a JVM with the requested number of cores is available, allow the requesting JVMJob to start before the remaining JVMs are initialized.; 2. Rather than starting all the JVMs in parallel, start JVMs serially *and also* start them in the order that they are requested. If we have three waiting JVM Jobs two requesting 1 core and one requesting 4 cores, prefer to start JVMs with 1 and 4 cores to JVMs with 2 or 8 cores. (2) might sound slower (why start serially when we can stat in parallel?) but it appears that 30 JVMs competing for CPU time dramatically slows down average start up time. In both main and this PR it takes about ~25s for all JVMs to be ready; however, in this PR, some jobs can start much sooner than 25s b/c their JVMs are started first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13870#issuecomment-1775517156:82,queue,queue,82,https://hail.is,https://github.com/hail-is/hail/pull/13870#issuecomment-1775517156,1,['queue'],['queue']
Performance,"> what ends up in /home/cotton/hail-20200407-1502-0.2.36-75a0f869d72d.log? What happens to the usual Spark/Hail master logs?. The client and the server are now separated by a machine boundary, so just the Python client logs end up in the client log. That's basically nothing and can probably be removed when using the client with the service. There are no Spark logs, the service is 100% Spark-free. The Hail master logs end up in the query service logs. Obviously a lot of this needs to be rethough and improved. The error checking and reporting needs to get improved at the service boundary, errors should be relative to the input, and clients probably shouldn't get a server-side stack trace. We're going to need additional tools for debugging pipelines on the master, and probably want an admin UI where you get the logs for each query, the IR getting executed, how it was transformed with lowering/optimization, statistics on timing of its execution, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8485#issuecomment-610628050:903,optimiz,optimization,903,https://hail.is,https://github.com/hail-is/hail/pull/8485#issuecomment-610628050,1,['optimiz'],['optimization']
Performance,">; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/kjd/idna/blob/master/HISTORY.rst"">idna's changelog</a>.</em></p>; <blockquote>; <p>3.7 (2024-04-11); ++++++++++++++++</p>; <ul>; <li>Fix issue where specially crafted inputs to encode() could; take exceptionally long amount of time to process. [CVE-2024-3651]</li>; </ul>; <p>Thanks to Guido Vranken for reporting the issue.</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/kjd/idna/commit/1d365e17e10d72d0b7876316fc7b9ca0eebdd38d""><code>1d365e1</code></a> Release v3.7</li>; <li><a href=""https://github.com/kjd/idna/commit/c1b3154939907fab67c5754346afaebe165ce8e6""><code>c1b3154</code></a> Merge pull request <a href=""https://redirect.github.com/kjd/idna/issues/172"">#172</a> from kjd/optimize-contextj</li>; <li><a href=""https://github.com/kjd/idna/commit/0394ec76ff022813e770ba1fd89658790ea35623""><code>0394ec7</code></a> Merge branch 'master' into optimize-contextj</li>; <li><a href=""https://github.com/kjd/idna/commit/cd58a23173d2b0a40b95ee680baf3e59e8d33966""><code>cd58a23</code></a> Merge pull request <a href=""https://redirect.github.com/kjd/idna/issues/152"">#152</a> from elliotwutingfeng/dev</li>; <li><a href=""https://github.com/kjd/idna/commit/5beb28b9dd77912c0dd656d8b0fdba3eb80222e7""><code>5beb28b</code></a> More efficient resolution of joiner contexts</li>; <li><a href=""https://github.com/kjd/idna/commit/1b121483ed04d9576a1291758f537e1318cddc8b""><code>1b12148</code></a> Update ossf/scorecard-action to v2.3.1</li>; <li><a href=""https://github.com/kjd/idna/commit/d516b874c3388047934938a500c7488d52c4e067""><code>d516b87</code></a> Update Github actions/checkout to v4</li>; <li><a href=""https://github.com/kjd/idna/commit/c095c75943413c75ebf8ac74179757031b7f80b7""><code>c095c75</code></a> Merge branch 'master' into dev</li>; <li><a href=""https://github.com/kjd/idna/commit/60a0a4cb61ec6834d74306bd8a1f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14464:1642,optimiz,optimize-contextj,1642,https://hail.is,https://github.com/hail-is/hail/pull/14464,7,['optimiz'],['optimize-contextj']
Performance,">; <h2>3.10.0 - 2024-03-27</h2>; <h3>Changed</h3>; <ul>; <li>Support serializing <code>numpy.float16</code> (<code>numpy.half</code>).</li>; <li>sdist uses metadata 2.3 instead of 2.1.</li>; <li>Improve Windows PyPI builds.</li>; </ul>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13 - 2024-02-03</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/ijl/orjson/commit/11c7de8e5846fa65449aa1f6ffc05c5a1090df03""><code>11c7de8</code></a> 3.10.0</li>; <li><a href=""https://github.com/ijl/orjson/commit/1fc3ed80c24864607be709d29e0d5f47fc507626""><code>1fc3ed8</code></a> Support numpy.float16</li>; <li><a href=""https://github.com/ijl/orjson/commit/56c1a03216426c54dfbe9a4b6c3f70013c65a1f8""><code>56c1a03</code></a> cargo update, build misc</li>; <li><a href=""https://github.com/ijl/orjson/commit/a348f59f0b55d92a1364523560f52f5b3cf9c12a""><code>a348f59</code></a> 3.9.15</li>; <li><a href=""https://github.com/ijl/orjson/commit/b0e4d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14427:2874,optimiz,optimization,2874,https://hail.is,https://github.com/hail-is/hail/pull/14427,1,['optimiz'],['optimization']
Performance,">; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/da6d0d034822f66966e4a84a3a1e2f37cc83e3b0""><code>da6d0d0</code></a> Remove unneeded &quot;update order&quot; consistency test</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/e85d8659733cb3e28d539a28db0fdd71672ab2e4""><code>e85d865</code></a> Simplify &quot;update order&quot; consistency test</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/7dc426c95a0c329d5514e6198d92080f1ffc1e5e""><code>7dc426c</code></a> Fix update() ordering to be more consistent with add() ordering (<a href=""https://github-redirect.dependabot.com/grantjenks/python-sortedcontainers/issues/159"">#159</a>)</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/13d30bc654eb9e6be092282ca502967fcb7f0113""><code>13d30bc</code></a> Bump version to 2.2.2</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/4997d0e849f2275d1931772a5432163ecc20e0b0""><code>4997d0e</code></a> Refactor small slice optimization in SortedList.<strong>getitem</strong></li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/6ee5d57fc8d691fbab4972b853a60348d0f922ef""><code>6ee5d57</code></a> improve SortedList.<strong>getitem</strong>() performance for small slices</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/ac80254fb6a08045ced7d9704412878ff8000fa7""><code>ac80254</code></a> suppress warning in test of deprecated function (<a href=""https://github-redirect.dependabot.com/grantjenks/python-sortedcontainers/issues/118"">#118</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/grantjenks/python-sortedcontainers/compare/v2.1.0...v2.4.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sortedcontainers&package-manager=pip&previous-version=2.1.0&new-version=2.4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11476:3017,optimiz,optimization,3017,https://hail.is,https://github.com/hail-is/hail/pull/11476,1,['optimiz'],['optimization']
Performance,">; <li>Added PyPy 3.10 and removed PyPy 3.8 <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7216"">#7216</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Added in_place argument to ImageOps.exif_transpose() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7092"">#7092</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Corrected error code <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7177"">#7177</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use &quot;not in&quot; <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7174"">#7174</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Only call text_layout once in getmask2 <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7206"">#7206</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fixed calling putpalette() on L and LA images before load() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7187"">#7187</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Removed unused INT64 definition <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7180"">#7180</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Updated xz to 5.4.3 <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7136"">#7136</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fixed saving TIFF multiframe images with LONG8 tag types <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7078"">#7078</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Do not set size unnecessarily if image fails to open <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7056"">#7056</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13321:4724,load,load,4724,https://hail.is,https://github.com/hail-is/hail/pull/13321,1,['load'],['load']
Performance,">; <ul>; <li>Update dependencies</li>; </ul>; <h2>5.2.0</h2>; <p>New features:</p>; <ul>; <li>Add <code>eachFile</code> method that adds an action to be applied to each source URL before it is downloaded. The action can be used to modify the filename of the target file.</li>; <li>Add <code>runAsync</code> method to download extension. This allows multiple files to be downloaded in parallel if the download extension is used. For normal download tasks, multiple files were downloaded in parallel already.</li>; </ul>; <h2>5.1.3</h2>; <p>Bug fixes:</p>; <ul>; <li>Initialize progress logger just before the download starts (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0f43ce67de72bd511d849c07bd7728c0d6f2e6dd""><code>0f43ce6</code></a> Document path and relativePath properties</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a8504f9d60d0264808894e4bb80d4a73b8086a3e""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12345:2448,cache,cached,2448,https://hail.is,https://github.com/hail-is/hail/pull/12345,1,['cache'],['cached']
Performance,">; <ul>; <li>Update dependencies</li>; </ul>; <h2>5.2.0</h2>; <p>New features:</p>; <ul>; <li>Add <code>eachFile</code> method that adds an action to be applied to each source URL before it is downloaded. The action can be used to modify the filename of the target file.</li>; <li>Add <code>runAsync</code> method to download extension. This allows multiple files to be downloaded in parallel if the download extension is used. For normal download tasks, multiple files were downloaded in parallel already.</li>; </ul>; <h2>5.1.3</h2>; <p>Bug fixes:</p>; <ul>; <li>Initialize progress logger just before the download starts (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.5</h2>; <p>Maintenance:</p>; <ul>; <li>Publish signed artifacts to Gradle plugin portal</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.4</h2>; <p>Bug fixes:</p>; <ul>; <li>Fix deadlock in <code>DownloadExtension</code> if <code>max-workers</code> equals 1 (thanks to <a href=""https://github.com/beatbrot""><code>@​beatbrot</code></a> for spotting this, see <a href=""https://github-redirect.dependabot.com/michel-kraeme",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:1729,cache,cached,1729,https://hail.is,https://github.com/hail-is/hail/pull/12332,1,['cache'],['cached']
Performance,"><code>@​akayunov</code></a></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/jmoiron/humanize/commit/a1514eb521c2befe40274674d61aba4f0fbf6137""><code>a1514eb</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jmoiron/humanize/issues/239"">#239</a> from hugovk/rm-3.6</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/48506d434fd315a976bbdc058a791b80086f7e7e""><code>48506d4</code></a> pre-commit autoupdate</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/8f2c8551e5e20cc6cc3bcaa241fa2c1760d07926""><code>8f2c855</code></a> Remove unused import</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/04bf8872908178b3d7d9fb4b316da8ce72916209""><code>04bf887</code></a> Drop support for Python 3.6</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/0f2ff42cbe632c47ddb6ac255c61890ab8a46fd4""><code>0f2ff42</code></a> Use actions/setup-python's pip cache and update other CI config</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/464de5965692765d29d1c3cfde1f87c4ceece440""><code>464de59</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jmoiron/humanize/issues/253"">#253</a> from hugovk/rm-VERSION</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/66b8a6322fbda9bffb2882500c6a9b6c96271401""><code>66b8a63</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jmoiron/humanize/issues/250"">#250</a> from carterbox/no-overflow-naturaldelta</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/e89c8c8e325ccb2b3ee78ef507e9d6805c47a175""><code>e89c8c8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jmoiron/humanize/issues/241"">#241</a> from samueljsb/remove-deprecated-private-function-ali...</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/ffe4bcfaa6cfbd95ba47315f8f71a206485af6ae""><code>ffe4bc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11517:5341,cache,cache,5341,https://hail.is,https://github.com/hail-is/hail/pull/11517,2,['cache'],['cache']
Performance,"><em>Sourced from <a href=""https://github.com/boto/boto3/blob/develop/CHANGELOG.rst"">boto3's changelog</a>.</em></p>; <blockquote>; <h1>1.26.15</h1>; <ul>; <li>bugfix:Endpoints: [<code>botocore</code>] Resolve endpoint with default partition when no region is set</li>; <li>bugfix:s3: [<code>botocore</code>] fixes missing x-amz-content-sha256 header for s3 object lambda</li>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] Adding support for Amazon AppFlow to transfer the data to Amazon Redshift databases through Amazon Redshift Data API service. This feature will support the Redshift destination connector on both public and private accessible Amazon Redshift Clusters and Amazon Redshift Serverless.</li>; <li>api-change:<code>kinesisanalyticsv2</code>: [<code>botocore</code>] Support for Apache Flink 1.15 in Kinesis Data Analytics.</li>; </ul>; <h1>1.26.14</h1>; <ul>; <li>api-change:<code>route53</code>: [<code>botocore</code>] Amazon Route 53 now supports the Asia Pacific (Hyderabad) Region (ap-south-2) for latency records, geoproximity records, and private DNS for Amazon VPCs in that region.</li>; </ul>; <h1>1.26.13</h1>; <ul>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] AppFlow provides a new API called UpdateConnectorRegistration to update a custom connector that customers have previously registered. With this API, customers no longer need to unregister and then register a connector to make an update.</li>; <li>api-change:<code>auditmanager</code>: [<code>botocore</code>] This release introduces a new feature for Audit Manager: Evidence finder. You can now use evidence finder to quickly query your evidence, and add the matching evidence results to an assessment report.</li>; <li>api-change:<code>chime-sdk-voice</code>: [<code>botocore</code>] Amazon Chime Voice Connector, Voice Connector Group and PSTN Audio Service APIs are now available in the Amazon Chime SDK Voice namespace. See <a href=""https://docs.aws.amazon.com/chime-sdk/late",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12498:1149,latency,latency,1149,https://hail.is,https://github.com/hail-is/hail/pull/12498,1,['latency'],['latency']
Performance,">= 354 partitions using an external spark cluster (i.e. specifying `master` in `hail.init`), the spark worker crashes with a SIGSEGV. The issue does not occur with `variant_qc` but we do not know know the extent of what specific operations trigger it. Below is a test that consistently triggers the issue:. Setup:. $ $SPARK_HOME/sbin/start-master.sh --host localhost --port 7077; $ $SPARK_HOME/sbin/start-shuffle-service.sh; $ $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077 --work-dir /scratch/local/. Test:. import hail; hail.init(master=""spark://localhost:7077""); P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = hail.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.n_hom_var > V*0.32); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception:; print(""\n[FAIL] with "", N, ""partitions""); break. Test Output (SIGSEGV is reported in Spark worker logs, see end):. 2020-06-10 10:29:56 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 	Setting default log level to ""WARN"".; 	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 	Running on Apache Spark version 2.4.5; 	SparkUI available at http://US0HPN0036.cm.cluster:4047; 	Welcome to; 		 __ __ <>__; 		/ /_/ /__ __/ /; 	 / __ / _ `/ / /; 	 /_/ /_/\_,_/_/_/ version 0.2.44-6cfa355a1954; 	LOGGING: writing to /bmrn/apps/bmrn-hugelib/0.3.0/test/hail-20200610-1029-0.2.44-6cfa355a1954.log; 	2020-06-10 10:29:59 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 1:==========================> (171 + 80) / 350]; 	[PASS] with 350 partitions: (50000, 984); 	2020-06-10 10:30:08 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 3:==========================> (169 + 80) / 351]; 	[PASS] with 351 partitions: (500",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:1219,load,load,1219,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['load'],['load']
Performance,">=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5476,cache,cached,5476,https://hail.is,https://github.com/hail-is/hail/issues/10197,2,['cache'],"['cached', 'cachetools-']"
Performance,">> FIXME. ```scala; joinF.invoke(region, rtyp.loadEleme; ``` . I was referring to this... I wanted to check that joinF.invoke still needed region, didn't quite follow the invocation here",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7903#issuecomment-575779083:46,load,loadEleme,46,https://hail.is,https://github.com/hail-is/hail/pull/7903#issuecomment-575779083,1,['load'],['loadEleme']
Performance,">> should be explicitly representable in the IR; >; > How would that work with MakeArray?. I have some thoughts on that, but I think it would require a bigger change to how we represent streams in the IR. One idea is to put stream producers and stream consumers on equal footing. Right now, a stream consumer is a node with a stream child that produces a non-stream value, e.g. `ArrayFor(stream, name, body)`. We could let stream consumers be values themselves, `ArrayFor(name, body)`, with other nodes for connecting producers with consumers, e.g. `RunStream(stream, ArrayFor(name, body))`. Then I think we would need two `MakeStream`s, one that can be unrolled, and one that can be zipped. The former just directly takes a consumer and produces a value (and can choose to duplicate the consumer code or wrap it in a method). The later constructs a stream producer, which can be freely used in zips, etc. It would definitely require some thought how to optimize such a representation (is there a normal form?). > I'm not sure I see that. If you don't duplicate the consumer, the push code is the same. The question is, do you use a variable + switch to track where you are in the MakeStream, or do you use the program counter via labels, where you get the latter from the former by code duplication. I think the easiest way to see it is to think about how you would zip two `MakeStream`s, both of which represent their state using the program counter. You could do it by statically fusing them into a single `MakeStream`, but what if it was something more complicated like `Zip(Map(MakeStream(...), ...), Filter(MakeStream(...), ...))`? You can't represent a compound state with a single program counter. At a higher level, you can only do the optimization of unrolling the `MakeStream` loop if that is the outermost loop in the generated code. A producer wanting to create the outermost loop is what I mean by a push stream. By contrast, to be able to zip arbitrary streams, you want the consumer to",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8148#issuecomment-591962972:954,optimiz,optimize,954,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-591962972,1,['optimiz'],['optimize']
Performance,">Can you help me understand why we can't set a very negative z-index on whatever element is creating the offset or use one of these shorter solutions?. Sure. These solutions don't work because they interfere with the layout of content. What you linked above works fine with 1 element, but not with adjacent elements. All of these solutions try to make up for the fact that the browser will position the element at the top of the browser (has no concept of non-0 offset). The javascript solution fixes this by introducing that non-0-offset. - I tried every permutation of these solutions, including every solution at the link you provided, in the original fix. They all interfere with layout in the presence of nested and adjacent modified elements. There is no z-index solution possible for all combinations, at least without JS (because you need adjacent elements to have descending ordered z-indices). MathJax was handling scrolling at page load. If you remove that line MathJax will scroll the page whenever it detects a hash in the URL. ""Since typesetting usually changes the vertical dimensions of the page, if the URL contains an anchor position, then after the page is typeset, you may no longer be positioned at the correct position on the page. MathJax can reposition to that location after it completes its initial typesetting of the page. This value controls whether MathJax will reposition the browser to the #hash location from the page URL after typesetting for the page"". * https://docs.mathjax.org/en/v2.7-latest/options/hub.html. . The reason we use history instead of say updating location.href is because there is no way to prevent the browser from scrolling to that location when you update that value. It's undefeatable, as mentioned in the comments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7334#issuecomment-544655748:943,load,load,943,https://hail.is,https://github.com/hail-is/hail/pull/7334#issuecomment-544655748,1,['load'],['load']
Performance,">Pandas 1.5.0rc0</h2>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pandas-dev/pandas/commit/8dab54d6573f7186ff0c3b6364d5e4dd635ff3e7""><code>8dab54d</code></a> RLS: 1.5.2</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/d78c5e624936ea5bc30568fd7d6fc9b5f42d0beb""><code>d78c5e6</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/49806"">#49806</a> on branch 1.5.x (DOC: Update what's new notes for 1.5.2 re...</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/98c6139ff12107b9aa34441d25ef1593b6a0adca""><code>98c6139</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/49579"">#49579</a> on Branch 1.5.x (BUG: Behaviour change in 1.5.0 when using...</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/9196f8d545d1118f1233c1b45e7b740cb95c370c""><code>9196f8d</code></a> Backport PR STYLE enable pylint: method-cache-max-size-none (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/49784"">#49784</a>)</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/8c4b559c87561ca68ccdc3e81ff3c5218c7b4db7""><code>8c4b559</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/49776"">#49776</a> on branch 1.5.x (REGR: arithmetic ops recursion error with...</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/1616fb3d2c00905a5f3af510db893206ae00ea09""><code>1616fb3</code></a> Backport PR Revert &quot;Add color and size to arguments (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/44856"">#44856</a>)&quot; (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/49752"">#49752</a>)</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/6f8e1745472c9d107367da1e38494425c3938234""><code>6f8e174</code></a> Backport PR <a href=""https://github-redire",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12564:3490,cache,cache-max-size-none,3490,https://hail.is,https://github.com/hail-is/hail/pull/12564,1,['cache'],['cache-max-size-none']
Performance,"@Sun-shan According to the error message you posted, Spark itself cannot find `/hail/test/BRCA1.raw_indel.vcf`:; ```; py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.; : org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/hail/test/BRCA1.raw_indel.vcf; ```. Looking at that error message, it looks like Spark is interpreting your path as a local file system path, _not_ a hadoop path. Moreover, earlier in your posted output this line:; ```; 17/08/15 08:58:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; ```; suggests that you're not actually connecting to a Spark cluster with a properly configured Hadoop installation. ---. Your Spark cluster appears improperly configured. I'm not sure if `pyspark` is even connecting to your cluster. You might try looking at [this StackOverflow post](https://stackoverflow.com/questions/34642292/cant-connect-pyspark-to-master) about connecting `pyspark` to a Spark cluster. I strongly recommend running `pyspark` again and executing:; ```; spark.sparkContext.master; ```; This should print the URL of your Spark master node. If this prints a String starting with `local`, then you're definitely not connecting to a Spark cluster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-322539635:591,load,load,591,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-322539635,1,['load'],['load']
Performance,"@akotlar Ok, so I finally managed to remove the internal requests. The asyncio learning curve was higher than I expected. The final product is a lot tighter than I expected. I eliminated all the run_forever and run_once stuff, all the threading, and I was able to move the log back into server.py. It involves three new things:; - [kill the whole loop if anything goes wrong](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R899), which works wonderfully with k8s' automatic pod restarting; - [use a concurrent thread pool](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R904) for any legacy blocking operations; - [a blocking-to-async convertor](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R823) and a [blocking iterator to async iterator](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R828) both of which stick blocking operations on a separate thread pool. Legacy blocking operations might end up queueing behind one another in the ""blocking pool"", but the rest of the application continues without interruption on the main event loop. I took the chance to reorder the k8s refresh and the k8s watch functions to be closer together, but that made the diff worse :/. Probably demands another review on Monday. cc: @cseed, possibly some asyncio engineering best practices in this",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-482744047:537,concurren,concurrent,537,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-482744047,4,"['concurren', 'queue']","['concurrent', 'queueing']"
Performance,"@armartin did, see the linked issue. While I think supporting this is the right thing in some sense, would make things easier for first-time users and loading small data sets without thinking too much, I have mixed feelings about spending time on it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3499#issuecomment-386690550:151,load,loading,151,https://hail.is,https://github.com/hail-is/hail/issues/3499#issuecomment-386690550,1,['load'],['loading']
Performance,"@asvetlov thanks for your reply, and for your work on the Sanic project! I was really curious about the Techempower issue. Do you know why Sanic, on past rounds failed to complete test subsets without error? I haven’t had much of a chance to look into that yet, but https://github.com/huge-success/sanic/issues/53 doesn’t divulge much, and my own attempts to give Sanic problems haven’t yielded anything worrisome (i.e asyncpg works great under 2000 simultaneous connection load, request standard deviation is about as tight as aiohttp, and number of extremes / timeouts is smaller than aiohttp). Techwmpower benchmark was on version 0.7, if not earlier (the linked file in the Techempower issue is 0.7), and that version may have been affected by the issue described here: https://github.com/huge-success/sanic/issues/1176 which seems to have been largely addressed. . Edit: furthermore, other recent tests showed no significant issues with Sanic https://fgimian.github.io/blog/2018/06/05/python-api-framework-benchmarks/. Still the addressing the Techempower issues may help people feel more confident.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461425481:474,load,load,474,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461425481,1,['load'],['load']
Performance,"@catoverdrive Assigned you since it will be good to get you acclimated to the services team code. We have a function called `is_transient_error` which checks if an error is a known-to-be-transient error. An error is transient if perpetually retrying the error-raising-operation will eventually lead to success. We consider most things transient, even 500 Internal Server Error, because we expect our systems work but sometimes fail under heavy load. Eventually load should drop.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9213#issuecomment-668750434:444,load,load,444,https://hail.is,https://github.com/hail-is/hail/pull/9213#issuecomment-668750434,2,['load'],['load']
Performance,"@catoverdrive I pushed a fix on here. I'll let Patrick review I think, unless you want to review my piece. Unfortunately, this kills performance. The no-key benchmark is about 5-6x faster (20s) than the keyed benchmark (110s)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6267#issuecomment-499585600:133,perform,performance,133,https://hail.is,https://github.com/hail-is/hail/pull/6267#issuecomment-499585600,1,['perform'],['performance']
Performance,@catoverdrive I rebased and pulled your `loadmatrix` branch. I will review.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2431#issuecomment-344159074:41,load,loadmatrix,41,https://hail.is,https://github.com/hail-is/hail/pull/2431#issuecomment-344159074,1,['load'],['loadmatrix']
Performance,"@catoverdrive I still need to give you someway to uniformly use either a compiled AST or a compiled IR, but maybe you can start playing with optimizer things with this. Adding a constant propagation pass to the IR would be neat. I've got some PR review to do first though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2545#issuecomment-350357452:141,optimiz,optimizer,141,https://hail.is,https://github.com/hail-is/hail/pull/2545#issuecomment-350357452,1,['optimiz'],['optimizer']
Performance,"@chrisvittal I took the plunge and also created a `MatrixHybridReader`. Changes (including above):. - FunctionBuilder now accepts `Code[Unit]` to be added to the `init` method of the function object; - SRVB now has an `init` method that should be called in the `init` method of a function object if many methods will share the SRVB; - `CodeChar` now exists; - `TextMatrixReader` exists which mimics `TextTableReader`; these should get unified at some point; - minor documentation fixes to `import_matrix_table`; - better error messages wrt using the name `row_id`, which is reserved for use by `import_matrix_table` when there are no keys specified; - several new tests, including:; - extensive testing of the product space of `header`, `delimiter`, `header`, and `entry_type` (including such weird things as using `9` as the missing value); - a pathological file: `9`-separated values with `8` representing the missing value; - several tests that trigger the pruner; - rename `LoadMatrix.scala` to `TextMatrixReader.scala`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6987#issuecomment-529001365:978,Load,LoadMatrix,978,https://hail.is,https://github.com/hail-is/hail/pull/6987#issuecomment-529001365,1,['Load'],['LoadMatrix']
Performance,"@chrisvittal This is all garbage, the failure makes little sense. I've started back from scratch with master, and have re-implemented all of the functionality, from this branch, needed to have the Dataproc test run (which relies on LoadVCF). . It works fine. Something else is amiss. I'm going to finish reimplementing everything in that clean slate, and when everything is running close this PR and reissue. The diff between them will show the problem area, which is in some kind of global state affecting sparkContext or RDD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-498334543:232,Load,LoadVCF,232,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-498334543,1,['Load'],['LoadVCF']
Performance,"@chrisvittal Would you mind reviewing at least the changes since @tpoterba's approval, since he's out this week? Summary of the original changes is at the top. The main changes since that, which I made while debugging failing tests, are:; * Add `PType.loadCheapSCodeField`. Both this and `loadCheapSCode` now return an `SValue`, with the former using fields instead of locals.; * `SingleCodeSCode.loadToSValue` uses fields",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10797#issuecomment-908587926:252,load,loadCheapSCodeField,252,https://hail.is,https://github.com/hail-is/hail/pull/10797#issuecomment-908587926,3,['load'],"['loadCheapSCode', 'loadCheapSCodeField', 'loadToSValue']"
Performance,@chrisvittal if you dismiss Tim's review it will show up on his ci queue,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12295#issuecomment-1431862227:67,queue,queue,67,https://hail.is,https://github.com/hail-is/hail/pull/12295#issuecomment-1431862227,1,['queue'],['queue']
Performance,"@cseed - this PR has a bad interaction with the changes made to [optimize inside ArrayAgg emit](https://github.com/hail-is/hail/pull/5765/files). . I've added a [test that catches the problem](https://github.com/hail-is/hail/pull/5710/files#diff-3273df362c814023cfa64428acf395cfR1122). The root of the issue is that we **cannot run NormalizeNames again** inside of that optimization pass -- it generates references that collide/overwrite existing bindings available when ArrayAgg is emitted. We should be creating globally-unique names inside ForwardLets, not normalized names.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5710#issuecomment-483001172:65,optimiz,optimize,65,https://hail.is,https://github.com/hail-is/hail/pull/5710#issuecomment-483001172,2,['optimiz'],"['optimization', 'optimize']"
Performance,"@cseed . ```python3; import hail as hl; hl.import_vcf(PATH, reference_genome='GRCh38').write('/tmp/vcfmt', overwrite=True); ```. On my laptop with a warmish filesystem cache takes 1 minute for this code and 1:20 for current master. That's actually pretty good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5828#issuecomment-481451647:168,cache,cache,168,https://hail.is,https://github.com/hail-is/hail/pull/5828#issuecomment-481451647,1,['cache'],['cache']
Performance,"@cseed @danking as far as I can tell, asm doesn't give you a way to look at class/method size and you have to external stuff. So the problem with the constant pool is that with structs, two things are loaded as constants; the index and the offset. I think it's kind of hard to work around this, especially on the `GetField` side.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3308#issuecomment-380197516:201,load,loaded,201,https://hail.is,https://github.com/hail-is/hail/pull/3308#issuecomment-380197516,1,['load'],['loaded']
Performance,"@cseed @tpoterba . See [the if section of AST.scala in my issue-813 branch](https://github.com/danking/hail/blob/issue-813/src/main/scala/org/broadinstitute/hail/expr/AST.scala#L1718-L1834). This doesn't work on a cluster because we need to do the class loading on the agents, not on the driver.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/813#issuecomment-251530669:254,load,loading,254,https://hail.is,https://github.com/hail-is/hail/issues/813#issuecomment-251530669,1,['load'],['loading']
Performance,"@cseed Added the redirect logic. It feels much slower (although there may be some small optimizations available). What do you think about using popup as the default, and then catch on error and send to redirect method? https://github.com/auth0/auth0.js/issues/868. This could work well as long as blocking happened rarely. So far, I haven't been able to trigger the block in any browser (Safari, Chrome, Firefox, all latest v), with content blockers enabled (which definitely block popups on other sites).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162#issuecomment-456667812:88,optimiz,optimizations,88,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-456667812,1,['optimiz'],['optimizations']
Performance,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996:72,load,loading,72,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996,16,['load'],['loading']
Performance,@cseed Can we reassign to someone else to determine:; 1. why the entries index is changing in the optimizer; 2. if this is actually a bug or not; 3. make a code change that either fixes the bug or fixes the equality check.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4527#issuecomment-429062373:98,optimiz,optimizer,98,https://hail.is,https://github.com/hail-is/hail/issues/4527#issuecomment-429062373,1,['optimiz'],['optimizer']
Performance,"@cseed I managed to reproduce the problem you were seeing on a cluster. It's not a problem with the size of the table (as I previously suspected), but rather a race condition caused by dropping a table then recreating it immediately. Adding a sleep avoids the problem for the time being. There are stil some merge conflicts due to the way that the metadata is stored. The best thing would be to factor out the code to do that so it can be shared by the Parquet and Kudu code paths. I'll have a go at doing that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-213006430:160,race condition,race condition,160,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-213006430,1,['race condition'],['race condition']
Performance,"@cseed I think this is probably ready for review. There's a couple of things that I'm going to look at/do more that aren't in this PR:; - add better docs for how random functions behave; - add more tests for randomness in various IR nodes; - add more tests and make sure that the optimization rules preserve the context that random functions are going to be evaluated in. @tpoterba and I talked briefly about the last one and it might involve restructuring simplify, since some rules are fine in isolation but will return different results when performed in combination (and sometimes one of the rules in question doesn't even involve the random node, which is kind of difficult to account for currently)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4104#issuecomment-411878967:280,optimiz,optimization,280,https://hail.is,https://github.com/hail-is/hail/pull/4104#issuecomment-411878967,4,"['optimiz', 'perform']","['optimization', 'performed']"
Performance,"@cseed I understand that BlockMatrix is useful when the sample by sample matrix is too large to store on each node. But for the current master we are still converting the result .toLocalMatrix on the driver, so each dimension on the square matrix cannot exceed the bound of 2^16 = 65k checked by the computeGrammianMatrix function anyway, as underlying Breeze DenseMatrix is backed by a single Java array. For samples > 65k, we could use BlockMatrix and write out via some sort of streaming over blocks (though it may be more performant to directly index chunks up to 65k samples each and computeGrammianMatrix on each pair of chunks). But if the number of samples is under 65k, it looks like we should still use computeGrammianMatrix.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/801#issuecomment-247862866:526,perform,performant,526,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247862866,1,['perform'],['performant']
Performance,"@cseed I've implemented `readPartitions` on `HailContext` and `writePartitions` on `RichRDD`, re-implemented read/write of blocks on `BlockMatrix` to preserve partitioning using these, and moved MatrixValue and KeyTable row read/write to use these. readPartitions and writePartitions use `unsafeReadPartition` and `writePartition` in RichHadoopConfig. `readRow` includes `close()`, and I've added `close()` to `readBlock` as well. At first I used `using` (see `readPartition` which is remarked out), but this closes the resource before the iterator iterated in the readRows case. Yet somehow, `testWriteRead` in VSMSuite passes even with `readPartition`. This test just creates a random VDS, writes, reads, compares. I added `testWriteRead2` which imports `sample.vcf`, writes, reads, compares. And indeed, the latter fails with `readPartition`. I have no explanation for why the behavior would differ (could the former be somehow cached?). I'm not happy with the asymmetry or lake of read safety if closing isn't propagated backward (I think it is for DataInputStream), but would like to get your feedback before messing around with these functions further. I've left the filename that had ReadRowsRDD with that name so it's easier to compare, but will change prior to merging since that class is gone. No class jumps out, should I just use the first one, `ArrayInputStream`, or a name that's not a class?. `ReadRDDPartition` in file ReadRowsRDDs and `IntPartition` in `BlockMatrix` have the same definition (the latter is used for more than just reading). I'm thinking of just having IntPartition located somewhere other than BlockMatrix. Any thoughts on where?. I added `sqrt` on blockSize in `blockMatrixGen` in order to get more cases with `blockSize < min(rows, cols)`. Only 1 of 10 had this property in `readWriteIdentityRandom` and its an important case to check with respect to the GridPartitioner dealing with block indices corrected (particularly with transpose in the mix).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2367:931,cache,cached,931,https://hail.is,https://github.com/hail-is/hail/pull/2367,1,['cache'],['cached']
Performance,"@cseed Still running the pipeline, but can confirm I got 293 partitions for chr22 and the import step took about 1 minute. ```; 2018-07-18 15:39:30 Hail: INFO: Number of BGEN files parsed: 1; 2018-07-18 15:39:30 Hail: INFO: Number of samples in BGEN files: 487409; 2018-07-18 15:39:30 Hail: INFO: Number of variants across all BGEN files: 1255683; 2018-07-18 15:40:37 Hail: INFO: Coerced almost-sorted dataset; 2018-07-18 15:40:39 Hail: INFO: interval filter loaded 5 of 293 partitions; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3945#issuecomment-405977581:459,load,loaded,459,https://hail.is,https://github.com/hail-is/hail/pull/3945#issuecomment-405977581,1,['load'],['loaded']
Performance,@cseed The current code only does a repartition if the number of variants per partition in the input dataset is greater than the queue size - should I always force a repartition to occur to rebalance after the first prune step?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1296#issuecomment-275287341:129,queue,queue,129,https://hail.is,https://github.com/hail-is/hail/pull/1296#issuecomment-275287341,1,['queue'],['queue']
Performance,"@cseed This should now be sufficient for Wed. I will probably add 2 features before then: reconnecting web sockets (though connection should infrequently drop, shouldn't affect workshop), and an admin panel to blow out users / delete notebooks. And maybe SSL. Edit: my commit https://github.com/hail-is/hail/pull/5162/commits/bac155c9713d99c68cd7d0605eb59585656c14ea makes reference to csrf. This may not be necessary: inspecting the login request I notice a nonce, generated by the auth0-js lib, sent with login and silent token rotation / refresh, which is nice. Assuming it's used to prevent replay attacks, in this case it has the same purpose as a CSRF token. Yay auth0. Edit2: Regarding performance. Scorecard page, before server-side caching takes 100ms on refresh, and 50ms on navigation from another page. This effectively means no overhead from my web implementation. Why? It takes ~50ms to return *anything* (including favicon.ico of 0 bytes), i.e 50ms is the time it takes from my computer to kubernetes and back, with no additional work done. We have 2 such requests currently when visiting app.hail.is/scorecard: one to to the web app server, one to scorecard/json. After caching (which is invalidated every 3 minutes), takes 50ms. So, if current scorecard.hail.is needed to hit a json endpoint to get data for its template, we would expect it to be no faster. Alternatively if we placed the json-generating function in the web-app's nodejs server it's response time would drop by ~50ms.; * I also am trying to use the internal DNS (SSR phase routes to http://scorecard/json, so should use kubernetes DNS; still take ~50ms to get the json... before caching).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162#issuecomment-460071524:693,perform,performance,693,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-460071524,2,"['perform', 'response time']","['performance', 'response time']"
Performance,"@cseed added these print statements. I've never used the output in debugging in production except to quickly realize a job is one of a given user's when I was looking at worker performance. Early on, I might have checked it to make sure the Docker config resource options and volume mounts were correct. But I can add those back when debugging if needed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9184#issuecomment-667107337:177,perform,performance,177,https://hail.is,https://github.com/hail-is/hail/pull/9184#issuecomment-667107337,1,['perform'],['performance']
Performance,"@cseed bump. This is blocking further testing on dbuf. Without this, there's a race condition where the image is deleted by CI before the entire StatefulSet starts.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7564#issuecomment-558382580:79,race condition,race condition,79,https://hail.is,https://github.com/hail-is/hail/pull/7564#issuecomment-558382580,1,['race condition'],['race condition']
Performance,@cseed rebased! I'll probably put a bit on performance characteristics and graphs in the discuss post to go with it once it's in.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1375#issuecomment-280150750:43,perform,performance,43,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-280150750,1,['perform'],['performance']
Performance,@cseed single branch has a very limited effect:; ```; # for i in $(seq 0 2) ; do rm -rf hail && time git clone https://github.com/danking/hail.git --branch ci-decompress --single-branch >/dev/null 2>/dev/null; done; git clone https://github.com/danking/hail.git --branch ci-decompress > 2> 3.61s user 1.63s system 53% cpu 9.846 total; git clone https://github.com/danking/hail.git --branch ci-decompress > 2> 3.50s user 1.57s system 54% cpu 9.333 total; git clone https://github.com/danking/hail.git --branch ci-decompress > 2> 3.65s user 1.66s system 54% cpu 9.817 total; # for i in $(seq 0 2) ; do rm -rf hail && time git clone https://github.com/danking/hail.git >/dev/null 2>/dev/null; done ; git clone https://github.com/danking/hail.git > /dev/null 2> /dev/null 4.26s user 1.88s system 52% cpu 11.637 total; git clone https://github.com/danking/hail.git > /dev/null 2> /dev/null 4.24s user 1.87s system 51% cpu 11.917 total; git clone https://github.com/danking/hail.git > /dev/null 2> /dev/null 4.05s user 1.77s system 54% cpu 10.661 total; ```; I also noticed that GitHub does some limited cacheing. The fresh pull of the full repo was 14s. As you can see subsequent pulls are only 11s.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7626#issuecomment-566697070:1098,cache,cacheing,1098,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-566697070,1,['cache'],['cacheing']
Performance,"@cseed this code in GenericGenotype is not in MutableGenotype. Do you think it belongs in read? Seems like it will hit (relative) performance more significantly in the mutable case. ```; require(unboxedGT >= -1, s""invalid _gt value: $unboxedGT""); require(unboxedDP >= -1, s""invalid _dp value: $unboxedDP""). if (isDosage) {; require(unboxedGQ == -1); if (unboxedPX == null); require(unboxedGT == -1); else {; require(unboxedPX.sum == 32768); require(unboxedGT == Genotype.gtFromLinear(unboxedPX).getOrElse(-1)); }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1559#issuecomment-287181344:130,perform,performance,130,https://hail.is,https://github.com/hail-is/hail/pull/1559#issuecomment-287181344,1,['perform'],['performance']
Performance,"@cseed, details below, exec summary: a series of confusions and confusing interfaces lead to cloud tools deployment issues. The resolution is to fix the deploy script to always deploy `cloudtools` for python 2 and 3 (because *it* works with both). Users will need `python2` somewhere on their PATH, but `gsutil` will guide them through that, i.e. not our problem. There is no problem except that I cannot approve my own PR. I noticed @catoverdrive wasn't actually assigned, so it wasn't in her scorecard queue. ---. There was a bit of confusion around this recently that @catoverdrive figured out. So, `gsutil` only supports `python2`, but it's happy to search through your path for a variety of binaries with suggestive names until it finds a valid python 2.x binary. `cloudtools` itself works with any version of python (there was a recent breaking change in the release of python 3.7, which broke cloud tools [but that's been resolved in an open PR](https://github.com/Nealelab/cloudtools/pull/91/files#diff-7decff7c08c5270a32982ea34483b8cbR11)). Now, wrt PYPI, @catoverdrive discovered that the version of python you use to run `setup.py bdist_wheel` determines which version of python is deployed. This is rather confusing and, since none (or many?) of us did not know this, we accidentally uploaded a variety of [cloud tools versions on pypi, as you've noted](https://pypi.org/simple/cloudtools/). There was some earlier confusion wherein I thought we should only support `python2`, but that was a mistake on my end. We should support both versions, and when the user tries to start a cluster, `gsutil` will provide an error message guiding them to install `python2`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588:504,queue,queue,504,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588,2,['queue'],['queue']
Performance,"@cseed,. > The point isn't to save the few untars, the point is to make the general facilities performant for everyone. Basically, I think directory outputs are untenable in the current design unless they are very small, which is why you're doing all this for your use case, and the tar is still probably as good (if not better) in that case. Fix it for everyone's use case so nobody has to go through this in the future. I agree directory outputs with large file counts are untenable. Are you suggesting we do this at the batch level for any directory? That feels a bit opaque, because users may expect that if they specify their output is a directory that they can selectively download certain subdirectories. At the build.yaml level, I see a couple explicit options:; ```; outputs:; - from: /io/repo/hail/resources; to: /resources.tgz; codec: gzip; ```; and; ```; inputs:; - from: /resources.tgz; to: /io/resources; codec: gzip; ```; ?. There's also this, which irks me a bit because it's punny, but:; ```; outputs:; - from: /io/repo/hail/resources; to: /resources.tgz; ```; and. ```; inputs:; - from: /resources.tgz; to: /io/resources; extract:; - .; ```; ```; inputs:; - from: /resources.tgz; to: /io/resources; extract:; - foo; - bar; ```. A little bit my reaction to this is that we're creating a DSL that provides minor value relative to the user using `tar` in their `runImage` steps. E.g. build hail is explicit and not verbose:; ```; ...; tar czf test.tar.gz -C python test; tar czf resources.tar.gz -C src/test resources; tar czf data.tar.gz -C python/hail/docs data; tar czf www-src.tar.gz www; tar czf cluster-tests.tar.gz python/cluster-tests; ```; and `test_hail_java` is fine:; ```; ...; tar xzf resources.tar.gz -C src/test; ...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7626#issuecomment-560470857:95,perform,performant,95,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560470857,1,['perform'],['performant']
Performance,"@daniel-goldstein, BackendUtils.scala `collectDArray` has an optimization for single partition jobs. They're run on the driver.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12736#issuecomment-1485546827:61,optimiz,optimization,61,https://hail.is,https://github.com/hail-is/hail/pull/12736#issuecomment-1485546827,1,['optimiz'],['optimization']
Performance,"@danking , I'm a bit stuck on how to proceed with the credential refreshing. Here's the layout of the problem:. 1. In normal Azure, we accept user-provided SAS tokens. Since they are user-provided, we have no way of obtaining new ones and the onus is on the user to obtain a SAS token for however long they expect to need to use it.; 2. This current design in Terra is to not make the user have to do that, because that seems annoying, and for terra-controlled ABS containers we have an endpoint we can hit to get a SAS token. Ok, but now we need to update our Azure FS infrastructure to refresh a credential if it expires. But, we use the azure client lib and don't control all http requests. For example, for `AzureStorageFS.open`, we call `downloader.readall()` if we want to load the whole file into memory. I went spelunking through their source and looks like `readall` mostly wraps a sequence of range reads, but regardless if we were to use that method we would have to catch credential expiration errors, reset credentials on the blob client and retry hoping that we didn't break any invariants -- I don't want to do that as I wouldn't trust a stream that encountered a non-transient error like that. It could be that getting rid of `downloader.readall` is the only thing we have to worry about, but it makes me uneasy not having control of the http requests we're making to ABS. Do you see a solution other than raking through our `aioazure.fs` and making sure that we only use ""quick"" methods and possibly retrying 401s? It just seems to me like we're going against the grain and even though it feels user-hostile the intention of SAS tokens are to have users own credential expiration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13944#issuecomment-1930482715:779,load,load,779,https://hail.is,https://github.com/hail-is/hail/pull/13944#issuecomment-1930482715,1,['load'],['load']
Performance,"@danking -; Let's compare with & without Hail install; I don't know where to find `load-spark-env.sh` so I only print the env once. ## Without Hail. ```sh; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-shell; /usr/bin/spark-shell; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-shell; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/; ; Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.21; Branch ; Compiled by user release on 2023-07-19T15:12:33Z; Revision ; Url ; Type --help for more information.; ```. ```sh; -rwxr-xr-x 1 root root 141 Jul 19 15:54 /usr/bin/spark-submit; /usr/bin/spark-submit; -rwxr-xr-x 1 root root 141 Jul 19 15:54 /usr/bin/spark-submit; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/; ; Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.21; Branch ; Compiled by user release on 2023-07-19T15:12:33Z; Revision ; Url ; Type --help for more information.; ```. ```sh; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-class; /usr/bin/spark-class; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-class; SPARK_SCALA_VERSION=; ```; <details><summary>>>>>>>>>>> before load-spark-env.sh <<<<<<<<<</summary>; <p>; ```sh; XDG_SESSION_ID=38; HOSTNAME=ip-192-168-96-172; TERM=xterm-256color; SHELL=/bin/bash; HISTSIZE=1000; SSH_CLIENT=103.37.196.84 57805 22; QTDIR=/usr/lib64/qt-3.3; QTINC=/usr/lib64/qt-3.3/include; SSH_TTY=/dev/pts/0; USER=hadoop; LS_COLORS=rs=0:di=38;5;27:ln=38;5;51:mh=44;38;5;15:pi=40;38;5;11:so=38;5;13:do=38;5;5:bd=48;5;232;38;5;11:cd=48;5;232;38;5;3:or=48;5;232;38;5;9:mi=05;48;5;232;38;5;15:su=48;5;196;38;5;15:sg=48;5;11;38;5;16:ca=48;5;196;38;5;226:tw=48;5;10;38;5;16:ow=48;5;10;38;5;21:st=48;5;21;38;5;15:ex=38;5;34:*.tar=38;5;9:*.tgz=38;5;9:*.arc=38;5;9:*.arj=38;5;9:*.taz=38;5;9:*.lha=38;5;9:*.lz4=38;5;9:*.lzh=38;5;9:*.lzma=38;5;9:*.tlz=38;5;9:*.txz=38;5;9",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045:83,load,load-spark-env,83,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045,1,['load'],['load-spark-env']
Performance,"@danking @catoverdrive OK, I think it's ready for another review. I made the suggested changes, as well as the changes we talked about on Tuesday during check-in. There is a ton of indirection now, which might be confusing but is good for future expansion. The matrix and table metadata are now polymorphic `RelationalSpec`s and have a `name` field so new relational types can be added. Here is `sample.hmt/metadata.json.gz`:. ```; {; ""components"": {; ""cols"": {; ""rel_path"": ""cols\/rows"",; ""name"": ""RVDComponentSpec""; },; ""rows"": ...,; ""partition_counts"": {; ""counts"": [; 346; ],; ""name"": ""PartitionCountsComponentSpec""; }; },; ""matrix_type"": ...,; ""references_rel_path"": ""references"",; ""hail_version"": ""devel-e6ef439"",; ""file_version"": 65536,; ""name"": ""MatrixTableSpec""; }; ```. A MT has currently has five components: globals, cols, rows, entries and partition_counts. The first four are `RVDComponents`, the counts its own thing. I wanted to make the references a component, but they need to be loaded before the types are parse, so `RelationalSpec`s have a path to the references. Components are future expandable. The MT directory structure has metadata and four directories for each main component which is a Table directory. Those directory names are stored in the metadata, so they can be changed or even point elsewhere. A Table directory has two directories: globals and rows, which are RVDs. Again, the directories are stored in the metadata and I use that here: the globals RVD for rows and cols tables are the rows RVD of the globals table of the MT (if you can understand this sentence you've got a handle on this PR.). RVDs now store their own metadata, the RVDSpec. A sample rows RVD metadata for the rows table of an MT, `sample.hmt/rows/rows/metadata.json.gz` looks like:. ```; {; ""jRangeBounds"": [],; ""partFiles"": [; ""part-0""; ],; ""codecSpec"": ...,; ""orvdType"": ...,; ""name"": ""OrderedRVDSpec""; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2858#issuecomment-364814856:998,load,loaded,998,https://hail.is,https://github.com/hail-is/hail/pull/2858#issuecomment-364814856,1,['load'],['loaded']
Performance,"@danking @cseed should be ready for testing soon. pruned portions we're not using atm, wrote docker files, tested. if you want to run both packages on your local machine, you could use the top-level docker file (or each package's). To start a local instance of the web app, simply run:; `npm install && npm bootstrap`. To get a hot-reloading version of the web app (links to your browser, refreshes all changes): `cd packages/public && npm run dev`. To start the gateway: `cd packages/hail-api-gateway && nodemon index.js`. Dev mode routing is slow. To see a production, minified build: `cd packages/public && npm run build && npm run prod-test`.; * Build is a kind of compilation process. Dev dependencies are pruned, the app is split into static bundles, and minified. Some optimizations, like inlining of some React functions also occurs. This is independent of anything V8 does . This will also show a neat readout of all bundles:; ```; Browser assets sizes after gzip:. 2.79 kB .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:776,optimiz,optimizations,776,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935,2,['optimiz'],['optimizations']
Performance,@danking Any idea why this is failing? On the CI I'm seeing:. > Failed to perform checkout on agent: Cannot find commit 8955cc391e0e811ecb3a5325a82dff96eb3a0824,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1463#issuecomment-283829662:74,perform,perform,74,https://hail.is,https://github.com/hail-is/hail/pull/1463#issuecomment-283829662,1,['perform'],['perform']
Performance,@danking Can you confirm whether we can just drop the table in one shot or do we need to use truncate or delete rows in chunks first? I'm just worried about performance here and I can't find a solid answer on whether this operation is instant or could take days.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12710#issuecomment-1443672380:157,perform,performance,157,https://hail.is,https://github.com/hail-is/hail/pull/12710#issuecomment-1443672380,1,['perform'],['performance']
Performance,"@danking I addressed your comments that weren't major changes. I see making three issues -- fix the __init__ methods for Job and Batch, fix the startup polling inconsistent state issues, and fix the database performance issues. Let me know if any of these still need to be addressed in this PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5934#issuecomment-486009826:208,perform,performance,208,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-486009826,1,['perform'],['performance']
Performance,"@danking I think I addressed most of your comments. Everything is passing. Can you look if you like the interface for `sync_check_call` etc.? If so, then can I translate your TLS commands in lists into a single string? Is there ever a reason to want to pass the list explicitly in this case?. Once you're happy with the changes, then I'll do another test round with dev deploy to make sure the cache is actually doing its thing and the garbage collection loop is working.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9095#issuecomment-665855101:394,cache,cache,394,https://hail.is,https://github.com/hail-is/hail/pull/9095#issuecomment-665855101,1,['cache'],['cache']
Performance,"@danking Looks like I'm still failing to configure a couple of settings related to references on the `ServiceBackend` but you can feel free to start looking. You'll notice that I made quite a substantial refactor in `ServiceBackend.scala` in an attempt to harmonize the scala backends a bit more. The rationale behind the refactor is I was having a hard time working with the various thunks passed around there. I saw them as a bit of poor-man's-object way to capture some state from the input file while keeping the `ServiceBackend` stateless. IMO there's no harm in keeping the `ServiceBackend` just as stateful as the other backends since it is single use. So I lifted a lot of that state into backend-creation time and created a harder delineation between which part of the input is for configuring the backend and which part is for the action being performed. This made it easier to reuse a couple of methods like `tableType` and such. I'm happy to take suggestions on ways to trim down this PR, but I thought you'd want to take a look at the whole thing given the time-sensitivity.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13797#issuecomment-1766521995:854,perform,performed,854,https://hail.is,https://github.com/hail-is/hail/pull/13797#issuecomment-1766521995,1,['perform'],['performed']
Performance,"@danking Sorry to keep making you break things out, but it is really helpful for me and the changes will go in faster. Can you make a separate PR with the following changes that don't relate to passing the indices and the new index code? Specifically, the following items from your list:. ```; added row_fields which prevents reading and allocation of LID and RSID (also improved python-type-checking for row_fields and entry_fields). I changed several asserts to if's with fatals, so as not to allocate strings. We no longer copy the genotype data into a buffer in the block reader. This was forcing the fastKeys to do an unnecessary data copy. I changed the contract on BgenRecord to require that getValue is called to ""consume"" the record before the next record is taken. getValue(null) just skips bytes (no copy, no decompression). I added RegionValueBuilder.unsafeAdvance which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work. I use RegionValueBuilder.unsafeAdvance to make loading a BGEN without entry fields very fast. I fixed Table.index to not trigger a partition key info gathering; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3727#issuecomment-397281018:1052,load,loading,1052,https://hail.is,https://github.com/hail-is/hail/pull/3727#issuecomment-397281018,1,['load'],['loading']
Performance,@danking What do you think about having a version ID inside the JAR file (MANIFEST???). We already download the JAR file on the worker. Not sure how much extra time it would be to look for the version inside the JAR (maybe cache this?) and then pass the right argument configuration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12222#issuecomment-1258646909:223,cache,cache,223,https://hail.is,https://github.com/hail-is/hail/pull/12222#issuecomment-1258646909,1,['cache'],['cache']
Performance,@danking do you have an intuition for what the block size and capacity defaults should be for the cache?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3095#issuecomment-371984225:98,cache,cache,98,https://hail.is,https://github.com/hail-is/hail/pull/3095#issuecomment-371984225,2,['cache'],['cache']
Performance,"@danking img: https://github.com/genuinetools/img. ""Standalone, daemon-less, unprivileged Dockerfile and OCI compatible container image builder. img is more cache-efficient than Docker and can also execute multiple build stages concurrently, as it internally uses BuildKit's DAG solver. The commands/UX are the same as docker {build,tag,push,pull,login,logout,save} so all you have to do is replace docker with img in your scripts, command line, and/or life."". Oops, seems it doesn't quite work unprivileged yet, see: https://github.com/genuinetools/img#running-with-docker. Waiting on an upstream docker change, no movement in two months. Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5623#issuecomment-474189290:157,cache,cache-efficient,157,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474189290,4,"['cache', 'concurren']","['cache-efficient', 'concurrently']"
Performance,"@danking, I started playing with your ASM experiment and wrote a library for lightweight bytecode generation. The primary abstractions are `FunctionBuilder` and `Code[T]`. The latter is an object that can generate bytecode to produce a value of type `T` on the top of the stack. I'm reasonably happy with the interface, see this example for factorial:. https://github.com/cseed/hail/commit/93d95982bccd16ffa531f67fa47163f3fc8cbdde#diff-e434fa9004c38142a8f6f64ffa73b48eR109. No ClassBuilder yet. Apart from that, all the major features are there. There are a bunch of missing operations (type conversions, for example) and I only have wrapper classes for `Int` and `Double`. Once we fill it out I think it will make an excellent stand-alone library. While I optimized conditional generation to be smart about converting between indicator values (0, 1) and branch targets, it still emits some unnecessary GOTOs for fall through and could be improved. There are two double comparison bytecodes (DCMPG and DCMPL) that treat NAN differently. I wasn't sure which one to use. We should probably emulate Java/Scala. I can't tell if ASM is generating short bytecodes for load from small local indices (e.g. ILOAD_2) or small constants (e.g., ICONST_3). It isn't clear if the pretty printer that comes with ASM makes a distinction. We probably need to dump to a file and run `javap`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/921:757,optimiz,optimized,757,https://hail.is,https://github.com/hail-is/hail/pull/921,2,"['load', 'optimiz']","['load', 'optimized']"
Performance,"@gregsmi Looks like the scopes we currently have set are insufficient for reading the storage account keys. Do you know best practice permissions for creating SAS tokens?. ```; The client '96fe73da-25e0-4a69-9cd8-0043e56d0d0a' with object id '96fe73da-25e0-4a69-9cd8-0043e56d0d0a' does not have authorization to perform action 'Microsoft.Storage/storageAccounts/listKeys/action' over scope '/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/haildev/providers/Microsoft.Storage/storageAccounts/hailtest' or the scope is invalid. If access was recently granted, please refresh your credentials.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13140#issuecomment-1577254412:312,perform,perform,312,https://hail.is,https://github.com/hail-is/hail/pull/13140#issuecomment-1577254412,1,['perform'],['perform']
Performance,"@huy-nguyen is getting a segfault on on current release (0.2.33-5d8cae649505):; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa4b25e18cd, pid=6637, tid=0x00007f9a4f1fc700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-1~deb9u1-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8240:473,load,loadBit,473,https://hail.is,https://github.com/hail-is/hail/issues/8240,1,['load'],['loadBit']
Performance,@jbloom22 That would be great. We have made Nirvana even faster recently. Also we are working on reduce the overhead (i.e. time used to load the Cache) for each Nirvana process. I will test on the best blockSize again when this is done.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3266#issuecomment-391058985:136,load,load,136,https://hail.is,https://github.com/hail-is/hail/pull/3266#issuecomment-391058985,2,"['Cache', 'load']","['Cache', 'load']"
Performance,"@jbloom22 We lost our AWS EC2 instance I used for performance testing previously. I tried to build HAIL on our cluster, but was out of luck due to some Python issues. We just get another EC2 instance very recently and I haven't had a chance to test the block size yet. One thing I want to mention is that every Nirvana process will spend several seconds on reading the Cache file and Nirvana can process 10k+ variants per second (on my laptop). (I deleted my estimation as it is incorrect... ) So Nirvana prefers larger chunks of data due to the overhead of loading Cache file and relatively faster annotation speed. With other possible overhead like disk I/O (I tested HAIL with Spark running in Standalone mode, which may not be ideal) and merging the results of different Nirvana process, more Nirvana processes do not always lead a shorter processing time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3266#issuecomment-386642917:50,perform,performance,50,https://hail.is,https://github.com/hail-is/hail/pull/3266#issuecomment-386642917,4,"['Cache', 'load', 'perform']","['Cache', 'loading', 'performance']"
Performance,"@jigold I screwed up the stacked PRs so I'll need reapprovals. Sorry :(; ---; 1. Separating the `apk update` from the `apk add` means that; if the apk package repository metadata changes (say, the; URL of some repository changes) and we change our `apk add`; line (say we add a new package), then the `apk add` will; fail (e.g. because it has an out of date URL for the; repository that should contain the new package). The; `apk add --no-cache ...` invocation is essentially the same; as `apk update && apk add ... && rm -rf /path/to/repo/cache`.; When using docker, it is good practice remove unnecessary; files so that they do not get included in the ""image diff""; for that line of the Dockerfile. `apk add --no-cache ...`; succinctly performs exactly what we want. 2. Keeping each package on a separate line and sorting those; lines makes diffs easy-to-read with one line per new package. 3. Because `COPY` moves all the *contents* of a source folder into; the contents of the destination path (creating it if it does not; exist), it seems more clear to say `/batch/batch/`, indicating; that we are moving data into a folder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4828:439,cache,cache,439,https://hail.is,https://github.com/hail-is/hail/pull/4828,4,"['cache', 'perform']","['cache', 'performs']"
Performance,"@jigold OK, so here's the summary of what I learned:. We don't have tabix files for GRCh38 and we only test on small positions. Many large positions without tabix files seems to cause a problem for VEP (and make it slow, unsurprisingly). Fix seems to be to download the *indexed* homo_sapiens cache https://ftp.ensembl.org/pub/release-95/variation/indexed_vep_cache/ and upload that to our QoB VEP bucket. I presume you copied from the data we use in Dataproc? If yes, we should update that to also have tabix files. Also, in Dataproc, we use highmem machines for VEP. We should change _service_vep to also use highmem machines. <details><summary>Listing the tabix files for GRCh38 and GRCh37</summary>. ```; (base) dking@wm28c-761 /tmp % gsutil ls gs://hail-qob-vep-grch38-us-central1/homo_sapiens/95_GRCh38/\*/\*.tbi; CommandException: One or more URLs matched no objects.; ```. ```; (base) dking@wm28c-761 /tmp % gsutil ls gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/\*/\*.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/1/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/10/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/11/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/12/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/13/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/14/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/15/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/16/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/17/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/18/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/19/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/2/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13989#issuecomment-1830868145:293,cache,cache,293,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1830868145,2,['cache'],['cache']
Performance,"@jigold Ok I changed this in a way that will hopefully be more clear. The rules are as follows:. 1. We only use `ci-intermediate` for anonymous images. Images are named auth, batch, etc. even when they are in tests or dev deploys.; 1. Every image draws from the main branch cache tag, named `<DOCKER_PREFIX>/<image_name>:cache`; 2. Every image has an additional cache tag that it draws from and pushes to. For deploys, that is the same as the main branch cache, for PRs, it is `cache-pr-<pr_number>`, for dev deploys it is `cache-<dev_username>`, and for deploys conducted by CIs in a non-default namespace, it is `cache-<namespace-CI-is-in>-deploy`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11999#issuecomment-1177641450:274,cache,cache,274,https://hail.is,https://github.com/hail-is/hail/pull/11999#issuecomment-1177641450,14,['cache'],"['cache', 'cache-pr']"
Performance,"@jigold https://azure.github.io/Storage/docs/application-and-user-data/code-samples/concurrent-uploads-with-versioning. Looks like this is the expected behavior from Azure Blob Storage when concurrently uploading blobs. It seems like *all* blocks are purged when any single upload succeeds. Unfortunately, it doesn't seem possible to distinguish between a truly invalid block list and this transiently invalid block list. I dislike this interface, but it is what we have. It seems to me that the right fix is to deduplicate the file list before uploading. Multiple files uploading to the same target should be an error if they're different source files.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13812#issuecomment-1882088862:84,concurren,concurrent-uploads-with-versioning,84,https://hail.is,https://github.com/hail-is/hail/pull/13812#issuecomment-1882088862,2,['concurren'],"['concurrent-uploads-with-versioning', 'concurrently']"
Performance,"@jigold https://dev.mysql.com/doc/refman/8.0/en/innodb-online-ddl-operations.html#online-ddl-primary-key-operations. Operation | Instant | In Place | Rebuilds Table | Permits Concurrent DML | Only Modifies Metadata; -- | -- | -- | -- | -- | --; Adding a primary key	|No | Yes* | Yes* | Yes | No; Dropping a primary key	|No | No | Yes | No | No; Dropping a primary key and adding another	|No | Yes | Yes | Yes | No. So, replacing the primary key can be done inplace while allowing data manipulation on the table. It sounds like we should use the syntax:; ```; ALTER TABLE tbl_name DROP PRIMARY KEY, ADD PRIMARY KEY (column), ALGORITHM=INPLACE, LOCK=NONE;; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13985#issuecomment-1804259410:175,Concurren,Concurrent,175,https://hail.is,https://github.com/hail-is/hail/pull/13985#issuecomment-1804259410,1,['Concurren'],['Concurrent']
Performance,"@jigold sorry about that CI frigged up, but things look good now, there's an error:; ```; =================================== FAILURES ===================================; ___________________ [doctest] hail.methods.impex.import_bgen ___________________; [gw0] linux -- Python 3.6.6 /home/hail/.conda/envs/hail/bin/python; 067 ; 068 Import a BGEN file as a matrix table with genotype dosage entry field:; 069 ; 070 >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; 071 ... entry_fields=['dosage'],; 072 ... sample_file=""data/example.8bits.sample""); 073 ; 074 Load a single variant from a BGEN file:; 075 ; 076 >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; UNEXPECTED EXCEPTION: TypeError(""import_bgen: parameter 'variants': expected (None or Sequence[hail.utils.struct.Struct] or hail.expr.expressions.typed_expressions.StructExpression or hail.table.Table), found list: [<StructExpression of type struct{locus: locus<GRCh37>, alleles: array<str>}>]"",); Traceback (most recent call last):. File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 487, in check_all; args_.append(checker.check(arg, name, arg_name)). File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 59, in check; raise TypecheckFailure(). hail.typecheck.check.TypecheckFailure. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/home/hail/.conda/envs/hail/lib/python3.6/doctest.py"", line 1330, in __run; compileflags, 1), test.globs). File ""<doctest hail.methods.impex.import_bgen[2]>"", line 4, in <module>. File ""<decorator-gen-904>"", line 2, in import_bgen. File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 559, in wrapper; args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method). File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 513, in check_all; )) from e. TypeError: import_bgen: parameter 'variants': exp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4291#issuecomment-421486590:567,Load,Load,567,https://hail.is,https://github.com/hail-is/hail/pull/4291#issuecomment-421486590,1,['Load'],['Load']
Performance,"@jjfarrell I've been working with some other Broadies that also noted unusual results in Hail's PC-Relate. In this case, we found that about a quarter of variants being used in PC-Relate had terrible HWE p-values. We're tracking this down a bit as a possible explanation for the poor performance of PC-Relate.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3490#issuecomment-388150598:284,perform,performance,284,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-388150598,1,['perform'],['performance']
Performance,"@jjfarrell Thanks for sharing that! This is really interesting information. I'm quite surprised, but the evidence is pointing to there being a PC that largely separates those 8 replicates from the entire remaining dataset (!!!). I'm personally quite surprised that 8 samples out of thousands could pull this off, but that definitely seems to be the issue, given that the PCs from PC-AiR avoid the issue. Thank you so much for hunting this down! It's very valuable information for us. ### Next Steps . So, clearly we need a solution for users that have substantial numbers of related individuals in their source dataset (especially if the pedigrees are unknown). For your _particular_ use case, I can add a blurb to the docs that recommends removing known replicates _before_ PCA and then projecting them using the loadings from PCA. A longer term solution is to simply implement PC-AiR in hail. I skimmed the implementation section of the paper earlier this week and it looks very straightforward. It seems to boil down to using the KING estimator to estimate relatedness, compute PCA on unrelated individuals, project related individuals into unrelated PC space. Finally, we can use pc_relate to improve on our original estimates of relatedness from KING. The timeline for the latter thing is kind of unclear and a bit further out given some other work I need to finish. I'll get the documentation improvement in this week. Is there anything else I can do that would have helped you avoid this issue? Is there anything else you need to resolve the issue now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3490#issuecomment-391739992:814,load,loadings,814,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-391739992,2,['load'],['loadings']
Performance,"@johnc1231 Had this idea for dealing with registry flakiness. I was hesitant at first because really I want buildkit to retry more transient errors, but maybe with its local cache it would be quick?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11666:174,cache,cache,174,https://hail.is,https://github.com/hail-is/hail/pull/11666,1,['cache'],['cache']
Performance,"@konradjk asked for weighted OLS, which is just a transformation of `x` and `y` by `sqrt(w)`. Currently `sqrt` is done `1 + len(x)` per record rather than once because you can't bind inside an aggregate. If that's a bottleneck, I could rework the aggregator to pass the w through to scala and avoid taking sqrt altogether. But for now this simple change at the Python level seems reasonable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4146:216,bottleneck,bottleneck,216,https://hail.is,https://github.com/hail-is/hail/pull/4146,1,['bottleneck'],['bottleneck']
Performance,"@mhebrard. Just to be clear: *after* installing Hail, `/usr/bin/spark-shell --version` now shows `2.12.13` but `pip3 show pyspark` still shows ""Warning: Package(s) not found: pyspark""?. Is `/usr/bin/spark-shell` a symlink? What does it point to? Is `/usr/lib/spark` a symlink? Does it point to the same place? Actually, let's just check a bunch of things:; ```; ls -al /usr/bin/spark-shell; echo $(which spark-shell); ls -al $(which spark-shell); spark-shell --version. ls -al /usr/bin/spark-submit; echo $(which spark-submit); ls -al $(which spark-submit); spark-submit --version. ls -al /usr/bin/spark-class; echo $(which spark-class); ls -al $(which spark-class). echo SPARK_SCALA_VERSION=$SPARK_SCALA_VERSION. echo "">>>>>>>>>> before load-spark-env.sh <<<<<<<<<""; env. load-spark-env.sh. echo "">>>>>>>>>> after load-spark-env.sh <<<<<<<<<""; env. which scala; ls -al $(which scala); cat $(which scala); ```. And one more thing, can you edit `$(which spark-shell)` to add `set -x` then try `spark-shell` and see if there's anything mysterious?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1771296222:738,load,load-spark-env,738,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1771296222,6,['load'],['load-spark-env']
Performance,@patrick-schultz slight performance improvement from doing the indexing without the intermediate struct like you suggested. Should be good to go.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10075#issuecomment-786736301:24,perform,performance,24,https://hail.is,https://github.com/hail-is/hail/pull/10075#issuecomment-786736301,1,['perform'],['performance']
Performance,"@patrick-schultz, sorry, I missed this in-line comment. For the sake of unifying the discussion, I'll reply in PR comment so we can continue in one thread for both discussions (which I think are intimately related). > Right, but this is on the generic key_by path, and this is no longer an obvious optimization in all cases. I think my real question is: what is the new semantics for key_by? If I want to change my key from [A, B] to [B], then probably it will shuffle and choose balanced partitions, keeping roughly the same amount of parallelism, but if the existing partitioner satisfies a somewhat obscure condition that I don't have much control over, it will instead coalesce partitions.; >; > What if we gate this behavior behind a flag on TableKeyBy, and expose a way to opt in to the optimization in python?. This behavior is only accessible when TableKeyBy isSorted=true. If you've used a hidden field (only accessible through a) my newly exposed `_key_by_assert_sorted` or b) writing IR yourself) to assert that your dataset is already in the order of the new key, I'm certain you would *not* want to shuffle. Moreover, switching from `[A, B]` to `[B]` could very well reduce your effective parallelism even after a shuffle because keys are not permitted to be split across partitions. Consider a dataset keyed by `[Locus, Alleles, Gene]` with 10k partitions. If we re-key to `[Gene]` and we only have 1000 genes annotated, we'll lose partitions. In fact, in the 1:1 partition case (the case we're optimizing here) you *must* lose parallelism because each partition contains exactly one value for the key `B`. Indeed, each partition contains only one record at all!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8864#issuecomment-637177344:298,optimiz,optimization,298,https://hail.is,https://github.com/hail-is/hail/pull/8864#issuecomment-637177344,3,['optimiz'],"['optimization', 'optimizing']"
Performance,"@patrick-schultz, you may have tuned out the long thread about all the issues with the first Python Chained Linear Regression and pruning, but this one is now clear of all of that and is purely a Python implementation of Scala's `LinearRegressionChained`: https://github.com/hail-is/hail/blob/main/hail/src/main/scala/is/hail/methods/LinearRegression.scala#L175",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9634#issuecomment-718124825:31,tune,tuned,31,https://hail.is,https://github.com/hail-is/hail/pull/9634#issuecomment-718124825,2,['tune'],['tuned']
Performance,"@rcownie you have to set an ""assignee"" for a PR to be placed in the appropriate queue on https://hail.is/tools/pr-scorecard.html, I've assigned cseed to this one for you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3595#issuecomment-389530695:80,queue,queue,80,https://hail.is,https://github.com/hail-is/hail/pull/3595#issuecomment-389530695,1,['queue'],['queue']
Performance,"@sjparsa I'll take a look! In the future, if you set the ""Assignee"" it shows in my Hail CI queue. You should have access to the Hail CI page now: https://ci.hail.is Your ""queue"" is at https://ci.hail.is/me . It shows you just the PRs you're working on or reviewing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13258#issuecomment-1644475278:91,queue,queue,91,https://hail.is,https://github.com/hail-is/hail/pull/13258#issuecomment-1644475278,2,['queue'],['queue']
Performance,"@tomwhite @jkeebler Added a parquet_genotypes flag to VDS.write:. ```; import hail; hc = hail.HailContext(); vds = hc.import_vcf('sample.vcf'); vds.count(); vds.write('sample.pq.vds', overwrite=True, parquet_genotypes=True); ```. Then in the Spark shell:. ```; scala> val df = spark.read.parquet(""sample.pq.vds/rdd.parquet""); scala> df.printSchema(); root; |-- variant: struct (nullable = true); [variant and annotation fields elided]; |-- gs: array (nullable = true); | |-- element: struct (containsNull = true); | | |-- gt: integer (nullable = true); | | |-- ad: array (nullable = true); | | | |-- element: integer (containsNull = true); | | |-- dp: integer (nullable = true); | | |-- gq: integer (nullable = true); | | |-- px: array (nullable = true); | | | |-- element: integer (containsNull = true); | | |-- fakeRef: boolean (nullable = true); | | |-- isDosage: boolean (nullable = true); ```. I added correctness tests, but no performance testing on the Hail side yet. Note, the `px` field is the `PL` in the case of sequence data and 16-bit fixed-point dosages in the case of array data (See `Gentoype` for more details.) We know if we have dosage or not globally (`VariantMetadata.isDosage`), so I can customize the resulting schema in v2. Finally, I'm seeing `containsNull = true` here, but I set it to `containsNull = false` when I constructed the schema programatically. Spark/Parquet seem to be consistently ignoring my non-missing hints. Have you seen this before? Any idea why it is happening?. From `Genotype.schema`:. ```; def schema: DataType = StructType(Array(; StructField(""gt"", IntegerType),; StructField(""ad"", ArrayType(IntegerType, containsNull = false)),; StructField(""dp"", IntegerType),; StructField(""gq"", IntegerType),; StructField(""px"", ArrayType(IntegerType, containsNull = false)),; StructField(""fakeRef"", BooleanType, nullable = false),; StructField(""isDosage"", BooleanType, nullable = false))); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1421:933,perform,performance,933,https://hail.is,https://github.com/hail-is/hail/pull/1421,1,['perform'],['performance']
Performance,"@tpoterba , can you take a look as well?. Notes:. 1. Azure uses Spark 3.0.2, so I need to build and publish a wheel for Spark 3.0.2.; 2. Azure provides Jupyter Notebooks already.; 3. hail/Makefile (for manual deploys) was missing some changes for deploy.sh, so I updated it.; 4. Azure sets the `AZURE_SPARK` environment variable inside hosted Jupyter Notebooks. 5. In Azure's Jupyter, if you set `extraClassPath` you break the extant classpath (e.g. you cannot load Scala stdlib classes). However, the JARs specified in `spark.jars` are added to the classpath properly, so, in Azure, it suffices to specify `spark.jars`. 6. Azure lacks requester pays, so I require Azure users download, untar, and upload the VEP files to their own bucket. 7. Instead of ""submit"", Azure installs Livy, a Java job-queue system. I have no idea how to set environment variables in Livy and Azure does not set AZURE_SPARK in Livy jobs; therefore, I search for `hdinsight` in the CLASSPATH.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11187:461,load,load,461,https://hail.is,https://github.com/hail-is/hail/pull/11187,2,"['load', 'queue']","['load', 'queue']"
Performance,"@tpoterba . This would some uses a bit more difficult:. (Ex: ExportBGen); ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the construct",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:443,cache,cachedVarid,443,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437,10,"['cache', 'load']","['cachedRsid', 'cachedVarid', 'loadField', 'loadString']"
Performance,"@tpoterba ; Hi Tim , thank you ,I tried the plink1.9, and it works. but when I use the ""importvcf"" command, there are some issues, I took the advice in ""http://www.slf4j.org/codes.html"", added one of the jars in my classpath,but the issue still appeared. (1) command and the info:; root hail $ ./build/install/hail/bin/hail importvcf src/test/resources/sample.vcf.gz -f write -o sample_4.vds; hail: info: running: importvcf src/test/resources/sample.vcf.gz -f; hail: info: running: write -o sample_4.vds; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; hail: info: while importing:; file:/***/hail/src/test/resources/sample.vcf.gz import clean; hail: info: timing:; importvcf: 736.849ms; write: 2.463s. (2) modify the classpath; I add the ""slf4j-nop.jar"" in the CLASSPATH,as follows:; root hail $ echo $CLASSPATH; .:/usr/share/java/slf4j/slf4j-nop.jar:/opt/BioDir/jdk/jdk1.8.0_91/lib/dt.jar:/opt/BioDir/jdk/jdk1.8.0_91/lib/tools.jar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/457#issuecomment-230429438:522,load,load,522,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230429438,1,['load'],['load']
Performance,"@tpoterba @cseed . If y'all can take a look at the docs, tests, and implementation, I want to merge this. I included a log of running on `profile.vcf.bgz` (which has 2500 samples) below, total time is about 3.5 minutes. I expect it to scale roughly like `O(nSamples^2)`. For Kyle's use case this performance is acceptable. Further performance, model, and usability improvements will be separate PRs. ```; dking@wmb16-359 # hail read -i profile.vds ibd -o foo; hail: info: running: read -i profile.vds; [Stage 0:==============> (1 + 3) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o foo; [Stage 8:==================================================> (197 + 4) / 214]hail: info: timing:; read: 3.523s; ibd: 3m33.8s. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/738#issuecomment-250288185:296,perform,performance,296,https://hail.is,https://github.com/hail-is/hail/pull/738#issuecomment-250288185,6,"['load', 'perform']","['load', 'performance']"
Performance,"@tpoterba @cseed. I ran into some issues building images that I addressed as a part of this PR. The main issue was that we needed the `beta` `gcloud` commands, so I added a line to the Dockerfile to load those. I also made a couple changes to the Docker build commands to ensure we at least reuse all images that are available in our GCR. I added `hail-pr-builder` as a cache source in an attempt to take advantage of successful local builds. This, unfortunately, does not allow us to reuse successful layers from a failing build. I noticed that if I build and fail once using `--cache-from`, then I cannot re-use the succeeding layers of the failed build, regardless of whether I supplied `--cache-from`. I also added `:latest` to the `docker images` command because I have a couple tagged images from earlier iterations of this script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4362#issuecomment-424066734:199,load,load,199,https://hail.is,https://github.com/hail-is/hail/pull/4362#issuecomment-424066734,4,"['cache', 'load']","['cache', 'cache-from', 'load']"
Performance,@tpoterba @danking fyi I changed the base branch back to master. I don't think it'll be a bad rebase since the changes are pretty much isolated to LoadMatrix.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2804#issuecomment-361132177:147,Load,LoadMatrix,147,https://hail.is,https://github.com/hail-is/hail/pull/2804#issuecomment-361132177,1,['Load'],['LoadMatrix']
Performance,"@tpoterba Back to you. Addressed comments. Nuked MemoryBlock, moved the array to MemoryBuffer. I think I made the safety tests in MemoryBuffer complete. I changed the array to Array[Byte]. It is working. There might still be an alignment issue (x86 supports unaligned loads but with possible performance penalty) but I'm OK leaving it to be addressed separately. I think ComplexType is good but I agree we can remove representation and just fundamentalType.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2074#issuecomment-321090734:268,load,loads,268,https://hail.is,https://github.com/hail-is/hail/pull/2074#issuecomment-321090734,2,"['load', 'perform']","['loads', 'performance']"
Performance,"@tpoterba FYI, I added MatrixTable._same (I never thought that should be user-visible.) and set min_block_size=0 in the tests so we can load sample.vcf with more than one partition. Let me know if you have objections.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2650#issuecomment-355372066:136,load,load,136,https://hail.is,https://github.com/hail-is/hail/pull/2650#issuecomment-355372066,1,['load'],['load']
Performance,"@tpoterba OK, this is ready for final review. Flags are now duplicated in Python so that service backend can perform all actions without starting a JVM. I have a test that verifies the flag sets, their envvars, and the default values, are all the same. I preserved the randomness behavior. We can address that in a separate PR. The flags now use the Hail `configuration_of` machinery which checks, in order:; - an explicit value (not relevant to flags); - a deprecated environment variable (these are the current flag envvars); - an environment variable with a mechanically derived name (e.g. `HAIL_QUERY_NO_WHOLE_STAGE_CODEGEN`); - the hail configuration file (usually: ""~/.config/hail/config.ini"") under the section ""query"". FWIW, hail configuration files look like this:. ```; (base) dking@wm28c-761 hail % cat ~/.config/hail/config.ini ; [query]; backend = spark; jar_url = gs://hail-query/jars/dking/0wfcw2e6sma9/f4fb19e3d387d6efc6cf0f19b95bec59c95b793a.jar. [batch]; remote_tmpdir = gs://1-day/dktmp/; billing_project = hail. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12423#issuecomment-1411200434:109,perform,perform,109,https://hail.is,https://github.com/hail-is/hail/pull/12423#issuecomment-1411200434,1,['perform'],['perform']
Performance,"@tpoterba did you still want me to check performance on exac metadata? If so, I'll do that tomorrow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2922#issuecomment-368339680:41,perform,performance,41,https://hail.is,https://github.com/hail-is/hail/pull/2922#issuecomment-368339680,1,['perform'],['performance']
Performance,"@tpoterba per your comments have largely left the PStruct varargs constructor in use in the PCanonicalStruct implementation. I think it would be simpler to just use the normal IndexedSeq constructor, and slightly more performant, and if you're interested in that could issue a separate PR. The only change in the implementation of PCanonicalStruct from the master version of PStruct is that I pass through requiredeness in all construction operations. Previously a few, like rename would not do this. Notably this only happened when they used the more complex varargs constructor, and seemed like a bug. The empty constructor was removed because it wasn't necessary.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7733:218,perform,performant,218,https://hail.is,https://github.com/hail-is/hail/pull/7733,1,['perform'],['performant']
Performance,"@tpoterba this builds on your recent PR https://github.com/hail-is/hail/pull/3882. partition counts are computed through Interpret on demand and memoized in the IR. fastPartitionCounts means get the partition counts if you have them (either because the IR know their partition counts, or they were previously computed by counting the RVD partitions). partitionCounts means compute (and memoize) if they aren't available the fast way. I think this is now optimal except that MatrixTable.count potentially runs things twice. We might be able to fix this with a MatrixLet in the case you're calling MatrixTable.count(). Next Table.index/MatrixTable.indexRows should use partitionCounts instead of zipWithIndex because computing the partition counts via the optimizer will potentially be much faster.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3891:754,optimiz,optimizer,754,https://hail.is,https://github.com/hail-is/hail/pull/3891,1,['optimiz'],['optimizer']
Performance,@tpoterba you were in here recently for performance so your eyes are appreciated. I simplified things a bit and localized almost all the parsing logic to `BgenRecord`. The contract for `advance` is that it is always called when `bfis` is pointing at the start of a record _or_ at or past the `end`. Advance will return the position to the start of a record or at or past the `end`. It returns true if there was a new record found. False otherwise. I avoided a couple allocating patterns. The rest of the diffs are copy pastes and some indentation changes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3783:40,perform,performance,40,https://hail.is,https://github.com/hail-is/hail/pull/3783,1,['perform'],['performance']
Performance,"@tpoterba you're referring to the numeric restriction? any numeric expression can go to a distributed tensor, though we don't yet have one and zero dimensional distributed tensors. converting a global to a dict. tensor is bad performance but not invalid.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5589#issuecomment-483794933:226,perform,performance,226,https://hail.is,https://github.com/hail-is/hail/issues/5589#issuecomment-483794933,1,['perform'],['performance']
Performance,@tpoterba: It looks like multiple tests failed on Jenkins. I should have time later this afternoon to look into this.; @cseed: It took 21 minutes for this commit to be tested. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:139. Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testGavinExample FAILED; org.broadinstitute.hail.FatalException at LoadBgenSuite.scala:37. Gradle suite > Gradle test > org.broadinstitute.hail.methods.RenameSamplesSuite.test FAILED; org.broadinstitute.hail.FatalException at RenameSamplesSuite.scala:59,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/291#issuecomment-210432089:232,Load,LoadBgenSuite,232,https://hail.is,https://github.com/hail-is/hail/pull/291#issuecomment-210432089,4,['Load'],['LoadBgenSuite']
Performance,@vladsaveliev I dismissed Jackie's review. That will make it show up in her reviewing queue in our CI system. In the future feel free to dismiss reviews if you've addressed the concerns!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11435#issuecomment-1055892980:86,queue,queue,86,https://hail.is,https://github.com/hail-is/hail/pull/11435#issuecomment-1055892980,1,['queue'],['queue']
Performance,A Hail value is data in a region (you don't know which region). we don't need to pass a region in `loadLength` because the region that the lazily decoded value might decode into isn't going to be that region anyway -- it must be the same region where the lazily decoded value is living.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7826#issuecomment-575730831:99,load,loadLength,99,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575730831,1,['load'],['loadLength']
Performance,"A [daemonset](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) which, given a list of images, constantly tries to keep those images cached on the nodes docker container. Because the preemptible nodes that execute CI builds are tainted, this will only execute on the main nodes and the non-preemptibles, which is the functionality we desire for now because this tool will be used to ensure `notebook` worker images are always cached. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4647:153,cache,cached,153,https://hail.is,https://github.com/hail-is/hail/pull/4647,2,['cache'],['cached']
Performance,"A few observations:; - `Prop` is essentially a named `Gen[Unit]`; - The type of `Prop.forAll` is `Gen[T] -> (U -> Boolean) -> Prop`; - The first two observations suggest: `Prop.forAll` has type: `Gen[T] -> (U -> Boolean) -> Gen[Unit]`; - A `Gen[Unit]` is a bit artificial because the test framework halts execution (presumably with an exception) when a counter-example is found. I instead prefer that `Prop.forAll` has type: `Gen[T] -> (U -> Boolean) -> Gen[Boolean]`; - Now `Prop.forAll` has the same type as `Gen.flatMap[Boolean]`. It seems the difference between `forAll` and `flatMap` is that `forAll` conceptually preforms a product operation while `flatMap` performs a sampling. However, I think they are, in reality, the same operation: sampling. The implementation for `GenProp3` looks like:. ``` scala; for (i <- 0 until p.count) {; val v1 = g1(p); val v2 = g2(p); val v3 = g3(p); val r = f(v1, v2, v3); if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; }; ```. Which could be re-written as:. ``` scala; for (i <- 0 until p.count) {; (for (v1 <- g1; v2 <- g2; v3 <- g3) {; if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; })(p); }; ```. The primary difference between `flatMap` and `forAll` seems to be in error reporting. We can fix this by noting `Gen[T]` is currently a Reader monad on `Parameters`. If we add a ""forAll stack"" to `Parameters` we could implement `forAll` as:. ``` scala; def forAll[T,U](gt: Gen[T], gu: T -> Gen[U]): Gen[U] =; for (t <- gt; u <- local(pushQuantified(t), gu(t)) yield u. def pushQuantified(x: Any)(Parameters p): Paramters =; new Parameters(p.rng, p.size, p.count, (x :: p.quanitifed)); ```. We complete the Reader monad transformation by adding the `local` operation to `class Gen[T]`. ``` scala; // in class Gen; def local(modify: Par",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/400#issuecomment-238901220:664,perform,performs,664,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-238901220,1,['perform'],['performs']
Performance,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7112:985,load,load,985,https://hail.is,https://github.com/hail-is/hail/pull/7112,2,['load'],['load']
Performance,"A number of people have observed the same behavior, but the prometheus team closes it as not a bug. It appears to be related to suddenly adding a very large number of metrics. This started happening when I started the scale tests, which keeps the k8s system operating with about 2000 pods, working through 30k pods over time. It's possible all the added pod information is bringing down prometheus. It appears k8s is restarting prometheus between every ten and twenty minutes. It seems likely that prometheus is spending more than ten minutes to load its database. This is surprising given that the database is a mere 31 GB:. ```; Filesystem Size Used Available Use% Mounted on; overlay 94.3G 46.4G 47.9G 49% /; tmpfs 64.0M 0 64.0M 0% /dev; tmpfs 14.7G 0 14.7G 0% /sys/fs/cgroup; /dev/sdd 49.0G 31.2G 17.8G 64% /prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /dev/termination-log; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/resolv.conf; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hostname; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hosts; shm 64.0M 0 64.0M 0% /dev/shm; tmpfs 14.7G 12.0K 14.7G 0% /var/run/secrets/kubernetes.io/serviceaccount; tmpfs 14.7G 0 14.7G 0% /proc/acpi; tmpfs 64.0M 0 64.0M 0% /proc/kcore; tmpfs 64.0M 0 64.0M 0% /proc/keys; tmpfs 64.0M 0 64.0M 0% /proc/timer_list; tmpfs 14.7G 0 14.7G 0% /proc/scsi; tmpfs 14.7G 0 14.7G 0% /sys/firmware; ```. Which isn't much larger than it was before the scaling tests. It appears to slowly increase the amount of memory it needs:; ```; 1 0 nobody S 30.9g103.7 1 11.5 /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.console.templates=/usr/share/prometheus/consoles --web.external; ```. caping out at 31.5 GB (the disk is 31.2 GB). Now, it is presumably trying to recover. It's been up for about 7 minutes. Still unavailable:; ```; /prometheus $ wget localhost:9090/monitoring/prometheus; Connecti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:546,load,load,546,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['load'],['load']
Performance,"A quick and dirty local test of different performances:; ```; 2017-09-22 18:05:57 Hail: INFO: baldingnichols: generating genotypes for 20 populations, 1000 samples, and 10000 variants...; [Stage 0:> (0 + 10) / 10]2017-09-22 18:05:58 Hail: INFO: Coerced sorted dataset; [Stage 374:==========================================> (3 + 1) / 4]. phi 27.4091310501. 2017-09-22 18:06:24 Hail: INFO: baldingnichols: generating genotypes for 20 populations, 1000 samples, and 10000 variants...; 2017-09-22 18:06:24 Hail: INFO: Coerced sorted dataset; [Stage 735:==========================================> (3 + 1) / 4]. phik2 34.3392460346. 2017-09-22 18:06:58 Hail: INFO: baldingnichols: generating genotypes for 20 populations, 1000 samples, and 10000 variants...; 2017-09-22 18:06:59 Hail: INFO: Coerced sorted dataset; [Stage 1192:==========================================> (3 + 1) / 4]. phik2k0 67.0002729893. 2017-09-22 18:08:05 Hail: INFO: baldingnichols: generating genotypes for 20 populations, 1000 samples, and 10000 variants...; 2017-09-22 18:08:06 Hail: INFO: Coerced sorted dataset; [Stage 1561:==========================================> (3 + 1) / 4]. all 102.006611109. ```. Time is in seconds. The most painful operation is clearly k0, but I bet most people will only want phi, maybe phi and k2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2249#issuecomment-331572653:42,perform,performances,42,https://hail.is,https://github.com/hail-is/hail/pull/2249#issuecomment-331572653,2,['perform'],['performances']
Performance,"A quick look at the source, it isn't loading mathjax.js. No idea why yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1225#issuecomment-271339637:37,load,loading,37,https://hail.is,https://github.com/hail-is/hail/pull/1225#issuecomment-271339637,1,['load'],['loading']
Performance,A read_filter=[] intervals=[/seq/references/HybSelOligos/whole_exome_agilent_1.1_refseq_plus_3_boosters/whole_exome_agilent_1.1_refseq_plus_3_boosters.Homo_sapiens_assembly19.targets.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=50 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.unannotated.vcf) snpEffFile=(RodBinding name=snpEffFile source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snpeff.vcf) dbsnp=(RodBinding name= source=UNBOUND) comp=[] resource=[] out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub annotation=[SnpEff] excludeAnnotation=[] group=[] expression=[] useAllAnnotations=false list=false alwaysAppendDbsnpId=false MendelViolationGeno,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:17544,perform,performanceLog,17544,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['perform'],['performanceLog']
Performance,A root file was modified so it has to test every piece. One of its batch jobs gets stuck in the queue and triggers a timeout. Hopefully this passes since there's no traffic right now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4924#issuecomment-447199915:96,queue,queue,96,https://hail.is,https://github.com/hail-is/hail/pull/4924#issuecomment-447199915,1,['queue'],['queue']
Performance,"A small PR for you, @cseed. 😀. If we are immediately looking at the rows or cols table of; a MatrixTable, then we need not load the entries in either case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3626:123,load,load,123,https://hail.is,https://github.com/hail-is/hail/pull/3626,1,['load'],['load']
Performance,"A summary of major changes:; - The genotype schema has changed from pl to px, where px is an Array[Int] that stores probabilifrom pl to px, where px is an Array[Int] that stores probabilities (phred or linear scaled). g.pl and g.dosage are used for accessing the PLs and/or dosages.; - The VariantMetadata includes information about whether the dataset is dosage data; - Can use indexbgen and importbgen to load BGEN files; - Can use importplink to load PLINK binary files; - Can use importgen to load GEN files and exportgen to export data in GEN format; - Reorganized the VCF import/export scripts to the io folder",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/542:407,load,load,407,https://hail.is,https://github.com/hail-is/hail/pull/542,3,['load'],['load']
Performance,"A summary of major changes:; - The genotype schema has changed from pl to px, where px is an Array[Int] that stores probabilifrom pl to px, where px is an Array[Int] that stores probabilities (phred or linear scaled). g.pl and g.dosage are used for accessing the PLs and/or dosages.; - The VariantMetadata includes information about whether the dataset is dosage data; - Can use indexbgen and importbgen to load BGEN files; - Can use importplink to load PLINK binary files; - Can use importgen to load GEN files and exportgen to export data in GEN format; - Reorganized the VCF import/export scripts to the io folder. Fix indentation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/573:407,load,load,407,https://hail.is,https://github.com/hail-is/hail/pull/573,3,['load'],['load']
Performance,"A user reported this error `concurrent.futures._base.TimeoutError` with no stack trace while copying files in a batch job. There's a comment in `is_transient_error` that we should catch this error, but I did not see it caught in the existing function.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11817:28,concurren,concurrent,28,https://hail.is,https://github.com/hail-is/hail/pull/11817,1,['concurren'],['concurrent']
Performance,"A very uninteresting PR but just to show you more bits of the codebase that are relevant to our earlier `python-dill` bug. We publish a small collection of images in DockerHub that users can use, like `python-dill` and a `hail` image that includes the whole hail pip package. You can use these like `j.image('hailgenetics/hail')`. However, DockerHub sets severe rate limits that would throttle a large batch from pulling those images on N workers for sufficiently large N. So, we mirror these images in our private image registry in GCP / Azure. If a user submits a job with one of these images, we instead pull from our own registry instead. This script does the mirroring from DockerHub -> internal registry. All I did in this PR is refactor the script. I don't honestly know why I used two lists instead of one, there was probably at one point some difference in how these images were handled that got deleted.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12231:385,throttle,throttle,385,https://hail.is,https://github.com/hail-is/hail/pull/12231,1,['throttle'],['throttle']
Performance,"AFAICT, FASTAs live at:; ```; ftp.ensembl.org/pub/release-95/fasta/homo_sapiens/dna_index/; ```; whereas the VEP cache lives at; ```; ftp.ensembl.org/pub/release-95/variation/indexed_vep_cache/homo_sapiens_merged_vep_95_GRCh38.tar.gz; ```; These seem to be two distinct sources of data, so my inclination is to not move the FASTAs inside the cache folder. That seems likely to cause confusion for ourselves in the future. Seems very reasonable to have `gs://bucket/cache/95_GRCh38/homo_sapiens_merged/...` and `gs://bucket/fasta/95_GRCh38/homo_sapiens/...`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14071#issuecomment-1846193943:113,cache,cache,113,https://hail.is,https://github.com/hail-is/hail/pull/14071#issuecomment-1846193943,3,['cache'],['cache']
Performance,"AFAICT, you didn't edit the release.sh script; do I misunderstand what you're worried about?. Can you run the dataproc tests via dev deploy and post the batch links here? I think this should do it. ```; hailctl dev deploy --branch jigold/fix-vep-grch38-cache -s test_dataproc-38 -s test_dataproc-37; ```. If those pass then I'm confident `vep-GRCh38.sh` is correct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14071#issuecomment-1883942042:253,cache,cache,253,https://hail.is,https://github.com/hail-is/hail/pull/14071#issuecomment-1883942042,1,['cache'],['cache']
Performance,AbstractChannelHandlerContext.java:336); 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294); 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357); 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343); 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911); 	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131); 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); 	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); 	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); 	at java.lang.Thread.run(Thread.java:748); 	Error: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.; 	This stopped SparkContext was created at:; 	; 	org.apache.spark.SparkContext.getOrCreate(SparkContext.scala); 	sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	java.lang.reflect.Method.invoke(Method.java:498); 	sparklyr.Invoke.invoke(invoke.scala:139); 	sparklyr.StreamHandler.handleMethodCall(stream.scala:123); 	sparklyr.StreamHandler.read(stream.scala:66); 	sparklyr.BackendHandler.channelRead0(handler.scala:51); 	sparklyr.BackendHandler.channelRead0(handler.scala:4); 	io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105); 	io.netty.channel.AbstractCha,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:6598,concurren,concurrent,6598,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['concurren'],['concurrent']
Performance,"AbstractChannelHandlerContext.java:336); 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294); 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357); 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343); 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911); 	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131); 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); 	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); 	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); 	at java.lang.Thread.run(Thread.java:748); </details>. <details>; <summary>Working hail.log</summary>. ```; 2018-10-09 15:04:33 Hail: INFO: SparkUI: http://10.32.119.167:4040; 2018-10-09 15:04:33 Hail: INFO: Running Hail version devel-17a988f2a628; 2018-10-09 15:04:33 SharedState: INFO: loading hive config file: file:/Users/michafla/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml; 2018-10-09 15:04:33 SharedState: INFO: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 15:04:33 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@16ba3696{/SQL,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletCon",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:13216,concurren,concurrent,13216,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['concurren'],['concurrent']
Performance,AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoogleClientRequest.java:490); 	at com.google.api.services.storage.Storage$Objects$Get.executeMedia(Storage.java:6523); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.load(HttpStorageRpc.java:726); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.lambda$readAllBytes$24(StorageImpl.java:574); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:60); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1476); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:574); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:563); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:288); 	at is.hail.services.package$.retryTransientErrors(package.scala:163); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:286); 	at is.hail.io.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13409:8472,load,load,8472,https://hail.is,https://github.com/hail-is/hail/issues/13409,1,['load'],['load']
Performance,"AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSch",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:214142,queue,queued,214142,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['queue'],['queued']
Performance,"Action items for getting this in:; - Delete Docker's copy of an image when deleting an expanded root filesystem; - Try docker save instead of docker export so docker doesn't create a container. If this doesn't work, delete the container before deleting the image; - Pull every time a job is run for authentication purposes; - Up the reserved image cache size to 30",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10648#issuecomment-879462826:348,cache,cache,348,https://hail.is,https://github.com/hail-is/hail/pull/10648#issuecomment-879462826,1,['cache'],['cache']
Performance,"Actually I get a different error when doing a small reproducible example, but looks related:. ```; ht = hl.utils.range_table(10).annotate_globals(test='yay'); ht2 = hl.utils.range_table(10).annotate_globals(test='yay'); ht.join(ht2).show(); ```; gives:; ```; Hail version: devel-c2508f35dc41; Error summary: HailException: optimization changed type!; before: Table{global:Struct{test:String,test_1:String},key:[idx],row:Struct{idx:Int32}}; after: Table{global:Struct{test:String},key:[idx],row:Struct{idx:Int32}}; Before IR:; ----------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))))); After IR:; ---------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global))))); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4311#issuecomment-420251986:323,optimiz,optimization,323,https://hail.is,https://github.com/hail-is/hail/issues/4311#issuecomment-420251986,1,['optimiz'],['optimization']
Performance,"Actually, maybe we just don't want to push to any cache at all for test deployments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152522572:50,cache,cache,50,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152522572,1,['cache'],['cache']
Performance,Add ForwardLets optimizer pass.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5245:16,optimiz,optimizer,16,https://hail.is,https://github.com/hail-is/hail/pull/5245,1,['optimiz'],['optimizer']
Performance,Add Maximize and MinimizeLets optimizer passes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5041:30,optimiz,optimizer,30,https://hail.is,https://github.com/hail-is/hail/pull/5041,1,['optimiz'],['optimizer']
Performance,Add PValue load/store.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8247:11,load,load,11,https://hail.is,https://github.com/hail-is/hail/pull/8247,1,['load'],['load']
Performance,Add Python type parser. Speed up Python API by ~3x with optimizations,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2979:56,optimiz,optimizations,56,https://hail.is,https://github.com/hail-is/hail/pull/2979,1,['optimiz'],['optimizations']
Performance,Add `ExtractIntervalFilters` optimizer pass.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5979:29,optimiz,optimizer,29,https://hail.is,https://github.com/hail-is/hail/pull/5979,1,['optimiz'],['optimizer']
Performance,"Add a code cache. 50 is was chosen somewhat randomly. Normalize incoming IR so name differences don't case a recompile. Move ApplyIR `conversion` since it shouldn't be involved in equality. Add hashCode to GR because you should always define hashCode, equals as a pair (and it was behaving very strangely without it). @chrisvittal I think this resolves the last of the issues you ran into on Friday.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5426:11,cache,cache,11,https://hail.is,https://github.com/hail-is/hail/pull/5426,1,['cache'],['cache']
Performance,"Add a codegen method `SNDArray.coiterate`, with signature; ```; def coiterate(cb: EmitCodeBuilder, region: Value[Region], indexVars: IndexedSeq[String], arrays: IndexedSeq[(SNDArrayCode, IndexedSeq[Int], String)], body: IndexedSeq[SSettable] => Unit, deepCopy: Boolean): Unit; ```; For example, the index expression `A[i, j] += B[j]` would be written; ```; coiterate(cb, region, Seq(""i"", ""j""), Seq((A, Seq(0, 1), ""A""), (B, Seq(1), ""B"")), {; case Seq(a, b) => cb.assign(a, SCode.add(cb, a, b)); }); ```; This generates a loop nest, with one loop per variable in `indexVars`. The inner loop is `indexVars(0)`, so that column major traversal is when index variables are increasing, as in `(A, Seq(0, 1), ""A"")`. Each index variable iterates over a dimension, with the size of the dimension inferred from its use. In the inner loop, each index variable `i0, i1, ...` has a value; `body` is run, with each of the `SSettable`s bound to an element of the corresponding input in `arrays`. For example, if the first element of `arrays` is `(A, IndexedSeq(1, 3), ""A"")`, then the `SSettable` will be the element of `A` at index `(i1, i3)`. However, it avoids computing the address of each element from the indices from scratch in the inner loop. This was motivated by the need to generate operations on ndarrays in the local whitening aggregator. I replaced a few uses of `forEachIndex` with `coiterate`, which may give a performance boost since it avoids index math in the inner loop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10583:1410,perform,performance,1410,https://hail.is,https://github.com/hail-is/hail/pull/10583,1,['perform'],['performance']
Performance,"Add an extra (currently unused) flag on PartitionNativeWriter and an; extra field on its return type. When the flag is true, we compute the; sum of the byte sizes of every row and return it as the extra field; `partitionByteSize`. We will use this to compute branch factors in LowerDistributedSort and; depending on computational cost, possible turn this on by default to; allow more optimization around deserializing/processing more than one; row at a time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11829:384,optimiz,optimization,384,https://hail.is,https://github.com/hail-is/hail/pull/11829,1,['optimiz'],['optimization']
Performance,"Add components and list of reviewers for those components to the index.html page. It looks like this:. <img width=""667"" alt=""screen shot 2019-02-14 at 11 07 43 am"" src=""https://user-images.githubusercontent.com/1244990/52800012-ea686700-3048-11e9-84bd-33adb86e4820.png"">. The order of the names is random each time the page is loaded. Idea is to take the first name for the component you're reviewing when creating PRs. @danking I removed you from Hail front-end. Now everyone appears on 2-3 components. @akotlar FYI, I ripped out the JSON endpoints since I'm happy with the Flask implementation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5353:327,load,loaded,327,https://hail.is,https://github.com/hail-is/hail/pull/5353,1,['load'],['loaded']
Performance,Add option to import_bgen to load only row fields,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3448:29,load,load,29,https://hail.is,https://github.com/hail-is/hail/issues/3448,1,['load'],['load']
Performance,Add rewrite rule to optimize a common case of Literal array contains,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5660:20,optimiz,optimize,20,https://hail.is,https://github.com/hail-is/hail/pull/5660,1,['optimiz'],['optimize']
Performance,Add selection to PCA to push down optimization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3888:34,optimiz,optimization,34,https://hail.is,https://github.com/hail-is/hail/pull/3888,1,['optimiz'],['optimization']
Performance,Add some optimizations for Ryan's pipeline,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4504:9,optimiz,optimizations,9,https://hail.is,https://github.com/hail-is/hail/pull/4504,1,['optimiz'],['optimizations']
Performance,"Add the following transformations:; ```scala; MakeStruct(""a"" -> GetField(o, ""x""), ...) -> CastRename(SelectFields(o, [""x""..]), newtype); If(IsNA(x), NA(x.typ), x) -> x; ```. The changes to `SStructView` (nee `SSubsetStruct`) were as a result of a bud that prevented subsetting and then renaming to an excluded field, ie `{x, y, z} subset {z} rename {x}`. Now `SStructView` leaves its parent `SType` unmodified and casts loads through the parent to the appropriate `SValue`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14231:420,load,loads,420,https://hail.is,https://github.com/hail-is/hail/pull/14231,1,['load'],['loads']
Performance,Added Chunk Cache to facilitate faster chunk interactions through less use of malloc and free.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10618:12,Cache,Cache,12,https://hail.is,https://github.com/hail-is/hail/pull/10618,1,['Cache'],['Cache']
Performance,"Added a panel for it [here](https://grafana.hail.is/d/TVytxvb7z/batch-driver-performance?orgId=1&var-namespace=dgoldste&from=1646327460490&to=1646328244432). I was kind of surprised to see that we don't have a queue building up, but the back-pressure could be somewhere else. I can also bring up the rate limit now that the DB can handle it",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11472:77,perform,performance,77,https://hail.is,https://github.com/hail-is/hail/pull/11472,2,"['perform', 'queue']","['performance', 'queue']"
Performance,Added additional support for loading (implicit parent data) and exporting (in the context of exportplink) fam files,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/483:29,load,loading,29,https://hail.is,https://github.com/hail-is/hail/pull/483,2,['load'],['loading']
Performance,"Added cache, persist, coalesce, count, exportGenotypes to GDS",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1584:6,cache,cache,6,https://hail.is,https://github.com/hail-is/hail/pull/1584,1,['cache'],['cache']
Performance,Added detailed PCA docs in Pandoc format (Markdown+LaTeX).; Rewrote PCA command and SamplePCA method accordingly.; Passes preliminary testing including handling of missingness. Tests still needed.; Variant sorting in loadings output still needed.; Further issues flagged in the doc.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/202:217,load,loadings,217,https://hail.is,https://github.com/hail-is/hail/pull/202,1,['load'],['loadings']
Performance,Added example of loading dict from file for rename_samples,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2018:17,load,loading,17,https://hail.is,https://github.com/hail-is/hail/pull/2018,1,['load'],['loading']
Performance,Added ir.Optimize,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3330:9,Optimiz,Optimize,9,https://hail.is,https://github.com/hail-is/hail/pull/3330,1,['Optimiz'],['Optimize']
Performance,"Added new load and extract files for additional datasets such as GERP++, variant_summary",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6803:10,load,load,10,https://hail.is,https://github.com/hail-is/hail/pull/6803,1,['load'],['load']
Performance,Added optimizer to prune dead fields,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3470:6,optimiz,optimizer,6,https://hail.is,https://github.com/hail-is/hail/pull/3470,1,['optimiz'],['optimizer']
Performance,"Added optional parameters to the StartChangeSet API to support tagging a resource while making a request to create it.</li>; <li>api-change:<code>rekognition</code>: [<code>botocore</code>] Adding support for ImageProperties feature to detect dominant colors and image brightness, sharpness, and contrast, inclusion and exclusion filters for labels and label categories, new fields to the API response, &quot;aliases&quot; and &quot;categories&quot;</li>; <li>api-change:<code>securityhub</code>: [<code>botocore</code>] Documentation updates for Security Hub</li>; <li>api-change:<code>ssm-incidents</code>: [<code>botocore</code>] RelatedItems now have an ID field which can be used for referencing them else where. Introducing event references in TimelineEvent API and increasing maximum length of &quot;eventData&quot; to 12K characters.</li>; </ul>; <h1>1.26.7</h1>; <ul>; <li>api-change:<code>autoscaling</code>: [<code>botocore</code>] This release adds a new price capacity optimized allocation strategy for Spot Instances to help customers optimize provisioning of Spot Instances via EC2 Auto Scaling, EC2 Fleet, and Spot Fleet. It allocates Spot Instances based on both spare capacity availability and Spot Instance price.</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] This release adds a new price capacity optimized allocation strategy for Spot Instances to help customers optimize provisioning of Spot Instances via EC2 Auto Scaling, EC2 Fleet, and Spot Fleet. It allocates Spot Instances based on both spare capacity availability and Spot Instance price.</li>; <li>api-change:<code>ecs</code>: [<code>botocore</code>] This release adds support for task scale-in protection with updateTaskProtection and getTaskProtection APIs. UpdateTaskProtection API can be used to protect a service managed task from being terminated by scale-in events and getTaskProtection API to get the scale-in protection status of a task.</li>; <li>api-change:<code>es</code>: [<code>botocore</c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12458:1989,optimiz,optimized,1989,https://hail.is,https://github.com/hail-is/hail/pull/12458,4,['optimiz'],"['optimize', 'optimized']"
Performance,"Addressed comments, ready for another look. Change to `Gen.partition` revealed a few other bugs in master, which don't have elegant solutions:; 1. orderedPartitioner either needs an `empty` boolean flag, or a special `EmptyOrderedPartitioner` where `partitioner.nPartitions` is 0. Otherwise, rdd.fullOuterJoin(other) crashes because it looks at partitioner nPartitions instead of rdd nPartitions. This fix involves more logic in VSM.read as well.; 2. OrderedLeftJoinRDD needs to cache partitions and pass them to OrderedRDDIterator. There is no guarantee that `getPartitions` can be called on an executor, and this was crashing in particular for ParallelCollectionRDDs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/717#issuecomment-244904495:479,cache,cache,479,https://hail.is,https://github.com/hail-is/hail/pull/717#issuecomment-244904495,1,['cache'],['cache']
Performance,"Adds BlockMatrix sparsify functions for:; - band matrix; - upper/lower triangle (special case of band); - a collection of rectangles. For diagonal band, I switched GridPartitioner.filterBand to go from lower to upper diagonal index, rather than taking a lower and upper bandwidth. This is more general, e.g. the diagonal itself need not be in the band. Band and triangle zero out elements in partially overlapping blocks by default. Rectangles currently only supports dropping whole blocks. Also adds `export_rectangles` for exporting rectangular regions to TSV in parallel.; I use parameters `path_in` and `path_out`, and switched `BlockMatrix.export` to this convention as well from `input` and `output` to avoid using the reserved word `input`. I have not exposed export methods directly on BlockMatrix for now as it'd be very easy for users to needlessly write and read an already written BlockMatrix. I could add these in a later PR with a warning, or we can wait until we've moved to IR and can optimize read followed by export to export on the file. It'd also be good to add compression options (and float formatting options to `export` and `export_rectanges`). Along the way I fixed NaN checking (due to Double.NaN != Double.NaN) on scalar and vector `/` sparse block ops and added NaN and Infinity checking to scalar and vector `*` sparse ops. Together with `sparsify_row_intervals` in the first sparse matrix PR, this PR exposes all the BlockMatrix functionality needed for big LD applications of Kate/Ran and Jacob/Masa.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3539:1001,optimiz,optimize,1001,https://hail.is,https://github.com/hail-is/hail/pull/3539,1,['optimiz'],['optimize']
Performance,Adds optimization available in lowering process if number of rows per partition from child TableIR is known,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10798:5,optimiz,optimization,5,https://hail.is,https://github.com/hail-is/hail/pull/10798,1,['optimiz'],['optimization']
Performance,"Adds the ability to rerun/retry queries from the nearest `CollectDistributedArray` (`CDA`) IR site. Computes a ""Semantic Hash"" of the top-level IR, which is split and shared among the various constituent `CDA` calls in a query. The `CDA` procedure looks in an execution cache for the results of each partition for that call and uses/updates the cache with successful partition computations. . The nature of the staged- lower and execute model means we don't know how many `CDA` calls that will be generated ahead of time. Thus we treat the ""Semantic Hash"" in a similar way to an RNG state variable and generate a key from the Semantic Hash every time every time we encounter a `CDA`. Since an `ExecutionContext` is re-used for multiple queries in tests while a `SemanticHash` is coupled to one query, the two were kept separate. To minimise the amount of manual state handling, the code was transformed to use a ""State"" monad (abstracted as `MonadLower`). Since the `ExecuteContext` is used nearly everywhere the semantic hash is required, the `ExecuteContext` was absorbed into the `MonadLower` interface. `Lower` is a simple, concrete instance of `MonadLower`, and is used to adapt statements into `MonadLower` expressions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13194:270,cache,cache,270,https://hail.is,https://github.com/hail-is/hail/pull/13194,2,['cache'],['cache']
Performance,"Adds the ability to rerun/retry queries from the nearest `CollectDistributedArray` (`CDA`) `IR` site. Computes a ""Semantic Hash"" of the top-level `IR` which is used to generate a key for the various constituent `CDA` calls in a query. The implementation for CDA, `BackendUtils.collectDArray`, uses that key to look into an the execution cache for the results of each partition for that call and uses/updates the cache with successful partition computations. The nature of the staged- lower and execute model means we don't know how many `CDA` calls that will be generated ahead of time. Thus we treat the ""Semantic Hash"" in a similar way to an RNG state variable and generate a key from the Semantic Hash every time every time we encounter a `CDA`. The execution cache is implemented on-top of a local or remote filesystem (configurable via the `HAIL_CACHE_DIR` environment variable). This defaults to `{tmpdir}/hail/{pip-version}`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12954:337,cache,cache,337,https://hail.is,https://github.com/hail-is/hail/pull/12954,3,['cache'],['cache']
Performance,"Adds the available [GIANT 2018 Exome Array Summary Statistics](https://portals.broadinstitute.org/collaboration/giant/index.php/GIANT_consortium_data_files#2018_Exome_Array_Summary_Statistics) datasets for WHR, BMI, and height as Hail Tables. For reproducibility, I added the notebook I used to generate the tables and schemas. The datasets were small in this case, and I ended up doing things locally on my machine. It didn't seem to make sense to try to redo things to fit into the older extract/load workflow once everything had already been generated.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10235:498,load,load,498,https://hail.is,https://github.com/hail-is/hail/pull/10235,1,['load'],['load']
Performance,"Adds unphased versions to `1000_Genomes_HighCov_autosomes` and `1000_Genomes_HighCov_chrX` datasets. Unphased versions contained multiallelic variants, so these were split with `split_multi_hts`. The `chrY` dataset had not had multiallelic variants split before, so this fixes that as well. The dataset `load` scripts were replaced with a notebook that contains more detail about the process (for both phased and unphased versions).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10607:304,load,load,304,https://hail.is,https://github.com/hail-is/hail/pull/10607,1,['load'],['load']
Performance,"After discussion with @danking, I redesigned this. The PR was failing due to timeouts since we weren't refreshing statuses in `wait` causing everything to loop forever. I split the API up into ""always hit the endpoint"", and ""if you use this function, hit the endpoint at most one time"". I think it's more explicit. We'll want to audit uses of `status()` to ensure that we're using the cached one if possible in several circumstances especially in `hailctl batch` itself, but that can be a future change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9510#issuecomment-703726796:385,cache,cached,385,https://hail.is,https://github.com/hail-is/hail/pull/9510#issuecomment-703726796,1,['cache'],['cached']
Performance,"After do_handshake, [schedule_loop_body](https://github.com/hail-is/hail/blob/2d019337114a972016ad843baabe76814dc8ad10/batch/batch/driver/instance_collection/pool.py#L371) is our biggest offender on the profiler. That in itself is not a bad thing, ideally we want the scheduling loop to be running as much as possible to give us the highest throughput, but the loop should still be efficient. All the queries in `user_runnable_jobs` show up the same on the profiler, since they are the same function call-stack just with different arguments. This should give us finer granularity into what's taking up our time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11358:341,throughput,throughput,341,https://hail.is,https://github.com/hail-is/hail/pull/11358,1,['throughput'],['throughput']
Performance,"After profiling read/count on a 600M ExAC sites KeyTable, cached types/requiredness (requiredness was the big one, calling a virtual function) in arrays. ```; MASTER; In [4]: %timeit -n 10 df.count(); 4.41 s per loop; In [3]: %timeit -n 10 df.count(); 10 loops, best of 3: 4.63 s per loop. avg of two: 4.525 s per loop; ```; ```; THIS BRANCH; In [7]: %timeit -n 10 df.count(); 10 loops, best of 3: 2.94 s per loop; In [3]: %timeit -n 10 df.count(); 10 loops, best of 3: 3.34 s per loop. avg of two: 3.14 s per loop; ```. 44% increase in decoding throughput for KeyTable, can't imagine VDS would lag far behind.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2762:58,cache,cached,58,https://hail.is,https://github.com/hail-is/hail/pull/2762,2,"['cache', 'throughput']","['cached', 'throughput']"
Performance,"Again, [looking at utilization](https://console.cloud.google.com/monitoring/dashboards/builder/982ec67a-4b20-4901-a0aa-af418813a9c4?project=hail-vdc&dashboardBuilderState=%257B%2522editModeEnabled%2522:false%257D&timeDomain=1m&f.rlabel.namespace_name=default&f.umlabel.app=batch-driver), the driver is generally not using its full request. The Python spikes are maybe 35% of utilization and the nginx spikes are maybe 15%. I set the requests to around the top of these spikes. That should ensure that normal daily load is handled without scale up, but during low periods we can pack much better. cc: @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12014:514,load,load,514,https://hail.is,https://github.com/hail-is/hail/pull/12014,1,['load'],['load']
Performance,"Agh, the unsafeRow and UnsafeIndexedSeq optimizations must be wrong, getting out of bounds memory errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9892#issuecomment-770085173:40,optimiz,optimizations,40,https://hail.is,https://github.com/hail-is/hail/pull/9892#issuecomment-770085173,1,['optimiz'],['optimizations']
Performance,"Agh, this isn't good enough. It feels like the right thing might be to lift up the lets, optimize, then push down. This has the added benefit of making it trivial to collapse multiple identical let bindings.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5041#issuecomment-449743945:89,optimiz,optimize,89,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-449743945,1,['optimiz'],['optimize']
Performance,"Agreed, we should understand the practical effects. I suspect this isn't relevant to Hail Query read or write at almost all scales. You'd need to issue more than 1000 write requests in a second or 5000 read requests in a second. How often are folks running clusters with 1000 cores?. I bet the limit is more apparent when doing highly concurrent operations like copying (there's essentially no compute, so I'm issuing requests as fast as I possibly can).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10836#issuecomment-914676654:335,concurren,concurrent,335,https://hail.is,https://github.com/hail-is/hail/pull/10836#issuecomment-914676654,1,['concurren'],['concurrent']
Performance,Ah sorry this wasn't in my CI queue,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12327#issuecomment-1318934632:30,queue,queue,30,https://hail.is,https://github.com/hail-is/hail/pull/12327#issuecomment-1318934632,1,['queue'],['queue']
Performance,"Ah sorry, I forgot to update the original commit message as that's not actually correct. The optimization is that we don't need to iterate through all blobs until we find the exact blob matching our path name. The list operation returns all blobs that start with the prefix of that path. If we see a blob with a different name that is a child of our path `f'{path}/foo`, then we know it's a directory and don't need to iterate anymore (although it could be a file as well, but in Scala we don't currently throw errors on paths that are both files and directories, so we just choose the first we see). If we see a blob that matches the path exactly, then we know it's a file and stop iterating. The only reason we need to iterate through more than one blob is if there's blobs that are like `'{path}zzzzz/foo` or `'{path}szzzzz`. We need to ignore these as they don't provide any information on whether `{path}` is a file or directory. This is where `isChildOf` is needed because we need to make sure the blob is actually a child of the path such as `'{path}/file` and not `{path}zzzzz/file`. As for `getValues` versus `iterateAll`, I just used the one that was in the Java documentation for using the `list` method.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13390#issuecomment-1673853274:93,optimiz,optimization,93,https://hail.is,https://github.com/hail-is/hail/pull/13390#issuecomment-1673853274,1,['optimiz'],['optimization']
Performance,"Ah, I thought I said I was happy to fix the optimized version rather than revert. I do think it can be simplified, though, per my comments. Cotton also had the suggestion of writing this function unstaged using two utility functions:; ```scala; def findFirstNonZeroByte(addr: Long, n: Long): Long; def allPresent(addr: Long, n: Long): Long // uses findFirstNonZeroByte; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7646#issuecomment-561448054:44,optimiz,optimized,44,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561448054,2,['optimiz'],['optimized']
Performance,"Ah, I understand. You're correct that pushing to the `cache` tag disassociates that tag with any image that was previously associated with it. That image is still in the registry though. I thought that was sufficient for the cache to work. (I was wrong, see below!). AFAICT, this change doesn't prevent PR tests from pushing to the `cache` tag. This change just makes the tests run by the CI-in-the-PR not overwrite the cache. Every image build for a PR (which is tested by default namespace CI) will still overwrite the cache tag. AFAICT, this; ```; --import-cache type=registry,ref=gcr.io/hail-vdc/foo; ```; Will use as a cache source the `latest` tag in the `gcr.io/hail-vdc/foo` repository. It is *not* sufficient for an image to be present in the repository and untagged or with a different tag from `latest`. In particular, every push to the `cache` tag prevents us from using other images even though they are in the registry! For example, I pushed two images to `cache`:. ```; (base) # gcloud container images list-tags gcr.io/hail-vdc/dktest; DIGEST TAGS TIMESTAMP; fb551d9bdb94 2022-06-10T14:16:39; afb4c5ad2d7b cache,latest 2022-06-10T14:15:55; ```. If I rebuild [1] the most recently pushed image with; ```; --import-cache type=registry,ref=gcr.io/hail-vdc/dktest:cache; ```; it succeeds in getting the cache. If I rebuild the other image with the same import-cache, it does not see that the (untagged) image is already there! . ---. This all suggests that all our attempts at image caching are failing terribly. Options:; 1. Only deploy builds push to a `:cache` tag, everyone uses that tag.; 2. List all the tags in the repository and include them all as --cache-from's (this doesn't actually work: https://github.com/moby/moby/issues/34715#issuecomment-425933774); 3. Push a tag for each git SHA and then include as --cache-from's the last ten git SHAs on this branch, the most recent common commit with main (i.e. `git merge-base origin/main this-branch`), maybe the current main, and ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800:54,cache,cache,54,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800,18,['cache'],['cache']
Performance,"Ah, right. Eventually we can remove those when we can optimize them away, but I guess we need them now",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3487#issuecomment-387232397:54,optimiz,optimize,54,https://hail.is,https://github.com/hail-is/hail/pull/3487#issuecomment-387232397,1,['optimiz'],['optimize']
Performance,"Allow -b 0.; Added typeclasses for converting to/from JSON.; Implemented for OrderedPartitioner and Locus.; Use in VSM.{read, write}.; Remove OrderedPartitioner.ascending. It wasn't properly implemented. Still try to load serialized .vds/partitioner if partitioner.json.gz; isn't there.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1139:217,load,load,217,https://hail.is,https://github.com/hail-is/hail/pull/1139,1,['load'],['load']
Performance,Allows for local computation of ld matrix when the matrix is relatively small to improve performance.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1937:89,perform,performance,89,https://hail.is,https://github.com/hail-is/hail/pull/1937,1,['perform'],['performance']
Performance,"Alright, so the goal of this PR is to make this work:. ```; HAIL_QUERY_BACKEND=service \; python3 -c 'import hail as hl; hl.utils.range_table(10).write(""gs://foo/bar.t"")`; ```. In particular, the Hail Query JAR is stored in a well known location. If we know the desired git revision, (we do, it should be the same git revision as our wheel), then we can deduce the JAR URL for the user. Moreover, if we're pointed at a namespace, we can still determine the correct location [1]. This PR provides three escape hatches to this behavior:; 1. Specify the `jar_url` parameter to `ServiceBackend`.; 2. Specify the `HAIL_JAR_URL` environment variable.; 3. Specify a JAR url in the user config: `hailctl config set query/jar_url gs://...`. This PR is unfortunately entangled with one other minor change. In `main`, we send the git revision *and* the JAR URL to the driver and the worker as a part of the ""command string"" (the JVMEntryway passes this array of strings to the `main` method of `ServiceBackendSocketAPI2` or `Worker`. After this change, the backend does not necessarily know the git revision. That's fine. The git revision was only ever used as:; 1. a cache key for the JAR cache, and; 2. a unique name for the JAR; Both of these uses are buggy anyway! If you re-use a HAIL_SHA with a different HAIL_JAR_URL and you land on a worker that previously pulled that HAIL_SHA, you'll get the previously pulled JAR, not the newly specified one. Instead I use the JAR URL directly as a cache key and unique name. ---. [1] Odds are good that the developer has not uploaded a JAR to this location, but they can do so by dev deploying `upload_query_jar` or by running `make -C query push-jar`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11643:1157,cache,cache,1157,https://hail.is,https://github.com/hail-is/hail/pull/11643,3,['cache'],['cache']
Performance,Also add my extremely specific conda installation location to `loadconda`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5576:63,load,loadconda,63,https://hail.is,https://github.com/hail-is/hail/pull/5576,1,['load'],['loadconda']
Performance,Also added it to third-party images so we're not pulling from DockerHub. Turns out the \ufeff bug we were seeing is hitting a lot of people and is addressed in this release. I put this up in my namespace to see that I can load it without error (though didn't try copying over dashboards and such).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12300:222,load,load,222,https://hail.is,https://github.com/hail-is/hail/pull/12300,1,['load'],['load']
Performance,"Also fixed getting the logs for a job. - I didn't realize the context manager for asyncio_timeout was throwing an asyncio.TimeoutError. Now, I handle the TimeoutError exception and then throw our own exception after we've uploaded the logs and cleaned up the container. This way it still shows up as an error. - I noticed the logs were being cached when a user gets the logs while the job is running and we don't update the cache until the job is complete. Therefore, I think from the code, if the user asks for the logs part-way through the job running, they wouldn't see any updates until the job is completed. I'm not sure why no-one has complained about this yet, so might be good to double check that this is indeed a bug.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8280:342,cache,cached,342,https://hail.is,https://github.com/hail-is/hail/pull/8280,2,['cache'],"['cache', 'cached']"
Performance,Also rename so that these don't pop up as spurious performance changes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10685:51,perform,performance,51,https://hail.is,https://github.com/hail-is/hail/pull/10685,1,['perform'],['performance']
Performance,"Also sorry for the high latency on a response, apparently Verizon has an outage in my neighborhood until tomorrow morning.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701854993:24,latency,latency,24,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701854993,1,['latency'],['latency']
Performance,"Also, interesting to note: . Home page (with menu bar, dark icon, not logged in): 1.5KB .gz . Logged in: 3.1KB. Bundle size: on order of 100KB. However, 30% of this is the auth0 client library; we can modify it to save space. I've commented on an issue with some light guidance on how to save 5.5KB of that. Effectively 70KB for React + React-Dom + Webpack tooling + all page js compares quite favorably with a jquery-only solution, while being faster than jQuery (https://github.com/jonmiles/react-performance-tests, https://medium.com/thothzocial-engineering/rendering-speed-performance-challenge-with-famous-front-end-framework-196c876a68af), far easier to manage, and with a much large ecosystem (and jquery-only solution would do nothing for universal rendering). The React side should drop this year substantially. They are also interested in writing a compiler to completely remove the vdom, compiling to optimized javascript or maybe web assembly. That may be something interesting to us as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-454608700:499,perform,performance-tests,499,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454608700,6,"['optimiz', 'perform']","['optimized', 'performance-challenge-with-famous-front-end-framework-', 'performance-tests']"
Performance,"Also, just to add the second part of our previous conversation here: we should ultimately (not in this PR) add a second web hook from GitHub that removes all tags for the PR cached images when the PR is closed. I don't think the web hook component is hard, but we'll probably want to take a look at the gcloud and azure apis. It looks like in gcloud we can untag all images matching a regex for the tag:. https://cloud.google.com/sdk/gcloud/reference/container/images/list-tags. ```; gcloud container images list-tags --filter=""'tags:30e5504145'""; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11999#issuecomment-1178042556:174,cache,cached,174,https://hail.is,https://github.com/hail-is/hail/pull/11999#issuecomment-1178042556,1,['cache'],['cached']
Performance,"Although it's possible for entries to be a scalar type like double right now, that won't be possible in 0.2's interface -- they'll always be structs, which follows from lifting all fields out to be top-level. How about something like this:. ```python; vds = hwe_normalize(vds.GT) # adds field gt_norm or something; loadings, pcs, eigenvalues = pca(vds.gt_norm); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2454#issuecomment-348534152:315,load,loadings,315,https://hail.is,https://github.com/hail-is/hail/pull/2454#issuecomment-348534152,1,['load'],['loadings']
Performance,"Although the test is still running now, I am pretty sure the following solution solved the problem. ```; #https://discuss.hail.is/t/i-get-a-negativearraysizeexception-when-loading-a-plink-file/899. export PYSPARK_SUBMIT_ARGS=""--driver-java-options '-XX:hashCode=0' --conf 'spark.executor.extraJavaOptions=-XX:hashCode=0' pyspark-shell"". ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168#issuecomment-1897770087:172,load,loading-a-plink-file,172,https://hail.is,https://github.com/hail-is/hail/issues/14168#issuecomment-1897770087,1,['load'],['loading-a-plink-file']
Performance,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869:993,optimiz,optimize,993,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869,2,['optimiz'],['optimize']
Performance,An additional detail that I fixed in this change is that we no longer need to load the log file into memory on the worker in order to send it to the batch pod (by using `FileResponse`). It would also be nice to do this; - when the worker is uploading the file; - on the batch pod by somehow chaining the StreamResponse from the worker/blob storage to the client. but these felt like bigger changes that would've needed to touch more of the code so I left those out for now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12666#issuecomment-1422737869:78,load,load,78,https://hail.is,https://github.com/hail-is/hail/pull/12666#issuecomment-1422737869,1,['load'],['load']
Performance,"And if you have performance benchmarks already, make a note of them here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1884#issuecomment-304078746:16,perform,performance,16,https://hail.is,https://github.com/hail-is/hail/pull/1884#issuecomment-304078746,1,['perform'],['performance']
Performance,"Another possible worry -- the partitioning stays the same as before `head`, right? Having thousands of empty partitions around is going to have bad performance characteristics for other modules, I think. Why does Spark make this so hard?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/446#issuecomment-228735638:148,perform,performance,148,https://hail.is,https://github.com/hail-is/hail/pull/446#issuecomment-228735638,1,['perform'],['performance']
Performance,"Any page that was not a root page did not render properly because it; pointed to relative locations for the css and js resources. Moreover,; the 404 page incorrectly used a relative load for the navbar. These; changes change the template.xslt to use an absolute; protocol-agnostic (useful for testing locally without TLS/SSL) URL and; change 404.xslt to use an absolute load of the navbar. Currently, 404 is the only page that might appear at a non-root URL.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4936:182,load,load,182,https://hail.is,https://github.com/hail-is/hail/pull/4936,2,['load'],['load']
Performance,Any performance numbers?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5828#issuecomment-481241636:4,perform,performance,4,https://hail.is,https://github.com/hail-is/hail/pull/5828#issuecomment-481241636,1,['perform'],['performance']
Performance,"Anyone using recent versions of the hail-base image to connect to Google Storage has encountered MethodNotFound errors like this:; ```; Activated service account credentials for: [dpalmer-o8fe7@hail-vdc.iam.gserviceaccount.com]; 2020-03-23 20:00:58 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.0; SparkUI available at http://59dd09c396e8:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-2684f0214a05; LOGGING: writing to /hail-20200323-2000-0.2.34-2684f0214a05.log; Traceback (most recent call last):; File ""/scripts/hail_test.py"", line 3, in <module>; bam = hl.import_table('gs://dalio_bipolar_w1_w2_hail_02/analysis/gene_sets/BP_including_BPSCZ_MAC5_gene_set_counts_per_sample.tsv'); File ""</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-1276>"", line 2, in import_table; File ""/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/hail/python/hail/methods/impex.py"", line 1511, in import_table; t = Table(TableRead(tr)); File ""/hail/python/hail/table.py"", line 334, in __init__; self._type = self._tir.typ; File ""/hail/python/hail/ir/base_ir.py"", line 303, in typ; self._compute_type(); File ""/hail/python/hail/ir/table_ir.py"", line 215, in _compute_type; self._type = Env.backend().table_type(self); File ""/hail/python/hail/backend/backend.py"", line 121, in table_type; jir = self._to_java_ir(tir); File ""/hail/python/hail/backend/backend.py"", line 105, in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); File ""/hail/python/hail/ir/base_ir.py"", line 311, in parse; return Env.hail().expr.ir.IRParser.parse_table_ir(code, ref_map, ir_map); File ""/spark-2.4.0-b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8343:286,load,load,286,https://hail.is,https://github.com/hail-is/hail/issues/8343,1,['load'],['load']
Performance,"Apparently distutils isn't a core python package anymore. Here's an example where I tried to build the hail ubuntu image on main https://ci.hail.is/batches/2214931. ```; #13 59.80 update-alternatives: using /usr/bin/python3.7 to provide /usr/bin/python3 (python3) in auto mode; #13 59.87 Traceback (most recent call last):; #13 59.87 File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; #13 59.87 ""__main__"", mod_spec); #13 59.87 File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code; #13 59.87 exec(code, run_globals); #13 59.87 File ""/usr/lib/python3/dist-packages/pip/__main__.py"", line 16, in <module>; #13 59.87 from pip._internal.cli.main import main as _main # isort:skip # noqa; #13 59.87 File ""/usr/lib/python3/dist-packages/pip/_internal/cli/main.py"", line 10, in <module>; #13 59.87 from pip._internal.cli.autocompletion import autocomplete; #13 59.87 File ""/usr/lib/python3/dist-packages/pip/_internal/cli/autocompletion.py"", line 9, in <module>; #13 59.87 from pip._internal.cli.main_parser import create_main_parser; #13 59.87 File ""/usr/lib/python3/dist-packages/pip/_internal/cli/main_parser.py"", line 7, in <module>; #13 59.87 from pip._internal.cli import cmdoptions; #13 59.87 File ""/usr/lib/python3/dist-packages/pip/_internal/cli/cmdoptions.py"", line 19, in <module>; #13 59.87 from distutils.util import strtobool; #13 59.87 ModuleNotFoundError: No module named 'distutils.util'; ```. My guess is that we haven't actually built this layer in a while and we've been riding the cache, but my recent experiments with our docker images might have invalidated it? Seems like it hasn't been a core module for a while so I'm surprised we haven't hit this issue sooner.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11791:1519,cache,cache,1519,https://hail.is,https://github.com/hail-is/hail/pull/11791,1,['cache'],['cache']
Performance,"Applies on top of #4713 . Compared to previous versions, this uses a priority queue rather than a linear search to create the list of returned items. We return tuples so that the caller can construct arrays as appropriate rather than constructing an array of values here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4719:78,queue,queue,78,https://hail.is,https://github.com/hail-is/hail/pull/4719,1,['queue'],['queue']
Performance,Apply the same logic as used in batch to gateway to get better cache behavior.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5807:63,cache,cache,63,https://hail.is,https://github.com/hail-is/hail/pull/5807,1,['cache'],['cache']
Performance,"Are you working with another branch at the same time in dev? If not, I feel like `hailctl dev deploy -b jigold:region-job-queue-fast-ci -s test_batch,test_hailtop_batch -e deploy_batch` would be a faster feedback loop and not take up a spot in the PR queue",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12263#issuecomment-1267535507:122,queue,queue-fast-ci,122,https://hail.is,https://github.com/hail-is/hail/pull/12263#issuecomment-1267535507,4,['queue'],"['queue', 'queue-fast-ci']"
Performance,ArrayAggsToRunAggsPass.transform total 0.129ms self 0.129ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerArrayAggsToRunAggsPass/is.hail.expr.ir.lowering.EmittableIR total 0.013ms self 0.013ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.525ms self 0.004ms children 0.521ms %children 99.28%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.503ms self 0.061ms children 0.443ms %children 87.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.ex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:213840,Optimiz,OptimizePass,213840,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"As discussed in Zulip, `BlockMatrix.write_from_entry_expr` throws OOM error when running on a cluster without `--properties 'core:fs.gs.outputstream.upload.chunk.size=1048576'`. The reason is as documented in [the Hail doc](https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html#hail.linalg.BlockMatrix.from_entry_expr), but I hope either Hail 1) makes this property as default, or 2) throws more appropriate error/warning message. > This method opens n_cols / block_size files concurrently per task. To not blow out memory when the number of columns is very large, limit the Hadoop write buffer size. Error:; ```20/03/03 21:39:46 ERROR org.apache.spark.util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1213,5,main]; java.lang.OutOfMemoryError: GC overhead limit exceeded; 20/03/03 21:39:50 ERROR org.apache.spark.executor.Executor: Exception in task 55.0 in stage 3.0 (TID 1197); java.lang.NullPointerException; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:515); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.create(GoogleCloudStorageFileSystem.java:261); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.createChannel(GoogleHadoopOutputStream.java:82); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.<init>(GoogleHadoopOutputStream.java:74); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.create(GoogleHadoopFileSystemBase.java:797); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:925); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:91); at is.hail.io.fs.HadoopFS.unsafeWriter(HadoopFS.scala:445); at is.hail.linalg.WriteBlocksRDD$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8239:484,concurren,concurrently,484,https://hail.is,https://github.com/hail-is/hail/issues/8239,1,['concurren'],['concurrently']
Performance,"As part of your review, can you try to load this remote state and `terraform plan` and verify nothing has changed?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12991#issuecomment-1540498273:39,load,load,39,https://hail.is,https://github.com/hail-is/hail/pull/12991#issuecomment-1540498273,1,['load'],['load']
Performance,"As per the discussion on Slack, Option should be the default and null is allowed in two places:; 1. At legacy/Java APIs that expect null. (Possibly) null values should be immediately converted to Option with `Option(expr)` and Options should be converted to possibly null values with `o.orNull`.; 2. When the scope of null values is well-defined and significant performance gains justify the added danger. Immediately two instances come to mind that can be improved:; - State has an nullable vds; - Args4j encourages the use of null and needs to go (for this and many reasons). We need to examine every instance of `null` and `= _` in the code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/236:362,perform,performance,362,https://hail.is,https://github.com/hail-is/hail/issues/236,1,['perform'],['performance']
Performance,"As written there are at least two issues:. 1. This function is not idempotent. If we retry the request while the first is still running, the second request will receive nothing from the first query and attempt to insert records into that empty space. 2. Intentionally concurrent updates will try to use the same update id because we do not take a FOR UPDATE lock on the gap after the last row. We should verify that what I have written here takes that gap lock.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14367:268,concurren,concurrent,268,https://hail.is,https://github.com/hail-is/hail/pull/14367,1,['concurren'],['concurrent']
Performance,"Aside from removing these for being unused, I think it makes more sense when we want to bring something of the like back for the combiner just to have an async eval that the client (the combiner) can then use to evaluate many IRs concurrently. I don't much see the utility of the *_many methods. cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12490:230,concurren,concurrently,230,https://hail.is,https://github.com/hail-is/hail/pull/12490,1,['concurren'],['concurrently']
Performance,Assigning Cotton for context since he implemented ArrayAgg back in January. It looks to me like this stuff was never actually getting optimized...,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5765:134,optimiz,optimized,134,https://hail.is,https://github.com/hail-is/hail/pull/5765,1,['optimiz'],['optimized']
Performance,Assigning to Tim so it shows up in his review queue. He has some fixes he wanted to get released that probably need to be added to the change log.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12891#issuecomment-1514980397:46,queue,queue,46,https://hail.is,https://github.com/hail-is/hail/pull/12891#issuecomment-1514980397,1,['queue'],['queue']
Performance,"Associate a region with each ptype, and remove region parameterizations of ptype methods, including load*, is*Missing, allocate. Reference from Zulip:. Alex Kotlar: What is our long term plan for load* methods, and do we need their region parameterizations? I would love to understand the design proposal for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7826:100,load,load,100,https://hail.is,https://github.com/hail-is/hail/issues/7826,7,['load'],"['load', 'loadElement', 'loadField']"
Performance,"At some point (highly likely that it was the Ubuntu 20.04 -> 22.04 upgrade) Batch went from using cgroups v1 to cgroups v2 for setting containers' CPU and memory limits. We mostly don't touch cgroups, the container runtime handles that for us, but we poll the `cgroupfs` for recoding memory usage and CPU utilization. The accounting mechanism changed between v1 and v2 so batch was silently failing to collect these metrics. Deploying these changes into my namespace got me back the following plots (compiling hail):. <img width=""701"" alt=""Screenshot 2023-09-14 at 5 47 24 PM"" src=""https://github.com/hail-is/hail/assets/24440116/0f470e5a-7feb-4b9e-bac6-f560c8366d8e"">. The reason why we fail silently when the file doesn't exist is because we are letting the container runtime manage the cgroup, and there is a race condition between the container exiting + the cgroup getting destroyed and our polling of this file. We could probably do a better job reporting an error, like this though, perhaps logging errors if we fail to read this file more than X number of times.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13626:812,race condition,race condition,812,https://hail.is,https://github.com/hail-is/hail/pull/13626,1,['race condition'],['race condition']
Performance,"At some point we started optimizing the MakeStruct to a SelectFields,; which is great, but not if it breaks important optimizations like the; avoid-a-shuffle rewrite rule!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7073:25,optimiz,optimizing,25,https://hail.is,https://github.com/hail-is/hail/pull/7073,2,['optimiz'],"['optimizations', 'optimizing']"
Performance,"At some point, we should think about how to improve the discoverability and machine-verifiability of our APIs. Currently the tightest type of job log is rather complex. If the performance is OK, I think we should move towards classes that define the request and response types of each call. ---. The main difference is `hail-pip-install` having `retry`. If pip exits with a non-zero exit code, we'll just rerun the command exactly, at most four more times. This mitigates missing retry logic in `pip` itself. For example, [this job](https://ci.hail.is/batches/167314/jobs/27) failed because pip encountered a connection reset while downloading a file. Ideally, pip would simply retry the download. Since we don't control the pip source code, I use a retry that treats all of pip as a black box. There's definitely a failure mode: if you specify a package that doesn't exist, pip will error five times in a row and take ~30 seconds before the retry logic gives up. I'm OK with this because pip should basically never fail for legitimate reasons.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9906#issuecomment-775241278:176,perform,performance,176,https://hail.is,https://github.com/hail-is/hail/pull/9906#issuecomment-775241278,2,['perform'],['performance']
Performance,"Autosummary now documents only the members specified in a module's; <code>__all__</code> attribute if :confval:<code>autosummary_ignore_module_all</code> is set to; <code>False</code>. The default behaviour is unchanged. Autogen also now supports; this behavior with the <code>--respect-module-all</code> switch.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9555"">#9555</a>: autosummary: Improve error messages on failure to load target object</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9800"">#9800</a>: extlinks: Emit warning if a hardcoded link is replaceable; by an extlink, suggesting a replacement.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9961"">#9961</a>: html: Support nested <!-- raw HTML omitted --> HTML elements in other HTML builders</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10013"">#10013</a>: html: Allow to change the loading method of JS via <code>loading_method</code>; parameter for :meth:<code>Sphinx.add_js_file()</code></li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9551"">#9551</a>: html search: &quot;Hide Search Matches&quot; link removes &quot;highlight&quot; parameter; from URL</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9815"">#9815</a>: html theme: Wrap sidebar components in div to allow customizing their; layout via CSS</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9827"">#9827</a>: i18n: Sort items in glossary by translated terms</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9899"">#9899</a>: py domain: Allows to specify cross-reference specifier (<code>.</code> and; <code>~</code>) as <code>:type:</code> option</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9894"">#9894</a>: linkcheck: add option <code>linkcheck_excl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11522:2453,load,loading,2453,https://hail.is,https://github.com/hail-is/hail/pull/11522,2,['load'],['loading']
Performance,Avoid query-service races when looking at the cache.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10309:46,cache,cache,46,https://hail.is,https://github.com/hail-is/hail/pull/10309,1,['cache'],['cache']
Performance,Azure seems to have pervasively higher latency than GCP. This should reduce the amount of warning logs we receive.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12473:39,latency,latency,39,https://hail.is,https://github.com/hail-is/hail/pull/12473,1,['latency'],['latency']
Performance,"B); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Usin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3672,cache,cached,3672,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,B); Collecting frozenlist==1.4.0; Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB); Collecting google-api-core==2.11.1; Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (8,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:35862,cache,cached,35862,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"B); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB); Collecting typer==0.9.0; Using cached typer-0.9.0-py3-none-any.whl (45 kB); Collecting typing-extensions==4.7.1; Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB); Collecting tzdata==2023.3; Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB); Collecting urllib3==1.26.16; Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB); Collecting uvloop==0.17.0; Using cached uvloop-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB); Collecting wrapt==1.15.0; Using cached wrapt-1.15.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB); Collecting xyzservices==2023.7.0; Using cached xyzservices-2023.7.0-py3-none-any.whl (56 kB); Collecting yarl==1.9.2; Using cached yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB); Building wheels for collected packages: avro; Building wheel for avro (pyproject.toml): started; Building wheel for avro (pyproject.toml): finished with status 'done'; Created wheel for avro: filename=avro-1.11.2-py2.py3-none-any.whl size=119738 sha256=d7f238f86de270b449b018590930a06270766887328bdb51066eccff2cd696a6; Stored in directory: /home/hadoop/.cache/pip/wheels/e3/a2/1e/5c1be0865f4170a89de34e0a798f32f674a7eaf63a93272c7f; Successfully built avro; Installing collected packages: sortedcontainers, pytz, py4j, commonmark, azure-common, xyzservices, wrapt, uvloop, urllib3, tzdata, typing-extensions, tornado, tenacity, tabulate, six, regex, pyyaml, python-json-logger, pyjwt, pygments, pycparser, pyasn1, protobuf, portalocker, pillow, packaging, orjs; on, oauthlib, numpy, nest-asyncio, multidict, markupsafe, jmespath, idna, humanize, google-crc32c, frozenlist, dill, decorator, charset-normalizer, certifi, ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:41111,cache,cached,41111,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,BGEN performance improvements,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1917:5,perform,performance,5,https://hail.is,https://github.com/hail-is/hail/pull/1917,1,['perform'],['performance']
Performance,BGEN performance on par with 0.1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4020:5,perform,performance,5,https://hail.is,https://github.com/hail-is/hail/issues/4020,1,['perform'],['performance']
Performance,BN is now implemented in Python. Improving performance will now involve improving general infrastructure for the IR / execution engine,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2314#issuecomment-422362729:43,perform,performance,43,https://hail.is,https://github.com/hail-is/hail/issues/2314#issuecomment-422362729,1,['perform'],['performance']
Performance,"Back to you. Couldn't resolve the loadvcf vcf report stuff, but everything else should be good to go",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/233#issuecomment-204477417:34,load,loadvcf,34,https://hail.is,https://github.com/hail-is/hail/pull/233#issuecomment-204477417,1,['load'],['loadvcf']
Performance,"BackendSocketAPI2.$anonfun$parseInputToCommandThunk$3(ServiceBackend.scala:650); at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:822); at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:447); at is.hail.backend.service.Main$.main(Main.scala:15); at is.hail.backend.service.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.124-87398e1b514e; Error summary: HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht; ```; </details>. The code is simple and clearly is running against a path that does not already exist:; ```; if not hl.hadoop_exists(get_aou_util_path('mt_sample_qc')):; print('Run sample qc MT.....'); mt = hl.read_matrix_table(ACAF_MT_PATH); mt = mt.filter_rows(mt.locus.in_autosome()); # mt = mt.filter_rows(mt.locus.contig == 'chr1'); ht = hl.sample_qc(mt, name='mt_sample_qc'); ht.write(get_aou_util_path('mt_sample_qc'), overwrite=args.overwrite); ```. Job log: https://batch.hail.is/batches/8058522/jobs/171029. <details>; <summary>The last TableIR logged</summary>. ```; 2023-10-13 02:14:44.213 : INFO: after optimize: darrayLowerer, after LowerAndExecute",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13809:6762,concurren,concurrent,6762,https://hail.is,https://github.com/hail-is/hail/issues/13809,1,['concurren'],['concurrent']
Performance,"Based off of discussion in #11907, this aims to avoid separate PRs from clobbering the image cache tag and sets up PR-specific cache tags per image. Note that using `ci-intermediate` was also detrimental to the image cache and I don't think different images sharing layers under the common name holds much value. I think we should ultimately get rid of `ci-intermediate` entirely and explicitly name our images so that they don't ruin each other's caches. I tested this in my namespace's CI. Here's the image build times from two consecutive dev deploys:. Before | After; :-------------------------:|:-------------------------:; ![Screen Shot 2022-07-05 at 6 14 36 PM](https://user-images.githubusercontent.com/24440116/177426924-5d5ade8c-0cee-4a0e-b477-2156d4e01e78.png) | ![Screen Shot 2022-07-05 at 6 14 45 PM](https://user-images.githubusercontent.com/24440116/177426882-c0029760-42ae-471d-b48c-daa0eadea448.png). I don't personally see the need for adding more SHAs to the cache as mentioned in #11907, a per-PR cache seems like exactly what you would want. The one drawback I can think of here is that a deploy won't make use of the cache from the PR that resulted in the commit to main. I believe the commit SHAs would be different because we squash so other than devising a way to trace the commit back to the PR I don't see how we can easily connect the two. Still, I feel like it's not a big deal since it will still use the previously deployed commit as a cache, so most deploys will still be very fast and no one's waiting on deploys in the same way as we wait on PRs and dev deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11999:93,cache,cache,93,https://hail.is,https://github.com/hail-is/hail/pull/11999,8,['cache'],"['cache', 'caches']"
Performance,"Based on #9076. This PR caches input files at `/cache` on the worker. I do not support wildcard characters in filenames. I had to change the Flock implementation to only lock the directories and not a file if a file name is given. This is because we don't know if a user means a file or directory a priori. For example, `gs://jigold/test`. Is that a directory test or a file test? I figured more coarse-grained locking was fine.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9095:24,cache,caches,24,https://hail.is,https://github.com/hail-is/hail/pull/9095,2,['cache'],"['cache', 'caches']"
Performance,"Basic abstraction of element-wise operations between BlockMatrix objects and ""small"" values (scalars, NDArrays). Major additions include:; - Addition of element type to BlockMatrixType; - BlockMatrixMap2 which applies a binary op to two BlockMatrixIR nodes of the same shape. This sets up groundwork for element-wise operations, though in the long term can be phased out and replaced with an OuterProduct/Map.; - BlockMatrixBroadcast which wraps a BlockMatrixIR to give it a new shape. Broadcasts are never actually realized and are matched against in the execute of BlockMatrixMap2 so the appropriate BlockMatrix method can be called to perform the broadcast operation. Since the only supported values that can be broadcast are scalars, row vectors and column vectors, there is a corresponding enumeration to indicate the direction of broadcast. This can be generalized to an arbitrary index expression when higher-dimensional broadcasts/transformations are supported.; - ValueToBlockMatrix node that takes any IR to a BlockMatrixIR. Currently only supports a F64 and MakeArray. As this could generalize to any IR node that reduces to a tensor, all BlockMatrixType fields are not expected to be derived values of the interpreted IR and left as inputs to the ValueToBlockMatrix IR node. ### Workarounds; - MakeArrays are used to wrap vector values that came from NDArrays. Since ValueToBlockMatrix requires a shape and the BlockMatrix interface can only construct matrices given a 1-D array of data, I just flatten 2D arrays on the python side.; - To satisfy the BlockMatrix interface, some row/col vectors need to be interpreted to arrays and some need to be further constructed into BlockMatrix instances. ### Remaining tasks; - Implement a BlockMatrixMap for Unary ops; - Use IR instead of ApplyBinaryPrimOp for BlockMatrixMap2; - Update Typecheck for IR in Map nodes and children IR nodes in ValueToBlockMatrix; - Test methods to check evaluation of Apply*Op on BlockMatrices",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5220:638,perform,perform,638,https://hail.is,https://github.com/hail-is/hail/pull/5220,1,['perform'],['perform']
Performance,"Batches that have a `pr` attribute now link back to the corresponding PR page. Also fixed a latent bug where we were only loading batches for a PR based on the PR number, so if we had multiple watched branches you could display batches for PRs of the same number across branches.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10295:122,load,loading,122,https://hail.is,https://github.com/hail-is/hail/pull/10295,1,['load'],['loading']
Performance,BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:78); at org.apache.spark.network.client.TransportResponseHandler.failOutstandingRequests(TransportResponseHandler.java:117); at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:108); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.net,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:219281,concurren,concurrent,219281,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"Because of the unpredictable way that git clone might realize the requirements files, I removed the pinned-requirements file as a dependency of the changed targets. In both cases, regenerating that file (either in CI as part of the deploy.yaml target or on a cluster for `install-on-cluster`) could cause a dataproc cluster running with untested dependency versions even if the requirements.txt files are unchanged. I do, however, require that the pinned-requirements files be compatible using the same check we do in CI. I performed the following manual testing:; 1. Creating a dataproc cluster through `hailctl dataproc start`; 2. ssh'ing into said cluster, cloning this branch and running `make -C hail install-on-cluster` to completion; 3. Updating the requirements.txt file to something incompatible and successfully installing on cluster again with updated pinned requirements. However, I'm not sure I'm actually doing this right. I checked that in step 2 I was *not* regenerating any pinned-requirments files, but in step 3 make updated the pinned requirements without me telling it to, I'm guessing because of the wheel's dependence on `PY_FILES` and I changed the source under hail/python. So I don't entirely understand why I have this desirable result.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12510#issuecomment-1338482256:524,perform,performed,524,https://hail.is,https://github.com/hail-is/hail/pull/12510#issuecomment-1338482256,1,['perform'],['performed']
Performance,"Before open batches, the `n_jobs` of a batch was a constant known before any jobs were added. Moreover, we did not start scheduling jobs until all the jobs were added to the database. Therefore, it was always safe to assume that the final ""bunch"" of jobs in the database was the last ""bunch"" ergo it spanned from its `start_job_id` to the job with id `n_jobs` (nb: job ids are 1-indexed). When open batches were added, the `n_jobs` became a mutable value. Moreover, `n_jobs` includes jobs in bunches *which have not yet been added to the database*. In particular, suppose two clients are each submitting a bunch of size 10. Each client independently ""reserves"" 10 job slots by atomically incrementing `n_jobs` by ten. `n_jobs` is now 20. Further suppose that the first bunch is added to the database and begins scheduling before the second bunch is added to the database. In this case, when calculating the size of this bunch (for use in the bunch cache, and *only* in the bunch cache), we see that this is the last (and only) bunch in the database and assume that `n_jobs` is the last job id in this bunch. This is incorrect because `n_jobs` includes the not-yet-visible second bunch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13399:948,cache,cache,948,https://hail.is,https://github.com/hail-is/hail/pull/13399,2,['cache'],['cache']
Performance,"Ben submitted a pipeline where the first 85% of jobs run in us-central1 while the last 15% run in us-east1. The autoscaler only looks at the head of the job queue and then sorts the result set to figure out the regions to spin up instances in. The scheduler looks at the entire job queue and then sorts the result set to figure out the regions to spin up instances in. The sort order placed us-east1 before us-central1. Concretely, the autoscaler is spinning up instances in us-central1 only while the scheduler is trying to schedule jobs in us-east1. This PR attempts to solve this problem by placing bounds on which jobs the scheduler can look at based on the records the autoscaler actually considered. This is a bit of a hack and I'm worried about the performance implications. On Ben's pipeline with 100K jobs, this will add 0.3 seconds per user considered by the autoscaler. However, the scheduler query got 5x faster with the bounds in place (0.05 seconds vs 0.25 seconds). ```; mysql> EXPLAIN SELECT jobs.job_id, spec, cores_mcpu, regions_bits_rep, time_ready; -> FROM jobs FORCE INDEX(jobs_batch_id_state_always_run_cancelled); -> LEFT JOIN jobs_telemetry ON jobs.batch_id = jobs_telemetry.batch_id AND jobs.job_id = jobs_telemetry.job_id; -> WHERE jobs.batch_id = BATCH_ID AND (jobs.batch_id < BATCH_ID OR (jobs.batch_id = BATCH_ID AND jobs.job_id <= 15000)) AND inst_coll = ""standard"" AND jobs.state = 'Ready' AND always_run = 0 AND cancelled = 0; -> ORDER BY jobs.batch_id, inst_coll, state, always_run, -n_regions DESC, regions_bits_rep, jobs.job_id; -> LIMIT 300;; +----+-------------+----------------+------------+--------+------------------------------------------+------------------------------------------+---------+-------------------------+-------+----------+----------------------------------------------------+; | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |; +----+-------------+----------------+------------+---",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13268:157,queue,queue,157,https://hail.is,https://github.com/hail-is/hail/pull/13268,3,"['perform', 'queue']","['performance', 'queue']"
Performance,Ben's disks still took over 10 minutes to be attached when the system was under load.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10646#issuecomment-879389079:80,load,load,80,https://hail.is,https://github.com/hail-is/hail/pull/10646#issuecomment-879389079,1,['load'],['load']
Performance,"Benchmarks show that this doesn't change performance, with the exception of making linear_regression_rows_nd about 4x slower. the unreliability of ndarray performance is being solved separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10490#issuecomment-846021565:41,perform,performance,41,https://hail.is,https://github.com/hail-is/hail/pull/10490#issuecomment-846021565,2,['perform'],['performance']
Performance,"Better place to post things like this would be discuss.hail.is (because it's probably a configuration issue with your cluster and not a bug in hail). I'd guess you don't have BLAS installed on your cluster. If you check the hail.log file, do you have lines like. ```; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS; ```. ?. See here: https://hail.is/docs/0.2/getting_started.html#common-installation-issues",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7008#issuecomment-529008947:278,load,load,278,https://hail.is,https://github.com/hail-is/hail/issues/7008#issuecomment-529008947,2,['load'],['load']
Performance,Big aggregator performance gainz,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/954:15,perform,performance,15,https://hail.is,https://github.com/hail-is/hail/pull/954,1,['perform'],['performance']
Performance,"Bigger than I expected, but:; 1. Re-enable the FS tests and create a Gradle target for them so they can be run locally.; 2. Allow the FS tests to be easily used locally by not hardcoding a particular key file path.; 3. Skip GoogleStorageFSSuite when `CLOUD` is not `gcp`; 4. Remove irrelevant env vars from non-FS Scala tests.; 5. Eliminate the ""hail_repl"" image and deployment which was scoped dev anyway and never used.; 6. Add hail_pip_installed_image which can be used to execute `hailtop.aiotools.copy`.; 7. Use copy in two places in build.yaml.; 8. Add a command line argument for configuring the number of concurrent transfers which sets an upper bound on the number of open source files (and, additionally, open destination files). On my MacBook, I can't seem to open 100 local files simultaneously. I set the default low enough that local use should work by default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11089:613,concurren,concurrent,613,https://hail.is,https://github.com/hail-is/hail/pull/11089,1,['concurren'],['concurrent']
Performance,"Bit packing is not limited to biallelic datasets. In the biallelic case, we can represent the three states as:. | state | bits |; | ----- | -----|; | homRef | `0`/`0` |; | het | `0`/`1` |; | homVar | `1`/`1` |. We can play the same trick with two bits per allele:. | state | bits |; | --- | --- |; | A/A | `00`/`00` |; | A/G | `00`/`01` |; | A/C | `00`/`10` |; | A/T | `00`/`11` |; | | |; | G/G | `01`/`01` |; | G/C | `01`/`10` |; | G/T | `01`/`11` |; | | |; | C/C | `10`/`10` |; | C/T | `10`/`11` |; | | |; | T/T | `11`/`11` |; | | |; | NA | `01`/`00` or `10`/`00` or `11`/`00` |. We can't play the same trick with three bits per allele (six per genotype) because the SSE/AVX registers cannot perform the shift appropriately for non-byte-aligned data. So the next step would be four bits per allele (eight per genotype) which allows 16 total alleles. For each bit-count, the algorithm is exactly the same, but with shifts changed. We should write a little DSL for specifying these bit operations which automatically generates C code for various allele counts.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1082:694,perform,perform,694,https://hail.is,https://github.com/hail-is/hail/issues/1082,1,['perform'],['perform']
Performance,"Both of these were noted by Bernick as a part of his security review. 1. `local_infile`: if this is on, a client could in theory read any file on the instance by loading it into a table.; 2. `skip_show_database`: this disables `SHOW DATABASES` by default; we can still grant certain users (e.g. the admin-pod) the `SHOW DATABASES` privilege. A bit of security by obscurity, IMO, but it does not bother me much. I can always see the list of databases via the GCP console.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12835:162,load,loading,162,https://hail.is,https://github.com/hail-is/hail/pull/12835,1,['load'],['loading']
Performance,"Brought up by TJ in a recent conversation. He wants to not use the browser to work on Jupyter notebooks for performance / IDE convenience reasons. From a brief look, there appear to be two issues in getting this to work. First, VS Code will need to be started with proxy flags. As its runtime is Electron, all Chromium flags will work, so could almost specify HAILCTL_CHROME=code hailctl connect ... , but this doesn't directly work because VS Code also needs a workspace (so the cli invocation will need to be slightly different). Second, password-less may not work without `disable_xsrf_check`. Relevant issue: https://github.com/microsoft/vscode-python/issues/7137. There may be ways to hijack a proxied localhost connection, so unless we fully understand those issues, if disable_xsrf_check is necessary to enable password-less, it would be better to generate a token.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9067:108,perform,performance,108,https://hail.is,https://github.com/hail-is/hail/issues/9067,1,['perform'],['performance']
Performance,"Building on #4487, removes `IntervalTree`, replacing its remaining uses (mostly in `RVDPartitioner`) with an optimized generic binary search implementation. The key observation is that standard binary search works on an array of intervals such that all left endpoints are non-decreasing, as are all left endpoints, using in the binary search the non-standard ordering on intervals that compares I < J if I is completely below J (without overlap). In this ordering ""equality"" (defined as neither less than nor greater) means overlapping. This should speed up all partitioner queries.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4511:109,optimiz,optimized,109,https://hail.is,https://github.com/hail-is/hail/pull/4511,1,['optimiz'],['optimized']
Performance,"Builds on #2236. . Implements a LoadMatrix function that loads a VariantSampleMatrix from TSV of [rowID, ints... ] with a header containing column IDs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2246:32,Load,LoadMatrix,32,https://hail.is,https://github.com/hail-is/hail/pull/2246,2,"['Load', 'load']","['LoadMatrix', 'loads']"
Performance,"Builds on @jigold's PR: https://github.com/hail-is/hail/pull/4582. First pipeline executed via API!. Start the server:. ```; $ hail hail/python/hail-apiserver/hail-apiserver.py; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2-cc8ca5cfae35; * Serving Flask app ""hail-apiserver"" (lazy loading); * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit); ```. Run a hail pipeline:. ```; $ hail; >>> import hail as hl; >>> hl.init(_backend=hl.backend.ServiceBackend('localhost', 5000)); Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2-cc8ca5cfae35; >>> t = hl.Table.parallelize([hl.struct(a=1), hl.struct(a=5)]); >>> t.count(); 2; >>> ; ```. and on the server logs:. ```; 2018-10-19 22:36:18 Hail: INFO: execute: (TableCount (TableParallelize None (MakeArray None (MakeStruct (a (I32 1))) (MakeStruct (a (I32 5)))))); 2018-10-19 22:36:18 Hail: INFO: result: {'type': 'int64', 'value': '2'}; 127.0.0.1 - - [19/Oct/2018 22:36:18] ""POST /execute HTTP/1.1"" 200 -; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4589:322,load,loading,322,https://hail.is,https://github.com/hail-is/hail/pull/4589,1,['load'],['loading']
Performance,"Builds on: https://github.com/hail-is/hail/pull/2074. Added optimized unsafe row add to region value builder. Tests are faster than toward_fullgeneric_4, 0.1 (8m2s vs 9m18s, 0.1: 8m20s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2081:60,optimiz,optimized,60,https://hail.is,https://github.com/hail-is/hail/pull/2081,1,['optimiz'],['optimized']
Performance,"Builds on: https://github.com/hail-is/hail/pull/2228. It is smaller, cleaner and more self-contained, but I could still break it into more pieces if needed. Some remarks:; - I removed the BroadcastTypeTree nonsense. This will kill KeyTable joins but we can fix that later.; - I sample keys as part of collecting the partition key info.; - I left off two features off the key ranges sampler vs OrderedRDD: I don't resample large partitions and I don't use weights to calculate the ranges.; - I included the minimal LoadVCF interface changes. VCF parser will come as a separate PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2236:514,Load,LoadVCF,514,https://hail.is,https://github.com/hail-is/hail/pull/2236,1,['Load'],['LoadVCF']
Performance,"Builds on: https://github.com/hail-is/hail/pull/2299. History of BTT is somewhat obscure. Now we just serialize the type in Unsafe{Row, IndexedSeq}. And some small additional improvements along the way:. Renamed UnsafeIndexedSeqAnnotation => UnsafeIndexedSeq.; Cache specialized Array types in UnsafeRow to avoid allocation.; Fixed ordering disagreement for Variant (added regression test).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2301:261,Cache,Cache,261,https://hail.is,https://github.com/hail-is/hail/pull/2301,1,['Cache'],['Cache']
Performance,Builds on: https://github.com/hail-is/hail/pull/2563. Didn't stack (yet). Had to remove the requiredness on gs since I can't produce it in the IR (minor). Fixed two bugs along the way: wrong TypeInfo in ArrayMap and loadPrimitive wasn't loading the array address.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2564:216,load,loadPrimitive,216,https://hail.is,https://github.com/hail-is/hail/pull/2564,2,['load'],"['loadPrimitive', 'loading']"
Performance,"Builds on: https://github.com/hail-is/hail/pull/5004. Convert all operations in table.py to IR (if possible). Here are the things that remain in order to get rid of Table._jt in table.py. Rewrite in Python:; - expandTypes; - flatten; - collectJSON: use aggregate/collect (@tpoterba, do you feel this will be significantly slower now?); - showString: rewrite in Python in terms of collect. Add IR:; - intervalJoin; - same; - groupByKey. Should only work with SparkBackend:; - toDF. Hmm:; - forceCount: remove? add force option to TableCount that disables optimization?; - nPartitions; - filterPartitions; - persist, unpersist",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5015:554,optimiz,optimization,554,https://hail.is,https://github.com/hail-is/hail/pull/5015,1,['optimiz'],['optimization']
Performance,"Bumps [aiohttp](https://github.com/aio-libs/aiohttp) from 3.9.3 to 3.9.4.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/releases"">aiohttp's releases</a>.</em></p>; <blockquote>; <h2>3.9.4</h2>; <h2>Bug fixes</h2>; <ul>; <li>; <p>The asynchronous internals now set the underlying causes; when assigning exceptions to the future objects; -- by :user:<code>webknjaz</code>.</p>; <p><em>Related issues and pull requests on GitHub:</em>; <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8089"">#8089</a>.</p>; </li>; <li>; <p>Treated values of <code>Accept-Encoding</code> header as case-insensitive when checking; for gzip files -- by :user:<code>steverep</code>.</p>; <p><em>Related issues and pull requests on GitHub:</em>; <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8104"">#8104</a>.</p>; </li>; <li>; <p>Improved the DNS resolution performance on cache hit -- by :user:<code>bdraco</code>.</p>; <p>This is achieved by avoiding an :mod:<code>asyncio</code> task creation in this case.</p>; <p><em>Related issues and pull requests on GitHub:</em>; <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8163"">#8163</a>.</p>; </li>; <li>; <p>Changed the type annotations to allow <code>dict</code> on :meth:<code>aiohttp.MultipartWriter.append</code>,; :meth:<code>aiohttp.MultipartWriter.append_json</code> and; :meth:<code>aiohttp.MultipartWriter.append_form</code> -- by :user:<code>cakemanny</code></p>; <p><em>Related issues and pull requests on GitHub:</em>; <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7741"">#7741</a>.</p>; </li>; <li>; <p>Ensure websocket transport is closed when client does not close it; -- by :user:<code>bdraco</code>.</p>; <p>The transport could remain open if the client did not close it. This; change ensures the transport is closed when the client does not close; it.</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14477:925,perform,performance,925,https://hail.is,https://github.com/hail-is/hail/pull/14477,12,"['cache', 'perform']","['cache', 'performance']"
Performance,"Bumps [aiorwlock](https://github.com/aio-libs/aiorwlock) from 1.0.0 to 1.3.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiorwlock/releases"">aiorwlock's releases</a>.</em></p>; <blockquote>; <h2>aiorwlock 1.2.0</h2>; <h1>Changes</h1>; <ul>; <li>Fix a bug that makes concurrent writes possible under some (rare) conjunctions (<a href=""https://github-redirect.dependabot.com/aio-libs/aiorwlock/issues/235"">#235</a>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiorwlock/blob/master/CHANGES.rst"">aiorwlock's changelog</a>.</em></p>; <blockquote>; <p>1.3.0 (2022-1-18); ^^^^^^^^^^^^^^^^^^</p>; <ul>; <li>Dropped Python 3.6 support</li>; <li>Python 3.10 is officially supported</li>; <li>Drop deprecated <code>loop</code> parameter from <code>RWLock</code> constructor</li>; </ul>; <p>1.2.0 (2021-11-09); ^^^^^^^^^^^^^^^^^^</p>; <ul>; <li>Fix a bug that makes concurrent writes possible under some (rare) conjunctions (<a href=""https://github-redirect.dependabot.com/aio-libs/aiorwlock/issues/235"">#235</a>)</li>; </ul>; <p>1.1.0 (2021-09-27); ^^^^^^^^^^^^^^^^^^</p>; <ul>; <li>Remove explicit loop usage in <code>asyncio.sleep()</code> call, make the library forward; compatible with Python 3.10</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/aiorwlock/commit/6599d10ba16f95f19d5b5963a00aa857bc98f656""><code>6599d10</code></a> Bump to 1.3.0</li>; <li><a href=""https://github.com/aio-libs/aiorwlock/commit/d4b41f54b57caf316c41c3973ab82bd53a418ff8""><code>d4b41f5</code></a> Drop deprecated 'loop' parameter from RWLock constructor</li>; <li><a href=""https://github.com/aio-libs/aiorwlock/commit/3edb2a1bc1636832df12671f035e21dd74440824""><code>3edb2a1</code></a> Fix tests</li>; <li><a href=""https://github.com/aio-libs/aiorwlock/commit/45a7418474a55defe9c53fd8e38df60af514cf",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11514:326,concurren,concurrent,326,https://hail.is,https://github.com/hail-is/hail/pull/11514,1,['concurren'],['concurrent']
Performance,"Bumps [botocore](https://github.com/boto/botocore) from 1.24.13 to 1.24.14.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/boto/botocore/blob/develop/CHANGELOG.rst"">botocore's changelog</a>.</em></p>; <blockquote>; <h1>1.24.14</h1>; <ul>; <li>api-change:<code>chime-sdk-meetings</code>: Adds support for Transcribe language identification feature to the StartMeetingTranscription API.</li>; <li>api-change:<code>ecs</code>: Amazon ECS UpdateService API now supports additional parameters: loadBalancers, propagateTags, enableECSManagedTags, and serviceRegistries</li>; <li>api-change:<code>migration-hub-refactor-spaces</code>: AWS Migration Hub Refactor Spaces documentation update.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/botocore/commit/5c6f8c8d8e6c5ed05b05302ba9ef83cc2f0c420f""><code>5c6f8c8</code></a> Merge branch 'release-1.24.14'</li>; <li><a href=""https://github.com/boto/botocore/commit/3042265ca9488b8d73c6442f703337309d6733a4""><code>3042265</code></a> Bumping version to 1.24.14</li>; <li><a href=""https://github.com/boto/botocore/commit/ba0d095eeb62a2a293abadb54111df5fc0e2f0c8""><code>ba0d095</code></a> Update to latest models</li>; <li><a href=""https://github.com/boto/botocore/commit/a8c5cc855ecb91f5f64d73f2a15dfebc9e5e20e0""><code>a8c5cc8</code></a> Merge branch 'release-1.24.13' into develop</li>; <li>See full diff in <a href=""https://github.com/boto/botocore/compare/1.24.13...1.24.14"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=botocore&package-manager=pip&previous-version=1.24.13&new-version=1.24.14)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11534:532,load,loadBalancers,532,https://hail.is,https://github.com/hail-is/hail/pull/11534,1,['load'],['loadBalancers']
Performance,"Bumps [cryptography](https://github.com/pyca/cryptography) from 41.0.2 to 41.0.3.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pyca/cryptography/blob/main/CHANGELOG.rst"">cryptography's changelog</a>.</em></p>; <blockquote>; <p>41.0.3 - 2023-08-01</p>; <pre><code>; * Fixed performance regression loading DH public keys.; * Fixed a memory leak when using; :class:`~cryptography.hazmat.primitives.ciphers.aead.ChaCha20Poly1305`.; * Updated Windows, macOS, and Linux wheels to be compiled with OpenSSL 3.1.2.; <p>.. _v41-0-2:; </code></pre></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pyca/cryptography/commit/b22271cf3c3dd8dc8978f8f4b00b5c7060b6538d""><code>b22271c</code></a> bump for 41.0.3 (<a href=""https://redirect.github.com/pyca/cryptography/issues/9330"">#9330</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/774a4a16cbd22a89fdb4195ade9e4fcee27a7afa""><code>774a4a1</code></a> Only check DH key validity when loading a private key. (<a href=""https://redirect.github.com/pyca/cryptography/issues/9071"">#9071</a>) (<a href=""https://redirect.github.com/pyca/cryptography/issues/9319"">#9319</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/bfa4d95f0f356f2d535efd5c775e0fb3efe90ef2""><code>bfa4d95</code></a> changelog for 41.0.3 (<a href=""https://redirect.github.com/pyca/cryptography/issues/9320"">#9320</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/0da7165aa73c0a4865b0a4d9e019db3c16eea55a""><code>0da7165</code></a> backport fix the memory leak in fixedpool (<a href=""https://redirect.github.com/pyca/cryptography/issues/9272"">#9272</a>) (<a href=""https://redirect.github.com/pyca/cryptography/issues/9309"">#9309</a>)</li>; <li>See full diff in <a href=""https://github.com/pyca/cryptography/compare/41.0.2...41.0.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.gith",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13357:318,perform,performance,318,https://hail.is,https://github.com/hail-is/hail/pull/13357,9,"['load', 'perform']","['loading', 'performance']"
Performance,"Bumps [cryptography](https://github.com/pyca/cryptography) from 41.0.5 to 41.0.6.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pyca/cryptography/blob/main/CHANGELOG.rst"">cryptography's changelog</a>.</em></p>; <blockquote>; <p>41.0.6 - 2023-11-27</p>; <pre><code>; * Fixed a null-pointer-dereference and segfault that could occur when loading; certificates from a PKCS#7 bundle. Credit to **pkuzco** for reporting the; issue. **CVE-2023-49083**; <p>.. _v41-0-5:; </code></pre></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pyca/cryptography/commit/f09c261ca10a31fe41b1262306db7f8f1da0e48a""><code>f09c261</code></a> 41.0.6 release (<a href=""https://redirect.github.com/pyca/cryptography/issues/9927"">#9927</a>)</li>; <li>See full diff in <a href=""https://github.com/pyca/cryptography/compare/41.0.5...41.0.6"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=cryptography&package-manager=pip&previous-version=41.0.5&new-version=41.0.6)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14046:380,load,loading,380,https://hail.is,https://github.com/hail-is/hail/pull/14046,3,['load'],['loading']
Performance,"Bumps [cryptography](https://github.com/pyca/cryptography) from 42.0.2 to 42.0.4.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pyca/cryptography/blob/main/CHANGELOG.rst"">cryptography's changelog</a>.</em></p>; <blockquote>; <p>42.0.4 - 2024-02-20</p>; <pre><code>; * Fixed a null-pointer-dereference and segfault that could occur when creating; a PKCS#12 bundle. Credit to **Alexander-Programming** for reporting the; issue. **CVE-2024-26130**; * Fixed ASN.1 encoding for PKCS7/SMIME signed messages. The fields ``SMIMECapabilities``; and ``SignatureAlgorithmIdentifier`` should now be correctly encoded according to the; definitions in :rfc:`2633` :rfc:`3370`.; <p>.. _v42-0-3:</p>; <p>42.0.3 - 2024-02-15; </code></pre></p>; <ul>; <li>Fixed an initialization issue that caused key loading failures for some; users.</li>; </ul>; <p>.. _v42-0-2:</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pyca/cryptography/commit/fe18470f7d05f963e7267e34fdf985d81ea6ceea""><code>fe18470</code></a> Bump for 42.0.4 release (<a href=""https://redirect.github.com/pyca/cryptography/issues/10445"">#10445</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/aaa2dd06ed470695de818405a982d4c459869803""><code>aaa2dd0</code></a> Fix ASN.1 issues in PKCS#7 and S/MIME signing (<a href=""https://redirect.github.com/pyca/cryptography/issues/10373"">#10373</a>) (<a href=""https://redirect.github.com/pyca/cryptography/issues/10442"">#10442</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/7a4d012991061974da5d9cb7614de65eac94f49b""><code>7a4d012</code></a> Fixes <a href=""https://redirect.github.com/pyca/cryptography/issues/10422"">#10422</a> -- don't crash when a PKCS#12 key and cert don't match (<a href=""https://redirect.github.com/pyca/cryptography/issues/10423"">#10423</a>) ...</li>; <li><a href=""https://github.com/pyca/cryptography/commit/df314bb182bdfd661333969a94325e4680d785f6""><",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14332:828,load,loading,828,https://hail.is,https://github.com/hail-is/hail/pull/14332,3,['load'],['loading']
Performance,"Bumps [de.undercouch.download](https://github.com/michel-kraemer/gradle-download-task) from 5.3.0 to 5.3.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/michel-kraemer/gradle-download-task/releases"">de.undercouch.download's releases</a>.</em></p>; <blockquote>; <h2>5.3.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Downgrade slf4j to fix warning on console about missing slf4j provider</li>; <li>Allow <code>download</code> and <code>verify</code> extensions to be created on demand in custom tasks, so these tasks can be made compatible with Gradle's configuration cache (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/284"">#284</a>). Thanks to <a href=""https://github.com/liblit""><code>@​liblit</code></a> for testing!</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; <li>Improve documentation</li>; <li>Add integration tests for Gradle 6.9.3 and 7.6</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a0374fc7c895ae53309ea351e989571204e0ea5f""><code>a0374fc</code></a> Bump up version number to 5.3.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/612f57a382b8640cc730dc5e75d1c809e3e772bd""><code>612f57a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/291"">#291</a> from michel-kraemer/dependabot/npm_and_yarn/screencas...</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/53af1049f5514afe58e884d487d7c57dae47759d""><code>53af104</code></a> Bump http-cache-semantics from 4.1.0 to 4.1.1 in /screencast</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/398c14c05c6448b380ac35c6095598299c5e23c5""><code>398c14c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12707:607,cache,cache,607,https://hail.is,https://github.com/hail-is/hail/pull/12707,1,['cache'],['cache']
Performance,"Bumps [google-auth](https://github.com/googleapis/google-auth-library-python) from 1.27.0 to 2.6.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/google-auth-library-python/releases"">google-auth's releases</a>.</em></p>; <blockquote>; <h2>v2.6.0</h2>; <h3>Features</h3>; <ul>; <li>ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/962"">#962</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/52c8ef90058120d7d04d3d201adc111664be526c"">52c8ef9</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>revert &quot;feat: add api key support (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/826"">#826</a>)&quot; (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/964"">#964</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/f9f23f4370f2a7a5b2c66ee56a5e700ef03b5b06"">f9f23f4</a>)</li>; </ul>; <h2>v2.5.0</h2>; <h3>Features</h3>; <ul>; <li>ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/956"">#956</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/a8eb4c8693055a3420cfe9c3420aae2bc8cd465a"">a8eb4c8</a>)</li>; </ul>; <h2>v2.4.1</h2>; <h3>Bug Fixes</h3>; <ul>; <li>urllib3 import (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/953"">#953</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/c8b5cae3da5eb9d40067d38dac51a4a8c1e0763e"">c8b5cae</a>)</li>; </ul>; <h2>v2.4.0</h2>; <h3>Features</h3>; <ul>; <li>add 'py.typed' declaration (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/919"">#919</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11546:348,load,load,348,https://hail.is,https://github.com/hail-is/hail/pull/11546,1,['load'],['load']
Performance,"Bumps [importlib-metadata](https://github.com/python/importlib_metadata) from 3.10.1 to 4.12.0.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/python/importlib_metadata/blob/main/CHANGES.rst"">importlib-metadata's changelog</a>.</em></p>; <blockquote>; <h1>v4.12.0</h1>; <ul>; <li>py-93259: Now raise <code>ValueError</code> when <code>None</code> or an empty; string are passed to <code>Distribution.from_name</code> (and other; callers).</li>; </ul>; <h1>v4.11.4</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/379"">#379</a>: In <code>PathDistribution._name_from_stem</code>, avoid including; parts of the extension in the result.</li>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/381"">#381</a>: In <code>PathDistribution._normalized_name</code>, ensure names; loaded from the stem of the filename are also normalized, ensuring; duplicate entry points by packages varying only by non-normalized; name are hidden.</li>; </ul>; <h1>v4.11.3</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/372"">#372</a>: Removed cast of path items in FastPath, not needed.</li>; </ul>; <h1>v4.11.2</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/369"">#369</a>: Fixed bug where <code>EntryPoint.extras</code> was returning; match objects and not the extras strings.</li>; </ul>; <h1>v4.11.1</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/367"">#367</a>: In <code>Distribution.requires</code> for egg-info, if <code>requires.txt</code>; is empty, return an empty list.</li>; </ul>; <h1>v4.11.0</h1>; <ul>; <li>bpo-46246: Added <code>__slots__</code> to <code>EntryPoints</code>.</li>; </ul>; <h1>v4.10.2</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/365"">#365</a> and bpo-46546: Avoid leakin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12000:895,load,loaded,895,https://hail.is,https://github.com/hail-is/hail/pull/12000,1,['load'],['loaded']
Performance,"Bumps [orjson](https://github.com/ijl/orjson) from 3.6.4 to 3.6.7.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/releases"">orjson's releases</a>.</em></p>; <blockquote>; <h2>3.6.7</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of deserializing almost-empty documents.</li>; <li>Publish arm7l <code>manylinux_2_17</code> wheels to PyPI.</li>; <li>Publish amd64 <code>musllinux_1_1</code> wheels to PyPI.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix build requiring <code>python</code> on <code>PATH</code>.</li>; </ul>; <h2>3.6.6</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing <code>datetime.datetime</code> using <code>tzinfo</code> that; are <code>zoneinfo.ZoneInfo</code>.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix invalid indexing in line and column number reporting in; <code>JSONDecodeError</code>.</li>; <li>Fix <code>orjson.OPT_STRICT_INTEGER</code> not raising an error on; values exceeding a 64-bit integer maximum.</li>; </ul>; <h2>3.6.5</h2>; <h3>Fixed</h3>; <ul>; <li>Fix build on macOS aarch64 CPython 3.10.</li>; <li>Fix build issue on 32-bit.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.6.7 - 2022-02-14</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of deserializing almost-empty documents.</li>; <li>Publish arm7l <code>manylinux_2_17</code> wheels to PyPI.</li>; <li>Publish amd4 <code>musllinux_1_1</code> wheels to PyPI.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix build requiring <code>python</code> on <code>PATH</code>.</li>; </ul>; <h2>3.6.6 - 2022-01-21</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing <code>datetime.datetime</code> using <code>tzinfo</code> that; are <code>zoneinfo.ZoneInfo</code>.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix invalid indexing in line and column number",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11572:281,perform,performance,281,https://hail.is,https://github.com/hail-is/hail/pull/11572,2,['perform'],['performance']
Performance,"Bumps [orjson](https://github.com/ijl/orjson) from 3.9.10 to 3.10.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/releases"">orjson's releases</a>.</em></p>; <blockquote>; <h2>3.10.0</h2>; <h3>Changed</h3>; <ul>; <li>Support serializing <code>numpy.float16</code> (<code>numpy.half</code>).</li>; <li>sdist uses metadata 2.3 instead of 2.1.</li>; <li>Improve Windows PyPI builds.</li>; </ul>; <h2>3.9.15</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12</h2>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.10.0 - 2024-03-27</h2>; <h3>Changed</h3>; <ul>; <li>Support serializing <code>numpy.float16</code> (<code>numpy.half</code>).</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14427:545,load,loads,545,https://hail.is,https://github.com/hail-is/hail/pull/14427,1,['load'],['loads']
Performance,"Bumps [orjson](https://github.com/ijl/orjson) from 3.9.10 to 3.9.15.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/releases"">orjson's releases</a>.</em></p>; <blockquote>; <h2>3.9.15</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12</h2>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14357:324,load,loads,324,https://hail.is,https://github.com/hail-is/hail/pull/14357,6,"['load', 'optimiz']","['loads', 'optimization']"
Performance,"Bumps [pandas](https://github.com/pandas-dev/pandas) from 1.3.0 to 1.4.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/pandas-dev/pandas/releases"">pandas's releases</a>.</em></p>; <blockquote>; <h2>Pandas 1.4.1</h2>; <p>This is the first patch release in the 1.4.x series and includes some regression fixes and bug fixes. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4.1/whatsnew/v1.4.1.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <h2>Pandas 1.4.0</h2>; <p>This release includes some new features, bug fixes, and performance improvements. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4.0/whatsnew/v1.4.0.html"">full whatsnew</a> for a list of all the changes. pandas 1.4.0 supports Python 3.8 and higher.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install -c conda-forge pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <h2>Pandas 1.4.0rc0</h2>; <p>We are pleased to announce a release candidate for pandas 1.4.0. If all goes well, we'll release pandas 1.4.0 in about two weeks.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4/whatsnew/v1.4.0.html"">whatsnew</a> for a list of all the changes. pandas 1.4.0 supports Python 3.8 and higher.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11539:1012,perform,performance,1012,https://hail.is,https://github.com/hail-is/hail/pull/11539,1,['perform'],['performance']
Performance,"Bumps [pandas](https://github.com/pandas-dev/pandas) from 1.3.5 to 1.5.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/pandas-dev/pandas/releases"">pandas's releases</a>.</em></p>; <blockquote>; <h2>Pandas 1.5.0</h2>; <p>This release includes some new features, bug fixes, and performance improvements. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5.0/whatsnew/v1.5.0.html"">full whatsnew</a> for a list of all the changes. pandas 1.5.0 supports Python 3.8 and higher.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <p><code>conda install -c conda-forge pandas</code></p>; <p>Or via PyPI:</p>; <p><code>python3 -m pip install --upgrade pandas</code></p>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <h2>Pandas 1.5.0rc0</h2>; <p>We are pleased to announce a release candidate for pandas 1.5.0. If all goes well, we'll release pandas 1.5.0 in about two weeks.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5/whatsnew/v1.5.0.html"">whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on conda-forge and PyPI.</p>; <p>The release can be installed from PyPI</p>; <pre><code>python -m pip install --upgrade --pre pandas==1.5.0rc0; </code></pre>; <p>Or from conda-forge</p>; <pre><code>conda install -c conda-forge/label/pandas_rc pandas==1.5.0rc0; </code></pre>; <p>Please report any issues with the release candidate on the pandas issue tracker.</p>; <h2>Pandas 1.4.4</h2>; <p>This is a patch release in the 1.4.x series and includes some regression and bug fixes. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4.4/whatsnew/v1.4.4.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12292:325,perform,performance,325,https://hail.is,https://github.com/hail-is/hail/pull/12292,1,['perform'],['performance']
Performance,"Bumps [protobuf](https://github.com/protocolbuffers/protobuf) from 3.19.6 to 4.21.12.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/protocolbuffers/protobuf/releases"">protobuf's releases</a>.</em></p>; <blockquote>; <h2>Protocol Buffers v3.20.3</h2>; <h1>Java</h1>; <ul>; <li>Refactoring java full runtime to reuse sub-message builders and prepare to; migrate parsing logic from parse constructor to builder.</li>; <li>Move proto wireformat parsing functionality from the private &quot;parsing; constructor&quot; to the Builder class.</li>; <li>Change the Lite runtime to prefer merging from the wireformat into mutable; messages rather than building up a new immutable object before merging. This; way results in fewer allocations and copy operations.</li>; <li>Make message-type extensions merge from wire-format instead of building up; instances and merging afterwards. This has much better performance.</li>; <li>Fix TextFormat parser to build up recurring (but supposedly not repeated); sub-messages directly from text rather than building a new sub-message and; merging the fully formed message into the existing field.</li>; <li>This release addresses a <a href=""https://github.com/protocolbuffers/protobuf/security/advisories/GHSA-h4h5-3hr4-j3g2"">Security Advisory for Java users</a></li>; </ul>; <h2>Protocol Buffers v3.20.2</h2>; <h1>C++</h1>; <ul>; <li>Reduce memory consumption of MessageSet parsing</li>; <li>This release addresses a <a href=""https://github.com/protocolbuffers/protobuf/security/advisories/GHSA-8gq9-2x98-w8hf"">Security Advisory for C++ and Python users</a></li>; </ul>; <h2>Protocol Buffers v3.20.1</h2>; <h1>PHP</h1>; <ul>; <li>Fix building packaged PHP extension (<a href=""https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/9727"">#9727</a>)</li>; <li>Fixed composer.json to only advertise compatibility with PHP 7.0+. (<a href=""https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/9",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12563:942,perform,performance,942,https://hail.is,https://github.com/hail-is/hail/pull/12563,1,['perform'],['performance']
Performance,"Bumps [protobuf](https://github.com/protocolbuffers/protobuf) from 3.20.2 to 4.21.9.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/protocolbuffers/protobuf/releases"">protobuf's releases</a>.</em></p>; <blockquote>; <h2>Protocol Buffers v3.20.3</h2>; <h1>Java</h1>; <ul>; <li>Refactoring java full runtime to reuse sub-message builders and prepare to; migrate parsing logic from parse constructor to builder.</li>; <li>Move proto wireformat parsing functionality from the private &quot;parsing; constructor&quot; to the Builder class.</li>; <li>Change the Lite runtime to prefer merging from the wireformat into mutable; messages rather than building up a new immutable object before merging. This; way results in fewer allocations and copy operations.</li>; <li>Make message-type extensions merge from wire-format instead of building up; instances and merging afterwards. This has much better performance.</li>; <li>Fix TextFormat parser to build up recurring (but supposedly not repeated); sub-messages directly from text rather than building a new sub-message and; merging the fully formed message into the existing field.</li>; <li>This release addresses a <a href=""https://github.com/protocolbuffers/protobuf/security/advisories/GHSA-h4h5-3hr4-j3g2"">Security Advisory for Java users</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/protocolbuffers/protobuf/commits"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=protobuf&package-manager=pip&previous-version=3.20.2&new-version=4.21.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12518:941,perform,performance,941,https://hail.is,https://github.com/hail-is/hail/pull/12518,1,['perform'],['performance']
Performance,"Bumps [pycodestyle](https://github.com/PyCQA/pycodestyle) from 2.8.0 to 2.9.1.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/PyCQA/pycodestyle/blob/main/CHANGES.txt"">pycodestyle's changelog</a>.</em></p>; <blockquote>; <h2>2.9.1 (2022-08-03)</h2>; <p>Changes:</p>; <ul>; <li>E275: fix false positive for yield expressions.</li>; </ul>; <h2>2.9.0 (2022-07-30)</h2>; <p>Changes:</p>; <ul>; <li>E221, E222, E223, E224: add support for <code>:=</code> operator. PR <a href=""https://github-redirect.dependabot.com/PyCQA/pycodestyle/issues/1032"">#1032</a>.</li>; <li>Drop python 2.7 / 3.5.</li>; <li>E262: consider non-breaking spaces (<code>\xa0</code>) as whitespace. PR <a href=""https://github-redirect.dependabot.com/PyCQA/pycodestyle/issues/1035"">#1035</a>.</li>; <li>Improve performance of <code>_is_binary_operator</code>. PR <a href=""https://github-redirect.dependabot.com/PyCQA/pycodestyle/issues/1052"">#1052</a>.</li>; <li>E275: requires whitespace around keywords. PR <a href=""https://github-redirect.dependabot.com/PyCQA/pycodestyle/issues/1063"">#1063</a>.</li>; <li>Add support for python 3.11. PR <a href=""https://github-redirect.dependabot.com/PyCQA/pycodestyle/issues/1070"">#1070</a>.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/PyCQA/pycodestyle/commit/10a4427c75740717b43448339fcf71f11bc33d1a""><code>10a4427</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/PyCQA/pycodestyle/issues/1092"">#1092</a> from PyCQA/2_9_1</li>; <li><a href=""https://github.com/PyCQA/pycodestyle/commit/c33e852a5938b823b04dd981260bd1664c643385""><code>c33e852</code></a> Release 2.9.1</li>; <li><a href=""https://github.com/PyCQA/pycodestyle/commit/c97e4f86bd60e449a64be6c0de5b5ec5bb28b8e9""><code>c97e4f8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/PyCQA/pycodestyle/issues/1091"">#1091</a> from asottile/E275-yield-expression</li>; <li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12476:819,perform,performance,819,https://hail.is,https://github.com/hail-is/hail/pull/12476,1,['perform'],['performance']
Performance,"Bumps [pylint](https://github.com/PyCQA/pylint) from 2.13.4 to 2.13.5.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/PyCQA/pylint/blob/main/ChangeLog"">pylint's changelog</a>.</em></p>; <blockquote>; <h1>What's New in Pylint 2.13.5?</h1>; <p>Release date: 2022-04-06</p>; <ul>; <li>; <p>Fix false positive regression in 2.13.0 for <code>used-before-assignment</code> for; homonyms between variable assignments in try/except blocks and variables in; subscripts in comprehensions.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6069"">#6069</a>; Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6136"">#6136</a></p>; </li>; <li>; <p><code>lru-cache-decorating-method</code> has been renamed to <code>cache-max-size-none</code> and; will only be emitted when <code>maxsize</code> is <code>None</code>.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6180"">#6180</a></p>; </li>; <li>; <p>Fix false positive for <code>unused-import</code> when disabling both <code>used-before-assignment</code> and <code>undefined-variable</code>.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6089"">#6089</a></p>; </li>; <li>; <p>Narrow the scope of the <code>unnecessary-ellipsis</code> checker to:</p>; <ul>; <li>functions &amp; classes which contain both a docstring and an ellipsis.</li>; <li>A body which contains an ellipsis <code>nodes.Expr</code> node &amp; at least one other statement.</li>; </ul>; </li>; <li>; <p>Fix false positive for <code>used-before-assignment</code> for assignments taking place via; nonlocal declarations after an earlier type annotation.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5394"">#5394</a></p>; </li>; <li>; <p>Fix crash for <code>redefined-slots-in-subclass</code> when the type of the slot is not a const or a string.</p>; <p>Closes <a href=""https://github-redi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11739:744,cache,cache-decorating-method,744,https://hail.is,https://github.com/hail-is/hail/pull/11739,2,['cache'],"['cache-decorating-method', 'cache-max-size-none']"
Performance,"Bumps [scipy](https://github.com/scipy/scipy) from 1.9.3 to 1.10.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/scipy/scipy/releases"">scipy's releases</a>.</em></p>; <blockquote>; <h1>SciPy 1.10.0 Release Notes</h1>; <p>SciPy <code>1.10.0</code> is the culmination of <code>6</code> months of hard work. It contains; many new features, numerous bug-fixes, improved test coverage and better; documentation. There have been a number of deprecations and API changes; in this release, which are documented below. All users are encouraged to; upgrade to this release, as there are a large number of bug-fixes and; optimizations. Before upgrading, we recommend that users check that; their own code does not use deprecated SciPy functionality (to do so,; run your code with <code>python -Wd</code> and check for <code>DeprecationWarning</code> s).; Our development attention will now shift to bug-fix releases on the; 1.10.x branch, and on adding new features on the main branch.</p>; <p>This release requires Python <code>3.8+</code> and NumPy <code>1.19.5</code> or greater.</p>; <p>For running on PyPy, PyPy3 <code>6.0+</code> is required.</p>; <h1>Highlights of this release</h1>; <ul>; <li>A new dedicated datasets submodule (<code>scipy.datasets</code>) has been added, and is; now preferred over usage of <code>scipy.misc</code> for dataset retrieval.</li>; <li>A new <code>scipy.interpolate.make_smoothing_spline</code> function was added. This; function constructs a smoothing cubic spline from noisy data, using the; generalized cross-validation (GCV) criterion to find the tradeoff between; smoothness and proximity to data points.</li>; <li><code>scipy.stats</code> has three new distributions, two new hypothesis tests, three; new sample statistics, a class for greater control over calculations; involving covariance matrices, and many other enhancements.</li>; </ul>; <h1>New features</h1>; <h1><code>scipy.datasets</code> introduction</h1>; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13227:659,optimiz,optimizations,659,https://hail.is,https://github.com/hail-is/hail/pull/13227,1,['optimiz'],['optimizations']
Performance,"Bumps [sortedcontainers](https://github.com/grantjenks/python-sortedcontainers) from 2.1.0 to 2.4.0.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/grantjenks/python-sortedcontainers/blob/master/HISTORY.rst"">sortedcontainers's changelog</a>.</em></p>; <blockquote>; <h2>2.4.0 (2021-05-16)</h2>; <p><strong>API Changes</strong></p>; <ul>; <li>Implement SortedDict methods: <strong>or</strong>, <strong>ror</strong>, and <strong>ior</strong> per PEP 584.</li>; </ul>; <h2>2.3.0 (2020-11-08)</h2>; <p><strong>Bugfixes</strong></p>; <ul>; <li>Make sort order stable when updating with large iterables.</li>; </ul>; <h2>2.2.2 (2020-06-07)</h2>; <p><strong>Miscellaneous</strong></p>; <ul>; <li>Add &quot;small slice&quot; optimization to <code>SortedList.__getitem__</code>.</li>; <li>Silence warning when testing <code>SortedList.iloc</code>.</li>; </ul>; <h2>2.2.1 (2020-06-06)</h2>; <p><strong>Miscellaneous</strong></p>; <ul>; <li>Fix a warning regarding <code>classifiers</code> in setup.py.</li>; </ul>; <h2>2.2.0 (2020-06-06)</h2>; <p><strong>Miscellaneous</strong></p>; <ul>; <li>Change SortedDict to avoid cycles for CPython reference counting.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/a1f52d6713dd2c2713a881d4f4d86ed68ff71cab""><code>a1f52d6</code></a> Bump version to 2.4.0</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/2678a78b6dacbe2352bff7876a26759d84971dac""><code>2678a78</code></a> Implement SortedDict methods: <strong>or</strong>, <strong>ror</strong>, and <strong>ior</strong> (<a href=""https://github-redirect.dependabot.com/grantjenks/python-sortedcontainers/issues/171"">#171</a>)</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/9887989b21fc21fe572e0b4c30a3f3aa1eabbdca""><code>9887989</code></a> Bump version to 2.3.0</li>; <li><a href=""https://github.com/gra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11476:760,optimiz,optimization,760,https://hail.is,https://github.com/hail-is/hail/pull/11476,1,['optimiz'],['optimization']
Performance,"Bumps [svelte](https://github.com/sveltejs/svelte) from 3.38.2 to 3.49.0.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/sveltejs/svelte/blob/master/CHANGELOG.md"">svelte's changelog</a>.</em></p>; <blockquote>; <h2>3.49.0</h2>; <ul>; <li>Improve performance of string escaping during SSR (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/pull/5701"">#5701</a>)</li>; <li>Add <code>ComponentType</code> and <code>ComponentProps</code> convenience types (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/pull/6770"">#6770</a>)</li>; <li>Add support for CSS <code>@layer</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7504"">#7504</a>)</li>; <li>Export <code>CompileOptions</code> from <code>svelte/compiler</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/pull/7658"">#7658</a>)</li>; <li>Fix DOM-less components not being properly destroyed (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7488"">#7488</a>)</li>; <li>Fix <code>class:</code> directive updates with <code>&lt;svelte:element&gt;</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7521"">#7521</a>, <a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7571"">#7571</a>)</li>; <li>Harden attribute escaping during SSR (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/pull/7530"">#7530</a>)</li>; </ul>; <h2>3.48.0</h2>; <ul>; <li>Allow creating cancelable custom events with <code>createEventDispatcher</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/4623"">#4623</a>)</li>; <li>Support <code>{@const}</code> tag in <code>{#if}</code> blocks <a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7241"">#7241</a></li>; <li>Return the context object in <code>setContext</code> <a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7427"">#7427</a></li>; <li>Allow comments ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12032:289,perform,performance,289,https://hail.is,https://github.com/hail-is/hail/pull/12032,3,['perform'],['performance']
Performance,"Bumps [urllib3](https://github.com/urllib3/urllib3) from 1.26.4 to 1.26.5.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/releases"">urllib3's releases</a>.</em></p>; <blockquote>; <h2>1.26.5</h2>; <p>:warning: <strong>IMPORTANT: urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting the authority component.</li>; </ul>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a></strong></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h2>1.26.5 (2021-05-26)</h2>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting; the authority component.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/d1616473df94b94f0f5ad19d2a6608cfe93b7cdf""><code>d161647</code></a> Release 1.26.5</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2d4a3fee6de2fa45eb82169361918f759269b4ec""><code>2d4a3fe</code></a> Improve performance of sub-authority splitting in URL</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2698537d52f8ff1f0bbb1d45cf018b118e91f637""><code>2698537</code></a> Update vendored six to 1.16.0</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/07bed791e9c391d8bf12950f76537dc3c6f90550""><code>07bed79</code></a> Fix deprecation warnings for Py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10544:595,perform,performance,595,https://hail.is,https://github.com/hail-is/hail/pull/10544,1,['perform'],['performance']
Performance,"Bumps [wrapt](https://github.com/GrahamDumpleton/wrapt) from 1.13.3 to 1.14.1.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/GrahamDumpleton/wrapt/blob/develop/docs/changes.rst"">wrapt's changelog</a>.</em></p>; <blockquote>; <h2>Version 1.14.1</h2>; <p><strong>Bugs Fixed</strong></p>; <ul>; <li>When the post import hooks mechanism was being used, and a Python package with; its own custom module importer was used, importing modules could fail if the; custom module importer didn't use the latest Python import hook finder/loader; APIs and instead used the deprecated API. This was actually occurring with the; <code>zipimporter</code> in Python itself, which was not updated to use the newer Python; APIs until Python 3.10.</li>; </ul>; <h2>Version 1.14.0</h2>; <p><strong>Bugs Fixed</strong></p>; <ul>; <li>; <p>Python 3.11 dropped <code>inspect.formatargspec()</code> which was used in creating; signature changing decorators. Now bundling a version of this function; which uses <code>Parameter</code> and <code>Signature</code> from <code>inspect</code> module when; available. The replacement function is exposed as <code>wrapt.formatargspec()</code>; if need it for your own code.</p>; </li>; <li>; <p>When using a decorator on a class, <code>isinstance()</code> checks wouldn't previously; work as expected and you had to manually use <code>Type.__wrapped__</code> to access; the real type when doing instance checks. The <code>__instancecheck__</code> hook is; now implemented such that you don't have to use <code>Type.__wrapped__</code> instead; of <code>Type</code> as last argument to <code>isinstance()</code>.</p>; </li>; <li>; <p>Eliminated deprecation warnings related to Python module import system, which; would have turned into broken code in Python 3.12. This was used by the post; import hook mechanism.</p>; </li>; </ul>; <p><strong>New Features</strong></p>; <ul>; <li>Binary wheels provided on PyPi for <code>aarch64</code> Linux s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12102:569,load,loader,569,https://hail.is,https://github.com/hail-is/hail/pull/12102,1,['load'],['loader']
Performance,"Bumps [zipp](https://github.com/jaraco/zipp) from 3.17.0 to 3.18.1.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jaraco/zipp/blob/main/NEWS.rst"">zipp's changelog</a>.</em></p>; <blockquote>; <h1>v3.18.1</h1>; <p>No significant changes.</p>; <h1>v3.18.0</h1>; <h2>Features</h2>; <ul>; <li>Bypass ZipFile.namelist in glob for better performance. (<a href=""https://redirect.github.com/jaraco/zipp/issues/106"">#106</a>)</li>; <li>Refactored glob functionality to support a more generalized solution with support for platform-specific path separators. (<a href=""https://redirect.github.com/jaraco/zipp/issues/108"">#108</a>)</li>; </ul>; <h2>Bugfixes</h2>; <ul>; <li>Add special accounting for pypy when computing the stack level for text encoding warnings. (<a href=""https://redirect.github.com/jaraco/zipp/issues/114"">#114</a>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/jaraco/zipp/commit/bfae83474a730e8cc9b8a71027fb859b46b3875c""><code>bfae834</code></a> Finalize</li>; <li><a href=""https://github.com/jaraco/zipp/commit/487066ec9757c3c82e96014d0b30906996c6280d""><code>487066e</code></a> Merge changelog into last release.</li>; <li><a href=""https://github.com/jaraco/zipp/commit/4584ee2dcfb10d5314ad319d9d5b140c90bc2951""><code>4584ee2</code></a> Move changelog entry, saved to the wrong location :(</li>; <li><a href=""https://github.com/jaraco/zipp/commit/3c06d30b91b37a118536d9d424e0a8b893e78a6e""><code>3c06d30</code></a> Finalize</li>; <li><a href=""https://github.com/jaraco/zipp/commit/48b72b8db6ae5f7712323aca6b340744db15f576""><code>48b72b8</code></a> Merge pull request <a href=""https://redirect.github.com/jaraco/zipp/issues/113"">#113</a> from jaraco/feature/glob-perf</li>; <li><a href=""https://github.com/jaraco/zipp/commit/171fa98236a1adfc316c3bc5cdc5eaa4b9548424""><code>171fa98</code></a> Add news fragment.</li>; <li><a href=""https://github.com/jaraco/zipp/commit/ac8ea7a5",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14473:376,perform,performance,376,https://hail.is,https://github.com/hail-is/hail/pull/14473,1,['perform'],['performance']
Performance,"Bundling working well now:. For instance, the addition of this file, which handles the auth0 callback/sets cookie, adds only *501B* despite importing Auth and react-easy-state :tada:. The entirety of Auth dependency, react-easy-state (just observable JS properties for easy event notification), js-cookie to simplify cookie management, all other imports that are used at least 2x, poly fills for IE11 compat (promises, object.assign) + React + React-Dom is 99KB, and served in parallel with the page, so initial render doesn't incur the cost. Not bad; we can get this down a bit by removing js-cookie (2KB). ```jsx; // TODO: Replace Loading component without Material UI; import { Component } from 'react';; import Router from 'next/router';; import { view } from 'react-easy-state';; import Auth from '../lib/Auth';. class Callback extends Component {; componentDidMount() {; Auth.handleAuthenticationAsync(err => {; // TODO: notify in modal if error; if (err) {; console.error('ERROR in callback!', err);; }. Router.push('/');; });; }. render() {; return !Auth.isAuthenticated() ? <div>Loading</div> : <div>Hello</div>;; }; }. export default view(Callback);; ```. <img width=""353"" alt=""screen shot 2018-12-19 at 5 06 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50251076-ad695680-03b0-11e9-88f2-28d3ff7daa33.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682:633,Load,Loading,633,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682,2,['Load'],['Loading']
Performance,"Buy it, use it, break it, fix it; Trash it, change it, mail - upgrade it; Charge it, point it, zoom it, press it; Snap it, work it, quick - erase it; Write it, cut it, paste it, save it; Load it, check it, quick - rewrite it; Plug it, play it, burn it, rip it; Drag and drop it, zip - unzip it; Lock it, fill it, call it, find it; View it, code it, jam - unlock it; Surf it, scroll it, pause it, click it; Cross it, crack it, switch - update it; Name it, read it, tune it, print it; Scan it, send it, fax - rename it; Touch it, bring it, pay it, watch it; Turn it, leave it, start - format it",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2036#issuecomment-318518817:187,Load,Load,187,https://hail.is,https://github.com/hail-is/hail/pull/2036#issuecomment-318518817,3,"['Load', 'tune']","['Load', 'tune']"
Performance,"By joining against the `batches` row in the insert `FOR UPDATE`, these queries try to grab an exclusive lock on the row in the `batches` table. This is a problem if the transaction already has a shared lock on that row, like it does in `mark_job_complete`. In that case any two concurrent MJCs from the same batch will deadlock.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14379:278,concurren,concurrent,278,https://hail.is,https://github.com/hail-is/hail/pull/14379,1,['concurren'],['concurrent']
Performance,"By looking at the full state on a dataset consist of 5000 copies of the same row, I've narrowed the search to this line giving the wrong values in the first row of `xdx` (not including upper left element) on a non-deterministic subset of rows despite `px` and `dpa` being correct:. ```; xdx(r0, r1) := px.t * dpa; ```. This is the only place `px` is used, and indeed, copying `px` is sufficient to fix the bug. I'd say it's a breeze concurrency bug, except that not only does it go away whenever no single computer processes more than one partition (i.e. big partition or single-core workers), it also doesn't occur on my laptop or google VM. I'd prefer a solution that avoids copying `px` per thread, since it's samples by covariates, rather than covariates by covariates (xdx) or covariates by 1 (xdy). But while I'm still working on what the heck is going on, I see no reason not to merge this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4229#issuecomment-416789786:433,concurren,concurrency,433,https://hail.is,https://github.com/hail-is/hail/pull/4229#issuecomment-416789786,1,['concurren'],['concurrency']
Performance,"CC: @danking . I tested this change works by replicating Lindo's job download (~80 Gi) on an overloaded node with 8 simultaneous jobs trying to download data in parallel. Before this proposed change and after I fixed some other issues in #10522, 75% of his jobs would fail with this error (the ones I presume on the persistent SSDs rather than the local SSDs):. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/site-packages/batch/copy/__main__.py"", line 34, in <module>; asyncio.run(main()); File ""/usr/local/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); concurrent.futures._base.CancelledError; ```. Now, all of the downloads succeed after my change. I found this link to be very helpful figuring out what the issue was. ; https://bugs.python.org/issue33413",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10534:925,concurren,concurrent,925,https://hail.is,https://github.com/hail-is/hail/pull/10534,1,['concurren'],['concurrent']
Performance,"CHANGELOG: Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to specify which cloud regions a job can run in. The default value is a job can run in any available region. Stacked on #12212 . This PR threads through region requests from the user and feeds that information into the scheduler. The architecture of a pool per machine type has not changed. We explicitly chose not to have a new pool per region x machine_type. Instead, the control loop looks at the front of the job queue and tries to predict which jobs are likely to be scheduled. From those jobs, we then find which regions the jobs can run in and create the number of corresponding instances. We use the fair share calculation to estimate how many jobs per user can be scheduled in 2.5 minutes assuming the scheduling loop runs once per second. We then grab this many jobs from the queue for each user and estimate the ""scheduling iteration"" at which each iteration of the scheduler each chunk of user jobs would be scheduled. We sort the overall set of jobs that we've chosen by the ""scheduling iteration"". We also include the regions as part of the sorting queries with None (any region) being sorted last. This is to compact the free cores across jobs so as to avoid fragmentation of instances created and for jobs with no region specifications to fill in the remaining cores in any region. For the hailtop.batch client, I added a new setting in `~/.config/hail` to set the default regions for all jobs in the ServiceBackend and a new method on `Job` that sets the list of regions to run in. Things to double check once everything is working is the sort orders on the scheduling queries are correct. . Once this PR goes in, then we can merge #11840 with some minor changes. There will also be a follow-up PR that gets rid of the CI-specific code in the scheduler.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221:519,queue,queue,519,https://hail.is,https://github.com/hail-is/hail/pull/12221,2,['queue'],['queue']
Performance,"CHANGELOG: BatchPoolExecutor now raises an informative error message for a variety of ""system"" errors, such as missing container images. If the main container fails for reasons beyond BatchPoolExecutor's control, such; as a missing container image, we previously did not report these errors. In; fact, we encountered errors when trying to load the output file that cannot; exist if the main container errors. Smaller included changes:; - directly use the asynchronous, low-level client instead of the synchronous,; low-level client; - introduce an `async_cancel` now that we have access to the async client.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9543:339,load,load,339,https://hail.is,https://github.com/hail-is/hail/pull/9543,1,['load'],['load']
Performance,"CHANGELOG: Eliminate quadratic behavior in `BlockMatrix.to_matrix_table_row_major`. Users should expect significant reduction in run-time. There are two significant changes in this PR:; - Teach `LZ4InputBlockBuffer` how to skip bytes without decompressing every block, and; - Teach BlockMatrix to use a small cache of rows when converting from a BlockMatrix to a row-wise RDD. ### Blocked LZ4 Byte Skipping. We compress in blocks of 16 KiB. The blocks begin with an 32-bit integer indicating the decompressed length. When we're skipping large numbers of bytes we can request an `LZ4InputBlockBuffer` to skip decompression if the entire block will be skipped. ### BlockMatrix Blocks to Rows Caching; Currently, for every row in every block, BM opens a file, skips to the appropriate location, reads that one row, writes it into an RVB, and then closes the file. This has terrible cache and I/O performance. Instead, we allocate 32 MiB to cache the rows of each block. We divide the cache evenly across all rows. The new implementation requires the cache can at least fit one row of the block, with 32 MiB we're good up to ~4 million (total) columns. We'll need to reimplement this to also use a tree-aggregate long before we get to 4 million columns. ### Benchmark Results. This branch vs main (3149211fb79b):; ```; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; to_matrix_table_row_major 716.3% 251.300 1800.000; ----------------------; Harmonic mean: 716.3%; Geometric mean: 716.3%; Arithmetic mean: 716.3%; Median: 716.3%; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9328:309,cache,cache,309,https://hail.is,https://github.com/hail-is/hail/pull/9328,6,"['cache', 'perform']","['cache', 'performance']"
Performance,CHANGELOG: Fix hl.import_plink docs to properly report the type of `is_case` and `quant_pheno`. I verified the correct types by checking `LoadPlink.scala`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9495:138,Load,LoadPlink,138,https://hail.is,https://github.com/hail-is/hail/pull/9495,1,['Load'],['LoadPlink']
Performance,CHANGELOG: Fixed bug causing poor performance and memory leaks for Matrix.annotate_rows aggregations,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12995:34,perform,performance,34,https://hail.is,https://github.com/hail-is/hail/pull/12995,1,['perform'],['performance']
Performance,"CHANGELOG: Implement the KING method for relationship inference as hl.methods.king. Just look at the last commit. The other commits are PRs that I hope will merge; on Tuesday. This PR implements `hl.methods.king` a new, relatively fast, method for; relationship inference on genotype data. I am eager for criticism of the ""Notes""; section in which I attempt to describe the KING method to a Hail user with only; a basic understanding of genotype matrices and Hail. I also include a benchmark which exercises MT->BM, matrix multiply, and; BM->MT. We have an opportunity for a substantial improvement in performance by; BM->replacing the BM interface by one which permits multiple entry fields. In; BM->particular, note that I have to convert from row-partitioning to; BM->block-partitioning four times!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9343:602,perform,performance,602,https://hail.is,https://github.com/hail-is/hail/pull/9343,1,['perform'],['performance']
Performance,"CHANGELOG: In Query-on-Batch, `naive_coalsce` no longer performs a full write/read of the dataset. It now operates identically to the Query-on-Spark implementation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13042:56,perform,performs,56,https://hail.is,https://github.com/hail-is/hail/pull/13042,1,['perform'],['performs']
Performance,"CHANGELOG: Introduce `hailctl fs sync` which robustly transfers one or more files between Amazon S3, Azure Blob Storage, and Google Cloud Storage. There are really two distinct conceptual changes remaining here. Given my waning time available, I am not going to split them into two pull requests. The changes are:. 1. `basename` always agrees with the [`basename` UNIX utility](https://en.wikipedia.org/wiki/Basename). In particular, the folder `/foo/bar/baz/`'s basename is *not* `''` it is `'baz'`. The only folders or objects whose basename is `''` are objects whose name literally ends in a slash, e.g. an *object* named `gs://foo/bar/baz/`. 2. `hailctl fs sync`, a robust copying tool with a user-friendly CLI. `hailctl fs sync` comprises two pieces: `plan.py` and `sync.py`. The latter, `sync.py` is simple: it delegates to our existing copy infrastructure. That copy infastructure has been lightly modified to support this use-case. The former, `plan.py`, is a concurrent file system `diff`. `plan.py` generates and `sync.py` consumes a ""plan folder"" containing these files:. 1. `matches` files whose names and sizes match. Two columns: source URL, destination URL. 2. `differs` files or folders whose names match but either differ in size or differ in type. Four columns: source URL, destination URL, source state, destination state. The states are either: `file`, `dif`, or a size. If either state is a size, both states are sizes. 3. `srconly` files only present in the source. One column: source URL. 4. `dstonly` files only present in the destination. One column: destination URL. 5. `plan` a proposed set of object-to-object copies. Two columns: source URL, destination URL. 6. `summary` a one-line file containing the total number of copies in plan and the total number of bytes which would be copied. As described in the CLI documentation, the intended use of these commands is:. ```; hailctl fs sync --make-plan plan1 --copy-to gs://gcs-bucket/a s3://s3-bucket/b; hailctl fs sync --use",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14248:968,concurren,concurrent,968,https://hail.is,https://github.com/hail-is/hail/pull/14248,1,['concurren'],['concurrent']
Performance,"CHANGELOG: MatrixTable.aggregate_cols no longer forces a distributed computation. This should be what you want in the majority of cases. In case you know the aggregation is very slow and should be parallelized, use mt.cols().aggregate instead. Most of the time, `aggregate_cols` will be much faster performing the aggregation locally. Currently, we generate a `TableAggregate` over a `TableParallelize` of the columns. We shouldn't try to optimize that to a local computation during compilation; `TableParallelize` should express the intent that the computation is expensive and really should be parallelized. This should be considered part of the semantics the compiler must preserve. This PR changes `aggregate_cols` to explicitly generate a local computation using `StreamAgg` (which was only exposed in Python relatively recently, which is why we haven't made this change sooner). Longer term, aggregating columns should probably get its own IR node, especially once we start partitioning along columns.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13405:299,perform,performing,299,https://hail.is,https://github.com/hail-is/hail/pull/13405,2,"['optimiz', 'perform']","['optimize', 'performing']"
Performance,"CHANGELOG: On some pipelines, since at least 0.2.58 (commit 23813afd5b), Hail could use essentially unbounded amounts of memory. This change removes ""optimization"" rules that accidentally caused that. Closes #13606",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13619:150,optimiz,optimization,150,https://hail.is,https://github.com/hail-is/hail/pull/13619,1,['optimiz'],['optimization']
Performance,"CHANGELOG: Reduce latency on simple pipelines by as much as 50% by reducing decoding time. Force count essentially tests decoding because it forces decoding but then just increments a counter by one. Analysis of profile results indicates that the array inplace decoder was perhaps 50% of time, but exactly what part of decoding was unclear. I attempted many different things. I eventually settled on loop unrolling as the primary benefit. After team meeting, I applied @patrick-schultz 's advice to use bit twiddling to further improve the speed. ---. I assessed the latency using `time python3` on this file:. ```python; import hail as hl; hl.init(master='local[1]'); hl._set_flags(write_ir_files='1'); hl.read_matrix_table('/Users/dking/projects/hail-data/foo.mt')._force_count_rows(); ```. `foo.mt` is a subset of the `variant_data` from a VDS with ~80k samples, ~300k variants, stored in ~1.6GiB. 1. This PR: 34s, 33s; 2. no twiddling: 43s, 43s https://github.com/hail-is/hail/compare/main...danking:hail:unroll-64; 3. no twiddling & 8 element blocks: 37s, 38s https://github.com/hail-is/hail/compare/main...danking:hail:unroll-8; 4. `main` (`481cfc201b [query] fix backoff code (#13713)`): 68s, 69s. In YourKit, I observe that (1) reads 50-70MB/s with one core whereas (4) reads 15-35MB/s. I also assessed the 10-core latency and JIT effects:. - (1) starts at ~12s, warms to ~6s (+- 0.5s). Peak bandwidth 490MB/s.; - (4) starts at ~17s and warms up to ~11s (+- 2s). Peak bandwidth ~250MB/s. I suspect, with this PR, the multi-core speed is fast enough to saturate any of our file stores (including my laptop, which I think taps out just around ~500MB/s). Big thanks to everyone who contributed, particularly @patrick-schultz, whose suggestion to use bit-twiddling, squeezeed another 10% off the 8 element blocks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13776:18,latency,latency,18,https://hail.is,https://github.com/hail-is/hail/pull/13776,3,['latency'],['latency']
Performance,"CHANGELOG: Remove memory leak in `BlockMatrix.to_matrix_table_row_major` and `BlockMatrix.to_table_row_major`. Also, make the cache size used in both methods configurable. The `ref` variable was holding entire blocks in memory for no reason. It was a; vestiage of debugging. Moreover, the configurable cache permits users to fine tune; memory usage.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9501:126,cache,cache,126,https://hail.is,https://github.com/hail-is/hail/pull/9501,3,"['cache', 'tune']","['cache', 'tune']"
Performance,CHANGELOG: Substantially improve the performance of `import_gtf`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8887:37,perform,performance,37,https://hail.is,https://github.com/hail-is/hail/pull/8887,1,['perform'],['performance']
Performance,"CHANGELOG: Use indexed VEP cache files for GRCh38 on both dataproc and QoB. Fixes #13989. In this PR, I did the following:; 1. Installed samtools into the Docker image to get rid of errors in the log output; 2. Added the `--merged` flag so that VEP will use the directory `homo_sapiens_merged` for the cache. Outstanding Issues:; 1. The FASTA files that are in `homo_sapiens/` were not present in the merged dataset. Do we keep both the `homo_sapiens` and `homo_sapiens_merged/` directories in our bucket or do we transfer the FASTA files to the merged directory?; 2. Once we decide the answer to (1), then I can fix this in dataproc. The easiest thing to do is to add the tar file with the `_merged` data to the dataproc vep folders and use the `--merged` flag. However, that will double the startup time for VEP on a worker node in dataproc. Before:; <img width=""617"" alt=""Screenshot 2023-12-05 at 12 42 16 PM"" src=""https://github.com/hail-is/hail/assets/1693348/bee7fff5-782c-4f19-aa88-26383ed386b7"">. After:; <img width=""619"" alt=""Screenshot 2023-12-05 at 12 46 30 PM"" src=""https://github.com/hail-is/hail/assets/1693348/3d731759-6c69-4f1c-9c73-92bfb05c239a"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14071:27,cache,cache,27,https://hail.is,https://github.com/hail-is/hail/pull/14071,2,['cache'],['cache']
Performance,"CHANGELOG: `hl.Table.parallelize` is much more flexible and now successfully imports most Hail-compatible data. I really wanted to load the hail-is/hail pull requests into Hail. I did not want to specify; the types of all 271 fields. I souped up Hail's `impute_type`:. - If an empty array, set, dict or `None` appears at any nesting level, but a ""peer"" is non-empty and; non-missing, we accept the peer's type.; - We take the union of two struct types as long as they agree on their intersection.; - If we discover a dict that cannot be imputed as a Hail dict, we try to impute it as a struct. If you like this change, I'll add tests. Note: I had to change `HailType` to include `None`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10045:131,load,load,131,https://hail.is,https://github.com/hail-is/hail/pull/10045,1,['load'],['load']
Performance,"CHANGELOG: `hl.import_table` is up to twice as fast for small tables. The big change is optimizing for the single file, no filters case in which; we need not scan for the first extant row, that row *must* be in the first; partition, if it exists at all. Unfortunately there is no zero-RPC way to; determine the number of partitions in a table, so I must catch an error; about the lack of a zeroth partition. I also did some refactoring:. 1. Move some functions to a utility file and add lots of indents and newlines to make them readable.; 2. Use `hl.format` for constructing strings.; 3. Make `should_filter_line` into `should_remove_line` for clarity of name.; 4. Modify `should_remove_line` to use short-circuiting and/or instead of array folds.; 5. Modify `should_remove_line` to indicate (via returning None) when there are no filters enabled.; 6. Add types.; 7. Fix a bug where we assumed that `.collect()[0]` would be `None` if there were no values in the table. (It raises an error); 8. Deduplicate `hail.utils.deduplicate` (haha: I mean, there is already code for doing field dedupe)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11782:88,optimiz,optimizing,88,https://hail.is,https://github.com/hail-is/hail/pull/11782,1,['optimiz'],['optimizing']
Performance,"CHANGELOG: early implementation of regenie. I'd like to get this basic version in and iterate. It works on local. Has a few todos and fixmes; I've seen Cotton, others use these, and they'll be gone in fairly short order, saves a bit of time over making a formal issue, for something that is clearly wip. . This looks like a scary amount of lines, but almost all of the work is held in regenie-batch.py. Example files are included in contrib/regenie/regenie, which is a redacted copy of their repo, and which is sufficient, lighter-weight than the full. When you have a chance, I'd like to discuss increasing the max capacity of the SSD (striping partitions). I want this as a fallback mechanism, when not enough memory can be allocated (lowmem). It will likely perform terribly with persistent storage. I'm happy to contribute that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194:761,perform,perform,761,https://hail.is,https://github.com/hail-is/hail/pull/9194,1,['perform'],['perform']
Performance,"CHANGELOG: make hail's optimization rewriting filters to interval-filters smarter and more robust. Completely rewrites ExtractIntervalFilters. Instead of matching against very specific patterns, and failing completely for things that don't quite match (e.g. an input is let bound, or the fold implementing ""locus is contained in a set of intervals"" is written slightly differently), this uses a standard abstract interpretation framework, which is almost completely insensitive to the form of the IR, only depending on the semantics. It also correctly handles missing key fields, where the previous implementation often produced an unsound transformation of the IR. Also adds a much more thorough test suite than we had before. At the top level, the analysis takes a boolean typed IR `cond` in an environment where there is a reference to some `key`, and produces a set `intervals`, such that `cond` is equivalent to `cond & intervals.contains(key)` (in other words `cond` implies `intervals.contains(key)`, or `intervals` contains all rows where `cond` is true). This means for instance it is safe to replace `TableFilter(t, cond)` with `TableFilter(TableFilterIntervals(t, intervals), cond)`. Then in a second pass it rewrites `cond` to `cond2`, such that `cond & (intervals.contains(key))` is equivalent to `cond2 & intervals.contains(key)` (in other words `cond` implies `cond2`, and `cond2 & intervals.contains(key)` implies `cond`). This means it is safe to replace the `TableFilter(t, cond)` with `TableFilter(TableFilterIntervals(t, intervals), cond2)`. A common example is when `cond` can be completely captured by the interval filter, i.e. `cond` is equivant to `intervals.contains(key)`, in which case we can take `cond2 = True`, and the `TableFilter` can be optimized away. This all happens in the function; ```scala; def extractPartitionFilters(ctx: ExecuteContext, cond: IR, ref: Ref, key: IndexedSeq[String]): Option[(IR, IndexedSeq[Interval])] = {; if (key.isEmpty) None; else {; val e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13355:23,optimiz,optimization,23,https://hail.is,https://github.com/hail-is/hail/pull/13355,1,['optimiz'],['optimization']
Performance,"CI gets the logs for every failing job in the batch and sends it with the deploy_status response. This performs O(n_jobs) work for a single request, but this API is developers only and used only by the deploy test which has a small batch. We can revisit if this becomes a problem.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8337:103,perform,performs,103,https://hail.is,https://github.com/hail-is/hail/pull/8337,1,['perform'],['performs']
Performance,"CI test failure means not known to be safe to merge into master. Agreed re: minimizing false failures (i.e. failure due to system load but it's actually an OK change). I think in practice much less than 30s is fine, this test has been in for a month or two and this is the first time I saw it fail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5503#issuecomment-470264865:130,load,load,130,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470264865,1,['load'],['load']
Performance,Cache `reference_entry_fields_to_keep` in the VDS combiner,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10963:0,Cache,Cache,0,https://hail.is,https://github.com/hail-is/hail/issues/10963,1,['Cache'],['Cache']
Performance,Cache blockmatrix type,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6775:0,Cache,Cache,0,https://hail.is,https://github.com/hail-is/hail/pull/6775,1,['Cache'],['Cache']
Performance,Cache from in notebook,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5016:0,Cache,Cache,0,https://hail.is,https://github.com/hail-is/hail/pull/5016,1,['Cache'],['Cache']
Performance,Cache regression results,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4557:0,Cache,Cache,0,https://hail.is,https://github.com/hail-is/hail/pull/4557,1,['Cache'],['Cache']
Performance,"Can I ask you to add the same scan test here for rows and cols? This will mostly help protect against us trying to introduce an optimization that inadvertently breaks scans, which we've done in the past for e.g. filter intervals.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7394#issuecomment-547637893:128,optimiz,optimization,128,https://hail.is,https://github.com/hail-is/hail/pull/7394#issuecomment-547637893,1,['optimiz'],['optimization']
Performance,Can load 5000 cols in 5 seconds. Previously 250 cols took 3 minutes. fixes #4153,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4159:4,load,load,4,https://hail.is,https://github.com/hail-is/hail/pull/4159,1,['load'],['load']
Performance,"Can we fix the docs to correctly list the `lz4` libraries as required? . I performed `make clean` and that resolved the `make[1]: *** No rule to make target lz4.h', needed by build/Decoder.o'. Stop.` issue. . Note that the build not only requires `liblz4-1` and `liblz4-dev` (Ubuntu) / `lz4` and `lz4-devel` (CentOS), it also requires `rsync` and `python-setuptools` libraries. . Can we fix the docs to include these?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10747#issuecomment-893876601:75,perform,performed,75,https://hail.is,https://github.com/hail-is/hail/issues/10747#issuecomment-893876601,1,['perform'],['performed']
Performance,"Can you double check we don't need to explicitly tell nginx to use more than 1 core? I'm looking here:. https://www.nginx.com/blog/thread-pools-boost-performance-9x/#Configuring-Thread-Pools; https://www.nginx.com/blog/thread-pools-boost-performance-9x/#Benchmarking. Otherwise, I think this change is fine, although we do already have a minimum of two copies of internal-gateway at any time. Is this change better than increasing the number of copies of internal-gateway? I assume that the response time to increases in load will be faster with your proposed change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11876#issuecomment-1145004769:150,perform,performance-,150,https://hail.is,https://github.com/hail-is/hail/pull/11876#issuecomment-1145004769,4,"['load', 'perform', 'response time']","['load', 'performance-', 'response time']"
Performance,"Can you take another look now?. I added two new fields to the jobs table to help with indexing and order bys. This should make the queries simpler and allow us to revert back to the old scheduler that Cotton wrote that was optimitzed. The regions_bits_rep is just a 0/1 for each region. So [us-east1, us-central1] could be ""1100000"". I also realized that I could aggregate the ready cores per user and then order them after unioning each user. I think this will perform better. From small tests, the autoscaler query seemed much better, but I'll want to do one last load test once you're okay with this approach.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1274632733:462,perform,perform,462,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1274632733,4,"['load', 'perform']","['load', 'perform']"
Performance,Cannot load a PLINK file containing 20 million variants,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5564:7,load,load,7,https://hail.is,https://github.com/hail-is/hail/issues/5564,1,['load'],['load']
